{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mooc-cube.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebFpXAN6bogQ"
      },
      "source": [
        "# EduRec MOOCCube Course Recommender\n",
        "> Disentangled Self-Supervision for Recommending Videos in MOOCs (Code using PyTorch). Finding best MOOC course for students based on the past activity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz8bpfVcZ1Qc"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWqq3yKXTWvK"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import copy\n",
        "import tqdm\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, RandomSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh2Q5DQqZ22t"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "WGzLwFMbTXwJ",
        "outputId": "53e752fe-f9f4-422c-f649-dd08fc696b0e"
      },
      "source": [
        "mooccube_df = pd.read_parquet('https://github.com/recohut/reco-data/raw/mooccube/mooccube/v1/interactions_processed.parquet.gzip')\n",
        "mooccube_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>video_ids</th>\n",
              "      <th>num_video_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>U_8126464</td>\n",
              "      <td>67,67,230,230,367,367,726,726,1353,1353,1683,1...</td>\n",
              "      <td>187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>U_8650752</td>\n",
              "      <td>77,124,274,835,1396,1779,2177,2245,2706,3543,4...</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>U_131074</td>\n",
              "      <td>149,940,2374,3230,6485,6969,10793,10929,12714,...</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>U_262145</td>\n",
              "      <td>1017,3322,4217,4333,4566,4853,5204,5512,6614,6...</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>U_1441801</td>\n",
              "      <td>3494,3494,9388,9845,9845,10364,11164,13695,136...</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id                                          video_ids  num_video_ids\n",
              "0  U_8126464  67,67,230,230,367,367,726,726,1353,1353,1683,1...            187\n",
              "1  U_8650752  77,124,274,835,1396,1779,2177,2245,2706,3543,4...             94\n",
              "2   U_131074  149,940,2374,3230,6485,6969,10793,10929,12714,...             27\n",
              "3   U_262145  1017,3322,4217,4333,4566,4853,5204,5512,6614,6...             43\n",
              "4  U_1441801  3494,3494,9388,9845,9845,10364,11164,13695,136...             17"
            ]
          },
          "execution_count": 2,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC2pjRhRZ52l"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxiuB59RZ3YQ"
      },
      "source": [
        "<img src='https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F6f99139d-2d56-4a5b-b4c9-98884cd86044%2FUntitled.png?table=block&id=70d88498-b33a-4bf2-b37a-004057d1be5c&spaceId=63b72b1f-0e90-4ab8-a6df-a060a6545a56&width=2000&userId=21ec183f-f0be-4b6b-9b3e-6f0d4e5c5469&cache=v2'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN3lpMX1ThLG",
        "outputId": "526d2e9e-5a97-4e90-fcfa-91dd5df6ef19"
      },
      "source": [
        "series_num_videos = mooccube_df['num_video_ids']\n",
        "sum_num_videos = series_num_videos.sum()\n",
        "avg_num_videos = series_num_videos.mean()\n",
        "median_num_videos = series_num_videos.median()\n",
        "first_quartile = series_num_videos.quantile(q=0.25)\n",
        "third_quartile = series_num_videos.quantile(q=0.75)\n",
        "max_num_videos = series_num_videos.max()\n",
        "min_num_videos = series_num_videos.min()\n",
        "num_students = mooccube_df.shape[0]\n",
        "\n",
        "item_set = set()\n",
        "for _, row in mooccube_df.iterrows():\n",
        "    items = row['video_ids'].split(',')\n",
        "    item_set = item_set | set(items)\n",
        "\n",
        "num_uniq_videos = len(item_set)\n",
        "\n",
        "print('Total number of students: {}'.format(num_students))\n",
        "print('Number of unique videos watched: {}'.format(num_uniq_videos))\n",
        "print('Number of videos watched: {}'.format(sum_num_videos))\n",
        "print('Average number of videos watched: {}'.format(round(avg_num_videos, 2)))\n",
        "print('Median number of videos watched: {}'.format(median_num_videos))\n",
        "print('1st quartile number of videos watched: {}'.format(first_quartile))\n",
        "print('3rd quartile of videos watched: {}'.format(third_quartile))\n",
        "print('Maximum number of videos watched: {}'.format(max_num_videos))\n",
        "print('Minimum number of videos watched: {}'.format(min_num_videos))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of students: 48640\n",
            "Number of unique videos watched: 34101\n",
            "Number of videos watched: 4874298\n",
            "Average number of videos watched: 100.21\n",
            "Median number of videos watched: 59.0\n",
            "1st quartile number of videos watched: 28.0\n",
            "3rd quartile of videos watched: 123.0\n",
            "Maximum number of videos watched: 3427\n",
            "Minimum number of videos watched: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "ok4jP7A1T7m8",
        "outputId": "68ee0009-ea41-44cb-c5b1-cbbeccb04bf5"
      },
      "source": [
        "# plot histogram for number of videos watched amongst all students\n",
        "plt.figure(0)\n",
        "plt.hist(series_num_videos, bins=128)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "2ie0hqPNT96M",
        "outputId": "38ea5f11-49e5-4853-d6d2-cebcff85cd59"
      },
      "source": [
        "# plot box plot for number of videos watched amongst all students\n",
        "plt.figure(1)\n",
        "plt.boxplot(series_num_videos)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "qGJfmRjsTkKn",
        "outputId": "a17639f5-fdaf-40d9-e95b-a1f3680c64b8"
      },
      "source": [
        "# plot ecdf for number of videos watched amongst all students\n",
        "x = np.sort(series_num_videos)\n",
        "y = np.arange(1, len(x) + 1) / len(x)\n",
        "plt.figure(2)\n",
        "plt.plot(x, y, marker='.', linestyle='none')\n",
        "plt.xlabel('Number of Videos watched')\n",
        "plt.ylabel('ECDF')\n",
        "plt.margins(0.02)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_2BRzbmZ8Sp"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUPvXSPwTrcY"
      },
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # some cudnn methods can be random even after fixing the seed\n",
        "    # unless you tell it to be deterministic\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def check_path(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "        print(f'{path} created')\n",
        "\n",
        "\n",
        "def neg_sample(item_set, item_size):\n",
        "    item = random.randint(1, item_size - 1)\n",
        "    while item in item_set:\n",
        "        item = random.randint(1, item_size - 1)\n",
        "    return item\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "\n",
        "    def __init__(self, checkpoint_path, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "\n",
        "    def compare(self, score):\n",
        "        for i in range(len(score)):\n",
        "            if score[i] > self.best_score[i] + self.delta:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def __call__(self, score, model):\n",
        "        # score HIT@10 NDCG@10\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.score_min = np.array([0] * len(score))\n",
        "            self.save_checkpoint(score, model)\n",
        "        elif self.compare(score):\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(score, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, score, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            # ({self.score_min:.6f} --> {score:.6f})\n",
        "            print(f'Validation score increased.  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.checkpoint_path)\n",
        "        self.score_min = score\n",
        "\n",
        "\n",
        "def generate_rating_matrix_valid(user_seq, num_users, num_items):\n",
        "    # three lists are used to construct sparse matrix\n",
        "    row = []\n",
        "    col = []\n",
        "    data = []\n",
        "    for user_id, item_list in enumerate(user_seq):\n",
        "        for item in item_list[:-2]:  #\n",
        "            row.append(user_id)\n",
        "            col.append(item)\n",
        "            data.append(1)\n",
        "\n",
        "    row = np.array(row)\n",
        "    col = np.array(col)\n",
        "    data = np.array(data)\n",
        "    rating_matrix = csr_matrix((data, (row, col)), shape=(num_users, num_items))\n",
        "\n",
        "    return rating_matrix\n",
        "\n",
        "\n",
        "def generate_rating_matrix_test(user_seq, num_users, num_items):\n",
        "    # three lists are used to construct sparse matrix\n",
        "    row = []\n",
        "    col = []\n",
        "    data = []\n",
        "    for user_id, item_list in enumerate(user_seq):\n",
        "        for item in item_list[:-1]:  #\n",
        "            row.append(user_id)\n",
        "            col.append(item)\n",
        "            data.append(1)\n",
        "\n",
        "    row = np.array(row)\n",
        "    col = np.array(col)\n",
        "    data = np.array(data)\n",
        "    rating_matrix = csr_matrix((data, (row, col)), shape=(num_users, num_items))\n",
        "\n",
        "    return rating_matrix\n",
        "\n",
        "\n",
        "def get_user_seqs_csv(data_file):\n",
        "    data_df = pd.read_csv(data_file)\n",
        "    user_seq = []\n",
        "    item_set = set()\n",
        "    user_set = set()\n",
        "    for _, row in data_df.iterrows():\n",
        "        items = row['video_ids'].split(',')\n",
        "        user_seq.append(items)\n",
        "        item_set = item_set | set(items)\n",
        "        user_set.add(row['id'])\n",
        "\n",
        "    num_users = len(user_set)\n",
        "    max_item = len(item_set)\n",
        "    num_items = max_item + 2\n",
        "    assert len(user_set) == len(user_seq)\n",
        "\n",
        "    valid_rating_matrix = generate_rating_matrix_valid(user_seq, num_users, num_items)\n",
        "    test_rating_matrix = generate_rating_matrix_test(user_seq, num_users, num_items)\n",
        "    return user_seq, max_item, valid_rating_matrix, test_rating_matrix\n",
        "\n",
        "\n",
        "def get_user_seqs_long_csv(data_file):\n",
        "    \"\"\"\n",
        "    :param data_file:\n",
        "    :return:\n",
        "    user_seq:\n",
        "        list of item sequences\n",
        "    max_item:\n",
        "        item with largest item id (number of items basically)\n",
        "    \"\"\"\n",
        "    data_df = pd.read_csv(data_file)\n",
        "    user_seq = []\n",
        "    item_set = set()\n",
        "    user_set = set()\n",
        "    for idx, row in data_df.iterrows():\n",
        "        items = row['video_ids'].split(',')\n",
        "        items = [int(item) for item in items]\n",
        "        user_seq.append(items)\n",
        "        item_set = item_set | set(items)\n",
        "        user_set.add(row['id'])\n",
        "    max_item = len(item_set)\n",
        "    assert len(user_set) == len(user_seq)\n",
        "    return user_seq, max_item\n",
        "\n",
        "\n",
        "def get_user_seqs_and_sample(data_file, sample_file):\n",
        "    data_df = pd.read_csv(data_file)\n",
        "    user_seq = []\n",
        "    item_set = set()\n",
        "    for _, row in data_df.iterrows():\n",
        "        items = row['video_ids'].split(',')\n",
        "        user_seq.append(items)\n",
        "        item_set = item_set | set(items)\n",
        "\n",
        "    max_item = len(item_set)\n",
        "\n",
        "    sample_df = pd.read_csv(sample_file)\n",
        "    sample_seq = []\n",
        "    for _, row in sample_df.iterrows():\n",
        "        items = row['video_ids'].split(',')\n",
        "        sample_seq.append(items)\n",
        "        item_set = item_set | set(items)\n",
        "\n",
        "    assert len(user_seq) == len(sample_seq)\n",
        "\n",
        "    return user_seq, max_item, sample_seq\n",
        "\n",
        "\n",
        "def get_metric(pred_list, topk=10):\n",
        "    NDCG = 0.0\n",
        "    HIT = 0.0\n",
        "    MRR = 0.0\n",
        "    # [batch] the answer's rank\n",
        "    for rank in pred_list:\n",
        "        MRR += 1.0 / (rank + 1.0)\n",
        "        if rank < topk:\n",
        "            NDCG += 1.0 / np.log2(rank + 2.0)\n",
        "            HIT += 1.0\n",
        "    return HIT / len(pred_list), NDCG / len(pred_list), MRR / len(pred_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3pvtaUBZ_g9"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TRwrBruaA61"
      },
      "source": [
        "def precision_at_k_per_sample(actual, predicted, topk):\n",
        "    num_hits = 0\n",
        "    for place in predicted:\n",
        "        if place in actual:\n",
        "            num_hits += 1\n",
        "    return num_hits / (topk + 0.0)\n",
        "\n",
        "\n",
        "def precision_at_k(actual, predicted, topk):\n",
        "    sum_precision = 0.0\n",
        "    num_users = len(predicted)\n",
        "    for i in range(num_users):\n",
        "        act_set = set(actual[i])\n",
        "        pred_set = set(predicted[i][:topk])\n",
        "        sum_precision += len(act_set & pred_set) / float(topk)\n",
        "\n",
        "    return sum_precision / num_users\n",
        "\n",
        "\n",
        "def recall_at_k(actual, predicted, topk):\n",
        "    sum_recall = 0.0\n",
        "    num_users = len(predicted)\n",
        "    true_users = 0\n",
        "    for i in range(num_users):\n",
        "        act_set = set(actual[i])\n",
        "        pred_set = set(predicted[i][:topk])\n",
        "        if len(act_set) != 0:\n",
        "            sum_recall += len(act_set & pred_set) / float(len(act_set))\n",
        "            true_users += 1\n",
        "    return sum_recall / true_users\n",
        "\n",
        "\n",
        "def apk(actual, predicted, k=10):\n",
        "    \"\"\"\n",
        "    Computes the average precision at k.\n",
        "    This function computes the average precision at k between two lists of\n",
        "    items.\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "             A list of elements that are to be predicted (order doesn't matter)\n",
        "    predicted : list\n",
        "                A list of predicted elements (order does matter)\n",
        "    k : int, optional\n",
        "        The maximum number of predicted elements\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The average precision at k over the input lists\n",
        "    \"\"\"\n",
        "    if len(predicted) > k:\n",
        "        predicted = predicted[:k]\n",
        "\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "\n",
        "def mapk(actual, predicted, k=10):\n",
        "    \"\"\"\n",
        "    Computes the mean average precision at k.\n",
        "    This function computes the mean average prescision at k between two lists\n",
        "    of lists of items.\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "             A list of lists of elements that are to be predicted\n",
        "             (order doesn't matter in the lists)\n",
        "    predicted : list\n",
        "                A list of lists of predicted elements\n",
        "                (order matters in the lists)\n",
        "    k : int, optional\n",
        "        The maximum number of predicted elements\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The mean average precision at k over the input lists\n",
        "    \"\"\"\n",
        "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
        "\n",
        "\n",
        "def ndcg_k(actual, predicted, topk):\n",
        "    res = 0\n",
        "    for user_id in range(len(actual)):\n",
        "        k = min(topk, len(actual[user_id]))\n",
        "        idcg = idcg_k(k)\n",
        "        dcg_k = sum([int(predicted[user_id][j] in\n",
        "                         set(actual[user_id])) / math.log(j + 2, 2) for j in range(topk)])\n",
        "        res += dcg_k / idcg\n",
        "    return res / float(len(actual))\n",
        "\n",
        "\n",
        "# Calculates the ideal discounted cumulative gain at k\n",
        "def idcg_k(k):\n",
        "    res = sum([1.0 / math.log(i + 2, 2) for i in range(k)])\n",
        "    if not res:\n",
        "        return 1.0\n",
        "    else:\n",
        "        return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kM0pEEFagiV"
      },
      "source": [
        "## Pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMXR4eNZG-Uu"
      },
      "source": [
        "class PretrainDataset(Dataset):\n",
        "\n",
        "    def __init__(self, args, user_seq):\n",
        "        self.args = args\n",
        "        self.user_seq = user_seq\n",
        "        self.max_len = args.max_seq_length\n",
        "        self.part_sequence = []\n",
        "        self.split_sequence()\n",
        "\n",
        "    def split_sequence(self):\n",
        "        for seq in self.user_seq:\n",
        "            input_ids = seq[-(self.max_len + 2):-2]  # keeping same as train set\n",
        "            for i in range(len(input_ids)):\n",
        "                self.part_sequence.append(input_ids[:(i+1) + 1])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.part_sequence)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        :param index:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # (all possible sequences made using all possible subsets of sequences)\n",
        "        sequence = self.part_sequence[index]  # pos_items\n",
        "\n",
        "        seq_len = len(sequence)\n",
        "        if seq_len == 2:\n",
        "            t = 1\n",
        "        else:\n",
        "            t = torch.randint(1, seq_len-1, (1,))\n",
        "        # input sub-sequence\n",
        "        inp_subseq = sequence[:t]\n",
        "        inp_pad_len = self.max_len - len(inp_subseq)\n",
        "        inp_pos_items = ([0] * inp_pad_len) + inp_subseq\n",
        "        inp_pos_items = inp_pos_items[-self.max_len:]\n",
        "\n",
        "        # label sub-sequence\n",
        "        label_subseq = sequence[t:]\n",
        "        label_pad_len = self.max_len - len(label_subseq)\n",
        "        label_pos_items = [0] * label_pad_len + label_subseq\n",
        "        label_pos_items = label_pos_items[-self.max_len:]\n",
        "        label_pos_items.reverse()\n",
        "        # next item\n",
        "        next_item = [sequence[t]]\n",
        "\n",
        "        assert len(inp_pos_items) == self.max_len\n",
        "        assert len(label_pos_items) == self.max_len\n",
        "\n",
        "        cur_tensors = (\n",
        "            torch.tensor(inp_pos_items, dtype=torch.long),  # actual input sub-sequence of items\n",
        "            torch.tensor(label_pos_items, dtype=torch.long),  # actual label sub-sequence of items\n",
        "            torch.tensor(next_item, dtype=torch.long)  # item next to input sub-sequence of items\n",
        "        )\n",
        "\n",
        "        return cur_tensors\n",
        "\n",
        "\n",
        "class FineTrainDataset(Dataset):\n",
        "\n",
        "    def __init__(self, args, user_seq, test_neg_items=None, data_type='train'):\n",
        "        self.args = args\n",
        "        self.user_seq = user_seq\n",
        "        self.test_neg_items = test_neg_items\n",
        "        self.data_type = data_type\n",
        "        self.max_len = args.max_seq_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        user_id = index\n",
        "        items = self.user_seq[index]\n",
        "\n",
        "        assert self.data_type in {\"train\", \"valid\", \"test\"}\n",
        "\n",
        "        # [0, 1, 2, 3, 4, 5, 6]\n",
        "        # train [0, 1, 2, 3]\n",
        "        # target [1, 2, 3, 4]\n",
        "\n",
        "        # valid [0, 1, 2, 3, 4]\n",
        "        # answer [5]\n",
        "\n",
        "        # test [0, 1, 2, 3, 4, 5]\n",
        "        # answer [6]\n",
        "        if self.data_type == \"train\":\n",
        "            input_ids = items[:-3]\n",
        "            target_pos = items[1:-2]\n",
        "            answer = [0]  # no use\n",
        "\n",
        "        elif self.data_type == 'valid':\n",
        "            input_ids = items[:-2]\n",
        "            target_pos = items[1:-1]\n",
        "            answer = [items[-2]]\n",
        "\n",
        "        else:\n",
        "            input_ids = items[:-1]\n",
        "            target_pos = items[1:]\n",
        "            answer = [items[-1]]\n",
        "\n",
        "        target_neg = []\n",
        "        seq_set = set(items)\n",
        "        for _ in input_ids:\n",
        "            target_neg.append(neg_sample(seq_set, self.args.item_size))\n",
        "\n",
        "        pad_len = self.max_len - len(input_ids)\n",
        "        input_ids = [0] * pad_len + input_ids\n",
        "        target_pos = [0] * pad_len + target_pos\n",
        "        target_neg = [0] * pad_len + target_neg\n",
        "\n",
        "        input_ids = input_ids[-self.max_len:]\n",
        "        target_pos = target_pos[-self.max_len:]\n",
        "        target_neg = target_neg[-self.max_len:]\n",
        "\n",
        "        assert len(input_ids) == self.max_len\n",
        "        assert len(target_pos) == self.max_len\n",
        "        assert len(target_neg) == self.max_len\n",
        "\n",
        "        if self.test_neg_items is not None:\n",
        "            test_samples = self.test_neg_items[index]\n",
        "\n",
        "            cur_tensors = (\n",
        "                torch.tensor(user_id, dtype=torch.long),  # user_id for testing\n",
        "                torch.tensor(input_ids, dtype=torch.long),\n",
        "                torch.tensor(target_pos, dtype=torch.long),\n",
        "                torch.tensor(target_neg, dtype=torch.long),\n",
        "                torch.tensor(answer, dtype=torch.long),\n",
        "                torch.tensor(test_samples, dtype=torch.long),\n",
        "            )\n",
        "        else:\n",
        "            cur_tensors = (\n",
        "                torch.tensor(user_id, dtype=torch.long),  # user_id for testing\n",
        "                torch.tensor(input_ids, dtype=torch.long),\n",
        "                torch.tensor(target_pos, dtype=torch.long),\n",
        "                torch.tensor(target_neg, dtype=torch.long),\n",
        "                torch.tensor(answer, dtype=torch.long),\n",
        "            )\n",
        "\n",
        "        return cur_tensors\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.user_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQBmCmmqaplu"
      },
      "source": [
        "## Custom Pytorch Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0812cw1VSTV3"
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps=1e-12):\n",
        "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "        \"\"\"\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.variance_epsilon = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.weight * x + self.bias\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from item, position.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args):\n",
        "        super(Embeddings, self).__init__()\n",
        "\n",
        "        self.item_embeddings = nn.Embedding(args.item_size, args.hidden_size, padding_idx=0)\n",
        "        self.position_embeddings = nn.Embedding(args.max_seq_length, args.hidden_size)\n",
        "\n",
        "        self.LayerNorm = LayerNorm(args.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(args.hidden_dropout_prob)\n",
        "\n",
        "        self.args = args\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        items_embeddings = self.item_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        embeddings = items_embeddings + position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        if args.hidden_size % args.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (args.hidden_size, args.num_attention_heads))\n",
        "        self.num_attention_heads = args.num_attention_heads\n",
        "        self.attention_head_size = int(args.hidden_size / args.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(args.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(args.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(args.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(args.attention_probs_dropout_prob)\n",
        "        self.dense = nn.Linear(args.hidden_size, args.hidden_size)\n",
        "        self.LayerNorm = LayerNorm(args.hidden_size, eps=1e-12)\n",
        "        self.out_dropout = nn.Dropout(args.hidden_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask):\n",
        "        mixed_query_layer = self.query(input_tensor)\n",
        "        mixed_key_layer = self.key(input_tensor)\n",
        "        mixed_value_layer = self.value(input_tensor)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        # [batch_size heads seq_len seq_len] scores\n",
        "        # [batch_size 1 1 seq_len]\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        # Fixme\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        hidden_states = self.dense(context_layer)\n",
        "        hidden_states = self.out_dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class PointWiseFeedForward(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(PointWiseFeedForward, self).__init__()\n",
        "        self.conv1d_1 = nn.Conv1d(args.hidden_size, args.hidden_size, kernel_size=(1,))\n",
        "        self.activation = nn.ReLU()\n",
        "        self.conv1d_2 = nn.Conv1d(args.hidden_size, args.hidden_size, kernel_size=(1,))\n",
        "        self.LayerNorm = LayerNorm(args.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(args.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "\n",
        "        hidden_states = self.conv1d_1(input_tensor.transpose(1, 2))\n",
        "        hidden_states = hidden_states.transpose(1, 2)\n",
        "        hidden_states = self.activation(hidden_states)\n",
        "\n",
        "        hidden_states = self.conv1d_2(hidden_states.transpose(1, 2))\n",
        "        hidden_states = hidden_states.transpose(1, 2)\n",
        "        hidden_states = self.activation(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class Layer(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Layer, self).__init__()\n",
        "        self.attention = SelfAttention(args)\n",
        "        self.intermediate = PointWiseFeedForward(args)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        attention_output = self.attention(hidden_states, attention_mask)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        return intermediate_output\n",
        "\n",
        "\n",
        "class SASEncoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(SASEncoder, self).__init__()\n",
        "        layer = Layer(args)\n",
        "        self.layer = nn.ModuleList([copy.deepcopy(layer)\n",
        "                                    for _ in range(args.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n",
        "        all_encoder_layers = []\n",
        "        for layer_module in self.layer:\n",
        "            hidden_states = layer_module(hidden_states, attention_mask)\n",
        "            if output_all_encoded_layers:\n",
        "                all_encoder_layers.append(hidden_states)\n",
        "        if not output_all_encoded_layers:\n",
        "            all_encoder_layers.append(hidden_states)\n",
        "        return all_encoder_layers\n",
        "\n",
        "\n",
        "class BiasLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, initializer):\n",
        "        super(BiasLayer, self).__init__()\n",
        "        if initializer == 'zeros':\n",
        "            self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "        elif initializer == 'normal':\n",
        "            self.bias = nn.Parameter(torch.randn(hidden_size) * (1 / np.sqrt(hidden_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.bias\n",
        "\n",
        "\n",
        "class DisentangledEncoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(DisentangledEncoder, self).__init__()\n",
        "        self.sas_encoder = SASEncoder(args)\n",
        "        # prototypical intention vector for each intention\n",
        "        self.prototypes = nn.ParameterList([nn.Parameter(torch.zeros(args.hidden_size))\n",
        "                                            for _ in range(args.num_intents)])\n",
        "\n",
        "        self.layernorm1 = LayerNorm(args.hidden_size, eps=1e-12)\n",
        "        self.layernorm2 = LayerNorm(args.hidden_size, eps=1e-12)\n",
        "        self.layernorm3 = LayerNorm(args.hidden_size, eps=1e-12)\n",
        "        self.layernorm4 = LayerNorm(args.hidden_size, eps=1e-12)\n",
        "        self.layernorm5 = LayerNorm(args.hidden_size, eps=1e-12)\n",
        "\n",
        "        self.w = nn.Linear(args.hidden_size, args.hidden_size)\n",
        "\n",
        "        self.b_prime = BiasLayer(args.hidden_size, 'zeros')\n",
        "\n",
        "        # individual alpha for each position\n",
        "        self.alphas = nn.Parameter(torch.zeros(args.max_seq_length, args.hidden_size))\n",
        "\n",
        "        self.beta_input_seq = nn.Parameter(torch.randn(args.num_intents, args.hidden_size) *\n",
        "                                           (1 / np.sqrt(args.hidden_size)))\n",
        "\n",
        "        self.beta_label_seq = nn.Parameter(torch.randn(args.num_intents, args.hidden_size) *\n",
        "                                           (1 / np.sqrt(args.hidden_size)))\n",
        "\n",
        "    def intention_clustering(self, z):\n",
        "        \"\"\"\n",
        "        Method to measure how likely the primary intention at position i\n",
        "        is related with kth latent category\n",
        "        :param z:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        z = self.layernorm1(z)\n",
        "        hidden_size = z.shape[2]\n",
        "        exp_normalized_numerators = list()\n",
        "        i = 0\n",
        "        for prototype_k in self.prototypes:\n",
        "            prototype_k = self.layernorm2(prototype_k)  # [D]\n",
        "            numerator = torch.matmul(z, prototype_k)  # [B, S]\n",
        "            exp_normalized_numerator = torch.exp(numerator / np.sqrt(hidden_size))  # [B, S]\n",
        "            exp_normalized_numerators.append(numerator)\n",
        "            if i == 0:\n",
        "                denominator = exp_normalized_numerator\n",
        "            else:\n",
        "                denominator = torch.add(denominator, exp_normalized_numerator)\n",
        "            i = i + 1\n",
        "\n",
        "        all_attentions_p_k_i = [torch.div(k, denominator)\n",
        "                                for k in exp_normalized_numerators]  # [B, S] K times\n",
        "        all_attentions_p_k_i = torch.stack(all_attentions_p_k_i, -1)  # [B, S, K]\n",
        "\n",
        "        return all_attentions_p_k_i\n",
        "\n",
        "    def intention_weighting(self, z):\n",
        "        \"\"\"\n",
        "        Method to measure how likely primary intention at position i\n",
        "        is important for predicting user's future intentions\n",
        "        :param z:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        hidden_size = z.shape[2]\n",
        "        keys_tilde_i = self.layernorm3(z + self.alphas)  # [B, S, D]\n",
        "        keys_i = keys_tilde_i + torch.relu(self.w(keys_tilde_i))  # [B, S, D]\n",
        "        query = self.layernorm4(self.b_prime(self.alphas[-1, :] + (z[:, -1, :])))  # [B, D]\n",
        "        query = torch.unsqueeze(query, -1)  # [B, D, 1]\n",
        "        numerators = torch.matmul(keys_i, query)  # [B, S, 1]\n",
        "        exp_normalized_numerators = torch.exp(numerators / np.sqrt(hidden_size))\n",
        "        sum_exp_normalized_numerators = exp_normalized_numerators.sum(1).unsqueeze(-1)  # [B, 1] to [B, 1, 1]\n",
        "        all_attentions_p_i = exp_normalized_numerators / sum_exp_normalized_numerators  # [B, S, 1]\n",
        "        all_attentions_p_i = all_attentions_p_i.squeeze(-1)  # [B, S]\n",
        "\n",
        "        return all_attentions_p_i\n",
        "\n",
        "    def intention_aggr(self, z, attention_weights_p_k_i, attention_weights_p_i, is_input_seq):\n",
        "        \"\"\"\n",
        "        Method to aggregate intentions collected at all positions according\n",
        "        to both kinds of attention weights\n",
        "        :param z:\n",
        "        :param attention_weights_p_k_i:\n",
        "        :param attention_weights_p_i:\n",
        "        :param is_input_seq:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        attention_weights_p_i = attention_weights_p_i.unsqueeze(-1)  # [B, S, 1]\n",
        "        attention_weights = torch.mul(attention_weights_p_k_i, attention_weights_p_i)  # [B, S, K]\n",
        "        attention_weights_transpose = attention_weights.transpose(1, 2)  # [B, K, S]\n",
        "        if is_input_seq:\n",
        "            disentangled_encoding = self.beta_input_seq + torch.matmul(attention_weights_transpose, z)\n",
        "        else:\n",
        "            disentangled_encoding = self.beta_label_seq + torch.matmul(attention_weights_transpose, z)\n",
        "\n",
        "        disentangled_encoding = self.layernorm5(disentangled_encoding)\n",
        "\n",
        "        return disentangled_encoding  # [K, D]\n",
        "\n",
        "    def forward(self, is_input_seq, hidden_states, attention_mask, output_all_encoded_layers=True):\n",
        "\n",
        "        z = self.sas_encoder(hidden_states, attention_mask, output_all_encoded_layers)[-1]\n",
        "        attention_weights_p_k_i = self.intention_clustering(z)  # [B, S, K]\n",
        "        attention_weights_p_i = self.intention_weighting(z)  # [B, S]\n",
        "        disentangled_encoding = self.intention_aggr(z,\n",
        "                                                    attention_weights_p_k_i,\n",
        "                                                    attention_weights_p_i,\n",
        "                                                    is_input_seq)\n",
        "\n",
        "        return disentangled_encoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkQojgwaa0lB"
      },
      "source": [
        "## EduRec Pytorch Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5VjKI89U1ST"
      },
      "source": [
        "class EduRecModel(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(EduRecModel, self).__init__()\n",
        "        self.item_embeddings = nn.Embedding(args.item_size, args.hidden_size, padding_idx=0)\n",
        "        self.position_embeddings = nn.Embedding(args.max_seq_length, args.hidden_size)\n",
        "        self.sas_encoder = SASEncoder(args)\n",
        "        self.disentangled_encoder = DisentangledEncoder(args)\n",
        "        self.LayerNorm = LayerNorm(args.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(args.hidden_dropout_prob)\n",
        "        self.args = args\n",
        "\n",
        "        self.criterion = nn.BCELoss(reduction='none')\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def seq2seqloss(self, inp_subseq_encodings, label_subseq_encodings):\n",
        "        product = torch.mul(inp_subseq_encodings, label_subseq_encodings)\n",
        "        normalized_dot_product = torch.sum(product, dim=-1) / np.sqrt(self.args.hidden_size)\n",
        "        numerator = torch.exp(normalized_dot_product)\n",
        "        seq2seq_loss_k = -torch.log2(numerator / torch.sum(numerator))\n",
        "        thresh = np.floor(self.args.lambda_ * self.args.pre_batch_size * self.args.num_intents)\n",
        "        conf_indicator = seq2seq_loss_k <= thresh\n",
        "        conf_seq2seq_loss_k = torch.mul(seq2seq_loss_k, conf_indicator)\n",
        "        seq2seq_loss = torch.sum(conf_seq2seq_loss_k)\n",
        "\n",
        "        return seq2seq_loss\n",
        "\n",
        "    def seq2itemloss(self, inp_subseq_encodings, next_item_emb):\n",
        "        next_item_emb = torch.transpose(next_item_emb, 1, 2)\n",
        "        dot_product = torch.matmul(inp_subseq_encodings, next_item_emb)  # [B, K, 1]\n",
        "        exp_normalized_dot_product = torch.exp(dot_product / np.sqrt(self.args.hidden_size))\n",
        "        numerator = torch.max(exp_normalized_dot_product, dim=1)[0]  # [B, 1]\n",
        "        seq2item_loss_k = -torch.log2(numerator / torch.sum(exp_normalized_dot_product))  # [B, 1]\n",
        "        seq2item_loss = torch.sum(seq2item_loss_k)\n",
        "\n",
        "        return seq2item_loss\n",
        "\n",
        "    def add_position_embedding(self, sequence):\n",
        "\n",
        "        seq_length = sequence.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=sequence.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(sequence)\n",
        "        item_embeddings = self.item_embeddings(sequence)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        sequence_emb = item_embeddings + position_embeddings\n",
        "        sequence_emb = self.LayerNorm(sequence_emb)\n",
        "        sequence_emb = self.dropout(sequence_emb)\n",
        "\n",
        "        return sequence_emb\n",
        "\n",
        "    def pretrain(self, inp_pos_items, label_pos_items, next_pos_item):\n",
        "\n",
        "        next_item_emb = self.item_embeddings(next_pos_item)  # [B, 1, D]\n",
        "\n",
        "        # Encode masked sequence\n",
        "        inp_sequence_emb = self.add_position_embedding(inp_pos_items)\n",
        "        inp_sequence_mask = (inp_pos_items == 0).float() * -1e8\n",
        "        inp_sequence_mask = torch.unsqueeze(torch.unsqueeze(inp_sequence_mask, 1), 1)\n",
        "\n",
        "        label_sequence_emb = self.add_position_embedding(label_pos_items)\n",
        "        label_sequence_mask = (label_pos_items == 0).float() * -1e8\n",
        "        label_sequence_mask = torch.unsqueeze(torch.unsqueeze(label_sequence_mask, 1), 1)\n",
        "\n",
        "        inp_seq_encodings = self.disentangled_encoder(True,\n",
        "                                                      inp_sequence_emb,\n",
        "                                                      inp_sequence_mask,\n",
        "                                                      output_all_encoded_layers=True)\n",
        "\n",
        "        label_seq_encodings = self.disentangled_encoder(False,\n",
        "                                                        label_sequence_emb,\n",
        "                                                        label_sequence_mask,\n",
        "                                                        output_all_encoded_layers=True)\n",
        "\n",
        "        # seq2item loss\n",
        "        seq2item_loss = self.seq2itemloss(inp_seq_encodings, next_item_emb)\n",
        "\n",
        "        # seq2seq loss\n",
        "        seq2seq_loss = self.seq2seqloss(inp_seq_encodings, label_seq_encodings)\n",
        "\n",
        "        return seq2item_loss, seq2seq_loss\n",
        "\n",
        "    # Fine tune\n",
        "    # same as SASRec\n",
        "    def finetune(self, input_ids):\n",
        "\n",
        "        attention_mask = (input_ids > 0).long()\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # torch.int64\n",
        "        max_len = attention_mask.size(-1)\n",
        "        attn_shape = (1, max_len, max_len)\n",
        "        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1)  # torch.uint8\n",
        "        subsequent_mask = (subsequent_mask == 0).unsqueeze(1)\n",
        "        subsequent_mask = subsequent_mask.long()\n",
        "\n",
        "        if self.args.cuda_condition:\n",
        "            subsequent_mask = subsequent_mask.cuda()\n",
        "\n",
        "        extended_attention_mask = extended_attention_mask * subsequent_mask\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        sequence_emb = self.add_position_embedding(input_ids)\n",
        "\n",
        "        item_encoded_layers = self.sas_encoder(sequence_emb,\n",
        "                                               extended_attention_mask,\n",
        "                                               output_all_encoded_layers=True)\n",
        "\n",
        "        sequence_output = item_encoded_layers[-1]\n",
        "        return sequence_output\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        \"\"\" Initialize the weights.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.args.initializer_range)\n",
        "        elif isinstance(module, LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bss_q413a8qP"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szjmXleyVDbg"
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, train_dataloader,\n",
        "                 eval_dataloader,\n",
        "                 test_dataloader, args):\n",
        "\n",
        "        self.args = args\n",
        "        self.cuda_condition = torch.cuda.is_available() and not self.args.no_cuda\n",
        "        self.device = torch.device(\"cuda\" if self.cuda_condition else \"cpu\")\n",
        "\n",
        "        self.model = model\n",
        "        if self.cuda_condition:\n",
        "            self.model.cuda()\n",
        "\n",
        "        # Setting the train and test data loader\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.eval_dataloader = eval_dataloader\n",
        "        self.test_dataloader = test_dataloader\n",
        "\n",
        "        # self.data_name = self.args.data_name\n",
        "        betas = (self.args.adam_beta1, self.args.adam_beta2)\n",
        "        self.optim = Adam(self.model.parameters(), lr=self.args.lr, betas=betas, weight_decay=self.args.weight_decay)\n",
        "\n",
        "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
        "        self.criterion = nn.BCELoss()\n",
        "\n",
        "    def train(self, epoch):\n",
        "        self.iteration(epoch, self.train_dataloader)\n",
        "\n",
        "    def valid(self, epoch, full_sort=False):\n",
        "        return self.iteration(epoch, self.eval_dataloader, full_sort, train=False)\n",
        "\n",
        "    def test(self, epoch, full_sort=False):\n",
        "        return self.iteration(epoch, self.test_dataloader, full_sort, train=False)\n",
        "\n",
        "    def iteration(self, epoch, dataloader, full_sort=False, train=True):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_sample_scores(self, epoch, pred_list):\n",
        "        pred_list = (-pred_list).argsort().argsort()[:, 0]\n",
        "        HIT_1, NDCG_1, MRR = get_metric(pred_list, 1)\n",
        "        HIT_5, NDCG_5, MRR = get_metric(pred_list, 5)\n",
        "        HIT_10, NDCG_10, MRR = get_metric(pred_list, 10)\n",
        "        post_fix = {\n",
        "            \"Epoch\": epoch,\n",
        "            \"HIT@1\": '{:.4f}'.format(HIT_1), \"NDCG@1\": '{:.4f}'.format(NDCG_1),\n",
        "            \"HIT@5\": '{:.4f}'.format(HIT_5), \"NDCG@5\": '{:.4f}'.format(NDCG_5),\n",
        "            \"HIT@10\": '{:.4f}'.format(HIT_10), \"NDCG@10\": '{:.4f}'.format(NDCG_10),\n",
        "            \"MRR\": '{:.4f}'.format(MRR),\n",
        "        }\n",
        "        print(post_fix)\n",
        "        with open(self.args.log_file, 'a') as f:\n",
        "            f.write(str(post_fix) + '\\n')\n",
        "        return [HIT_1, NDCG_1, HIT_5, NDCG_5, HIT_10, NDCG_10, MRR], str(post_fix)\n",
        "\n",
        "    def get_full_sort_score(self, epoch, answers, pred_list):\n",
        "        recall, ndcg = [], []\n",
        "        for k in [5, 10, 15, 20]:\n",
        "            recall.append(recall_at_k(answers, pred_list, k))\n",
        "            ndcg.append(ndcg_k(answers, pred_list, k))\n",
        "        post_fix = {\n",
        "            \"Epoch\": epoch,\n",
        "            \"HIT@5\": '{:.4f}'.format(recall[0]), \"NDCG@5\": '{:.4f}'.format(ndcg[0]),\n",
        "            \"HIT@10\": '{:.4f}'.format(recall[1]), \"NDCG@10\": '{:.4f}'.format(ndcg[1]),\n",
        "            \"HIT@20\": '{:.4f}'.format(recall[3]), \"NDCG@20\": '{:.4f}'.format(ndcg[3])\n",
        "        }\n",
        "        print(post_fix)\n",
        "        with open(self.args.log_file, 'a') as f:\n",
        "            f.write(str(post_fix) + '\\n')\n",
        "        return [recall[0], ndcg[0], recall[1], ndcg[1], recall[3], ndcg[3]], str(post_fix)\n",
        "\n",
        "    def save(self, file_name):\n",
        "        torch.save(self.model.cpu().state_dict(), file_name)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def load(self, file_name):\n",
        "        self.model.load_state_dict(torch.load(file_name))\n",
        "\n",
        "    def cross_entropy(self, seq_out, pos_ids, neg_ids):\n",
        "        # [batch seq_len hidden_size]\n",
        "        pos_emb = self.model.item_embeddings(pos_ids)\n",
        "        neg_emb = self.model.item_embeddings(neg_ids)\n",
        "        # [batch*seq_len hidden_size]\n",
        "        pos = pos_emb.view(-1, pos_emb.size(2))\n",
        "        neg = neg_emb.view(-1, neg_emb.size(2))\n",
        "        seq_emb = seq_out.view(-1, self.args.hidden_size)  # [batch*seq_len hidden_size]\n",
        "        pos_logits = torch.sum(pos * seq_emb, -1)  # [batch*seq_len]\n",
        "        neg_logits = torch.sum(neg * seq_emb, -1)\n",
        "        istarget = (pos_ids > 0).view(pos_ids.size(0) * self.model.args.max_seq_length).float()  # [batch*seq_len]\n",
        "        loss = torch.sum(\n",
        "            - torch.log(torch.sigmoid(pos_logits) + 1e-24) * istarget -\n",
        "            torch.log(1 - torch.sigmoid(neg_logits) + 1e-24) * istarget\n",
        "        ) / torch.sum(istarget)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def predict_sample(self, seq_out, test_neg_sample):\n",
        "        # [batch 100 hidden_size]\n",
        "        test_item_emb = self.model.item_embeddings(test_neg_sample)\n",
        "        # [batch hidden_size]\n",
        "        test_logits = torch.bmm(test_item_emb, seq_out.unsqueeze(-1)).squeeze(-1)  # [B 100]\n",
        "        return test_logits\n",
        "\n",
        "    def predict_full(self, seq_out):\n",
        "        # [item_num hidden_size]\n",
        "        test_item_emb = self.model.item_embeddings.weight\n",
        "        # [batch hidden_size ]\n",
        "        rating_pred = torch.matmul(seq_out, test_item_emb.transpose(0, 1))\n",
        "        return rating_pred\n",
        "\n",
        "\n",
        "class PretrainTrainer(Trainer):\n",
        "\n",
        "    def __init__(self, model,\n",
        "                 train_dataloader,\n",
        "                 eval_dataloader,\n",
        "                 test_dataloader, args):\n",
        "        super(PretrainTrainer, self).__init__(\n",
        "            model,\n",
        "            train_dataloader,\n",
        "            eval_dataloader,\n",
        "            test_dataloader, args\n",
        "        )\n",
        "\n",
        "    def pretrain(self, epoch, pretrain_dataloader):\n",
        "        desc = f'S2I-{self.args.s2i_weight}-' \\\n",
        "               f'S2S-{self.args.s2s_weight}'\n",
        "\n",
        "        pretrain_data_iter = tqdm.tqdm(enumerate(pretrain_dataloader),\n",
        "                                       desc=f\"{self.args.model_name}-{self.args.data_name} Epoch:{epoch}\",\n",
        "                                       total=len(pretrain_dataloader),\n",
        "                                       bar_format=\"{l_bar}{r_bar}\")\n",
        "\n",
        "        self.model.train()\n",
        "        seq2item_loss_avg = 0.0\n",
        "        seq2seq_loss_avg = 0.0\n",
        "        total_loss_avg = 0.0\n",
        "\n",
        "        for i, batch in pretrain_data_iter:\n",
        "            # 0. batch_data will be sent into the device(GPU or CPU)\n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "            inp_pos_items, label_pos_items, next_pos_item = batch\n",
        "\n",
        "            seq2item_loss, seq2seq_loss = self.model.pretrain(inp_pos_items, label_pos_items, next_pos_item)\n",
        "\n",
        "            joint_loss = self.args.s2i_weight * seq2item_loss + \\\n",
        "                         self.args.s2s_weight * seq2seq_loss\n",
        "\n",
        "            self.optim.zero_grad()\n",
        "            joint_loss.backward()\n",
        "            self.optim.step()\n",
        "            # print('seq2item loss', seq2item_loss.item())\n",
        "            # print('seq2seq loss', seq2seq_loss.item())\n",
        "            seq2item_loss_avg += seq2item_loss.item()\n",
        "            seq2seq_loss_avg += seq2seq_loss.item()\n",
        "            total_loss_avg += (seq2item_loss_avg + seq2seq_loss_avg)\n",
        "\n",
        "        num = len(pretrain_data_iter) * self.args.pre_batch_size\n",
        "        post_fix = {\n",
        "            \"epoch\": epoch,\n",
        "            \"seq2item_loss_avg\": '{:.4f}'.format(seq2item_loss_avg / num),\n",
        "            \"seq2seq_loss_avg\": '{:.4f}'.format(seq2seq_loss_avg / num),\n",
        "            \"total_loss_avg\": '{:.4f}'.format(total_loss_avg / num)\n",
        "        }\n",
        "        print(desc)\n",
        "        print(str(post_fix))\n",
        "        with open(self.args.log_file, 'a') as f:\n",
        "            f.write(str(desc) + '\\n')\n",
        "            f.write(str(post_fix) + '\\n')\n",
        "\n",
        "\n",
        "class FinetuneTrainer(Trainer):\n",
        "\n",
        "    def __init__(self, model,\n",
        "                 train_dataloader,\n",
        "                 eval_dataloader,\n",
        "                 test_dataloader, args):\n",
        "        super(FinetuneTrainer, self).__init__(\n",
        "            model,\n",
        "            train_dataloader,\n",
        "            eval_dataloader,\n",
        "            test_dataloader, args\n",
        "        )\n",
        "\n",
        "    def iteration(self, epoch, dataloader, full_sort=False, train=True):\n",
        "\n",
        "        str_code = \"train\" if train else \"test\"\n",
        "\n",
        "        # Setting the tqdm progress bar\n",
        "        rec_data_iter = tqdm.tqdm(enumerate(dataloader),\n",
        "                                  desc=\"Recommendation EP_%s:%d\" % (str_code, epoch),\n",
        "                                  total=len(dataloader),\n",
        "                                  bar_format=\"{l_bar}{r_bar}\")\n",
        "        if train:\n",
        "            self.model.train()\n",
        "            rec_avg_loss = 0.0\n",
        "            rec_cur_loss = 0.0\n",
        "\n",
        "            for i, batch in rec_data_iter:\n",
        "                # 0. batch_data will be sent into the device(GPU or CPU)\n",
        "                batch = tuple(t.to(self.device) for t in batch)\n",
        "                _, input_ids, target_pos, target_neg, _ = batch\n",
        "                # Binary cross_entropy\n",
        "                sequence_output = self.model.finetune(input_ids)\n",
        "                loss = self.cross_entropy(sequence_output, target_pos, target_neg)\n",
        "                self.optim.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optim.step()\n",
        "\n",
        "                rec_avg_loss += loss.item()\n",
        "                rec_cur_loss = loss.item()\n",
        "\n",
        "            post_fix = {\n",
        "                \"epoch\": epoch,\n",
        "                \"rec_avg_loss\": '{:.4f}'.format(rec_avg_loss / len(rec_data_iter)),\n",
        "                \"rec_cur_loss\": '{:.4f}'.format(rec_cur_loss),\n",
        "            }\n",
        "\n",
        "            if (epoch + 1) % self.args.log_freq == 0:\n",
        "                print(str(post_fix))\n",
        "\n",
        "            with open(self.args.log_file, 'a') as f:\n",
        "                f.write(str(post_fix) + '\\n')\n",
        "\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "            pred_list = None\n",
        "\n",
        "            if full_sort:\n",
        "                answer_list = None\n",
        "                for i, batch in rec_data_iter:\n",
        "                    # 0. batch_data will be sent into the device(GPU or cpu)\n",
        "                    batch = tuple(t.to(self.device) for t in batch)\n",
        "                    user_ids, input_ids, target_pos, target_neg, answers = batch\n",
        "                    recommend_output = self.model.finetune(input_ids)\n",
        "\n",
        "                    recommend_output = recommend_output[:, -1, :]\n",
        "\n",
        "                    rating_pred = self.predict_full(recommend_output)\n",
        "\n",
        "                    rating_pred = rating_pred.cpu().data.numpy().copy()\n",
        "                    batch_user_index = user_ids.cpu().numpy()\n",
        "                    rating_pred[self.args.train_matrix[batch_user_index].toarray() > 0] = 0\n",
        "                    # reference: https://stackoverflow.com/a/23734295, https://stackoverflow.com/a/20104162\n",
        "                    ind = np.argpartition(rating_pred, -20)[:, -20:]\n",
        "                    arr_ind = rating_pred[np.arange(len(rating_pred))[:, None], ind]\n",
        "                    arr_ind_argsort = np.argsort(arr_ind)[np.arange(len(rating_pred)), ::-1]\n",
        "                    batch_pred_list = ind[np.arange(len(rating_pred))[:, None], arr_ind_argsort]\n",
        "\n",
        "                    if i == 0:\n",
        "                        pred_list = batch_pred_list\n",
        "                        answer_list = answers.cpu().data.numpy()\n",
        "                    else:\n",
        "                        pred_list = np.append(pred_list, batch_pred_list, axis=0)\n",
        "                        answer_list = np.append(answer_list, answers.cpu().data.numpy(), axis=0)\n",
        "                return self.get_full_sort_score(epoch, answer_list, pred_list)\n",
        "\n",
        "            else:\n",
        "                for i, batch in rec_data_iter:\n",
        "                    # 0. batch_data will be sent into the device(GPU or cpu)\n",
        "                    batch = tuple(t.to(self.device) for t in batch)\n",
        "                    user_ids, input_ids, target_pos, target_neg, answers, sample_negs = batch\n",
        "                    recommend_output = self.model.finetune(input_ids)\n",
        "                    test_neg_items = torch.cat((answers, sample_negs), -1)\n",
        "                    recommend_output = recommend_output[:, -1, :]\n",
        "\n",
        "                    test_logits = self.predict_sample(recommend_output, test_neg_items)\n",
        "                    test_logits = test_logits.cpu().detach().numpy().copy()\n",
        "                    if i == 0:\n",
        "                        pred_list = test_logits\n",
        "                    else:\n",
        "                        pred_list = np.append(pred_list, test_logits, axis=0)\n",
        "\n",
        "                return self.get_sample_scores(epoch, pred_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRI4VRrTbClo"
      },
      "source": [
        "## Start the Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCjKmi49cfSw"
      },
      "source": [
        "> Note: We are training only for 3 epochs just to validate the functionality as our focus is on undertanding the overall process of this type of modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WzpH4ybdXhB"
      },
      "source": [
        "mooccube_df.to_csv('mooccube.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHwtoVN_VOOp",
        "outputId": "17a53bdf-3067-492e-d46f-8a25c8889ef1"
      },
      "source": [
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--data_dir', default='.', type=str)\n",
        "    parser.add_argument('--plot_dir', default='plot', type=str)\n",
        "    parser.add_argument('--output_dir', default='output', type=str)\n",
        "    parser.add_argument('--data_name', default='mooccube', type=str)\n",
        "\n",
        "    # model args\n",
        "    parser.add_argument(\"--model_name\", default='Pretrain', type=str)\n",
        "\n",
        "    parser.add_argument(\"--hidden_size\", type=int, default=64, help=\"hidden size of transformer model\")\n",
        "    parser.add_argument(\"--num_hidden_layers\", type=int, default=2, help=\"number of layers\")\n",
        "    parser.add_argument('--num_attention_heads', default=1, type=int)\n",
        "    parser.add_argument('--hidden_act', default=\"gelu\", type=str)  # gelu relu\n",
        "    parser.add_argument(\"--attention_probs_dropout_prob\", type=float, default=0.5, help=\"attention dropout p\")\n",
        "    parser.add_argument(\"--hidden_dropout_prob\", type=float, default=0.5, help=\"hidden dropout p\")\n",
        "    parser.add_argument(\"--initializer_range\", type=float, default=0.02)\n",
        "    parser.add_argument('--max_seq_length', default=50, type=int)\n",
        "    parser.add_argument('--num_intents', default=4, type=int)\n",
        "    parser.add_argument('--lambda_', default=0.5, type=float)\n",
        "\n",
        "    # train args\n",
        "    parser.add_argument(\"--lr\", type=float, default=0.001, help=\"learning rate of adam\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=128, help=\"number of batch_size\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=200, help=\"number of epochs\")\n",
        "    parser.add_argument(\"--no_cuda\", action=\"store_true\")\n",
        "    parser.add_argument(\"--log_freq\", type=int, default=1, help=\"per epoch print res\")\n",
        "    parser.add_argument(\"--seed\", default=42, type=int)\n",
        "\n",
        "    # pre train args\n",
        "    parser.add_argument(\"--pre_epochs\", type=int, default=300, help=\"number of pre_train epochs\")\n",
        "    parser.add_argument(\"--pre_batch_size\", type=int, default=100)\n",
        "    parser.add_argument('--ckp', default=20, type=int, help=\"pretrain epochs 10, 20, 30...\")\n",
        "\n",
        "    parser.add_argument(\"--mask_p\", type=float, default=0.2, help=\"mask probability\")\n",
        "    parser.add_argument(\"--s2i_weight\", type=float, default=1.0, help=\"seq2item loss weight\")\n",
        "    parser.add_argument(\"--s2s_weight\", type=float, default=1.0, help=\"seq2seq loss weight\")\n",
        "\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=0.0, help=\"weight_decay of adam\")\n",
        "    parser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"adam first beta value\")\n",
        "    parser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"adam second beta value\")\n",
        "    parser.add_argument(\"--gpu_id\", type=str, default=\"0\", help=\"gpu_id\")\n",
        "\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    set_seed(args.seed)\n",
        "    check_path(args.output_dir)\n",
        "\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_id\n",
        "    args.cuda_condition = torch.cuda.is_available() and not args.no_cuda\n",
        "\n",
        "    args.data_file = os.path.join(args.data_dir, args.data_name + '.csv')\n",
        "\n",
        "    user_seq, max_item = get_user_seqs_long_csv(args.data_file)\n",
        "    # args.ckp = 20\n",
        "    args_str = f'{args.model_name}-{args.data_name}-epochs-{args.ckp}'\n",
        "    checkpoint = args_str + '.pt'\n",
        "    args.checkpoint_path = os.path.join(args.output_dir, checkpoint)\n",
        "\n",
        "    args.item_size = max_item + 2\n",
        "    args.mask_id = max_item + 1\n",
        "    # save model args\n",
        "    args_str = f'{args.model_name}-{args.data_name}'\n",
        "    args.log_file = os.path.join(args.output_dir, args_str + '.txt')\n",
        "    print(args)\n",
        "    with open(args.log_file, 'a') as f:\n",
        "        f.write(str(args) + '\\n')\n",
        "\n",
        "    model = EduRecModel(args=args)\n",
        "    trainer = PretrainTrainer(model, None, None, None, args)\n",
        "\n",
        "    # to resume training from last trained epoch\n",
        "    if os.path.exists(args.checkpoint_path):\n",
        "        trainer.load(args.checkpoint_path)\n",
        "        print(f'Resume training from epoch={args.ckp} for pre-training!')\n",
        "        init_epoch = int(args.ckp)\n",
        "    else:\n",
        "        init_epoch = -1\n",
        "    for epoch in range(args.pre_epochs):\n",
        "        if epoch <= init_epoch:\n",
        "          continue\n",
        "        pretrain_dataset = PretrainDataset(args, user_seq)\n",
        "        pretrain_sampler = RandomSampler(pretrain_dataset)\n",
        "        pretrain_dataloader = DataLoader(pretrain_dataset, sampler=pretrain_sampler, batch_size=args.pre_batch_size)\n",
        "\n",
        "        trainer.pretrain(epoch, pretrain_dataloader)\n",
        "\n",
        "        # if (epoch + 1) % 10 == 0:\n",
        "        ckp = f'{args.model_name}-{args.data_name}-epochs-{epoch+1}.pt'\n",
        "        checkpoint_path = os.path.join(args.output_dir, ckp)\n",
        "        trainer.save(checkpoint_path)\n",
        "\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(adam_beta1=0.9, adam_beta2=0.999, attention_probs_dropout_prob=0.5, batch_size=128, checkpoint_path='output/Pretrain-mooccube-epochs-20.pt', ckp=20, cuda_condition=True, data_dir='.', data_file='./mooccube.csv', data_name='mooccube', epochs=200, gpu_id='0', hidden_act='gelu', hidden_dropout_prob=0.5, hidden_size=64, initializer_range=0.02, item_size=34103, lambda_=0.5, log_file='output/Pretrain-mooccube.txt', log_freq=1, lr=0.001, mask_id=34102, mask_p=0.2, max_seq_length=50, model_name='Pretrain', no_cuda=False, num_attention_heads=1, num_hidden_layers=2, num_intents=4, output_dir='output', plot_dir='plot', pre_batch_size=100, pre_epochs=300, s2i_weight=1.0, s2s_weight=1.0, seed=42, weight_decay=0.0)\n",
            "Total Parameters: 2295616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Pretrain-mooccube Epoch:0: 100%|| 18708/18708 [09:38<00:00, 32.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "S2I-1.0-S2S-1.0\n",
            "{'epoch': 0, 'seq2item_loss_avg': '6.8654', 'seq2seq_loss_avg': '34.5771', 'total_loss_avg': '389116.3282'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Pretrain-mooccube Epoch:1: 100%|| 18708/18708 [09:43<00:00, 32.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "S2I-1.0-S2S-1.0\n",
            "{'epoch': 1, 'seq2item_loss_avg': '6.6616', 'seq2seq_loss_avg': '34.5748', 'total_loss_avg': '385779.0027'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Pretrain-mooccube Epoch:2: 100%|| 18708/18708 [09:50<00:00, 31.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "S2I-1.0-S2S-1.0\n",
            "{'epoch': 2, 'seq2item_loss_avg': '6.6537', 'seq2seq_loss_avg': '34.5746', 'total_loss_avg': '385690.2441'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Pretrain-mooccube Epoch:3:  31%|| 5715/18708 [02:58<06:41, 32.36it/s]"
          ]
        }
      ]
    }
  ]
}