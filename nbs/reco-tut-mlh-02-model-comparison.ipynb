{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"reco-tut-mlh-02-model-comparison.ipynb","provenance":[{"file_id":"1WgX5gqwulFU6zMuRI-sXbyIlSPj58LBh","timestamp":1628676317350},{"file_id":"1AqVI1vgtpjVbcx5XdPgFnkJCmWXMAg7G","timestamp":1628676023810}],"collapsed_sections":[],"mount_file_id":"1qal5YmJsYrefA9qpvL6Bv-DxT-9Ay7eK","authorship_tag":"ABX9TyNZQ/Aq0zpCkV6LLQZnuS6x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"UV_mis-jdwLd","executionInfo":{"status":"ok","timestamp":1628676759690,"user_tz":-330,"elapsed":905,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["import os\n","project_name = \"reco-tut-mlh\"; branch = \"main\"; account = \"sparsh-ai\"\n","project_path = os.path.join('/content', project_name)"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"KRGLEjqMd3dV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628676763112,"user_tz":-330,"elapsed":3429,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"29ab6abd-12bc-41ef-e1fd-ef3e7e0f0675"},"source":["if not os.path.exists(project_path):\n","    !cp /content/drive/MyDrive/mykeys.py /content\n","    import mykeys\n","    !rm /content/mykeys.py\n","    path = \"/content/\" + project_name; \n","    !mkdir \"{path}\"\n","    %cd \"{path}\"\n","    import sys; sys.path.append(path)\n","    !git config --global user.email \"recotut@recohut.com\"\n","    !git config --global user.name  \"reco-tut\"\n","    !git init\n","    !git remote add origin https://\"{mykeys.git_token}\":x-oauth-basic@github.com/\"{account}\"/\"{project_name}\".git\n","    !git pull origin \"{branch}\"\n","    !git checkout main\n","else:\n","    %cd \"{project_path}\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/reco-tut-mlh\n","Initialized empty Git repository in /content/reco-tut-mlh/.git/\n","remote: Enumerating objects: 50, done.\u001b[K\n","remote: Counting objects: 100% (50/50), done.\u001b[K\n","remote: Compressing objects: 100% (35/35), done.\u001b[K\n","remote: Total 50 (delta 11), reused 46 (delta 8), pack-reused 0\u001b[K\n","Unpacking objects: 100% (50/50), done.\n","From https://github.com/sparsh-ai/reco-tut-mlh\n"," * branch            main       -> FETCH_HEAD\n"," * [new branch]      main       -> origin/main\n","Branch 'main' set up to track remote branch 'main' from 'origin'.\n","Switched to a new branch 'main'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HWliEWwod3dX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628677574362,"user_tz":-330,"elapsed":776,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"6b0b8876-8be9-43d1-944a-bb869cb88c72"},"source":["!git status"],"execution_count":34,"outputs":[{"output_type":"stream","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\n","\t\u001b[31mcode/models/GCN.py\u001b[m\n","\n","nothing added to commit but untracked files present (use \"git add\" to track)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dGCJpyjLd3dY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628677579588,"user_tz":-330,"elapsed":1324,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"d321e789-c3a3-4802-c754-081960e6c7b8"},"source":["!git add . && git commit -m 'commit' && git push origin \"{branch}\""],"execution_count":35,"outputs":[{"output_type":"stream","text":["[main b8748e9] commit\n"," 1 file changed, 193 insertions(+)\n"," create mode 100644 code/models/GCN.py\n","Counting objects: 5, done.\n","Delta compression using up to 2 threads.\n","Compressing objects: 100% (5/5), done.\n","Writing objects: 100% (5/5), 2.52 KiB | 2.52 MiB/s, done.\n","Total 5 (delta 2), reused 0 (delta 0)\n","remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n","To https://github.com/sparsh-ai/reco-tut-mlh.git\n","   2f2caf4..b8748e9  main -> main\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ClFYXwQcqWub","executionInfo":{"status":"ok","timestamp":1628676808721,"user_tz":-330,"elapsed":465,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["import sys\n","sys.path.insert(0, './code')"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t0QPGVexfcaM"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"BEudW5MKLC8E"},"source":["# Collaborative Filtering Comparison\n","\n","In this notebook we compare different recommendation systems starting with the state-of-the-art LightGCN and going back to the winning algorithm for 2009's Netflix Prize competition, SVD++.\n","\n","Models include in order are LightGCN, NGCF, SVAE, SVD++, and SVD. Each model has their own individual notebooks where we go more indepth, especially LightGCN and NGCF, where we implemented them from scratch in Tensorflow. \n","\n","The last cell compares the performance of the different models using ranking metrics:\n","\n","\n","*   Precision@k\n","*   Recall@k\n","*   Mean Average Precision (MAP)\n","*   Normalized Discounted Cumulative Gain (NDCG)\n","\n","where $k=10$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eESDthIVHdOY"},"source":["# Imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sKYWteSRsyl0","executionInfo":{"status":"ok","timestamp":1628676795829,"user_tz":-330,"elapsed":26139,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"0394f9a4-1641-4959-ce68-2770d748e50b"},"source":["!pip install -q surprise"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 11.8 MB 6.6 MB/s \n","\u001b[?25h  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m10O1R5R8IUO","executionInfo":{"status":"ok","timestamp":1628676846212,"user_tz":-330,"elapsed":1151,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["import math\n","import numpy as np\n","import os\n","import pandas as pd\n","import random\n","import requests\n","import scipy.sparse as sp\n","import surprise\n","import tensorflow as tf\n","\n","from sklearn.model_selection import train_test_split\n","from tensorflow.python.framework.ops import disable_eager_execution\n","from tqdm import tqdm\n","\n","from utils import stratified_split, numpy_stratified_split\n","import build_features\n","import metrics\n","from models import SVAE\n","from models.GCN import LightGCN, NGCF"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_c_Xsn7eHgpU"},"source":["# Prepare data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":376},"id":"BTTB4DsY1_Ch","executionInfo":{"status":"ok","timestamp":1628676850862,"user_tz":-330,"elapsed":717,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"60391cfa-406b-401e-cc9f-2e93241f7ea3"},"source":["fp = os.path.join('./data/bronze', 'u.data')\n","raw_data = pd.read_csv(fp, sep='\\t', names=['userId', 'movieId', 'rating', 'timestamp'])\n","print(f'Shape: {raw_data.shape}')\n","raw_data.sample(10, random_state=123)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Shape: (100000, 4)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>userId</th>\n","      <th>movieId</th>\n","      <th>rating</th>\n","      <th>timestamp</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>42083</th>\n","      <td>600</td>\n","      <td>651</td>\n","      <td>4</td>\n","      <td>888451492</td>\n","    </tr>\n","    <tr>\n","      <th>71825</th>\n","      <td>607</td>\n","      <td>494</td>\n","      <td>5</td>\n","      <td>883879556</td>\n","    </tr>\n","    <tr>\n","      <th>99535</th>\n","      <td>875</td>\n","      <td>1103</td>\n","      <td>5</td>\n","      <td>876465144</td>\n","    </tr>\n","    <tr>\n","      <th>47879</th>\n","      <td>648</td>\n","      <td>238</td>\n","      <td>3</td>\n","      <td>882213535</td>\n","    </tr>\n","    <tr>\n","      <th>36734</th>\n","      <td>113</td>\n","      <td>273</td>\n","      <td>4</td>\n","      <td>875935609</td>\n","    </tr>\n","    <tr>\n","      <th>48636</th>\n","      <td>536</td>\n","      <td>213</td>\n","      <td>5</td>\n","      <td>882360704</td>\n","    </tr>\n","    <tr>\n","      <th>59566</th>\n","      <td>684</td>\n","      <td>395</td>\n","      <td>2</td>\n","      <td>878762243</td>\n","    </tr>\n","    <tr>\n","      <th>44826</th>\n","      <td>608</td>\n","      <td>423</td>\n","      <td>4</td>\n","      <td>880406727</td>\n","    </tr>\n","    <tr>\n","      <th>51584</th>\n","      <td>697</td>\n","      <td>628</td>\n","      <td>4</td>\n","      <td>882622016</td>\n","    </tr>\n","    <tr>\n","      <th>4368</th>\n","      <td>130</td>\n","      <td>930</td>\n","      <td>3</td>\n","      <td>876251072</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       userId  movieId  rating  timestamp\n","42083     600      651       4  888451492\n","71825     607      494       5  883879556\n","99535     875     1103       5  876465144\n","47879     648      238       3  882213535\n","36734     113      273       4  875935609\n","48636     536      213       5  882360704\n","59566     684      395       2  878762243\n","44826     608      423       4  880406727\n","51584     697      628       4  882622016\n","4368      130      930       3  876251072"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":376},"id":"e9qxAgkc2mAc","executionInfo":{"status":"ok","timestamp":1628676860325,"user_tz":-330,"elapsed":1065,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"cdcc1ca9-ee67-40ed-95d7-979a9cedcac0"},"source":["# Load movie titles.\n","fp = os.path.join('./data/bronze', 'u.item')\n","movie_titles = pd.read_csv(fp, sep='|', names=['movieId', 'title'], usecols = range(2), encoding='iso-8859-1')\n","print(f'Shape: {movie_titles.shape}')\n","movie_titles.sample(10, random_state=123)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Shape: (1682, 2)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>movieId</th>\n","      <th>title</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>304</th>\n","      <td>305</td>\n","      <td>Ice Storm, The (1997)</td>\n","    </tr>\n","    <tr>\n","      <th>450</th>\n","      <td>451</td>\n","      <td>Grease (1978)</td>\n","    </tr>\n","    <tr>\n","      <th>691</th>\n","      <td>692</td>\n","      <td>American President, The (1995)</td>\n","    </tr>\n","    <tr>\n","      <th>1408</th>\n","      <td>1409</td>\n","      <td>Swan Princess, The (1994)</td>\n","    </tr>\n","    <tr>\n","      <th>1075</th>\n","      <td>1076</td>\n","      <td>Pagemaster, The (1994)</td>\n","    </tr>\n","    <tr>\n","      <th>103</th>\n","      <td>104</td>\n","      <td>Theodore Rex (1995)</td>\n","    </tr>\n","    <tr>\n","      <th>167</th>\n","      <td>168</td>\n","      <td>Monty Python and the Holy Grail (1974)</td>\n","    </tr>\n","    <tr>\n","      <th>1460</th>\n","      <td>1461</td>\n","      <td>Here Comes Cookie (1935)</td>\n","    </tr>\n","    <tr>\n","      <th>1189</th>\n","      <td>1190</td>\n","      <td>That Old Feeling (1997)</td>\n","    </tr>\n","    <tr>\n","      <th>1438</th>\n","      <td>1439</td>\n","      <td>Jason's Lyric (1994)</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      movieId                                   title\n","304       305                   Ice Storm, The (1997)\n","450       451                           Grease (1978)\n","691       692          American President, The (1995)\n","1408     1409               Swan Princess, The (1994)\n","1075     1076                  Pagemaster, The (1994)\n","103       104                     Theodore Rex (1995)\n","167       168  Monty Python and the Holy Grail (1974)\n","1460     1461                Here Comes Cookie (1935)\n","1189     1190                 That Old Feeling (1997)\n","1438     1439                    Jason's Lyric (1994)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MhdGfZOW24rT","executionInfo":{"status":"ok","timestamp":1628676880673,"user_tz":-330,"elapsed":2196,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"1757f467-67e1-4716-b513-dadc57c731a8"},"source":["train_size = 0.75\n","train, test = stratified_split(raw_data, 'userId', train_size)\n","\n","print(f'Train Shape: {train.shape}')\n","print(f'Test Shape: {test.shape}')\n","print(f'Do they have the same users?: {set(train.userId) == set(test.userId)}')"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Train Shape: (74992, 4)\n","Test Shape: (25008, 4)\n","Do they have the same users?: True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kV-Q7up827yC","executionInfo":{"status":"ok","timestamp":1628676880675,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"6f633f98-4bbe-4d8f-abdf-9f0f2150fed6"},"source":["combined = train.append(test)\n","\n","n_users = combined['userId'].nunique()\n","print('Number of users:', n_users)\n","\n","n_movies = combined['movieId'].nunique()\n","print('Number of movies:', n_movies)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Number of users: 943\n","Number of movies: 1682\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RRzUqVW027v1","executionInfo":{"status":"ok","timestamp":1628676880676,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["# Create DataFrame with reset index of 0-n_movies.\n","movie_new = combined[['movieId']].drop_duplicates()\n","movie_new['movieId_new'] = np.arange(len(movie_new))\n","\n","train_reindex = pd.merge(train, movie_new, on='movieId', how='left')\n","# Reset index to 0-n_users.\n","train_reindex['userId_new'] = train_reindex['userId'] - 1  \n","train_reindex = train_reindex[['userId_new', 'movieId_new', 'rating']]\n","\n","test_reindex = pd.merge(test, movie_new, on='movieId', how='left')\n","# Reset index to 0-n_users.\n","test_reindex['userId_new'] = test_reindex['userId'] - 1\n","test_reindex = test_reindex[['userId_new', 'movieId_new', 'rating']]\n","\n","# Create dictionaries so we can convert to and from indexes\n","item2id = dict(zip(movie_new['movieId'], movie_new['movieId_new']))\n","id2item = dict(zip(movie_new['movieId_new'], movie_new['movieId']))\n","user2id = dict(zip(train['userId'], train_reindex['userId_new']))\n","id2user = dict(zip(train_reindex['userId_new'], train['userId']))"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pjwETtvX27rK","executionInfo":{"status":"ok","timestamp":1628676895186,"user_tz":-330,"elapsed":11964,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"aab6cdd0-6bc0-458b-c854-a55673833452"},"source":["# Create user-item graph (sparse matix where users are rows and movies are columns.\n","# 1 if a user reviewed that movie, 0 if they didn't).\n","R = sp.dok_matrix((n_users, n_movies), dtype=np.float32)\n","R[train_reindex['userId_new'], train_reindex['movieId_new']] = 1\n","\n","# Create the adjaceny matrix with the user-item graph.\n","adj_mat = sp.dok_matrix((n_users + n_movies, n_users + n_movies), dtype=np.float32)\n","\n","# List of lists.\n","adj_mat.tolil()\n","R = R.tolil()\n","\n","# Put together adjacency matrix. Movies and users are nodes/vertices.\n","# 1 if the movie and user are connected.\n","adj_mat[:n_users, n_users:] = R\n","adj_mat[n_users:, :n_users] = R.T\n","\n","adj_mat"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<2625x2625 sparse matrix of type '<class 'numpy.float32'>'\n","\twith 149984 stored elements in Dictionary Of Keys format>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"-hzlq37t27mv","executionInfo":{"status":"ok","timestamp":1628676895188,"user_tz":-330,"elapsed":17,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["# Calculate degree matrix D (for every row count the number of nonzero entries)\n","D_values = np.array(adj_mat.sum(1))\n","\n","# Square root and inverse.\n","D_inv_values = np.power(D_values  + 1e-9, -0.5).flatten()\n","D_inv_values[np.isinf(D_inv_values)] = 0.0\n","\n"," # Create sparse matrix with the values of D^(-0.5) are the diagonals.\n","D_inv_sq_root = sp.diags(D_inv_values)\n","\n","# Eval (D^-0.5 * A * D^-0.5).\n","norm_adj_mat = D_inv_sq_root.dot(adj_mat).dot(D_inv_sq_root)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TjPnmm4l3GDK","executionInfo":{"status":"ok","timestamp":1628676899585,"user_tz":-330,"elapsed":4412,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"4eab03b6-5471-4b16-abdb-ece3821c236c"},"source":["# to COOrdinate format first ((row, column), data)\n","coo = norm_adj_mat.tocoo().astype(np.float32)\n","\n","# create an index that will tell SparseTensor where the non-zero points are\n","indices = np.mat([coo.row, coo.col]).transpose()\n","\n","# covert to sparse tensor\n","A_tilde = tf.SparseTensor(indices, coo.data, coo.shape)\n","A_tilde"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fa2ffa9e790>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"YuRBZNznJpD6"},"source":["# Train models"]},{"cell_type":"markdown","metadata":{"id":"KshSoBWYI0rC"},"source":["## Graph Convoultional Networks (GCNs)"]},{"cell_type":"markdown","metadata":{"id":"kFRUrT-4HnW1"},"source":["### Light Graph Convolution Network (LightGCN)"]},{"cell_type":"code","metadata":{"id":"p7-joA-u3GBB","executionInfo":{"status":"ok","timestamp":1628676899586,"user_tz":-330,"elapsed":32,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["light_model = LightGCN(A_tilde,\n","                 n_users = n_users,\n","                 n_items = n_movies,\n","                 n_layers = 3)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7wUU0odC3F8f","executionInfo":{"status":"ok","timestamp":1628676953665,"user_tz":-330,"elapsed":54108,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"0ca4667d-445d-4af6-b5f4-05de045a2e6f"},"source":["optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n","light_model.fit(epochs=25, batch_size=1024, optimizer=optimizer)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Epoch 1/25\n","74/74 [==============================] - 5s 26ms/step - training loss: 0.4139\n","Epoch 2/25\n","74/74 [==============================] - 2s 26ms/step - training loss: 0.2455\n","Epoch 3/25\n","74/74 [==============================] - 2s 25ms/step - training loss: 0.2250\n","Epoch 4/25\n","74/74 [==============================] - 2s 25ms/step - training loss: 0.2061\n","Epoch 5/25\n","74/74 [==============================] - 2s 26ms/step - training loss: 0.1849\n","Epoch 6/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1713\n","Epoch 7/25\n","74/74 [==============================] - 2s 26ms/step - training loss: 0.1682\n","Epoch 8/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1647\n","Epoch 9/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1575\n","Epoch 10/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1532\n","Epoch 11/25\n","74/74 [==============================] - 2s 28ms/step - training loss: 0.1497\n","Epoch 12/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1459\n","Epoch 13/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1382\n","Epoch 14/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1344\n","Epoch 15/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1357\n","Epoch 16/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1293\n","Epoch 17/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1270\n","Epoch 18/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1190\n","Epoch 19/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1202\n","Epoch 20/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1174\n","Epoch 21/25\n","74/74 [==============================] - 2s 26ms/step - training loss: 0.1135\n","Epoch 22/25\n","74/74 [==============================] - 2s 26ms/step - training loss: 0.1125\n","Epoch 23/25\n","74/74 [==============================] - 2s 26ms/step - training loss: 0.1089\n","Epoch 24/25\n","74/74 [==============================] - 2s 26ms/step - training loss: 0.1039\n","Epoch 25/25\n","74/74 [==============================] - 2s 26ms/step - training loss: 0.1036\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oRp6NESrHqPf"},"source":["### Neural Graph Collaborative Filtering (NGCF)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z0q0dyg1zTQP","executionInfo":{"status":"ok","timestamp":1628677002247,"user_tz":-330,"elapsed":48603,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"8732edb2-198c-44a0-d2d1-1e151c10b995"},"source":["ngcf_model = NGCF(A_tilde,\n","                  n_users = n_users,\n","                  n_items = n_movies,\n","                  n_layers = 3\n","                  )\n","\n","ngcf_model.fit(epochs=25, batch_size=1024, optimizer=optimizer)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Epoch 1/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.5046\n","Epoch 2/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.2480\n","Epoch 3/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.2374\n","Epoch 4/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.2236\n","Epoch 5/25\n","74/74 [==============================] - 2s 26ms/step - training loss: 0.2037\n","Epoch 6/25\n","74/74 [==============================] - 2s 26ms/step - training loss: 0.1918\n","Epoch 7/25\n","74/74 [==============================] - 2s 26ms/step - training loss: 0.1831\n","Epoch 8/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1771\n","Epoch 9/25\n","74/74 [==============================] - 2s 26ms/step - training loss: 0.1740\n","Epoch 10/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1696\n","Epoch 11/25\n","74/74 [==============================] - 2s 26ms/step - training loss: 0.1662\n","Epoch 12/25\n","74/74 [==============================] - 2s 26ms/step - training loss: 0.1638\n","Epoch 13/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1612\n","Epoch 14/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1622\n","Epoch 15/25\n","74/74 [==============================] - 2s 27ms/step - training loss: 0.1601\n","Epoch 16/25\n","74/74 [==============================] - 2s 28ms/step - training loss: 0.1574\n","Epoch 17/25\n","74/74 [==============================] - 2s 28ms/step - training loss: 0.1554\n","Epoch 18/25\n","74/74 [==============================] - 2s 28ms/step - training loss: 0.1569\n","Epoch 19/25\n","74/74 [==============================] - 2s 28ms/step - training loss: 0.1516\n","Epoch 20/25\n","74/74 [==============================] - 2s 28ms/step - training loss: 0.1468\n","Epoch 21/25\n","74/74 [==============================] - 2s 28ms/step - training loss: 0.1451\n","Epoch 22/25\n","74/74 [==============================] - 2s 28ms/step - training loss: 0.1433\n","Epoch 23/25\n","74/74 [==============================] - 2s 28ms/step - training loss: 0.1424\n","Epoch 24/25\n","74/74 [==============================] - 2s 28ms/step - training loss: 0.1374\n","Epoch 25/25\n","74/74 [==============================] - 2s 28ms/step - training loss: 0.1358\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Vsq8nRDbIqUm"},"source":["### Recommend with LightGCN and NGCF"]},{"cell_type":"code","metadata":{"id":"n_V5VkktAwMh","executionInfo":{"status":"ok","timestamp":1628677003039,"user_tz":-330,"elapsed":818,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["# Convert test user ids to the new ids\n","users = np.array([user2id[x] for x in test['userId'].unique()])\n","\n","recs = []\n","for model in [light_model, ngcf_model]:\n","    recommendations = model.recommend(users, k=10)\n","    recommendations = recommendations.replace({'userId': id2user, 'movieId': id2item})\n","    recommendations = recommendations.merge(movie_titles,\n","                                                    how='left',\n","                                                    on='movieId'\n","                                                    )[['userId', 'movieId', 'title', 'prediction']]\n","\n","    # Create column with the predicted movie's rank for each user \n","    top_k = recommendations.copy()\n","    top_k['rank'] = recommendations.groupby('userId', sort=False).cumcount() + 1  # For each user, only include movies recommendations that are also in the test set\n","\n","    recs.append(top_k)"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VI56qeyiIAQE"},"source":["## Standard Variational Autoencoder (SVAE)"]},{"cell_type":"code","metadata":{"id":"t7VC52PGVFlD","executionInfo":{"status":"ok","timestamp":1628677022498,"user_tz":-330,"elapsed":3948,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["# Binarize the data (only keep ratings >= 4)\n","df_preferred = raw_data[raw_data['rating'] > 3.5]\n","df_low_rating = raw_data[raw_data['rating'] <= 3.5]\n","\n","df = df_preferred.groupby('userId').filter(lambda x: len(x) >= 5)\n","df = df.groupby('movieId').filter(lambda x: len(x) >= 1)\n","\n","# Obtain both usercount and itemcount after filtering\n","usercount = df[['userId']].groupby('userId', as_index = False).size()\n","itemcount = df[['movieId']].groupby('movieId', as_index = False).size()\n","\n","unique_users =sorted(df.userId.unique())\n","np.random.seed(123)\n","unique_users = np.random.permutation(unique_users)\n","\n","HELDOUT_USERS = 200\n","\n","# Create train/validation/test users\n","n_users = len(unique_users)\n","train_users = unique_users[:(n_users - HELDOUT_USERS * 2)]\n","val_users = unique_users[(n_users - HELDOUT_USERS * 2) : (n_users - HELDOUT_USERS)]\n","test_users = unique_users[(n_users - HELDOUT_USERS):]\n","\n","train_set = df.loc[df['userId'].isin(train_users)]\n","val_set = df.loc[df['userId'].isin(val_users)]\n","test_set = df.loc[df['userId'].isin(test_users)]\n","unique_train_items = pd.unique(train_set['movieId'])\n","val_set = val_set.loc[val_set['movieId'].isin(unique_train_items)]\n","test_set = test_set.loc[test_set['movieId'].isin(unique_train_items)]\n","\n","# Instantiate the sparse matrix generation for train, validation and test sets\n","# use list of unique items from training set for all sets\n","am_train = build_features.AffinityMatrix(df=train_set, items_list=unique_train_items)\n","am_val = build_features.AffinityMatrix(df=val_set, items_list=unique_train_items)\n","am_test = build_features.AffinityMatrix(df=test_set, items_list=unique_train_items)\n","\n","# Obtain the sparse matrix for train, validation and test sets\n","train_data, _, _ = am_train.gen_affinity_matrix()\n","val_data, val_map_users, val_map_items = am_val.gen_affinity_matrix()\n","test_data, test_map_users, test_map_items = am_test.gen_affinity_matrix()\n","\n","# Split validation and test data into training and testing parts\n","val_data_tr, val_data_te = numpy_stratified_split(val_data, ratio=0.75, seed=123)\n","test_data_tr, test_data_te = numpy_stratified_split(test_data, ratio=0.75, seed=123)\n","\n","# Binarize train, validation and test data\n","train_data = np.where(train_data > 3.5, 1.0, 0.0)\n","val_data = np.where(val_data > 3.5, 1.0, 0.0)\n","test_data = np.where(test_data > 3.5, 1.0, 0.0)\n","\n","# Binarize validation data\n","val_data_tr = np.where(val_data_tr > 3.5, 1.0, 0.0)\n","val_data_te_ratings = val_data_te.copy()\n","val_data_te = np.where(val_data_te > 3.5, 1.0, 0.0)\n","\n","# Binarize test data: training part \n","test_data_tr = np.where(test_data_tr > 3.5, 1.0, 0.0)\n","\n","# Binarize test data: testing part (save non-binary version in the separate object, will be used for calculating NDCG)\n","test_data_te_ratings = test_data_te.copy()\n","test_data_te = np.where(test_data_te > 3.5, 1.0, 0.0)\n","\n","# retrieve real ratings from initial dataset \n","test_data_te_ratings=pd.DataFrame(test_data_te_ratings)\n","val_data_te_ratings=pd.DataFrame(val_data_te_ratings)\n","\n","for index,i in df_low_rating.iterrows():\n","    user_old= i['userId'] # old value \n","    item_old=i['movieId'] # old value \n","\n","    if (test_map_users.get(user_old) is not None)  and (test_map_items.get(item_old) is not None) :\n","        user_new=test_map_users.get(user_old) # new value \n","        item_new=test_map_items.get(item_old) # new value \n","        rating=i['rating'] \n","        test_data_te_ratings.at[user_new,item_new]= rating   \n","\n","    if (val_map_users.get(user_old) is not None)  and (val_map_items.get(item_old) is not None) :\n","        user_new=val_map_users.get(user_old) # new value \n","        item_new=val_map_items.get(item_old) # new value \n","        rating=i['rating'] \n","        val_data_te_ratings.at[user_new,item_new]= rating   \n","\n","\n","val_data_te_ratings=val_data_te_ratings.to_numpy()    \n","test_data_te_ratings=test_data_te_ratings.to_numpy()    "],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rM8THbGWVFik","executionInfo":{"status":"ok","timestamp":1628677182024,"user_tz":-330,"elapsed":155241,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"73eeb9a6-c72e-4cf1-9f7a-4c613a2bdd26"},"source":["disable_eager_execution()\n","svae_model = SVAE.StandardVAE(n_users=train_data.shape[0],\n","                                   original_dim=train_data.shape[1], \n","                                   intermediate_dim=200, \n","                                   latent_dim=64, \n","                                   n_epochs=400, \n","                                   batch_size=100, \n","                                   k=10,\n","                                   verbose=0,\n","                                   seed=123,\n","                                   drop_encoder=0.5,\n","                                   drop_decoder=0.5,\n","                                   annealing=False,\n","                                   beta=1.0\n","                                   )\n","\n","svae_model.fit(x_train=train_data,\n","          x_valid=val_data,\n","          x_val_tr=val_data_tr,\n","          x_val_te=val_data_te_ratings,\n","          mapper=am_val\n","          )"],"execution_count":27,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_v1.py:1246: UserWarning: `model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`model.fit_generator` is deprecated and '\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"LK6CVrxqI_Ve"},"source":["### Recommend with SVAE"]},{"cell_type":"code","metadata":{"id":"N9jsBPW-XQIn","executionInfo":{"status":"ok","timestamp":1628677182026,"user_tz":-330,"elapsed":30,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["# Model prediction on the training part of test set \n","top_k =  svae_model.recommend_k_items(x=test_data_tr,k=10,remove_seen=True)\n","\n","# Convert sparse matrix back to df\n","recommendations = am_test.map_back_sparse(top_k, kind='prediction')\n","test_df = am_test.map_back_sparse(test_data_te_ratings, kind='ratings') # use test_data_te_, with the original ratings\n","\n","# Create column with the predicted movie's rank for each user \n","top_k = recommendations.copy()\n","top_k['rank'] = recommendations.groupby('userId', sort=False).cumcount() + 1  # For each user, only include movies recommendations that are also in the test set\n","\n","recs.append(top_k)"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJllF7bdJHHN"},"source":["## Singular Value Decomposition (SVD)"]},{"cell_type":"markdown","metadata":{"id":"uzwll5SnITUq"},"source":["### SVD++"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qbRc2KJ_8S2W","executionInfo":{"status":"ok","timestamp":1628677324150,"user_tz":-330,"elapsed":142147,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"14c6711d-aada-4eff-dc57-70e560ee3c12"},"source":["surprise_train = surprise.Dataset.load_from_df(train.drop('timestamp', axis=1), reader=surprise.Reader('ml-100k')).build_full_trainset()\n","svdpp = surprise.SVDpp(random_state=0, n_factors=64, n_epochs=10, verbose=True)\n","svdpp.fit(surprise_train)"],"execution_count":29,"outputs":[{"output_type":"stream","text":[" processing epoch 0\n"," processing epoch 1\n"," processing epoch 2\n"," processing epoch 3\n"," processing epoch 4\n"," processing epoch 5\n"," processing epoch 6\n"," processing epoch 7\n"," processing epoch 8\n"," processing epoch 9\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<surprise.prediction_algorithms.matrix_factorization.SVDpp at 0x7fa2f36c02d0>"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"od_CZ4pCJQef"},"source":["### SVD"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C52AB0rs-kOu","executionInfo":{"status":"ok","timestamp":1628677326290,"user_tz":-330,"elapsed":2169,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"4f95672f-1643-471c-9672-d1af4b7d0a47"},"source":["svd = surprise.SVD(random_state=0, n_factors=64, n_epochs=10, verbose=True)\n","svd.fit(surprise_train)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Processing epoch 0\n","Processing epoch 1\n","Processing epoch 2\n","Processing epoch 3\n","Processing epoch 4\n","Processing epoch 5\n","Processing epoch 6\n","Processing epoch 7\n","Processing epoch 8\n","Processing epoch 9\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7fa2ff51f3d0>"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"XkGR-_YyJSz6"},"source":["### Recommend with SVD++ and SVD"]},{"cell_type":"code","metadata":{"id":"kLUfa3Xf9vaJ","executionInfo":{"status":"ok","timestamp":1628677532805,"user_tz":-330,"elapsed":206521,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["for model in [svdpp, svd]:\n","    predictions = []\n","    users = train['userId'].unique()\n","    items = train['movieId'].unique()\n","\n","    for user in users:\n","            for item in items:\n","                predictions.append([user, item, model.predict(user, item).est])\n","\n","    predictions = pd.DataFrame(predictions, columns=['userId', 'movieId', 'prediction'])\n","\n","    # Remove movies already seen by users\n","    # Create column of all 1s\n","    temp = train[['userId', 'movieId']].copy()\n","    temp['seen'] = 1\n","\n","    # Outer join and remove movies that have alread been seen (seen=1)\n","    merged = pd.merge(temp, predictions, on=['userId', 'movieId'], how=\"outer\")\n","    merged = merged[merged['seen'].isnull()].drop('seen', axis=1)\n","\n","    # Create filter for users that appear in both the train and test set\n","    common_users = set(test['userId']).intersection(set(predictions['userId']))\n","\n","    # Filter the test and predictions so they have the same users between them\n","    test_common = test[test['userId'].isin(common_users)]\n","    svd_pred_common = merged[merged['userId'].isin(common_users)]\n","\n","    if len(set(merged['userId'])) != len(set(test['userId'])):\n","        print('Number of users in train and test are NOT equal')\n","        print(f\"# of users in train and test respectively: {len(set(merged['userId']))}, {len(set(test['userId']))}\")\n","        print(f\"# of users in BOTH train and test: {len(set(svd_pred_common['userId']))}\")\n","        continue\n","        \n","    # From the predictions, we want only the top k for each user,\n","    # not all the recommendations.\n","    # Extract the top k recommendations from the predictions\n","    top_movies = svd_pred_common.groupby('userId', as_index=False).apply(lambda x: x.nlargest(10, 'prediction')).reset_index(drop=True)\n","    top_movies['rank'] = top_movies.groupby('userId', sort=False).cumcount() + 1\n","    \n","    top_k = top_movies.copy()\n","    top_k['rank'] = top_movies.groupby('userId', sort=False).cumcount() + 1  # For each user, only include movies recommendations that are also in the test set\n","    \n","    recs.append(top_k)"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VybicxKFJZXP"},"source":["# Compare performance"]},{"cell_type":"markdown","metadata":{"id":"8R5oKqnp_4Q4"},"source":["Looking at all 5 of our models, we can see that the state-of-the-art model LightGCN vastly outperforms all other models. When compared to SVD++, a widely used algorithm during the Netflix Prize competition, LightGCN achieves an increase in **Percision@k by 29%, Recall@k by 18%, MAP by 12%, and NDCG by 35%**.\n","\n","NGCF is the older sister model to LightGCN, but only by a single year. We can see how LightGCN improves in ranking metrics compared to NGCF by simply removing unnecessary operations. \n","\n","In conclusion, this demonstrates how far recommendation systems have advanced since 2009, and how new model architectures with notable performance increases can be developed in the span of just 1-2 years."]},{"cell_type":"code","metadata":{"id":"ZA8JdPhf555b","executionInfo":{"status":"ok","timestamp":1628677532806,"user_tz":-330,"elapsed":28,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["model_names = ['LightGCN', 'NGCF', 'SVAE', 'SVD++', 'SVD']\n","comparison = pd.DataFrame(columns=['Algorithm', 'Precision@k', 'Recall@k', 'MAP', 'NDCG'])\n","\n","# Convert test user ids to the new ids\n","users = np.array([user2id[x] for x in test['userId'].unique()])\n","\n","for rec, name in zip(recs, model_names):\n","    tester = test_df if name == 'SVAE' else test\n","\n","    pak = metrics.precision_at_k(rec, tester, 'userId', 'movieId', 'rank')\n","    rak = metrics.recall_at_k(rec, tester, 'userId', 'movieId', 'rank')\n","    map = metrics.mean_average_precision(rec, tester, 'userId', 'movieId', 'rank')\n","    ndcg = metrics.ndcg(rec, tester, 'userId', 'movieId', 'rank')\n","\n","    comparison.loc[len(comparison)] = [name, pak, rak, map, ndcg]"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"ECyE_Qls7KkQ","executionInfo":{"status":"ok","timestamp":1628677532808,"user_tz":-330,"elapsed":26,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"32c8dfd6-a4a1-46b6-bd44-c2e05ae479d5"},"source":["comparison"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Algorithm</th>\n","      <th>Precision@k</th>\n","      <th>Recall@k</th>\n","      <th>MAP</th>\n","      <th>NDCG</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>LightGCN</td>\n","      <td>0.400848</td>\n","      <td>0.213777</td>\n","      <td>0.135546</td>\n","      <td>0.454352</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NGCF</td>\n","      <td>0.367550</td>\n","      <td>0.193680</td>\n","      <td>0.123132</td>\n","      <td>0.423777</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>SVAE</td>\n","      <td>0.366500</td>\n","      <td>0.097485</td>\n","      <td>0.047921</td>\n","      <td>0.359759</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>SVD++</td>\n","      <td>0.108271</td>\n","      <td>0.038600</td>\n","      <td>0.015655</td>\n","      <td>0.114023</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>SVD</td>\n","      <td>0.093531</td>\n","      <td>0.033000</td>\n","      <td>0.011672</td>\n","      <td>0.092656</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Algorithm  Precision@k  Recall@k       MAP      NDCG\n","0  LightGCN     0.400848  0.213777  0.135546  0.454352\n","1      NGCF     0.367550  0.193680  0.123132  0.423777\n","2      SVAE     0.366500  0.097485  0.047921  0.359759\n","3     SVD++     0.108271  0.038600  0.015655  0.114023\n","4       SVD     0.093531  0.033000  0.011672  0.092656"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"9DUTuMtTLFcA"},"source":["# References:\n","\n","1.   Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang & Meng Wang, LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation, 2020, https://arxiv.org/abs/2002.02126\n","2.   Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, & Tata-Seng Chua, Neural Graph Collaorative Filtering, 2019, https://arxiv.org/abs/1905.08108\n","3.   Microsoft SVAE implementation: https://github.com/microsoft/recommenders/blob/main/examples/02_model_collaborative_filtering/standard_vae_deep_dive.ipynb\n","4. Simon Gower, Netflix Prize and SVD, 2014, https://www.semanticscholar.org/paper/Netflix-Prize-and-SVD-Gower/ce7b81b46939d7852dbb30538a7796e69fdd407c\n"]}]}