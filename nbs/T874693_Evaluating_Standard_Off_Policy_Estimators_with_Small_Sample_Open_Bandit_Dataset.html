
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset &#8212; Reco Book</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Optimal Off-Policy Evaluation from Multiple Logging Policies" href="T792262_Optimal_Off_Policy_Evaluation_from_Multiple_Logging_Policies.html" />
    <link rel="prev" title="Evaluation of Multiple Off-Policy Estimators on Synthetic Dataset" href="T705904_Evaluation_of_Multiple_Off_Policy_Estimators_on_Synthetic_Dataset.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Reco Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Tutorials in Jupyter notebook format
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  User Stories
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="US780867_Transformer_based_Recommenders.html">
   Transformer-based Recommenders
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="T034923_BERT4Rec_on_ML1M_in_PyTorch.html">
     BERT4Rec on ML-1M in PyTorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T595874_BERT4Rec_on_ML25M_in_PyTorch_Lightning.html">
     BERT4Rec on ML-25M
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T088416_BST_Implementation_in_MXNet.html">
     BST Implementation in MXNet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T602245_BST_implementation_in_PyTorch.html">
     BST implementation in PyTorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T007665_BST_on_ML1M_in_Keras.html">
     A Transformer-based recommendation system
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T881207_BST_PTLightning_ML1M.html">
     Rating prediction using the Behavior Sequence Transformer (BST) model on ML-1M dataset in PyTorch Lightning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T757997_SASRec_PyTorch.html">
     SASRec implementation with PyTorch Library
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T225287_SASRec_PaddlePaddle.html">
     SASRec implementation with Paddle Library
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T701627_SR_SAN_Session_based_Model.html">
     SR-SAN Session-based Recommender
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T975104_SSEPT_ML1M_Tensorflow1x.html">
     SSE-PT Personalized Transformer Recommender on ML-1M in Tensorflow 1.x
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Prototypes
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="T382881_DeepWalk_Karateclub.html">
   DeepWalk from scratch referencing Karateclub library
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T384270_DeepWalk_pure_python.html">
   DeepWalk in pure python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T677598_Jaccard_Cosine_SVD_DeepWalk_ML100K.html">
   Recommender System with DeepWalk Graph Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T815556_Node2vec_Karateclub.html">
   Node2vec from scratch referencing Karateclub library
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T894941_Node2vec_MovieLens_Keras.html">
   Graph representation learning with node2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T611050_Node2vec_PyG.html">
   Node2vec from scratch in PyTorch Geometric
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T186367_Node2vec_library.html">
   Node2vec from scratch referencing node2vec library
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T331379_bayesian_personalized_ranking.html">
   BPR from scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T081831_Data_Poisoning_Attacks_on_Factorization_Based_Collaborative_Filtering.html">
   Data Poisoning Attacks on Factorization-Based Collaborative Filtering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T102448_Adversarial_Learning_for_Recommendation.html">
   Adversarial Training (Regularization) on a Recommender System
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T865035_Simulating_Data_Poisoning_Attacks_against_Twitter_Recommender.html">
   Load and process dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T711285_Data_Poisoning_Attack_using_LFM_and_ItemAE_on_Synthetic_Dataset.html">
   Injection attack using LFM and ItemAE model trained on Toy dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T355514_Black_box_Attack_on_Sequential_Recs.html">
   Black-box Attack on Sequential Recs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T873451_Statistics_fundamentals.html">
   Statictics Fundamentals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T890478_Batch_Learning_from_Bandit_Feedback_%28BLBF%29.html">
   Imports
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T257798_Off_Policy_Learning_in_Two_stage_Recommender_Systems.html">
   Off-Policy Learning in Two-stage Recommender Systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T471827_Adaptive_Estimator_Selection_for_Off_Policy_Evaluation.html">
   Adaptive Estimator Selection for Off-Policy Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T902666_Evaluating_the_Robustness_of_Off_Policy_Evaluation.html">
   Evaluating the Robustness of Off-Policy Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T705904_Evaluation_of_Multiple_Off_Policy_Estimators_on_Synthetic_Dataset.html">
   Evaluation of Multiple Off-Policy Estimators on Synthetic Dataset
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T792262_Optimal_Off_Policy_Evaluation_from_Multiple_Logging_Policies.html">
   Optimal Off-Policy Evaluation from Multiple Logging Policies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T167249_Offline_Policy_Evaluation_with_VW_Command_Line.html">
   Offline Policy Evaluation with VW Command Line
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T966055_OBP_Library_Workshop_Tutorials.html">
   OBP Library Workshop Tutorials
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T632722_PyTorch_Fundamentals_Part_1.html">
   PyTorch Fundamentals Part 1
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T472467_PyTorch_Fundamentals_Part_2.html">
   PyTorch Fundamentals Part 2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T206654_PyTorch_Fundamentals_Part_3.html">
   PyTorch Fundamentals Part 3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T536348_Attention_Mechanisms.html">
   Imports
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T500796_Agricultural_Satellite_Image_Segmentation.html">
   Agricultural Satellite Image Segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T611432_Image_Analysis_with_Tensorflow.html">
   Image Analysis with Tensorflow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T925716_MongoDB_to_CSV_Conversion.html">
   MongoDB to CSV conversion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T396469_PDF_to_Word_Cloud_via_Email.html">
   PDF to WordCloud via Email
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T030890_Job_Scraping_and_Clustering.html">
   Job scraping and clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T897054_Scene_Text_Recognition.html">
   Scene Text Recognition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T034809_Large_scale_Document_Retrieval_with_Elastic_Search.html">
   Large-scale Document Retrieval with ElasticSearch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T467251_vowpal_wabbit_contextual_recommender.html">
   Simulating a news personalization scenario using Contextual Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T686684_similar_product_recommender.html">
   Similar Product Recommender system using Deep Learning for an online e-commerce store
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T132203_Retail_Product_Recommendations_using_Word2vec.html">
   Retail Product Recommendations using word2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T501828_Recommender_Implicit_Negative_Feedback.html">
   Retail Product Recommendation with Negative Implicit Feedback
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T315965_Sequence_Aware_Recommenders_Music.html">
   Sequence Aware Recommender Systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T051777_image_similarity_recommendations.html">
   Similar Product Recommendations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990172_recobook_diversity_aware_book_recommender.html">
   Diversity Aware Book Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T488549_Goodreads_Diversity_Aware_Book_Recommender.html">
   Diversity Aware Book Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T023535_Kafka_MongoDB_Real_time_Streaming.html">
   Kafka MongoDB Real-time Streaming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T622304_Session_based_Recommender_Using_Word2vec.html">
   Session-based recommendation using word2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T416854_bandit_based_recommender_using_thompson_sampling_app.html">
   Bandit-based Online Learning using Thompson Sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T198578_Booking_dot_com_Trip_Recommendation.html">
   Booking.com Trip Recommendation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T519734_Vowpal_Wabbit_Contextual_Bandit.html">
   Vowpal Wabbit Contextual Bandit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T871537_Recommendation_Systems_using_Olist_Dataset.html">
   Recommendation systems using Olist dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T057885_Offline_Replayer_Evaluation.html">
   Offline Replayer Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T915054_method_for_effective_online_testing.html">
   Methods for effective online testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T513987_Recsys_2020_Feature_Engineering_Tutorial.html">
   Recsys’20 Feature Engineering Tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T227901_amazon_personalize_batch_job.html">
   Amazon Personalize Batch Job
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T022961_Amazon_Personalize_Workshop.html">
   Amazon Personalize Workshop
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T424437_Collaborative_Filtering_on_ML_latest_small.html">
   Collaborative Filtering on ML-latest-small
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T539160_Building_and_Deploying_ASOS_Fashion_Recommender.html">
   Building and deploying ASOS fashion recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T757697_Simple_Similarity_based_Recommender.html">
   Simple Similarity based Recommmendations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T051594_Analytics_Zoo.html">
   Analytics Zoo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T313645_A_B_Testing.html">
   A/B Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T475711_PinSage_Graph_based_Recommender.html">
   PinSage Graph-based Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T516490_Graph_Embeddings.html">
   Learn Embeddings using Graph Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T822164_movielens_milvus_redis_efficient_retrieval.html">
   Recommender with Redis and Milvus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T845186_Anime_Recommender.html">
   RekoNet Anime Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T855843_kafka_spark_streaming_colab.html">
   Kafka and Spark Streaming in Colab
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T460437_Building_Models_From_Scratch.html">
   Building Models from scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T855971_Conet_Model_for_Movie_Recommender.html">
   CoNet model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T996996_content_based_and_collaborative_movielens.html">
   Movie Recommendation with Content-Based and Collaborative Filtering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T239418_Simple_Movie_Recommenders.html">
   Simple Movie Recommenders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T138337_Simple_Movie_Recommender.html">
   Simple movie recommender in implicit, explicit, and cold-start settings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T935440_The_importance_of_Rating_Normalization.html">
   The importance of Rating Normalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T612622_cornac_examples.html">
   Cornac Examples
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T561435_Flower_Classification.html">
   Flower classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T680910_Trivago_Session_based_Recommender.html">
   Trivago Session-based Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_ads_selection_using_bandits.html">
   Best Ads detection using bandit methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_book_crossing_surprise_svd_nmf.html">
   Book-Crossing Recommendation System
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_book_recommender_kubeflow.html">
   Books recommendations with Kubeflow Pipelines on Scaleway Kapsule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_build_a_kubeflow_pipeline.html">
   Build a Kubeflow Pipeline
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_causal_inference.html">
   Causal Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_concept_embedding_nlp.html">
   Exploring Word Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_concept_neural_net.html">
   Neural Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_concept_nlp_basics.html">
   Natural Language Processing 101
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_concept_transformer_lm.html">
   TransformerLM Quick Start and Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_content_based_music_recommender_lyricsfreak.html">
   Content-based method for song recommendation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_evaluation_metrics_basics.html">
   Recommender System Evaluations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_implicit_synthetic.html">
   Comparing Implicit Models on Synthetic Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_movie_recommender_tensorflow.html">
   Recommendation Systems with TensorFlow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_movie_recommender_tensorflow_sagemaker.html">
   Movie recommender using Tensorflow in Sagemaker
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_movielens_eda_modeling.html">
   Movielens EDA and Modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_read_data_from_cassandra_into_pandas.html">
   Read Cassandra Data Snapshot as DataFrame
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_rl_in_action.html">
   Reinforcement Learning fundamentals in action
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_rnn_cnn_basics.html">
   Processing sequences using RNNs and CNNs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_tf_serving_in_action.html">
   TF Serving in action
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_training_indexing_movie_recommender.html">
   Training and indexing movie recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T207114_EduRec_MOOCCube_Course_Recommender.html">
   EduRec MOOCCube Course Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T273184_Multi_Task_Learning.html">
   Multi-task Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T661108_Book_Recommender_API.html">
   Book Recommender API
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T912764_Simple_Movie_Recommender_App.html">
   Simple Movie Recommender App
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T964554_Career_Village_Questions_Recommendation.html">
   CareerVillage Questions Recommendation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_amazon_women_apparel_tfidf_word2vec.html">
   Amazon Product Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_anime_recommender_graph_network.html">
   Anime Recommender with Bi-partite Graph Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_concept_self_attention.html">
   Self-Attention
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_course_recommender_svd_flask.html">
   Course Recommender with SVD based similarity
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_data_mining_similarity_measures.html">
   Concept - Data Mining Similarity Measures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_jaccard_recommender.html">
   Jaccard Similarity based Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_live_streamer_recommender.html">
   Live Streamer Recommender with Implicit feedback
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_songs_embedding_skipgram_recommender.html">
   Song Embeddings - Skipgram Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_toy_example_car_recommender_knn.html">
   Toy example - Car Recommender using KNN method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_wikirecs_recommender.html">
   WikiRecs
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/nbs/T874693_Evaluating_Standard_Off_Policy_Estimators_with_Small_Sample_Open_Bandit_Dataset.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sparsh-ai/reco-book/main?urlpath=tree/nbs/T874693_Evaluating_Standard_Off_Policy_Estimators_with_Small_Sample_Open_Bandit_Dataset.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataloader">
   Dataloader
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ope-estimators">
   OPE Estimators
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#utils">
     Utils
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#abstract-class">
     Abstract Class
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#direct-method">
     Direct Method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#doubly-robust">
     Doubly Robust
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inverse-probability-weighting">
     Inverse Probability Weighting
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#off-policy-evaluation-class">
   Off-Policy Evaluation Class
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#base-models">
   Base Models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Utils
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-model">
     Regression Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policies">
   Policies
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#base-context-free-policy">
     Base Context Free Policy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#epsilon-greedy">
     Epsilon Greedy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random">
     Random
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bernoullits">
     BernoulliTS
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#run">
   Run
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><a href="https://colab.research.google.com/github/sparsh-ai/reco-book/blob/stage/nbs/T874693_Evaluating_Standard_Off_Policy_Estimators_with_Small_Sample_Open_Bandit_Dataset.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<div class="section" id="evaluating-standard-off-policy-estimators-with-small-sample-open-bandit-dataset">
<h1>Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset<a class="headerlink" href="#evaluating-standard-off-policy-estimators-with-small-sample-open-bandit-dataset" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p><strong>Evaluating 3 standard off-policy estimators (DirectMethod, DoublyRobust, and InverseProbabilityWeighting) on small sample open-bandit dataset</strong>.</p>
<p>These OPE estimators will estimate the performance of BernoulliTS policy (counterfactual/evaluation policy) using data generated by Random policy (behavior policy).</p>
<p>Imports</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">delayed</span>
<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">DataFrame</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">import</span> <span class="nn">yaml</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="dataloader">
<h2>Dataloader<a class="headerlink" href="#dataloader" title="Permalink to this headline">¶</a></h2>
<p>Abstract Base Class for Logged Bandit Feedback.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABCMeta</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">abstractmethod</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BaseBanditDataset</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base Class for Synthetic Bandit Dataset.&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Obtain batch logged bandit feedback.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BaseRealBanditDataset</span><span class="p">(</span><span class="n">BaseBanditDataset</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base Class for Real-World Bandit Dataset.&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">load_raw_data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Load raw dataset.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">pre_process</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Preprocess raw dataset.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</pre></div>
</div>
</div>
</div>
<p>Dataset Class for Real-World Logged Bandit Feedback.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">logging</span> <span class="kn">import</span> <span class="n">basicConfig</span>
<span class="kn">from</span> <span class="nn">logging</span> <span class="kn">import</span> <span class="n">getLogger</span>
<span class="kn">from</span> <span class="nn">logging</span> <span class="kn">import</span> <span class="n">INFO</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">rankdata</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># dataset type</span>
<span class="n">BanditFeedback</span> <span class="o">=</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">logger</span> <span class="o">=</span> <span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">INFO</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">OBD_DATA_PATH</span> <span class="o">=</span> <span class="s1">&#39;/content/zr-obp/obd&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">OpenBanditDataset</span><span class="p">(</span><span class="n">BaseRealBanditDataset</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Class for loading and preprocessing Open Bandit Dataset.</span>
<span class="sd">    Note</span>
<span class="sd">    -----</span>
<span class="sd">    Users are free to implement their own feature engineering by overriding the `pre_process` method.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    behavior_policy: str</span>
<span class="sd">        Name of the behavior policy that generated the logged bandit feedback data.</span>
<span class="sd">        Must be either &#39;random&#39; or &#39;bts&#39;.</span>
<span class="sd">    campaign: str</span>
<span class="sd">        One of the three possible campaigns considered in ZOZOTOWN.</span>
<span class="sd">        Must be one of &quot;all&quot;, &quot;men&quot;, or &quot;women&quot;.</span>
<span class="sd">    data_path: str or Path, default=None</span>
<span class="sd">        Path where the Open Bandit Dataset is stored.</span>
<span class="sd">    dataset_name: str, default=&#39;obd&#39;</span>
<span class="sd">        Name of the dataset.</span>
<span class="sd">    References</span>
<span class="sd">    ------------</span>
<span class="sd">    Yuta Saito, Shunsuke Aihara, Megumi Matsutani, Yusuke Narita.</span>
<span class="sd">    &quot;Open Bandit Dataset and Pipeline: Towards Realistic and Reproducible Off-Policy Evaluation.&quot;, 2020.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">behavior_policy</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">campaign</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">data_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dataset_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;obd&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Open Bandit Dataset Class.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">behavior_policy</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="s2">&quot;bts&quot;</span><span class="p">,</span>
            <span class="s2">&quot;random&quot;</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;behavior_policy must be either of &#39;bts&#39; or &#39;random&#39;, but </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">behavior_policy</span><span class="si">}</span><span class="s2"> is given&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">campaign</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="s2">&quot;all&quot;</span><span class="p">,</span>
            <span class="s2">&quot;men&quot;</span><span class="p">,</span>
            <span class="s2">&quot;women&quot;</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;campaign must be one of &#39;all&#39;, &#39;men&#39;, or &#39;women&#39;, but </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">campaign</span><span class="si">}</span><span class="s2"> is given&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">OBD_DATA_PATH</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="n">Path</span><span class="p">):</span>
                <span class="k">pass</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_path</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;data_path must be a string or Path&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_path</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">behavior_policy</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">campaign</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">raw_data_file</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">campaign</span><span class="si">}</span><span class="s2">.csv&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_raw_data</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pre_process</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">n_rounds</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Total number of rounds contained in the logged bandit dataset.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">n_actions</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Number of actions.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dim_context</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Dimensions of context vectors.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">len_list</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Length of recommendation lists.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">calc_on_policy_policy_value_estimate</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">behavior_policy</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">campaign</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">data_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Path</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">test_size</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span>
        <span class="n">is_timeseries_split</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Calculate on-policy policy value estimate (used as a ground-truth policy value).</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        behavior_policy: str</span>
<span class="sd">            Name of the behavior policy that generated the log data.</span>
<span class="sd">            Must be either &#39;random&#39; or &#39;bts&#39;.</span>
<span class="sd">        campaign: str</span>
<span class="sd">            One of the three possible campaigns considered in ZOZOTOWN (i.e., &quot;all&quot;, &quot;men&quot;, and &quot;women&quot;).</span>
<span class="sd">        data_path: Path, default=None</span>
<span class="sd">            Path where the Open Bandit Dataset exists.</span>
<span class="sd">        test_size: float, default=0.3</span>
<span class="sd">            Proportion of the dataset included in the test split.</span>
<span class="sd">            If float, should be between 0.0 and 1.0.</span>
<span class="sd">            This argument matters only when `is_timeseries_split=True` (the out-sample case).</span>
<span class="sd">        is_timeseries_split: bool, default=False</span>
<span class="sd">            If true, split the original logged bandit feedback data by time series.</span>
<span class="sd">        Returns</span>
<span class="sd">        ---------</span>
<span class="sd">        on_policy_policy_value_estimate: float</span>
<span class="sd">            Policy value of the behavior policy estimated by on-policy estimation, i.e., :math:`\\mathbb{E}_{\\mathcal{D}} [r_t]`.</span>
<span class="sd">            where :math:`\\mathbb{E}_{\\mathcal{D}}[\\cdot]` is the empirical average over :math:`T` observations in :math:`\\mathcal{D}`.</span>
<span class="sd">            This parameter is used as a ground-truth policy value in the evaluation of OPE estimators.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">bandit_feedback</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">behavior_policy</span><span class="o">=</span><span class="n">behavior_policy</span><span class="p">,</span> <span class="n">campaign</span><span class="o">=</span><span class="n">campaign</span><span class="p">,</span> <span class="n">data_path</span><span class="o">=</span><span class="n">data_path</span>
        <span class="p">)</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span>
            <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span> <span class="n">is_timeseries_split</span><span class="o">=</span><span class="n">is_timeseries_split</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">is_timeseries_split</span><span class="p">:</span>
            <span class="n">bandit_feedback_test</span> <span class="o">=</span> <span class="n">bandit_feedback</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bandit_feedback_test</span> <span class="o">=</span> <span class="n">bandit_feedback</span>
        <span class="k">return</span> <span class="n">bandit_feedback_test</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">load_raw_data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Load raw open bandit dataset.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_path</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">raw_data_file</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">item_context</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data_path</span> <span class="o">/</span> <span class="s2">&quot;item_context.csv&quot;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">&quot;timestamp&quot;</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;item_id&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="o">=</span> <span class="p">(</span><span class="n">rankdata</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;position&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="s2">&quot;dense&quot;</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span>
            <span class="nb">int</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;click&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pscore</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;propensity_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

    <span class="k">def</span> <span class="nf">pre_process</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Preprocess raw open bandit dataset.</span>
<span class="sd">        Note</span>
<span class="sd">        -----</span>
<span class="sd">        This is the default feature engineering and please override this method to</span>
<span class="sd">        implement your own preprocessing.</span>
<span class="sd">        see https://github.com/st-tech/zr-obp/blob/master/examples/examples_with_obd/custom_dataset.py for example.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">user_cols</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s2">&quot;user_feature&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">user_cols</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span><span class="o">.</span><span class="n">values</span>
        <span class="n">item_feature_0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">item_context</span><span class="p">[</span><span class="s2">&quot;item_feature_0&quot;</span><span class="p">]</span>
        <span class="n">item_feature_cat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">item_context</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;item_feature_0&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
            <span class="n">LabelEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_context</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">item_feature_cat</span><span class="p">,</span> <span class="n">item_feature_0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>

    <span class="k">def</span> <span class="nf">obtain_batch_bandit_feedback</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">test_size</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">is_timeseries_split</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">BanditFeedback</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">BanditFeedback</span><span class="p">,</span> <span class="n">BanditFeedback</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Obtain batch logged bandit feedback.</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        test_size: float, default=0.3</span>
<span class="sd">            Proportion of the dataset included in the test split.</span>
<span class="sd">            If float, should be between 0.0 and 1.0.</span>
<span class="sd">            This argument matters only when `is_timeseries_split=True` (the out-sample case).</span>
<span class="sd">        is_timeseries_split: bool, default=False</span>
<span class="sd">            If true, split the original logged bandit feedback data into train and test sets based on time series.</span>
<span class="sd">        Returns</span>
<span class="sd">        --------</span>
<span class="sd">        bandit_feedback: BanditFeedback</span>
<span class="sd">            A dictionary containing batch logged bandit feedback data collected by a behavior policy.</span>
<span class="sd">            The keys of the dictionary are as follows.</span>
<span class="sd">            - n_rounds: number of rounds (size) of the logged bandit data</span>
<span class="sd">            - n_actions: number of actions (:math:`|\mathcal{A}|`)</span>
<span class="sd">            - action: action variables sampled by a behavior policy</span>
<span class="sd">            - position: positions where actions are recommended</span>
<span class="sd">            - reward: reward variables</span>
<span class="sd">            - pscore: action choice probabilities by a behavior policy</span>
<span class="sd">            - context: context vectors such as user-related features and user-item affinity scores</span>
<span class="sd">            - action_context: item-related context vectors</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">is_timeseries_split</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`is_timeseries_split` must be a bool, but </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">is_timeseries_split</span><span class="p">)</span><span class="si">}</span><span class="s2"> is given&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">is_timeseries_split</span><span class="p">:</span>
            <span class="n">check_scalar</span><span class="p">(</span>
                <span class="n">test_size</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;target_size&quot;</span><span class="p">,</span>
                <span class="n">target_type</span><span class="o">=</span><span class="p">(</span><span class="nb">float</span><span class="p">),</span>
                <span class="n">min_val</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                <span class="n">max_val</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">n_rounds_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_rounds</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">test_size</span><span class="p">))</span>
            <span class="n">bandit_feedback_train</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
                <span class="n">n_rounds</span><span class="o">=</span><span class="n">n_rounds_train</span><span class="p">,</span>
                <span class="n">n_actions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">[:</span><span class="n">n_rounds_train</span><span class="p">],</span>
                <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">[:</span><span class="n">n_rounds_train</span><span class="p">],</span>
                <span class="n">reward</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">[:</span><span class="n">n_rounds_train</span><span class="p">],</span>
                <span class="n">pscore</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pscore</span><span class="p">[:</span><span class="n">n_rounds_train</span><span class="p">],</span>
                <span class="n">context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="p">[:</span><span class="n">n_rounds_train</span><span class="p">],</span>
                <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">bandit_feedback_test</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
                <span class="n">n_rounds</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_rounds</span> <span class="o">-</span> <span class="n">n_rounds_train</span><span class="p">),</span>
                <span class="n">n_actions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">[</span><span class="n">n_rounds_train</span><span class="p">:],</span>
                <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">[</span><span class="n">n_rounds_train</span><span class="p">:],</span>
                <span class="n">reward</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">[</span><span class="n">n_rounds_train</span><span class="p">:],</span>
                <span class="n">pscore</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pscore</span><span class="p">[</span><span class="n">n_rounds_train</span><span class="p">:],</span>
                <span class="n">context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="p">[</span><span class="n">n_rounds_train</span><span class="p">:],</span>
                <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">bandit_feedback_train</span><span class="p">,</span> <span class="n">bandit_feedback_test</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
                <span class="n">n_rounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_rounds</span><span class="p">,</span>
                <span class="n">n_actions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">,</span>
                <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span>
                <span class="n">reward</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">pscore</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pscore</span><span class="p">,</span>
                <span class="n">context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="p">,</span>
                <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample_bootstrap_bandit_feedback</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">test_size</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span>
        <span class="n">is_timeseries_split</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BanditFeedback</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Obtain bootstrap logged bandit feedback.</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        sample_size: int, default=None</span>
<span class="sd">            Number of data sampled by bootstrap.</span>
<span class="sd">            When None is given, the original data size (n_rounds) is used as `sample_size`.</span>
<span class="sd">            The value must be smaller than the original data size.</span>
<span class="sd">        test_size: float, default=0.3</span>
<span class="sd">            Proportion of the dataset included in the test split.</span>
<span class="sd">            If float, should be between 0.0 and 1.0.</span>
<span class="sd">            This argument matters only when `is_timeseries_split=True` (the out-sample case).</span>
<span class="sd">        is_timeseries_split: bool, default=False</span>
<span class="sd">            If true, split the original logged bandit feedback data into train and test sets based on time series.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        Returns</span>
<span class="sd">        --------</span>
<span class="sd">        bandit_feedback: BanditFeedback</span>
<span class="sd">            A dictionary containing logged bandit feedback data sampled independently from the original data with replacement.</span>
<span class="sd">            The keys of the dictionary are as follows.</span>
<span class="sd">            - n_rounds: number of rounds (size) of the logged bandit data</span>
<span class="sd">            - n_actions: number of actions</span>
<span class="sd">            - action: action variables sampled by a behavior policy</span>
<span class="sd">            - position: positions where actions are recommended by a behavior policy</span>
<span class="sd">            - reward: reward variables</span>
<span class="sd">            - pscore: action choice probabilities by a behavior policy</span>
<span class="sd">            - context: context vectors such as user-related features and user-item affinity scores</span>
<span class="sd">            - action_context: item-related context vectors</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">is_timeseries_split</span><span class="p">:</span>
            <span class="n">bandit_feedback</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span>
                <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span> <span class="n">is_timeseries_split</span><span class="o">=</span><span class="n">is_timeseries_split</span>
            <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bandit_feedback</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span>
                <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span> <span class="n">is_timeseries_split</span><span class="o">=</span><span class="n">is_timeseries_split</span>
            <span class="p">)</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;n_rounds&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">sample_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sample_size</span> <span class="o">=</span> <span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;n_rounds&quot;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">check_scalar</span><span class="p">(</span>
                <span class="n">sample_size</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;sample_size&quot;</span><span class="p">,</span>
                <span class="n">target_type</span><span class="o">=</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span>
                <span class="n">min_val</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">max_val</span><span class="o">=</span><span class="n">n_rounds</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">random_</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">bootstrap_idx</span> <span class="o">=</span> <span class="n">random_</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">sample_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">key_</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="s2">&quot;position&quot;</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="s2">&quot;context&quot;</span><span class="p">]:</span>
            <span class="n">bandit_feedback</span><span class="p">[</span><span class="n">key_</span><span class="p">]</span> <span class="o">=</span> <span class="n">bandit_feedback</span><span class="p">[</span><span class="n">key_</span><span class="p">][</span><span class="n">bootstrap_idx</span><span class="p">]</span>
        <span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;n_rounds&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sample_size</span>
        <span class="k">return</span> <span class="n">bandit_feedback</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="ope-estimators">
<h2>OPE Estimators<a class="headerlink" href="#ope-estimators" title="Permalink to this headline">¶</a></h2>
<div class="section" id="utils">
<h3>Utils<a class="headerlink" href="#utils" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABCMeta</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>
<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">check_array</span><span class="p">(</span>
    <span class="n">array</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">expected_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="ne">ValueError</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Input validation on an array.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -------------</span>
<span class="sd">    array: object</span>
<span class="sd">        Input object to check.</span>
<span class="sd">    name: str</span>
<span class="sd">        Name of the input array.</span>
<span class="sd">    expected_dim: int, default=1</span>
<span class="sd">        Expected dimension of the input array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must be </span><span class="si">{</span><span class="n">expected_dim</span><span class="si">}</span><span class="s2">D array, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">array</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">array</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="n">expected_dim</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must be </span><span class="si">{</span><span class="n">expected_dim</span><span class="si">}</span><span class="s2">D array, but got </span><span class="si">{</span><span class="n">array</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">D array&quot;</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">check_confidence_interval_arguments</span><span class="p">(</span>
    <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">ValueError</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Check confidence interval arguments.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    alpha: float, default=0.05</span>
<span class="sd">        Significance level.</span>
<span class="sd">    n_bootstrap_samples: int, default=10000</span>
<span class="sd">        Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        Controls the random seed in bootstrap sampling.</span>
<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    estimated_confidence_interval: Dict[str, float]</span>
<span class="sd">        Dictionary storing the estimated mean and upper-lower confidence bounds.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">check_scalar</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">check_scalar</span><span class="p">(</span><span class="n">n_bootstrap_samples</span><span class="p">,</span> <span class="s2">&quot;n_bootstrap_samples&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">estimate_confidence_interval_by_bootstrap</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Estimate confidence interval by nonparametric bootstrap-like procedure.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    samples: array-like</span>
<span class="sd">        Empirical observed samples to be used to estimate cumulative distribution function.</span>
<span class="sd">    alpha: float, default=0.05</span>
<span class="sd">        Significance level.</span>
<span class="sd">    n_bootstrap_samples: int, default=10000</span>
<span class="sd">        Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        Controls the random seed in bootstrap sampling.</span>
<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    estimated_confidence_interval: Dict[str, float]</span>
<span class="sd">        Dictionary storing the estimated mean and upper-lower confidence bounds.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_confidence_interval_arguments</span><span class="p">(</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span>
    <span class="p">)</span>

    <span class="n">boot_samples</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="n">random_</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_bootstrap_samples</span><span class="p">):</span>
        <span class="n">boot_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">random_</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
    <span class="n">lower_bound</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">boot_samples</span><span class="p">,</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">upper_bound</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">boot_samples</span><span class="p">,</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">boot_samples</span><span class="p">),</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span><span class="si">}</span><span class="s2">% CI (lower)&quot;</span><span class="p">:</span> <span class="n">lower_bound</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span><span class="si">}</span><span class="s2">% CI (upper)&quot;</span><span class="p">:</span> <span class="n">upper_bound</span><span class="p">,</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">check_ope_inputs</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">action</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pscore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">ValueError</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Check inputs for ope.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">        Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">    position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">    action: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">    reward: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Observed rewards (or outcome) in each round, i.e., :math:`r_t`.</span>
<span class="sd">    pscore: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Propensity scores, the probability of selecting each action by behavior policy,</span>
<span class="sd">        in the given logged bandit data.</span>
<span class="sd">    estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list), default=None</span>
<span class="sd">        Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># action_dist</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action_dist&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;action_dist must be a probability distribution&quot;</span><span class="p">)</span>

    <span class="c1"># position</span>
    <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">position</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;position&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">position</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `position.shape[0] == action_dist.shape[0]`, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">position</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)</span> <span class="ow">and</span> <span class="n">position</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;position elements must be non-negative integers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;position elements must be smaller than `action_dist.shape[2]`&quot;</span>
            <span class="p">)</span>
    <span class="k">elif</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;position elements must be given when `action_dist.shape[2] &gt; 1`&quot;</span>
        <span class="p">)</span>

    <span class="c1"># estimated_rewards_by_reg_model</span>
    <span class="k">if</span> <span class="n">estimated_rewards_by_reg_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">estimated_rewards_by_reg_model</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `estimated_rewards_by_reg_model.shape == action_dist.shape`, but found it False&quot;</span>
            <span class="p">)</span>

    <span class="c1"># action, reward</span>
    <span class="k">if</span> <span class="n">action</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reward</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `action.shape[0] == reward.shape[0]`, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)</span> <span class="ow">and</span> <span class="n">action</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;action elements must be non-negative integers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">action</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;action elements must be smaller than `action_dist.shape[1]`&quot;</span>
            <span class="p">)</span>

    <span class="c1"># pscore</span>
    <span class="k">if</span> <span class="n">pscore</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pscore</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;pscore must be 1-dimensional&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">pscore</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `action.shape[0] == reward.shape[0] == pscore.shape[0]`, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">pscore</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;pscore must be positive&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">estimate_bias_in_ope</span><span class="p">(</span>
    <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">iw</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">iw_hat</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">q_hat</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Helper to estimate a bias in OPE.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    reward: array-like, shape (n_rounds,)</span>
<span class="sd">        Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">    iw: array-like, shape (n_rounds,)</span>
<span class="sd">        Importance weight in each round of the logged bandit feedback, i.e., :math:`w(x,a)=\\pi_e(a|x)/ \\pi_b(a|x)`.</span>
<span class="sd">    iw_hat: array-like, shape (n_rounds,)</span>
<span class="sd">        Importance weight (IW) modified by a hyparpareter. How IW is modified depends on the estimator as follows.</span>
<span class="sd">            - clipping: :math:`\\hat{w}(x,a) := \\min \\{ \\lambda, w(x,a) \\}`</span>
<span class="sd">            - switching: :math:`\\hat{w}(x,a) := w(x,a) \\cdot \\mathbb{I} \\{ w(x,a) &lt; \\lambda \\}`</span>
<span class="sd">            - shrinkage: :math:`\\hat{w}(x,a) := (\\lambda w(x,a)) / (\\lambda + w^2(x,a))`</span>
<span class="sd">        where :math:`\\lambda` is a hyperparameter value.</span>
<span class="sd">    q_hat: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Estimated expected reward given context :math:`x_t` and action :math:`a_t`.</span>
<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    estimated_bias: float</span>
<span class="sd">        Estimated the bias in OPE.</span>
<span class="sd">        This is based on the direct bias estimation stated on page 17 of Su et al.(2020).</span>
<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.</span>
<span class="sd">    &quot;Doubly Robust Off-Policy Evaluation with Shrinkage.&quot;, 2020.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">q_hat</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">q_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">)</span>
    <span class="n">estimated_bias_arr</span> <span class="o">=</span> <span class="p">(</span><span class="n">iw</span> <span class="o">-</span> <span class="n">iw_hat</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">q_hat</span><span class="p">)</span>
    <span class="n">estimated_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">estimated_bias_arr</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">estimated_bias</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="abstract-class">
<h3>Abstract Class<a class="headerlink" href="#abstract-class" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseOffPolicyEstimator</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for OPE estimators.&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_estimate_round_rewards</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate round-wise (or sample-wise) rewards.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">estimate_policy_value</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">estimate_interval</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate confidence interval of policy value by nonparametric bootstrap procedure.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="direct-method">
<h3>Direct Method<a class="headerlink" href="#direct-method" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">DirectMethod</span><span class="p">(</span><span class="n">BaseOffPolicyEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Direct Method (DM).</span>
<span class="sd">    Note</span>
<span class="sd">    -------</span>
<span class="sd">    DM first learns a supervised machine learning model, such as ridge regression and gradient boosting,</span>
<span class="sd">    to estimate the mean reward function (:math:`q(x,a) = \\mathbb{E}[r|x,a]`).</span>
<span class="sd">    It then uses it to estimate the policy value as follows.</span>
<span class="sd">    .. math::</span>
<span class="sd">        \\hat{V}_{\\mathrm{DM}} (\\pi_e; \\mathcal{D}, \\hat{q})</span>
<span class="sd">        &amp;:= \\mathbb{E}_{\\mathcal{D}} \\left[ \\sum_{a \\in \\mathcal{A}} \\hat{q} (x_t,a) \\pi_e(a|x_t) \\right],    \\\\</span>
<span class="sd">        &amp; =  \\mathbb{E}_{\\mathcal{D}}[\\hat{q} (x_t,\\pi_e)],</span>
<span class="sd">    where :math:`\\mathcal{D}=\\{(x_t,a_t,r_t)\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by</span>
<span class="sd">    a behavior policy :math:`\\pi_b`. :math:`\\mathbb{E}_{\\mathcal{D}}[\\cdot]` is the empirical average over :math:`T` observations in :math:`\\mathcal{D}`.</span>
<span class="sd">    :math:`\\hat{q} (x,a)` is an estimated expected reward given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\hat{q} (x_t,\\pi):= \\mathbb{E}_{a \\sim \\pi(a|x)}[\\hat{q}(x,a)]` is the expectation of the estimated reward function over :math:`\\pi`.</span>
<span class="sd">    To estimate the mean reward function, please use `obp.ope.regression_model.RegressionModel`, which supports several fitting methods specific to OPE.</span>
<span class="sd">    If the regression model (:math:`\\hat{q}`) is a good approximation to the true mean reward function,</span>
<span class="sd">    this estimator accurately estimates the policy value of the evaluation policy.</span>
<span class="sd">    If the regression function fails to approximate the mean reward function well,</span>
<span class="sd">    however, the final estimator is no longer consistent.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator_name: str, default=&#39;dm&#39;.</span>
<span class="sd">        Name of the estimator.</span>
<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Alina Beygelzimer and John Langford.</span>
<span class="sd">    &quot;The offset tree for learning with partial labels.&quot;, 2009.</span>
<span class="sd">    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.</span>
<span class="sd">    &quot;Doubly Robust Policy Evaluation and Optimization.&quot;, 2014.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">estimator_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;dm&quot;</span>

    <span class="k">def</span> <span class="nf">_estimate_round_rewards</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like or Tensor, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">        position: array-like or Tensor, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_rewards: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Rewards of each round estimated by the DM estimator.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">position</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">q_hat_at_position</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span>
        <span class="p">]</span>
        <span class="n">pi_e_at_position</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
                <span class="n">q_hat_at_position</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="n">pi_e_at_position</span><span class="p">,</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;action must be 1D array&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">estimate_policy_value</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        V_hat: float</span>
<span class="sd">            Estimated policy value (performance) of a given evaluation policy.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span>
            <span class="n">array</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span><span class="p">,</span>
            <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">estimate_interval</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate confidence interval of policy value by nonparametric bootstrap procedure.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=10000</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_confidence_interval: Dict[str, float]</span>
<span class="sd">            Dictionary storing the estimated mean and upper-lower confidence bounds.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span>
            <span class="n">array</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span><span class="p">,</span>
            <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">estimated_round_rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">estimate_confidence_interval_by_bootstrap</span><span class="p">(</span>
            <span class="n">samples</span><span class="o">=</span><span class="n">estimated_round_rewards</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="doubly-robust">
<h3>Doubly Robust<a class="headerlink" href="#doubly-robust" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">DoublyRobust</span><span class="p">(</span><span class="n">BaseOffPolicyEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Doubly Robust (DR) Estimator.</span>
<span class="sd">    Note</span>
<span class="sd">    -------</span>
<span class="sd">    Similar to DM, DR first learns a supervised machine learning model, such as ridge regression and gradient boosting,</span>
<span class="sd">    to estimate the mean reward function (:math:`q(x,a) = \\mathbb{E}[r|x,a]`).</span>
<span class="sd">    It then uses it to estimate the policy value as follows.</span>
<span class="sd">    .. math::</span>
<span class="sd">        \\hat{V}_{\\mathrm{DR}} (\\pi_e; \\mathcal{D}, \\hat{q})</span>
<span class="sd">        := \\mathbb{E}_{\\mathcal{D}}[\\hat{q}(x_t,\\pi_e) +  w(x_t,a_t) (r_t - \\hat{q}(x_t,a_t))],</span>
<span class="sd">    where :math:`\\mathcal{D}=\\{(x_t,a_t,r_t)\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by</span>
<span class="sd">    a behavior policy :math:`\\pi_b`.</span>
<span class="sd">    :math:`w(x,a):=\\pi_e (a|x)/\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\mathbb{E}_{\\mathcal{D}}[\\cdot]` is the empirical average over :math:`T` observations in :math:`\\mathcal{D}`.</span>
<span class="sd">    :math:`\\hat{q} (x,a)` is an estimated expected reward given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\hat{q} (x_t,\\pi):= \\mathbb{E}_{a \\sim \\pi(a|x)}[\\hat{q}(x,a)]` is the expectation of the estimated reward function over :math:`\\pi`.</span>
<span class="sd">    When the weight-clipping is applied, a large importance weight is clipped as :math:`\\hat{w}(x,a) := \\min \\{ \\lambda, w(x,a) \\}`</span>
<span class="sd">    where :math:`\\lambda (&gt;0)` is a hyperparameter that decides a maximum allowed importance weight.</span>
<span class="sd">    To estimate the mean reward function, please use `obp.ope.regression_model.RegressionModel`,</span>
<span class="sd">    which supports several fitting methods specific to OPE such as *more robust doubly robust*.</span>
<span class="sd">    DR mimics IPW to use a weighted version of rewards, but DR also uses the estimated mean reward</span>
<span class="sd">    function (the regression model) as a control variate to decrease the variance.</span>
<span class="sd">    It preserves the consistency of IPW if either the importance weight or</span>
<span class="sd">    the mean reward estimator is accurate (a property called double robustness).</span>
<span class="sd">    Moreover, DR is semiparametric efficient when the mean reward estimator is correctly specified.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    lambda_: float, default=np.inf</span>
<span class="sd">        A maximum possible value of the importance weight.</span>
<span class="sd">        When a positive finite value is given, importance weights larger than `lambda_` will be clipped.</span>
<span class="sd">        DoublyRobust with a finite positive `lambda_` corresponds to Doubly Robust with Pessimistic Shrinkage of Su et al.(2020) or CAB-DR of Su et al.(2019).</span>
<span class="sd">    estimator_name: str, default=&#39;dr&#39;.</span>
<span class="sd">        Name of the estimator.</span>
<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.</span>
<span class="sd">    &quot;Doubly Robust Policy Evaluation and Optimization.&quot;, 2014.</span>
<span class="sd">    Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh.</span>
<span class="sd">    &quot;More Robust Doubly Robust Off-policy Evaluation.&quot;, 2018.</span>
<span class="sd">    Yi Su, Lequn Wang, Michele Santacatterina, and Thorsten Joachims.</span>
<span class="sd">    &quot;CAB: Continuous Adaptive Blending Estimator for Policy Evaluation and Learning&quot;, 2019.</span>
<span class="sd">    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudík.</span>
<span class="sd">    &quot;Doubly robust off-policy evaluation with shrinkage.&quot;, 2020.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">lambda_</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    <span class="n">estimator_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;dr&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Class.&quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;lambda_&quot;</span><span class="p">,</span>
            <span class="n">target_type</span><span class="o">=</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">),</span>
            <span class="n">min_val</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;lambda_ must not be nan&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_estimate_round_rewards</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate round-wise (or sample-wise) rewards.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        action: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        pscore: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model or Tensor: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">        position: array-like or Tensor, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_rewards: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Rewards of each round estimated by the DR estimator.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="c1"># weight clipping</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">iw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">)</span>
        <span class="n">q_hat_at_position</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span>
        <span class="p">]</span>
        <span class="n">q_hat_factual</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span>
        <span class="p">]</span>
        <span class="n">pi_e_at_position</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">estimated_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
                <span class="n">q_hat_at_position</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="n">pi_e_at_position</span><span class="p">,</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;reward must be 1D array&quot;</span><span class="p">)</span>

        <span class="n">estimated_rewards</span> <span class="o">+=</span> <span class="n">iw</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">q_hat_factual</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">estimated_rewards</span>

    <span class="k">def</span> <span class="nf">estimate_policy_value</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        V_hat: float</span>
<span class="sd">            Policy value estimated by the DR estimator.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span>
            <span class="n">array</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span><span class="p">,</span>
            <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">estimate_interval</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate confidence interval of policy value by nonparametric bootstrap procedure.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=10000</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_confidence_interval: Dict[str, float]</span>
<span class="sd">            Dictionary storing the estimated mean and upper-lower confidence bounds.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span>
            <span class="n">array</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span><span class="p">,</span>
            <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">estimated_round_rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">estimate_confidence_interval_by_bootstrap</span><span class="p">(</span>
            <span class="n">samples</span><span class="o">=</span><span class="n">estimated_round_rewards</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_estimate_mse_score</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_bias_upper_bound</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the MSE score of a given clipping hyperparameter to conduct hyperparameter tuning.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">        use_bias_upper_bound: bool, default=True</span>
<span class="sd">            Whether to use bias upper bound in hyperparameter tuning.</span>
<span class="sd">            If False, direct bias estimator is used to estimate the MSE.</span>
<span class="sd">        delta: float, default=0.05</span>
<span class="sd">            A confidence delta to construct a high probability upper bound based on the Bernstein’s inequality.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_mse_score: float</span>
<span class="sd">            Estimated MSE score of a given clipping hyperparameter `lambda_`.</span>
<span class="sd">            MSE score is the sum of (high probability) upper bound of bias and the sample variance.</span>
<span class="sd">            This is estimated using the automatic hyperparameter tuning procedure</span>
<span class="sd">            based on Section 5 of Su et al.(2020).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># estimate the sample variance of DR with clipping</span>
        <span class="n">sample_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
                <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
                <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">sample_variance</span> <span class="o">/=</span> <span class="n">n_rounds</span>

        <span class="c1"># estimate the (high probability) upper bound of the bias of DR with clipping</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="k">if</span> <span class="n">use_bias_upper_bound</span><span class="p">:</span>
            <span class="n">bias_term</span> <span class="o">=</span> <span class="n">estimate_high_probability_upper_bound_bias</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">iw</span><span class="o">=</span><span class="n">iw</span><span class="p">,</span>
                <span class="n">iw_hat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">),</span>
                <span class="n">q_hat</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span>
                <span class="p">],</span>
                <span class="n">delta</span><span class="o">=</span><span class="n">delta</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bias_term</span> <span class="o">=</span> <span class="n">estimate_bias_in_ope</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">iw</span><span class="o">=</span><span class="n">iw</span><span class="p">,</span>
                <span class="n">iw_hat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">),</span>
                <span class="n">q_hat</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span>
                <span class="p">],</span>
            <span class="p">)</span>
        <span class="n">estimated_mse_score</span> <span class="o">=</span> <span class="n">sample_variance</span> <span class="o">+</span> <span class="p">(</span><span class="n">bias_term</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">estimated_mse_score</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="inverse-probability-weighting">
<h3>Inverse Probability Weighting<a class="headerlink" href="#inverse-probability-weighting" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">InverseProbabilityWeighting</span><span class="p">(</span><span class="n">BaseOffPolicyEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Inverse Probability Weighting (IPW) Estimator.</span>
<span class="sd">    Note</span>
<span class="sd">    -------</span>
<span class="sd">    Inverse Probability Weighting (IPW) estimates the policy value of evaluation policy :math:`\\pi_e` by</span>
<span class="sd">    .. math::</span>
<span class="sd">        \\hat{V}_{\\mathrm{IPW}} (\\pi_e; \\mathcal{D}) := \\mathbb{E}_{\\mathcal{D}} [ w(x_t,a_t) r_t],</span>
<span class="sd">    where :math:`\\mathcal{D}=\\{(x_t,a_t,r_t)\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by</span>
<span class="sd">    a behavior policy :math:`\\pi_b`. :math:`w(x,a):=\\pi_e (a|x)/\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\mathbb{E}_{\\mathcal{D}}[\\cdot]` is the empirical average over :math:`T` observations in :math:`\\mathcal{D}`.</span>
<span class="sd">    When the weight-clipping is applied, a large importance weight is clipped as :math:`\\hat{w}(x,a) := \\min \\{ \\lambda, w(x,a) \\}`</span>
<span class="sd">    where :math:`\\lambda (&gt;0)` is a hyperparameter that decides a maximum allowed importance weight.</span>
<span class="sd">    IPW re-weights the rewards by the ratio of the evaluation policy and behavior policy (importance weight).</span>
<span class="sd">    When the behavior policy is known, IPW is unbiased and consistent for the true policy value.</span>
<span class="sd">    However, it can have a large variance, especially when the evaluation policy significantly deviates from the behavior policy.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ------------</span>
<span class="sd">    lambda_: float, default=np.inf</span>
<span class="sd">        A maximum possible value of the importance weight.</span>
<span class="sd">        When a positive finite value is given, importance weights larger than `lambda_` will be clipped.</span>
<span class="sd">    estimator_name: str, default=&#39;ipw&#39;.</span>
<span class="sd">        Name of the estimator.</span>
<span class="sd">    References</span>
<span class="sd">    ------------</span>
<span class="sd">    Alex Strehl, John Langford, Lihong Li, and Sham M Kakade.</span>
<span class="sd">    &quot;Learning from Logged Implicit Exploration Data&quot;., 2010.</span>
<span class="sd">    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.</span>
<span class="sd">    &quot;Doubly Robust Policy Evaluation and Optimization.&quot;, 2014.</span>
<span class="sd">    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.</span>
<span class="sd">    &quot;Doubly Robust Off-Policy Evaluation with Shrinkage.&quot;, 2020.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">lambda_</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    <span class="n">estimator_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;ipw&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Class.&quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;lambda_&quot;</span><span class="p">,</span>
            <span class="n">target_type</span><span class="o">=</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">),</span>
            <span class="n">min_val</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;lambda_ must not be nan&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_estimate_round_rewards</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate round-wise (or sample-wise) rewards.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        action: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        pscore: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        position: array-like or Tensor, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_rewards: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Rewards of each round estimated by IPW.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="c1"># weight clipping</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">iw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">reward</span> <span class="o">*</span> <span class="n">iw</span>

    <span class="k">def</span> <span class="nf">estimate_policy_value</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        V_hat: float</span>
<span class="sd">            Estimated policy value (performance) of a given evaluation policy.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">estimate_interval</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate confidence interval of policy value by nonparametric bootstrap procedure.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=10000</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_confidence_interval: Dict[str, float]</span>
<span class="sd">            Dictionary storing the estimated mean and upper-lower confidence bounds.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">estimated_round_rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">estimate_confidence_interval_by_bootstrap</span><span class="p">(</span>
            <span class="n">samples</span><span class="o">=</span><span class="n">estimated_round_rewards</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_estimate_mse_score</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_bias_upper_bound</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the MSE score of a given clipping hyperparameter to conduct hyperparameter tuning.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">        use_bias_upper_bound: bool, default=True</span>
<span class="sd">            Whether to use bias upper bound in hyperparameter tuning.</span>
<span class="sd">            If False, direct bias estimator is used to estimate the MSE.</span>
<span class="sd">        delta: float, default=0.05</span>
<span class="sd">            A confidence delta to construct a high probability upper bound based on the Bernstein’s inequality.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_mse_score: float</span>
<span class="sd">            Estimated MSE score of a given clipping hyperparameter `lambda_`.</span>
<span class="sd">            MSE score is the sum of (high probability) upper bound of bias and the sample variance.</span>
<span class="sd">            This is estimated using the automatic hyperparameter tuning procedure</span>
<span class="sd">            based on Section 5 of Su et al.(2020).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># estimate the sample variance of IPW with clipping</span>
        <span class="n">sample_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
                <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">sample_variance</span> <span class="o">/=</span> <span class="n">n_rounds</span>

        <span class="c1"># estimate the (high probability) upper bound of the bias of IPW with clipping</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="k">if</span> <span class="n">use_bias_upper_bound</span><span class="p">:</span>
            <span class="n">bias_term</span> <span class="o">=</span> <span class="n">estimate_high_probability_upper_bound_bias</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">iw</span><span class="o">=</span><span class="n">iw</span><span class="p">,</span> <span class="n">iw_hat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">),</span> <span class="n">delta</span><span class="o">=</span><span class="n">delta</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bias_term</span> <span class="o">=</span> <span class="n">estimate_bias_in_ope</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">iw</span><span class="o">=</span><span class="n">iw</span><span class="p">,</span>
                <span class="n">iw_hat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="n">estimated_mse_score</span> <span class="o">=</span> <span class="n">sample_variance</span> <span class="o">+</span> <span class="p">(</span><span class="n">bias_term</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">estimated_mse_score</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="off-policy-evaluation-class">
<h2>Off-Policy Evaluation Class<a class="headerlink" href="#off-policy-evaluation-class" title="Permalink to this headline">¶</a></h2>
<p>Off-Policy Evaluation Class to Streamline OPE.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">logging</span> <span class="kn">import</span> <span class="n">getLogger</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">DataFrame</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">logger</span> <span class="o">=</span> <span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">OffPolicyEvaluation</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Class to conduct OPE by multiple estimators simultaneously.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    bandit_feedback: BanditFeedback</span>
<span class="sd">        Logged bandit feedback data used to conduct OPE.</span>
<span class="sd">    ope_estimators: List[BaseOffPolicyEstimator]</span>
<span class="sd">        List of OPE estimators used to evaluate the policy value of evaluation policy.</span>
<span class="sd">        Estimators must follow the interface of `obp.ope.BaseOffPolicyEstimator`.</span>
<span class="sd">    Examples</span>
<span class="sd">    ----------</span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">        # a case for implementing OPE of the BernoulliTS policy</span>
<span class="sd">        # using log data generated by the Random policy</span>
<span class="sd">        &gt;&gt;&gt; from obp.dataset import OpenBanditDataset</span>
<span class="sd">        &gt;&gt;&gt; from obp.policy import BernoulliTS</span>
<span class="sd">        &gt;&gt;&gt; from obp.ope import OffPolicyEvaluation, InverseProbabilityWeighting as IPW</span>
<span class="sd">        # (1) Data loading and preprocessing</span>
<span class="sd">        &gt;&gt;&gt; dataset = OpenBanditDataset(behavior_policy=&#39;random&#39;, campaign=&#39;all&#39;)</span>
<span class="sd">        &gt;&gt;&gt; bandit_feedback = dataset.obtain_batch_bandit_feedback()</span>
<span class="sd">        &gt;&gt;&gt; bandit_feedback.keys()</span>
<span class="sd">        dict_keys([&#39;n_rounds&#39;, &#39;n_actions&#39;, &#39;action&#39;, &#39;position&#39;, &#39;reward&#39;, &#39;pscore&#39;, &#39;context&#39;, &#39;action_context&#39;])</span>
<span class="sd">        # (2) Off-Policy Learning</span>
<span class="sd">        &gt;&gt;&gt; evaluation_policy = BernoulliTS(</span>
<span class="sd">            n_actions=dataset.n_actions,</span>
<span class="sd">            len_list=dataset.len_list,</span>
<span class="sd">            is_zozotown_prior=True, # replicate the policy in the ZOZOTOWN production</span>
<span class="sd">            campaign=&quot;all&quot;,</span>
<span class="sd">            random_state=12345</span>
<span class="sd">        )</span>
<span class="sd">        &gt;&gt;&gt; action_dist = evaluation_policy.compute_batch_action_dist(</span>
<span class="sd">            n_sim=100000, n_rounds=bandit_feedback[&quot;n_rounds&quot;]</span>
<span class="sd">        )</span>
<span class="sd">        # (3) Off-Policy Evaluation</span>
<span class="sd">        &gt;&gt;&gt; ope = OffPolicyEvaluation(bandit_feedback=bandit_feedback, ope_estimators=[IPW()])</span>
<span class="sd">        &gt;&gt;&gt; estimated_policy_value = ope.estimate_policy_values(action_dist=action_dist)</span>
<span class="sd">        &gt;&gt;&gt; estimated_policy_value</span>
<span class="sd">        {&#39;ipw&#39;: 0.004553...}</span>
<span class="sd">        # policy value improvement of BernoulliTS over the Random policy estimated by IPW</span>
<span class="sd">        &gt;&gt;&gt; estimated_policy_value_improvement = estimated_policy_value[&#39;ipw&#39;] / bandit_feedback[&#39;reward&#39;].mean()</span>
<span class="sd">        # our OPE procedure suggests that BernoulliTS improves Random by 19.81%</span>
<span class="sd">        &gt;&gt;&gt; print(estimated_policy_value_improvement)</span>
<span class="sd">        1.198126...</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">bandit_feedback</span><span class="p">:</span> <span class="n">BanditFeedback</span>
    <span class="n">ope_estimators</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseOffPolicyEstimator</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize class.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">key_</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="s2">&quot;position&quot;</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="s2">&quot;pscore&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">key_</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit_feedback</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Missing key of </span><span class="si">{</span><span class="n">key_</span><span class="si">}</span><span class="s2"> in &#39;bandit_feedback&#39;.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_model_dependent</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="p">[</span><span class="n">estimator</span><span class="o">.</span><span class="n">estimator_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">DirectMethod</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">DoublyRobust</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">is_model_dependent</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_create_estimator_inputs</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Create input dictionary to estimate policy value using subclasses of `BaseOffPolicyEstimator`&quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action_dist&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">estimated_rewards_by_reg_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">estimated_rewards_by_reg_model</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">check_array</span><span class="p">(</span>
                    <span class="n">array</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;estimated_rewards_by_reg_model[</span><span class="si">{</span><span class="n">estimator_name</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">,</span>
                    <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Expected `estimated_rewards_by_reg_model[</span><span class="si">{</span><span class="n">estimator_name</span><span class="si">}</span><span class="s2">].shape == action_dist.shape`, but found it False.&quot;</span>
                    <span class="p">)</span>
        <span class="k">elif</span> <span class="n">estimated_rewards_by_reg_model</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `estimated_rewards_by_reg_model.shape == action_dist.shape`, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">estimator_name</span><span class="p">:</span> <span class="p">{</span>
                <span class="n">input_</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="n">input_</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">input_</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="s2">&quot;position&quot;</span><span class="p">,</span> <span class="s2">&quot;pscore&quot;</span><span class="p">]</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">estimator_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span>
        <span class="p">}</span>

        <span class="k">for</span> <span class="n">estimator_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="p">:</span>
            <span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span><span class="s2">&quot;action_dist&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">action_dist</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">estimator_name</span> <span class="ow">in</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span>
                    <span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span>
                        <span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span>
                    <span class="p">]</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span>
                        <span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span>
                    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span>
                    <span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span>

        <span class="k">return</span> <span class="n">estimator_inputs</span>

    <span class="k">def</span> <span class="nf">estimate_policy_values</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ------------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given each round, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        policy_value_dict: Dict[str, float]</span>
<span class="sd">            Dictionary containing estimated policy values by OPE estimators.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_model_dependent</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">estimated_rewards_by_reg_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;When model dependent estimators such as DM or DR are used, `estimated_rewards_by_reg_model` must be given&quot;</span>
                <span class="p">)</span>

        <span class="n">policy_value_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_estimator_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">policy_value_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">estimate_policy_value</span><span class="p">(</span>
                <span class="o">**</span><span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">policy_value_dict</span>

    <span class="k">def</span> <span class="nf">estimate_intervals</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate confidence intervals of policy values using nonparametric bootstrap procedure.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ------------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=100</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        policy_value_interval_dict: Dict[str, Dict[str, float]]</span>
<span class="sd">            Dictionary containing confidence intervals of estimated policy value estimated</span>
<span class="sd">            using nonparametric bootstrap procedure.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_model_dependent</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">estimated_rewards_by_reg_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;When model dependent estimators such as DM or DR are used, `estimated_rewards_by_reg_model` must be given&quot;</span>
                <span class="p">)</span>

        <span class="n">check_confidence_interval_arguments</span><span class="p">(</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">policy_value_interval_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_estimator_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">policy_value_interval_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">estimate_interval</span><span class="p">(</span>
                <span class="o">**</span><span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">],</span>
                <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">policy_value_interval_dict</span>

    <span class="k">def</span> <span class="nf">summarize_off_policy_estimates</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Summarize policy values and their confidence intervals estimated by OPE estimators.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ------------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given each round, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=100</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        (policy_value_df, policy_value_interval_df): Tuple[DataFrame, DataFrame]</span>
<span class="sd">            Policy values and their confidence intervals Estimated by OPE estimators.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">policy_value_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimate_policy_values</span><span class="p">(</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;estimated_policy_value&quot;</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">policy_value_interval_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimate_intervals</span><span class="p">(</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">policy_value_of_behavior_policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">policy_value_df</span> <span class="o">=</span> <span class="n">policy_value_df</span><span class="o">.</span><span class="n">T</span>
        <span class="k">if</span> <span class="n">policy_value_of_behavior_policy</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Policy value of the behavior policy is </span><span class="si">{</span><span class="n">policy_value_of_behavior_policy</span><span class="si">}</span><span class="s2"> (&lt;=0); relative estimated policy value is set to np.nan&quot;</span>
            <span class="p">)</span>
            <span class="n">policy_value_df</span><span class="p">[</span><span class="s2">&quot;relative_estimated_policy_value&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">policy_value_df</span><span class="p">[</span><span class="s2">&quot;relative_estimated_policy_value&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">policy_value_df</span><span class="o">.</span><span class="n">estimated_policy_value</span> <span class="o">/</span> <span class="n">policy_value_of_behavior_policy</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">policy_value_df</span><span class="p">,</span> <span class="n">policy_value_interval_df</span><span class="o">.</span><span class="n">T</span>

    <span class="k">def</span> <span class="nf">visualize_off_policy_estimates</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">is_relative</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">fig_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Path</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">fig_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;estimated_policy_value.png&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Visualize policy values estimated by OPE estimators.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=100</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        is_relative: bool, default=False,</span>
<span class="sd">            If True, the method visualizes the estimated policy values of evaluation policy</span>
<span class="sd">            relative to the ground-truth policy value of behavior policy.</span>
<span class="sd">        fig_dir: Path, default=None</span>
<span class="sd">            Path to store the bar figure.</span>
<span class="sd">            If &#39;None&#39; is given, the figure will not be saved.</span>
<span class="sd">        fig_name: str, default=&quot;estimated_policy_value.png&quot;</span>
<span class="sd">            Name of the bar figure.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">fig_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fig_dir</span><span class="p">,</span> <span class="n">Path</span><span class="p">),</span> <span class="s2">&quot;fig_dir must be a Path&quot;</span>
        <span class="k">if</span> <span class="n">fig_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fig_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">),</span> <span class="s2">&quot;fig_dir must be a string&quot;</span>

        <span class="n">estimated_round_rewards_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_estimator_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">estimated_round_rewards_dict</span><span class="p">[</span>
                <span class="n">estimator_name</span>
            <span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span><span class="o">**</span><span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">])</span>
        <span class="n">estimated_round_rewards_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">estimated_round_rewards_dict</span><span class="p">)</span>
        <span class="n">estimated_round_rewards_df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span>
            <span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">key</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">estimated_round_rewards_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()},</span>
            <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">is_relative</span><span class="p">:</span>
            <span class="n">estimated_round_rewards_df</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;ggplot&quot;</span><span class="p">)</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span>
            <span class="n">data</span><span class="o">=</span><span class="n">estimated_round_rewards_df</span><span class="p">,</span>
            <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
            <span class="n">ci</span><span class="o">=</span><span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">),</span>
            <span class="n">n_boot</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;OPE Estimators&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Estimated Policy Value (± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">))</span><span class="si">}</span><span class="s2">% CI)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span>
        <span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">fig_dir</span><span class="p">:</span>
            <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">fig_dir</span> <span class="o">/</span> <span class="n">fig_name</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">evaluate_performance_of_estimators</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ground_truth_policy_value</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relative-ee&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Evaluate estimation performance of OPE estimators.</span>
<span class="sd">        Note</span>
<span class="sd">        ------</span>
<span class="sd">        Evaluate the estimation performance of OPE estimators by relative estimation error (relative-EE) or squared error (SE):</span>
<span class="sd">        .. math ::</span>
<span class="sd">            \\text{Relative-EE} (\\hat{V}; \\mathcal{D}) = \\left|  \\frac{\\hat{V}(\\pi; \\mathcal{D}) - V(\\pi)}{V(\\pi)} \\right|,</span>
<span class="sd">        .. math ::</span>
<span class="sd">            \\text{SE} (\\hat{V}; \\mathcal{D}) = \\left(\\hat{V}(\\pi; \\mathcal{D}) - V(\\pi) \\right)^2,</span>
<span class="sd">        where :math:`V({\\pi})` is the ground-truth policy value of the evalation policy :math:`\\pi_e` (often estimated using on-policy estimation).</span>
<span class="sd">        :math:`\\hat{V}(\\pi; \\mathcal{D})` is an estimated policy value by an OPE estimator :math:`\\hat{V}` and logged bandit feedback :math:`\\mathcal{D}`.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ground_truth policy value: float</span>
<span class="sd">            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\pi_e)`.</span>
<span class="sd">            With Open Bandit Dataset, we use an on-policy estimate of the policy value as its ground-truth.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        metric: str, default=&quot;relative-ee&quot;</span>
<span class="sd">            Evaluation metric used to evaluate and compare the estimation performance of OPE estimators.</span>
<span class="sd">            Must be &quot;relative-ee&quot; or &quot;se&quot;.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        eval_metric_ope_dict: Dict[str, float]</span>
<span class="sd">            Dictionary containing evaluation metric for evaluating the estimation performance of OPE estimators.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span>
            <span class="n">ground_truth_policy_value</span><span class="p">,</span>
            <span class="s2">&quot;ground_truth_policy_value&quot;</span><span class="p">,</span>
            <span class="nb">float</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">metric</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;relative-ee&quot;</span><span class="p">,</span> <span class="s2">&quot;se&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;metric must be either &#39;relative-ee&#39; or &#39;se&#39;, but </span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2"> is given&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;relative-ee&quot;</span> <span class="ow">and</span> <span class="n">ground_truth_policy_value</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;ground_truth_policy_value must be non-zero when metric is relative-ee&quot;</span>
            <span class="p">)</span>

        <span class="n">eval_metric_ope_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_estimator_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">estimated_policy_value</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">estimate_policy_value</span><span class="p">(</span>
                <span class="o">**</span><span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;relative-ee&quot;</span><span class="p">:</span>
                <span class="n">relative_ee_</span> <span class="o">=</span> <span class="n">estimated_policy_value</span> <span class="o">-</span> <span class="n">ground_truth_policy_value</span>
                <span class="n">relative_ee_</span> <span class="o">/=</span> <span class="n">ground_truth_policy_value</span>
                <span class="n">eval_metric_ope_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">relative_ee_</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;se&quot;</span><span class="p">:</span>
                <span class="n">se_</span> <span class="o">=</span> <span class="p">(</span><span class="n">estimated_policy_value</span> <span class="o">-</span> <span class="n">ground_truth_policy_value</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
                <span class="n">eval_metric_ope_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">se_</span>
        <span class="k">return</span> <span class="n">eval_metric_ope_dict</span>

    <span class="k">def</span> <span class="nf">summarize_estimators_comparison</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ground_truth_policy_value</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relative-ee&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataFrame</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Summarize performance comparisons of OPE estimators.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ground_truth policy value: float</span>
<span class="sd">            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\pi_e)`.</span>
<span class="sd">            With Open Bandit Dataset, we use an on-policy estimate of the policy value as ground-truth.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list), default=None</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        metric: str, default=&quot;relative-ee&quot;</span>
<span class="sd">            Evaluation metric used to evaluate and compare the estimation performance of OPE estimators.</span>
<span class="sd">            Must be either &quot;relative-ee&quot; or &quot;se&quot;.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        eval_metric_ope_df: DataFrame</span>
<span class="sd">            Evaluation metric to evaluate and compare the estimation performance of OPE estimators.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">eval_metric_ope_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_performance_of_estimators</span><span class="p">(</span>
                <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">ground_truth_policy_value</span><span class="p">,</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
                <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="n">metric</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">eval_metric_ope_df</span><span class="o">.</span><span class="n">T</span>

    <span class="k">def</span> <span class="nf">visualize_off_policy_estimates_of_multiple_policies</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">policy_name_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">action_dist_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">is_relative</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">fig_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Path</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">fig_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;estimated_policy_value.png&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Visualize policy values estimated by OPE estimators.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        policy_name_list: List[str]</span>
<span class="sd">            List of the names of evaluation policies.</span>
<span class="sd">        action_dist_list: List[array-like, shape (n_rounds, n_actions, len_list)]</span>
<span class="sd">            List of action choice probabilities by the evaluation policies (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of an estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=100</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        is_relative: bool, default=False,</span>
<span class="sd">            If True, the method visualizes the estimated policy values of evaluation policy</span>
<span class="sd">            relative to the ground-truth policy value of behavior policy.</span>
<span class="sd">        fig_dir: Path, default=None</span>
<span class="sd">            Path to store the bar figure.</span>
<span class="sd">            If &#39;None&#39; is given, the figure will not be saved.</span>
<span class="sd">        fig_name: str, default=&quot;estimated_policy_value.png&quot;</span>
<span class="sd">            Name of the bar figure.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">policy_name_list</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">action_dist_list</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;the length of policy_name_list must be the same as action_dist_list&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">fig_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fig_dir</span><span class="p">,</span> <span class="n">Path</span><span class="p">),</span> <span class="s2">&quot;fig_dir must be a Path&quot;</span>
        <span class="k">if</span> <span class="n">fig_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fig_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">),</span> <span class="s2">&quot;fig_dir must be a string&quot;</span>

        <span class="n">estimated_round_rewards_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">estimator_name</span><span class="p">:</span> <span class="p">{}</span> <span class="k">for</span> <span class="n">estimator_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span>
        <span class="p">}</span>

        <span class="k">for</span> <span class="n">policy_name</span><span class="p">,</span> <span class="n">action_dist</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">policy_name_list</span><span class="p">,</span> <span class="n">action_dist_list</span><span class="p">):</span>
            <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_estimator_inputs</span><span class="p">(</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">estimated_round_rewards_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span>
                    <span class="n">policy_name</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
                    <span class="o">**</span><span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span>
                <span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;ggplot&quot;</span><span class="p">)</span>
        <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mf">6.2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="p">)))</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">estimator_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="p">):</span>
            <span class="n">estimated_round_rewards_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span>
                <span class="n">estimated_round_rewards_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">is_relative</span><span class="p">:</span>
                <span class="n">estimated_round_rewards_df</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

            <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">action_dist_list</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span>
                <span class="n">data</span><span class="o">=</span><span class="n">estimated_round_rewards_df</span><span class="p">,</span>
                <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                <span class="n">ci</span><span class="o">=</span><span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">),</span>
                <span class="n">n_boot</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
                <span class="n">seed</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Estimated Policy Value (± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">))</span><span class="si">}</span><span class="s2">% CI)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span>
            <span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">policy_name_list</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">fig_dir</span><span class="p">:</span>
            <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">fig_dir</span> <span class="o">/</span> <span class="n">fig_name</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="base-models">
<h2>Base Models<a class="headerlink" href="#base-models" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3>Utils<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">check_bandit_feedback_inputs</span><span class="p">(</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">expected_reward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pscore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">action_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">ValueError</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Check inputs for bandit learning or simulation.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    context: array-like, shape (n_rounds, dim_context)</span>
<span class="sd">        Context vectors in each round, i.e., :math:`x_t`.</span>
<span class="sd">    action: array-like, shape (n_rounds,)</span>
<span class="sd">        Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">    reward: array-like, shape (n_rounds,)</span>
<span class="sd">        Observed rewards (or outcome) in each round, i.e., :math:`r_t`.</span>
<span class="sd">    expected_reward: array-like, shape (n_rounds, n_actions), default=None</span>
<span class="sd">        Expected rewards (or outcome) in each round, i.e., :math:`\\mathbb{E}[r_t]`.</span>
<span class="sd">    position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">    pscore: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Propensity scores, the probability of selecting each action by behavior policy,</span>
<span class="sd">        in the given logged bandit data.</span>
<span class="sd">    action_context: array-like, shape (n_actions, dim_action_context)</span>
<span class="sd">        Context vectors characterizing each action.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)</span> <span class="ow">and</span> <span class="n">action</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;action elements must be non-negative integers&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">expected_reward</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">expected_reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;expected_reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">expected_reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `context.shape[0] == action.shape[0] == reward.shape[0] == expected_reward.shape[0]`&quot;</span>
                <span class="s2">&quot;, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">action</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">expected_reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;action elements must be smaller than `expected_reward.shape[1]`&quot;</span>
            <span class="p">)</span>
    <span class="k">if</span> <span class="n">pscore</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">pscore</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `context.shape[0] == action.shape[0] == reward.shape[0] == pscore.shape[0]`&quot;</span>
                <span class="s2">&quot;, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">pscore</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;pscore must be positive&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">position</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;position&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">position</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `context.shape[0] == action.shape[0] == reward.shape[0] == position.shape[0]`&quot;</span>
                <span class="s2">&quot;, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">position</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)</span> <span class="ow">and</span> <span class="n">position</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;position elements must be non-negative integers&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `context.shape[0] == action.shape[0] == reward.shape[0]`&quot;</span>
                <span class="s2">&quot;, but found it False&quot;</span>
            <span class="p">)</span>
    <span class="k">if</span> <span class="n">action_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action_context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action_context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">action</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">action_context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;action elements must be smaller than `action_context.shape[0]`&quot;</span>
            <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="regression-model">
<h3>Regression Model<a class="headerlink" href="#regression-model" title="Permalink to this headline">¶</a></h3>
<p>Regression Model Class for Estimating Mean Reward Functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">clone</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">is_classifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">RegressionModel</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Machine learning model to estimate the mean reward function (:math:`q(x,a):= \\mathbb{E}[r|x,a]`).</span>
<span class="sd">    Note</span>
<span class="sd">    -------</span>
<span class="sd">    Reward (or outcome) :math:`r` must be either binary or continuous.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ------------</span>
<span class="sd">    base_model: BaseEstimator</span>
<span class="sd">        A machine learning model used to estimate the mean reward function.</span>
<span class="sd">    n_actions: int</span>
<span class="sd">        Number of actions.</span>
<span class="sd">    len_list: int, default=1</span>
<span class="sd">        Length of a list of actions recommended in each impression.</span>
<span class="sd">        When Open Bandit Dataset is used, 3 should be set.</span>
<span class="sd">    action_context: array-like, shape (n_actions, dim_action_context), default=None</span>
<span class="sd">        Context vector characterizing action (i.e., vector representation of each action).</span>
<span class="sd">        If not given, one-hot encoding of the action variable is used as default.</span>
<span class="sd">    fitting_method: str, default=&#39;normal&#39;</span>
<span class="sd">        Method to fit the regression model.</span>
<span class="sd">        Must be one of [&#39;normal&#39;, &#39;iw&#39;, &#39;mrdr&#39;] where &#39;iw&#39; stands for importance weighting and</span>
<span class="sd">        &#39;mrdr&#39; stands for more robust doubly robust.</span>
<span class="sd">    References</span>
<span class="sd">    -----------</span>
<span class="sd">    Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh.</span>
<span class="sd">    &quot;More Robust Doubly Robust Off-policy Evaluation.&quot;, 2018.</span>
<span class="sd">    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.</span>
<span class="sd">    &quot;Doubly Robust Off-Policy Evaluation with Shrinkage.&quot;, 2020.</span>
<span class="sd">    Yusuke Narita, Shota Yasui, and Kohei Yata.</span>
<span class="sd">    &quot;Off-policy Bandit and Reinforcement Learning.&quot;, 2020.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">base_model</span><span class="p">:</span> <span class="n">BaseEstimator</span>
    <span class="n">n_actions</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">len_list</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">action_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">fitting_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;normal&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Class.&quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="s2">&quot;n_actions&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">,</span> <span class="s2">&quot;len_list&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="s2">&quot;iw&quot;</span><span class="p">,</span> <span class="s2">&quot;mrdr&quot;</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;fitting_method must be one of &#39;normal&#39;, &#39;iw&#39;, or &#39;mrdr&#39;, but </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span><span class="si">}</span><span class="s2"> is given&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_model</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;base_model must be BaseEstimator or a child class of BaseEstimator&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_model</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_context</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">action_context</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Fit the regression model on given logged bandit feedback data.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        context: array-like, shape (n_rounds, dim_context)</span>
<span class="sd">            Context vectors observed in each round of the logged bandit feedback, i.e., :math:`x_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">            When None is given, behavior policy is assumed to be uniform.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            If None is set, a regression model assumes that there is only one position.</span>
<span class="sd">            When `len_list` &gt; 1, this position argument has to be set.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list), default=None</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">            When either of &#39;iw&#39; or &#39;mrdr&#39; is used as the &#39;fitting_method&#39; argument, then `action_dist` must be given.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_bandit_feedback_inputs</span><span class="p">(</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">position</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;position elements must be smaller than len_list, but the maximum value is </span><span class="si">{</span><span class="n">position</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">}</span><span class="s2"> (&gt;= </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;iw&quot;</span><span class="p">,</span> <span class="s2">&quot;mrdr&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;when fitting_method is either &#39;iw&#39; or &#39;mrdr&#39;, action_dist (a 3-dimensional ndarray) must be given&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;shape of action_dist must be (n_rounds, n_actions, len_list)=(</span><span class="si">{</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="si">}</span><span class="s2">), but is </span><span class="si">{</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;action_dist must be a probability distribution&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pscore</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pscore</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span>

        <span class="k">for</span> <span class="n">position_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">position</span> <span class="o">==</span> <span class="n">position_</span>
            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_process_for_reg_model</span><span class="p">(</span>
                <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
                <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No training data at position </span><span class="si">{</span><span class="n">position_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="c1"># train the base model according to the given `fitting method`</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="o">==</span> <span class="s2">&quot;normal&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">reward</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">action_dist_at_position</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span>
                    <span class="n">action</span><span class="p">,</span>
                    <span class="n">position_</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
                <span class="p">][</span><span class="n">idx</span><span class="p">]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="o">==</span> <span class="s2">&quot;iw&quot;</span><span class="p">:</span>
                    <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">action_dist_at_position</span> <span class="o">/</span> <span class="n">pscore</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                        <span class="n">X</span><span class="p">,</span> <span class="n">reward</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="o">==</span> <span class="s2">&quot;mrdr&quot;</span><span class="p">:</span>
                    <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">action_dist_at_position</span>
                    <span class="n">sample_weight</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">pscore</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                    <span class="n">sample_weight</span> <span class="o">/=</span> <span class="n">pscore</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                        <span class="n">X</span><span class="p">,</span> <span class="n">reward</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span>
                    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Predict the mean reward function.</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        context: array-like, shape (n_rounds_of_new_data, dim_context)</span>
<span class="sd">            Context vectors of new data.</span>
<span class="sd">        Returns</span>
<span class="sd">        -----------</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds_of_new_data, n_actions, len_list)</span>
<span class="sd">            Expected rewards of new data estimated by the regression model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_rounds_of_new_data</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">ones_n_rounds_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_rounds_of_new_data</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
        <span class="n">estimated_rewards_by_reg_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">n_rounds_of_new_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">action_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">position_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
                <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_process_for_reg_model</span><span class="p">(</span>
                    <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
                    <span class="n">action</span><span class="o">=</span><span class="n">action_</span> <span class="o">*</span> <span class="n">ones_n_rounds_arr</span><span class="p">,</span>
                    <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">estimated_rewards_</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">]</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">])</span>
                    <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds_of_new_data</span><span class="p">),</span>
                    <span class="n">action_</span> <span class="o">*</span> <span class="n">ones_n_rounds_arr</span><span class="p">,</span>
                    <span class="n">position_</span> <span class="o">*</span> <span class="n">ones_n_rounds_arr</span><span class="p">,</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">estimated_rewards_</span>
        <span class="k">return</span> <span class="n">estimated_rewards_by_reg_model</span>

    <span class="k">def</span> <span class="nf">fit_predict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_folds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Fit the regression model on given logged bandit feedback data and predict the reward function of the same data.</span>
<span class="sd">        Note</span>
<span class="sd">        ------</span>
<span class="sd">        When `n_folds` is larger than 1, then the cross-fitting procedure is applied.</span>
<span class="sd">        See the reference for the details about the cross-fitting technique.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        context: array-like, shape (n_rounds, dim_context)</span>
<span class="sd">            Context vectors observed in each round of the logged bandit feedback, i.e., :math:`x_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Observed rewards (or outcome) in each round, i.e., :math:`r_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Action choice probabilities (propensity score) of a behavior policy</span>
<span class="sd">            in the training logged bandit feedback.</span>
<span class="sd">            When None is given, the the behavior policy is assumed to be a uniform one.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            If None is set, a regression model assumes that there is only one position.</span>
<span class="sd">            When `len_list` &gt; 1, this position argument has to be set.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list), default=None</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">            When either of &#39;iw&#39; or &#39;mrdr&#39; is used as the &#39;fitting_method&#39; argument, then `action_dist` must be given.</span>
<span class="sd">        n_folds: int, default=1</span>
<span class="sd">            Number of folds in the cross-fitting procedure.</span>
<span class="sd">            When 1 is given, the regression model is trained on the whole logged bandit feedback data.</span>
<span class="sd">            Please refer to https://arxiv.org/abs/2002.08536 about the details of the cross-fitting procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            `random_state` affects the ordering of the indices, which controls the randomness of each fold.</span>
<span class="sd">            See https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html for the details.</span>
<span class="sd">        Returns</span>
<span class="sd">        -----------</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards of new data estimated by the regression model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_bandit_feedback_inputs</span><span class="p">(</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">check_scalar</span><span class="p">(</span><span class="n">n_folds</span><span class="p">,</span> <span class="s2">&quot;n_folds&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">position</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;position elements must be smaller than len_list, but the maximum value is </span><span class="si">{</span><span class="n">position</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">}</span><span class="s2"> (&gt;= </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;iw&quot;</span><span class="p">,</span> <span class="s2">&quot;mrdr&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;when fitting_method is either &#39;iw&#39; or &#39;mrdr&#39;, action_dist (a 3-dimensional ndarray) must be given&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;shape of action_dist must be (n_rounds, n_actions, len_list)=(</span><span class="si">{</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="si">}</span><span class="s2">), but is </span><span class="si">{</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="n">pscore</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pscore</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span>

        <span class="k">if</span> <span class="n">n_folds</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
                <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">estimated_rewards_by_reg_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_folds</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">kf</span><span class="o">.</span><span class="n">get_n_splits</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">context</span><span class="p">):</span>
            <span class="n">action_dist_tr</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">action_dist</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span> <span class="k">if</span> <span class="n">action_dist</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">action_dist</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span>
                <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span>
                <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_tr</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span><span class="n">test_idx</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
                <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">estimated_rewards_by_reg_model</span>

    <span class="k">def</span> <span class="nf">_pre_process_for_reg_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Preprocess feature vectors to train a regression model.</span>
<span class="sd">        Note</span>
<span class="sd">        -----</span>
<span class="sd">        Please override this method if you want to use another feature enginnering</span>
<span class="sd">        for training the regression model.</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        context: array-like, shape (n_rounds,)</span>
<span class="sd">            Context vectors observed in each round of the logged bandit feedback, i.e., :math:`x_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        action_context: array-like, shape shape (n_actions, dim_action_context)</span>
<span class="sd">            Context vector characterizing action (i.e., vector representation of each action).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">context</span><span class="p">,</span> <span class="n">action_context</span><span class="p">[</span><span class="n">action</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="policies">
<h2>Policies<a class="headerlink" href="#policies" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
  
<span class="kn">import</span> <span class="nn">enum</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>

<span class="c1"># import pkg_resources</span>
<span class="kn">import</span> <span class="nn">yaml</span>

<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABCMeta</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">writefile</span> <span class="n">prior_bts</span><span class="o">.</span><span class="n">yaml</span>
<span class="nb">all</span><span class="p">:</span>
  <span class="n">alpha</span><span class="p">:</span>
    <span class="o">-</span> <span class="mf">47.0</span>
    <span class="o">-</span> <span class="mf">8.0</span>
    <span class="o">-</span> <span class="mf">62.0</span>
    <span class="o">-</span> <span class="mf">142.0</span>
    <span class="o">-</span> <span class="mf">3.0</span>
    <span class="o">-</span> <span class="mf">14.0</span>
    <span class="o">-</span> <span class="mf">7.0</span>
    <span class="o">-</span> <span class="mf">857.0</span>
    <span class="o">-</span> <span class="mf">12.0</span>
    <span class="o">-</span> <span class="mf">15.0</span>
    <span class="o">-</span> <span class="mf">6.0</span>
    <span class="o">-</span> <span class="mf">100.0</span>
    <span class="o">-</span> <span class="mf">48.0</span>
    <span class="o">-</span> <span class="mf">23.0</span>
    <span class="o">-</span> <span class="mf">71.0</span>
    <span class="o">-</span> <span class="mf">61.0</span>
    <span class="o">-</span> <span class="mf">13.0</span>
    <span class="o">-</span> <span class="mf">16.0</span>
    <span class="o">-</span> <span class="mf">518.0</span>
    <span class="o">-</span> <span class="mf">30.0</span>
    <span class="o">-</span> <span class="mf">7.0</span>
    <span class="o">-</span> <span class="mf">4.0</span>
    <span class="o">-</span> <span class="mf">23.0</span>
    <span class="o">-</span> <span class="mf">8.0</span>
    <span class="o">-</span> <span class="mf">10.0</span>
    <span class="o">-</span> <span class="mf">11.0</span>
    <span class="o">-</span> <span class="mf">11.0</span>
    <span class="o">-</span> <span class="mf">18.0</span>
    <span class="o">-</span> <span class="mf">121.0</span>
    <span class="o">-</span> <span class="mf">11.0</span>
    <span class="o">-</span> <span class="mf">11.0</span>
    <span class="o">-</span> <span class="mf">10.0</span>
    <span class="o">-</span> <span class="mf">14.0</span>
    <span class="o">-</span> <span class="mf">9.0</span>
    <span class="o">-</span> <span class="mf">204.0</span>
    <span class="o">-</span> <span class="mf">58.0</span>
    <span class="o">-</span> <span class="mf">3.0</span>
    <span class="o">-</span> <span class="mf">19.0</span>
    <span class="o">-</span> <span class="mf">42.0</span>
    <span class="o">-</span> <span class="mf">1013.0</span>
    <span class="o">-</span> <span class="mf">2.0</span>
    <span class="o">-</span> <span class="mf">328.0</span>
    <span class="o">-</span> <span class="mf">15.0</span>
    <span class="o">-</span> <span class="mf">31.0</span>
    <span class="o">-</span> <span class="mf">14.0</span>
    <span class="o">-</span> <span class="mf">138.0</span>
    <span class="o">-</span> <span class="mf">45.0</span>
    <span class="o">-</span> <span class="mf">55.0</span>
    <span class="o">-</span> <span class="mf">23.0</span>
    <span class="o">-</span> <span class="mf">38.0</span>
    <span class="o">-</span> <span class="mf">10.0</span>
    <span class="o">-</span> <span class="mf">401.0</span>
    <span class="o">-</span> <span class="mf">52.0</span>
    <span class="o">-</span> <span class="mf">6.0</span>
    <span class="o">-</span> <span class="mf">3.0</span>
    <span class="o">-</span> <span class="mf">6.0</span>
    <span class="o">-</span> <span class="mf">5.0</span>
    <span class="o">-</span> <span class="mf">32.0</span>
    <span class="o">-</span> <span class="mf">35.0</span>
    <span class="o">-</span> <span class="mf">133.0</span>
    <span class="o">-</span> <span class="mf">52.0</span>
    <span class="o">-</span> <span class="mf">820.0</span>
    <span class="o">-</span> <span class="mf">43.0</span>
    <span class="o">-</span> <span class="mf">195.0</span>
    <span class="o">-</span> <span class="mf">8.0</span>
    <span class="o">-</span> <span class="mf">42.0</span>
    <span class="o">-</span> <span class="mf">40.0</span>
    <span class="o">-</span> <span class="mf">4.0</span>
    <span class="o">-</span> <span class="mf">32.0</span>
    <span class="o">-</span> <span class="mf">30.0</span>
    <span class="o">-</span> <span class="mf">9.0</span>
    <span class="o">-</span> <span class="mf">22.0</span>
    <span class="o">-</span> <span class="mf">6.0</span>
    <span class="o">-</span> <span class="mf">23.0</span>
    <span class="o">-</span> <span class="mf">5.0</span>
    <span class="o">-</span> <span class="mf">54.0</span>
    <span class="o">-</span> <span class="mf">8.0</span>
    <span class="o">-</span> <span class="mf">22.0</span>
    <span class="o">-</span> <span class="mf">65.0</span>
    <span class="o">-</span> <span class="mf">246.0</span>
  <span class="n">beta</span><span class="p">:</span>
    <span class="o">-</span> <span class="mf">12198.0</span>
    <span class="o">-</span> <span class="mf">3566.0</span>
    <span class="o">-</span> <span class="mf">15993.0</span>
    <span class="o">-</span> <span class="mf">35522.0</span>
    <span class="o">-</span> <span class="mf">2367.0</span>
    <span class="o">-</span> <span class="mf">4609.0</span>
    <span class="o">-</span> <span class="mf">3171.0</span>
    <span class="o">-</span> <span class="mf">181745.0</span>
    <span class="o">-</span> <span class="mf">4372.0</span>
    <span class="o">-</span> <span class="mf">4951.0</span>
    <span class="o">-</span> <span class="mf">3100.0</span>
    <span class="o">-</span> <span class="mf">24665.0</span>
    <span class="o">-</span> <span class="mf">13210.0</span>
    <span class="o">-</span> <span class="mf">7061.0</span>
    <span class="o">-</span> <span class="mf">18061.0</span>
    <span class="o">-</span> <span class="mf">17449.0</span>
    <span class="o">-</span> <span class="mf">5644.0</span>
    <span class="o">-</span> <span class="mf">6787.0</span>
    <span class="o">-</span> <span class="mf">111326.0</span>
    <span class="o">-</span> <span class="mf">8776.0</span>
    <span class="o">-</span> <span class="mf">3334.0</span>
    <span class="o">-</span> <span class="mf">2271.0</span>
    <span class="o">-</span> <span class="mf">7389.0</span>
    <span class="o">-</span> <span class="mf">2659.0</span>
    <span class="o">-</span> <span class="mf">3665.0</span>
    <span class="o">-</span> <span class="mf">4724.0</span>
    <span class="o">-</span> <span class="mf">3561.0</span>
    <span class="o">-</span> <span class="mf">5085.0</span>
    <span class="o">-</span> <span class="mf">27407.0</span>
    <span class="o">-</span> <span class="mf">4601.0</span>
    <span class="o">-</span> <span class="mf">4756.0</span>
    <span class="o">-</span> <span class="mf">4120.0</span>
    <span class="o">-</span> <span class="mf">4736.0</span>
    <span class="o">-</span> <span class="mf">3788.0</span>
    <span class="o">-</span> <span class="mf">45292.0</span>
    <span class="o">-</span> <span class="mf">14719.0</span>
    <span class="o">-</span> <span class="mf">2189.0</span>
    <span class="o">-</span> <span class="mf">5589.0</span>
    <span class="o">-</span> <span class="mf">11995.0</span>
    <span class="o">-</span> <span class="mf">222255.0</span>
    <span class="o">-</span> <span class="mf">2308.0</span>
    <span class="o">-</span> <span class="mf">70034.0</span>
    <span class="o">-</span> <span class="mf">4801.0</span>
    <span class="o">-</span> <span class="mf">8274.0</span>
    <span class="o">-</span> <span class="mf">5421.0</span>
    <span class="o">-</span> <span class="mf">31912.0</span>
    <span class="o">-</span> <span class="mf">12213.0</span>
    <span class="o">-</span> <span class="mf">13576.0</span>
    <span class="o">-</span> <span class="mf">6230.0</span>
    <span class="o">-</span> <span class="mf">10382.0</span>
    <span class="o">-</span> <span class="mf">4141.0</span>
    <span class="o">-</span> <span class="mf">85731.0</span>
    <span class="o">-</span> <span class="mf">12811.0</span>
    <span class="o">-</span> <span class="mf">2707.0</span>
    <span class="o">-</span> <span class="mf">2250.0</span>
    <span class="o">-</span> <span class="mf">2668.0</span>
    <span class="o">-</span> <span class="mf">2886.0</span>
    <span class="o">-</span> <span class="mf">9581.0</span>
    <span class="o">-</span> <span class="mf">9465.0</span>
    <span class="o">-</span> <span class="mf">28336.0</span>
    <span class="o">-</span> <span class="mf">12062.0</span>
    <span class="o">-</span> <span class="mf">162793.0</span>
    <span class="o">-</span> <span class="mf">12107.0</span>
    <span class="o">-</span> <span class="mf">41240.0</span>
    <span class="o">-</span> <span class="mf">3162.0</span>
    <span class="o">-</span> <span class="mf">11604.0</span>
    <span class="o">-</span> <span class="mf">10818.0</span>
    <span class="o">-</span> <span class="mf">2923.0</span>
    <span class="o">-</span> <span class="mf">8897.0</span>
    <span class="o">-</span> <span class="mf">8654.0</span>
    <span class="o">-</span> <span class="mf">4000.0</span>
    <span class="o">-</span> <span class="mf">6580.0</span>
    <span class="o">-</span> <span class="mf">3174.0</span>
    <span class="o">-</span> <span class="mf">6766.0</span>
    <span class="o">-</span> <span class="mf">2602.0</span>
    <span class="o">-</span> <span class="mf">14506.0</span>
    <span class="o">-</span> <span class="mf">3968.0</span>
    <span class="o">-</span> <span class="mf">7523.0</span>
    <span class="o">-</span> <span class="mf">16532.0</span>
    <span class="o">-</span> <span class="mf">51964.0</span>
<span class="n">men</span><span class="p">:</span>
  <span class="n">alpha</span><span class="p">:</span>
    <span class="o">-</span> <span class="mf">47.0</span>
    <span class="o">-</span> <span class="mf">8.0</span>
    <span class="o">-</span> <span class="mf">62.0</span>
    <span class="o">-</span> <span class="mf">142.0</span>
    <span class="o">-</span> <span class="mf">3.0</span>
    <span class="o">-</span> <span class="mf">6.0</span>
    <span class="o">-</span> <span class="mf">100.0</span>
    <span class="o">-</span> <span class="mf">48.0</span>
    <span class="o">-</span> <span class="mf">23.0</span>
    <span class="o">-</span> <span class="mf">71.0</span>
    <span class="o">-</span> <span class="mf">61.0</span>
    <span class="o">-</span> <span class="mf">13.0</span>
    <span class="o">-</span> <span class="mf">16.0</span>
    <span class="o">-</span> <span class="mf">518.0</span>
    <span class="o">-</span> <span class="mf">30.0</span>
    <span class="o">-</span> <span class="mf">7.0</span>
    <span class="o">-</span> <span class="mf">4.0</span>
    <span class="o">-</span> <span class="mf">23.0</span>
    <span class="o">-</span> <span class="mf">8.0</span>
    <span class="o">-</span> <span class="mf">10.0</span>
    <span class="o">-</span> <span class="mf">11.0</span>
    <span class="o">-</span> <span class="mf">11.0</span>
    <span class="o">-</span> <span class="mf">18.0</span>
    <span class="o">-</span> <span class="mf">121.0</span>
    <span class="o">-</span> <span class="mf">11.0</span>
    <span class="o">-</span> <span class="mf">4.0</span>
    <span class="o">-</span> <span class="mf">32.0</span>
    <span class="o">-</span> <span class="mf">30.0</span>
    <span class="o">-</span> <span class="mf">9.0</span>
    <span class="o">-</span> <span class="mf">22.0</span>
    <span class="o">-</span> <span class="mf">6.0</span>
    <span class="o">-</span> <span class="mf">23.0</span>
    <span class="o">-</span> <span class="mf">5.0</span>
    <span class="o">-</span> <span class="mf">54.0</span>
  <span class="n">beta</span><span class="p">:</span>
    <span class="o">-</span> <span class="mf">12198.0</span>
    <span class="o">-</span> <span class="mf">3566.0</span>
    <span class="o">-</span> <span class="mf">15993.0</span>
    <span class="o">-</span> <span class="mf">35522.0</span>
    <span class="o">-</span> <span class="mf">2367.0</span>
    <span class="o">-</span> <span class="mf">3100.0</span>
    <span class="o">-</span> <span class="mf">24665.0</span>
    <span class="o">-</span> <span class="mf">13210.0</span>
    <span class="o">-</span> <span class="mf">7061.0</span>
    <span class="o">-</span> <span class="mf">18061.0</span>
    <span class="o">-</span> <span class="mf">17449.0</span>
    <span class="o">-</span> <span class="mf">5644.0</span>
    <span class="o">-</span> <span class="mf">6787.0</span>
    <span class="o">-</span> <span class="mf">111326.0</span>
    <span class="o">-</span> <span class="mf">8776.0</span>
    <span class="o">-</span> <span class="mf">3334.0</span>
    <span class="o">-</span> <span class="mf">2271.0</span>
    <span class="o">-</span> <span class="mf">7389.0</span>
    <span class="o">-</span> <span class="mf">2659.0</span>
    <span class="o">-</span> <span class="mf">3665.0</span>
    <span class="o">-</span> <span class="mf">4724.0</span>
    <span class="o">-</span> <span class="mf">3561.0</span>
    <span class="o">-</span> <span class="mf">5085.0</span>
    <span class="o">-</span> <span class="mf">27407.0</span>
    <span class="o">-</span> <span class="mf">4601.0</span>
    <span class="o">-</span> <span class="mf">2923.0</span>
    <span class="o">-</span> <span class="mf">8897.0</span>
    <span class="o">-</span> <span class="mf">8654.0</span>
    <span class="o">-</span> <span class="mf">4000.0</span>
    <span class="o">-</span> <span class="mf">6580.0</span>
    <span class="o">-</span> <span class="mf">3174.0</span>
    <span class="o">-</span> <span class="mf">6766.0</span>
    <span class="o">-</span> <span class="mf">2602.0</span>
    <span class="o">-</span> <span class="mf">14506.0</span>
<span class="n">women</span><span class="p">:</span>
  <span class="n">alpha</span><span class="p">:</span>
    <span class="o">-</span> <span class="mf">12.0</span>
    <span class="o">-</span> <span class="mf">7.0</span>
    <span class="o">-</span> <span class="mf">984.0</span>
    <span class="o">-</span> <span class="mf">13.0</span>
    <span class="o">-</span> <span class="mf">15.0</span>
    <span class="o">-</span> <span class="mf">15.0</span>
    <span class="o">-</span> <span class="mf">11.0</span>
    <span class="o">-</span> <span class="mf">14.0</span>
    <span class="o">-</span> <span class="mf">9.0</span>
    <span class="o">-</span> <span class="mf">200.0</span>
    <span class="o">-</span> <span class="mf">72.0</span>
    <span class="o">-</span> <span class="mf">3.0</span>
    <span class="o">-</span> <span class="mf">14.0</span>
    <span class="o">-</span> <span class="mf">49.0</span>
    <span class="o">-</span> <span class="mf">1278.0</span>
    <span class="o">-</span> <span class="mf">3.0</span>
    <span class="o">-</span> <span class="mf">325.0</span>
    <span class="o">-</span> <span class="mf">14.0</span>
    <span class="o">-</span> <span class="mf">27.0</span>
    <span class="o">-</span> <span class="mf">14.0</span>
    <span class="o">-</span> <span class="mf">169.0</span>
    <span class="o">-</span> <span class="mf">48.0</span>
    <span class="o">-</span> <span class="mf">47.0</span>
    <span class="o">-</span> <span class="mf">18.0</span>
    <span class="o">-</span> <span class="mf">40.0</span>
    <span class="o">-</span> <span class="mf">12.0</span>
    <span class="o">-</span> <span class="mf">447.0</span>
    <span class="o">-</span> <span class="mf">46.0</span>
    <span class="o">-</span> <span class="mf">5.0</span>
    <span class="o">-</span> <span class="mf">3.0</span>
    <span class="o">-</span> <span class="mf">5.0</span>
    <span class="o">-</span> <span class="mf">7.0</span>
    <span class="o">-</span> <span class="mf">35.0</span>
    <span class="o">-</span> <span class="mf">34.0</span>
    <span class="o">-</span> <span class="mf">99.0</span>
    <span class="o">-</span> <span class="mf">30.0</span>
    <span class="o">-</span> <span class="mf">880.0</span>
    <span class="o">-</span> <span class="mf">51.0</span>
    <span class="o">-</span> <span class="mf">182.0</span>
    <span class="o">-</span> <span class="mf">6.0</span>
    <span class="o">-</span> <span class="mf">45.0</span>
    <span class="o">-</span> <span class="mf">39.0</span>
    <span class="o">-</span> <span class="mf">10.0</span>
    <span class="o">-</span> <span class="mf">24.0</span>
    <span class="o">-</span> <span class="mf">72.0</span>
    <span class="o">-</span> <span class="mf">229.0</span>
  <span class="n">beta</span><span class="p">:</span>
    <span class="o">-</span> <span class="mf">3612.0</span>
    <span class="o">-</span> <span class="mf">3173.0</span>
    <span class="o">-</span> <span class="mf">204484.0</span>
    <span class="o">-</span> <span class="mf">4517.0</span>
    <span class="o">-</span> <span class="mf">4765.0</span>
    <span class="o">-</span> <span class="mf">5331.0</span>
    <span class="o">-</span> <span class="mf">4131.0</span>
    <span class="o">-</span> <span class="mf">4728.0</span>
    <span class="o">-</span> <span class="mf">4028.0</span>
    <span class="o">-</span> <span class="mf">44280.0</span>
    <span class="o">-</span> <span class="mf">17918.0</span>
    <span class="o">-</span> <span class="mf">2309.0</span>
    <span class="o">-</span> <span class="mf">4339.0</span>
    <span class="o">-</span> <span class="mf">12922.0</span>
    <span class="o">-</span> <span class="mf">270771.0</span>
    <span class="o">-</span> <span class="mf">2480.0</span>
    <span class="o">-</span> <span class="mf">68475.0</span>
    <span class="o">-</span> <span class="mf">5129.0</span>
    <span class="o">-</span> <span class="mf">7367.0</span>
    <span class="o">-</span> <span class="mf">5819.0</span>
    <span class="o">-</span> <span class="mf">38026.0</span>
    <span class="o">-</span> <span class="mf">13047.0</span>
    <span class="o">-</span> <span class="mf">11604.0</span>
    <span class="o">-</span> <span class="mf">5394.0</span>
    <span class="o">-</span> <span class="mf">10912.0</span>
    <span class="o">-</span> <span class="mf">4439.0</span>
    <span class="o">-</span> <span class="mf">94485.0</span>
    <span class="o">-</span> <span class="mf">10700.0</span>
    <span class="o">-</span> <span class="mf">2679.0</span>
    <span class="o">-</span> <span class="mf">2319.0</span>
    <span class="o">-</span> <span class="mf">2578.0</span>
    <span class="o">-</span> <span class="mf">3288.0</span>
    <span class="o">-</span> <span class="mf">9566.0</span>
    <span class="o">-</span> <span class="mf">9775.0</span>
    <span class="o">-</span> <span class="mf">20120.0</span>
    <span class="o">-</span> <span class="mf">7317.0</span>
    <span class="o">-</span> <span class="mf">172026.0</span>
    <span class="o">-</span> <span class="mf">13673.0</span>
    <span class="o">-</span> <span class="mf">37329.0</span>
    <span class="o">-</span> <span class="mf">3365.0</span>
    <span class="o">-</span> <span class="mf">10911.0</span>
    <span class="o">-</span> <span class="mf">10734.0</span>
    <span class="o">-</span> <span class="mf">4278.0</span>
    <span class="o">-</span> <span class="mf">7574.0</span>
    <span class="o">-</span> <span class="mf">16826.0</span>
    <span class="o">-</span> <span class="mf">47462.0</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing prior_bts.yaml
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># configurations to replicate the Bernoulli Thompson Sampling policy used in ZOZOTOWN production</span>
<span class="n">prior_bts_file</span> <span class="o">=</span> <span class="s2">&quot;prior_bts.yaml&quot;</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">prior_bts_file</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">production_prior_for_bts</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">safe_load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PolicyType</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Policy type.</span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    CONTEXT_FREE:</span>
<span class="sd">        The policy type is contextfree.</span>
<span class="sd">    CONTEXTUAL:</span>
<span class="sd">        The policy type is contextual.</span>
<span class="sd">    OFFLINE:</span>
<span class="sd">        The policy type is offline.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">CONTEXT_FREE</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>
    <span class="n">CONTEXTUAL</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>
    <span class="n">OFFLINE</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>

        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="base-context-free-policy">
<h3>Base Context Free Policy<a class="headerlink" href="#base-context-free-policy" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseContextFreePolicy</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for context-free bandit policies.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_actions: int</span>
<span class="sd">        Number of actions.</span>
<span class="sd">    len_list: int, default=1</span>
<span class="sd">        Length of a list of actions recommended in each impression.</span>
<span class="sd">        When Open Bandit Dataset is used, 3 should be set.</span>
<span class="sd">    batch_size: int, default=1</span>
<span class="sd">        Number of samples used in a batch parameter update.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        Controls the random seed in sampling actions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">n_actions</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">len_list</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Class.&quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="s2">&quot;n_actions&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">,</span> <span class="s2">&quot;len_list&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_trial</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_counts_temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts_temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">policy_type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PolicyType</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Type of the bandit policy.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">PolicyType</span><span class="o">.</span><span class="n">CONTEXT_FREE</span>

    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Parameters.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_trial</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_counts_temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts_temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Select a list of actions.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">update_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Update policy parameters.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="epsilon-greedy">
<h3>Epsilon Greedy<a class="headerlink" href="#epsilon-greedy" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">EpsilonGreedy</span><span class="p">(</span><span class="n">BaseContextFreePolicy</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Epsilon Greedy policy.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_actions: int</span>
<span class="sd">        Number of actions.</span>
<span class="sd">    len_list: int, default=1</span>
<span class="sd">        Length of a list of actions recommended in each impression.</span>
<span class="sd">        When Open Bandit Dataset is used, 3 should be set.</span>
<span class="sd">    batch_size: int, default=1</span>
<span class="sd">        Number of samples used in a batch parameter update.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        Controls the random seed in sampling actions.</span>
<span class="sd">    epsilon: float, default=1.</span>
<span class="sd">        Exploration hyperparameter that must take value in the range of [0., 1.].</span>
<span class="sd">    policy_name: str, default=f&#39;egreedy_{epsilon}&#39;.</span>
<span class="sd">        Name of bandit policy.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Class.&quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span> <span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;egreedy_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__post_init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Select a list of actions.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        selected_actions: array-like, shape (len_list, )</span>
<span class="sd">            List of selected actions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_counts</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">predicted_rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_counts</span>
            <span class="k">return</span> <span class="n">predicted_rewards</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Update policy parameters.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        action: int</span>
<span class="sd">            Selected action by the policy.</span>
<span class="sd">        reward: float</span>
<span class="sd">            Observed reward for the chosen action and position.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_trial</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_counts_temp</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts_temp</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_trial</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">action_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_counts_temp</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_counts_temp</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="random">
<h3>Random<a class="headerlink" href="#random" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">Random</span><span class="p">(</span><span class="n">EpsilonGreedy</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Random policy</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_actions: int</span>
<span class="sd">        Number of actions.</span>
<span class="sd">    len_list: int, default=1</span>
<span class="sd">        Length of a list of actions recommended in each impression.</span>
<span class="sd">        When Open Bandit Dataset is used, 3 should be set.</span>
<span class="sd">    batch_size: int, default=1</span>
<span class="sd">        Number of samples used in a batch parameter update.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        Controls the random seed in sampling actions.</span>
<span class="sd">    epsilon: float, default=1.</span>
<span class="sd">        Exploration hyperparameter that must take value in the range of [0., 1.].</span>
<span class="sd">    policy_name: str, default=&#39;random&#39;.</span>
<span class="sd">        Name of bandit policy.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">policy_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;random&quot;</span>

    <span class="k">def</span> <span class="nf">compute_batch_action_dist</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_rounds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Compute the distribution over actions by Monte Carlo simulation.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        n_rounds: int, default=1</span>
<span class="sd">            Number of rounds in the distribution over actions.</span>
<span class="sd">            (the size of the first axis of `action_dist`)</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Probability estimates of each arm being the best one for each sample, action, and position.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">action_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span>
            <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">action_dist</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="bernoullits">
<h3>BernoulliTS<a class="headerlink" href="#bernoullits" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BernoulliTS</span><span class="p">(</span><span class="n">BaseContextFreePolicy</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Bernoulli Thompson Sampling Policy</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_actions: int</span>
<span class="sd">        Number of actions.</span>
<span class="sd">    len_list: int, default=1</span>
<span class="sd">        Length of a list of actions recommended in each impression.</span>
<span class="sd">        When Open Bandit Dataset is used, 3 should be set.</span>
<span class="sd">    batch_size: int, default=1</span>
<span class="sd">        Number of samples used in a batch parameter update.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        Controls the random seed in sampling actions.</span>
<span class="sd">    alpha: array-like, shape (n_actions, ), default=None</span>
<span class="sd">        Prior parameter vector for Beta distributions.</span>
<span class="sd">    beta: array-like, shape (n_actions, ), default=None</span>
<span class="sd">        Prior parameter vector for Beta distributions.</span>
<span class="sd">    is_zozotown_prior: bool, default=False</span>
<span class="sd">        Whether to use hyperparameters for the beta distribution used</span>
<span class="sd">        at the start of the data collection period in ZOZOTOWN.</span>
<span class="sd">    campaign: str, default=None</span>
<span class="sd">        One of the three possible campaigns considered in ZOZOTOWN, &quot;all&quot;, &quot;men&quot;, and &quot;women&quot;.</span>
<span class="sd">    policy_name: str, default=&#39;bts&#39;</span>
<span class="sd">        Name of bandit policy.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">alpha</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">beta</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">is_zozotown_prior</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">campaign</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">policy_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;bts&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize class.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__post_init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_zozotown_prior</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">campaign</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                    <span class="s2">&quot;`campaign` must be specified when `is_zozotown_prior` is True.&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">production_prior_for_bts</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">campaign</span><span class="p">][</span><span class="s2">&quot;alpha&quot;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">production_prior_for_bts</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">campaign</span><span class="p">][</span><span class="s2">&quot;beta&quot;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>

    <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Select a list of actions.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        selected_actions: array-like, shape (len_list, )</span>
<span class="sd">            List of selected actions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">predicted_rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span>
            <span class="n">a</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_counts</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">b</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_counts</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">predicted_rewards</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">update_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Update policy parameters.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        action: int</span>
<span class="sd">            Selected action by the policy.</span>
<span class="sd">        reward: float</span>
<span class="sd">            Observed reward for the chosen action and position.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_trial</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_counts_temp</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts_temp</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_trial</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">action_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_counts_temp</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_counts_temp</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">compute_batch_action_dist</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_rounds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">n_sim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100000</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Compute the distribution over actions by Monte Carlo simulation.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        n_rounds: int, default=1</span>
<span class="sd">            Number of rounds in the distribution over actions.</span>
<span class="sd">            (the size of the first axis of `action_dist`)</span>
<span class="sd">        n_sim: int, default=100000</span>
<span class="sd">            Number of simulations in the Monte Carlo simulation to compute the distribution over actions.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Probability estimates of each arm being the best one for each sample, action, and position.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">action_count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
            <span class="n">selected_actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">select_action</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
                <span class="n">action_count</span><span class="p">[</span><span class="n">selected_actions</span><span class="p">[</span><span class="n">pos</span><span class="p">],</span> <span class="n">pos</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">action_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
            <span class="n">action_count</span> <span class="o">/</span> <span class="n">n_sim</span><span class="p">,</span>
            <span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">action_dist</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="run">
<h2>Run<a class="headerlink" href="#run" title="Permalink to this headline">¶</a></h2>
<p>Policies</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">evaluation_policy_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">bts</span><span class="o">=</span><span class="n">BernoulliTS</span><span class="p">,</span> <span class="n">random</span><span class="o">=</span><span class="n">Random</span><span class="p">)</span>
<span class="n">evaluation_policy_dict</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;bts&#39;: __main__.BernoulliTS, &#39;random&#39;: __main__.Random}
</pre></div>
</div>
</div>
</div>
<p>Base models</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">writefile</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">yaml</span>
<span class="n">lightgbm</span><span class="p">:</span>
    <span class="n">n_estimators</span><span class="p">:</span> <span class="mi">30</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="mf">0.01</span>
    <span class="n">max_depth</span><span class="p">:</span> <span class="mi">5</span>
    <span class="n">min_samples_leaf</span><span class="p">:</span> <span class="mi">10</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="mi">12345</span>
<span class="n">logistic_regression</span><span class="p">:</span>
    <span class="n">max_iter</span><span class="p">:</span> <span class="mi">10000</span>
    <span class="n">C</span><span class="p">:</span> <span class="mi">100</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="mi">12345</span>
<span class="n">random_forest</span><span class="p">:</span>
    <span class="n">n_estimators</span><span class="p">:</span> <span class="mi">30</span>
    <span class="n">max_depth</span><span class="p">:</span> <span class="mi">5</span>
    <span class="n">min_samples_leaf</span><span class="p">:</span> <span class="mi">10</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="mi">12345</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Overwriting hyperparams.yaml
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># hyperparameters of the regression model used in model dependent OPE estimators</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;hyperparams.yaml&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">hyperparams</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">safe_load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="n">hyperparams</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;lightgbm&#39;: {&#39;learning_rate&#39;: 0.01,
  &#39;max_depth&#39;: 5,
  &#39;min_samples_leaf&#39;: 10,
  &#39;n_estimators&#39;: 30,
  &#39;random_state&#39;: 12345},
 &#39;logistic_regression&#39;: {&#39;C&#39;: 100, &#39;max_iter&#39;: 10000, &#39;random_state&#39;: 12345},
 &#39;random_forest&#39;: {&#39;max_depth&#39;: 5,
  &#39;min_samples_leaf&#39;: 10,
  &#39;n_estimators&#39;: 30,
  &#39;random_state&#39;: 12345}}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">base_model_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">logistic_regression</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">,</span>
    <span class="n">lightgbm</span><span class="o">=</span><span class="n">GradientBoostingClassifier</span><span class="p">,</span>
    <span class="n">random_forest</span><span class="o">=</span><span class="n">RandomForestClassifier</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">base_model_dict</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;lightgbm&#39;: sklearn.ensemble._gb.GradientBoostingClassifier,
 &#39;logistic_regression&#39;: sklearn.linear_model._logistic.LogisticRegression,
 &#39;random_forest&#39;: sklearn.ensemble._forest.RandomForestClassifier}
</pre></div>
</div>
</div>
</div>
<p>OPE estimators</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ope_estimators</span> <span class="o">=</span> <span class="p">[</span><span class="n">DirectMethod</span><span class="p">(),</span> <span class="n">InverseProbabilityWeighting</span><span class="p">(),</span> <span class="n">DoublyRobust</span><span class="p">()]</span>
<span class="n">ope_estimators</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[DirectMethod(estimator_name=&#39;dm&#39;),
 InverseProbabilityWeighting(lambda_=inf, estimator_name=&#39;ipw&#39;),
 DoublyRobust(lambda_=inf, estimator_name=&#39;dr&#39;)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;evaluate off-policy estimators.&quot;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s2">&quot;--n_runs&quot;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s2">&quot;number of bootstrap sampling in the experiment.&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s2">&quot;--evaluation_policy&quot;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
    <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;bts&quot;</span><span class="p">,</span> <span class="s2">&quot;random&quot;</span><span class="p">],</span>
    <span class="n">default</span><span class="o">=</span><span class="s1">&#39;bts&#39;</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s2">&quot;evaluation policy, bts or random.&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s2">&quot;--base_model&quot;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
    <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;logistic_regression&quot;</span><span class="p">,</span> <span class="s2">&quot;lightgbm&quot;</span><span class="p">,</span> <span class="s2">&quot;random_forest&quot;</span><span class="p">],</span>
    <span class="n">default</span><span class="o">=</span><span class="s1">&#39;lightgbm&#39;</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s2">&quot;base ML model for regression model, logistic_regression, random_forest or lightgbm.&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s2">&quot;--behavior_policy&quot;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
    <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;bts&quot;</span><span class="p">,</span> <span class="s2">&quot;random&quot;</span><span class="p">],</span>
    <span class="n">default</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s2">&quot;behavior policy, bts or random.&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s2">&quot;--campaign&quot;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
    <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="s2">&quot;men&quot;</span><span class="p">,</span> <span class="s2">&quot;women&quot;</span><span class="p">],</span>
    <span class="n">default</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s2">&quot;campaign name, men, women, or all.&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s2">&quot;--n_sim_to_compute_action_dist&quot;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="mi">1000000</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s2">&quot;number of monte carlo simulation to compute the action distribution of bts.&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s2">&quot;--n_jobs&quot;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the maximum number of concurrently running jobs.&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--random_state&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">12345</span><span class="p">)</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">{})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Namespace(base_model=&#39;lightgbm&#39;, behavior_policy=&#39;random&#39;, campaign=&#39;all&#39;, evaluation_policy=&#39;bts&#39;, n_jobs=1, n_runs=1, n_sim_to_compute_action_dist=1000000, random_state=12345)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># configurations</span>
<span class="n">n_runs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_runs</span>
<span class="n">base_model</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">base_model</span>
<span class="n">evaluation_policy</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">evaluation_policy</span>
<span class="n">behavior_policy</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">behavior_policy</span>
<span class="n">campaign</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">campaign</span>
<span class="n">n_sim_to_compute_action_dist</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_sim_to_compute_action_dist</span>
<span class="n">n_jobs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_jobs</span>
<span class="n">random_state</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">random_state</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!git clone https://github.com/st-tech/zr-obp.git
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cloning into &#39;zr-obp&#39;...
remote: Enumerating objects: 4993, done.
remote: Counting objects: 100% (2007/2007), done.
remote: Compressing objects: 100% (860/860), done.
remote: Total 4993 (delta 1404), reused 1661 (delta 1135), pack-reused 2986
Receiving objects: 100% (4993/4993), 27.54 MiB | 29.23 MiB/s, done.
Resolving deltas: 100% (3306/3306), done.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">obd</span> <span class="o">=</span> <span class="n">OpenBanditDataset</span><span class="p">(</span><span class="n">behavior_policy</span><span class="o">=</span><span class="n">behavior_policy</span><span class="p">,</span> <span class="n">campaign</span><span class="o">=</span><span class="n">campaign</span><span class="p">,</span> <span class="n">data_path</span><span class="o">=</span><span class="n">Path</span><span class="p">(</span><span class="s1">&#39;/content/zr-obp/obd&#39;</span><span class="p">))</span>
<span class="n">obd</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>OpenBanditDataset(behavior_policy=&#39;random&#39;, campaign=&#39;all&#39;, data_path=PosixPath(&#39;/content/zr-obp/obd/random/all&#39;), dataset_name=&#39;obd&#39;)
</pre></div>
</div>
</div>
</div>
<p>Compute action distribution by evaluation policy</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">obd</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">len_list</span><span class="o">=</span><span class="n">obd</span><span class="o">.</span><span class="n">len_list</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span>
<span class="p">)</span>

<span class="k">if</span> <span class="n">evaluation_policy</span> <span class="o">==</span> <span class="s2">&quot;bts&quot;</span><span class="p">:</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;is_zozotown_prior&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;campaign&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">campaign</span>

<span class="n">policy</span> <span class="o">=</span> <span class="n">evaluation_policy_dict</span><span class="p">[</span><span class="n">evaluation_policy</span><span class="p">](</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">action_dist_single_round</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">compute_batch_action_dist</span><span class="p">(</span>
    <span class="n">n_sim</span><span class="o">=</span><span class="n">n_sim_to_compute_action_dist</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Ground-truth policy value of an evaluation policy, which is estimated with factual (observed) rewards (on-policy estimation)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ground_truth_policy_value</span> <span class="o">=</span> <span class="n">OpenBanditDataset</span><span class="o">.</span><span class="n">calc_on_policy_policy_value_estimate</span><span class="p">(</span>
    <span class="n">behavior_policy</span><span class="o">=</span><span class="n">evaluation_policy</span><span class="p">,</span>
    <span class="n">campaign</span><span class="o">=</span><span class="n">campaign</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="c1"># sample bootstrap from batch logged bandit feedback</span>
    <span class="n">bandit_feedback</span> <span class="o">=</span> <span class="n">obd</span><span class="o">.</span><span class="n">sample_bootstrap_bandit_feedback</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">b</span><span class="p">)</span>
    <span class="c1"># estimate the mean reward function with an ML model</span>
    <span class="n">regression_model</span> <span class="o">=</span> <span class="n">RegressionModel</span><span class="p">(</span>
        <span class="n">n_actions</span><span class="o">=</span><span class="n">obd</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
        <span class="n">len_list</span><span class="o">=</span><span class="n">obd</span><span class="o">.</span><span class="n">len_list</span><span class="p">,</span>
        <span class="n">action_context</span><span class="o">=</span><span class="n">obd</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
        <span class="n">base_model</span><span class="o">=</span><span class="n">base_model_dict</span><span class="p">[</span><span class="n">base_model</span><span class="p">](</span><span class="o">**</span><span class="n">hyperparams</span><span class="p">[</span><span class="n">base_model</span><span class="p">]),</span>
    <span class="p">)</span>
    <span class="n">estimated_rewards_by_reg_model</span> <span class="o">=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span>
        <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
        <span class="n">action</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span>
        <span class="n">reward</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span>
        <span class="n">position</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;position&quot;</span><span class="p">],</span>
        <span class="n">pscore</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">],</span>
        <span class="n">n_folds</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>  <span class="c1"># 3-fold cross-fitting</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># evaluate estimators&#39; performances using relative estimation error (relative-ee)</span>
    <span class="n">ope</span> <span class="o">=</span> <span class="n">OffPolicyEvaluation</span><span class="p">(</span>
        <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">,</span>
        <span class="n">ope_estimators</span><span class="o">=</span><span class="n">ope_estimators</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">action_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
        <span class="n">action_dist_single_round</span><span class="p">,</span> <span class="p">(</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;n_rounds&quot;</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">relative_ee_b</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">evaluate_performance_of_estimators</span><span class="p">(</span>
        <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">ground_truth_policy_value</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">relative_ee_b</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">processed</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">50</span><span class="p">)([</span><span class="n">delayed</span><span class="p">(</span><span class="n">process</span><span class="p">)(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_runs</span><span class="p">)])</span>

<span class="n">relative_ee_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">est</span><span class="o">.</span><span class="n">estimator_name</span><span class="p">:</span> <span class="nb">dict</span><span class="p">()</span> <span class="k">for</span> <span class="n">est</span> <span class="ow">in</span> <span class="n">ope_estimators</span><span class="p">}</span>

<span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">relative_ee_b</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">processed</span><span class="p">):</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">estimator_name</span><span class="p">,</span> <span class="n">relative_ee_</span><span class="p">)</span> <span class="ow">in</span> <span class="n">relative_ee_b</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">relative_ee_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">relative_ee_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.0s finished
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">relative_ee_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">relative_ee_dict</span><span class="p">)</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;random_state=</span><span class="si">{</span><span class="n">random_state</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">relative_ee_df</span><span class="p">[[</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;std&quot;</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==============================
random_state=12345
------------------------------
         mean  std
dm   0.034354  NaN
ipw  0.100573  NaN
dr   0.096567  NaN
==============================
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># save results of the evaluation of off-policy estimators in &#39;./logs&#39; directory.</span>
<span class="n">log_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;./logs&quot;</span><span class="p">)</span> <span class="o">/</span> <span class="n">behavior_policy</span> <span class="o">/</span> <span class="n">campaign</span>
<span class="n">log_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">relative_ee_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">log_path</span> <span class="o">/</span> <span class="s2">&quot;relative_ee_of_ope_estimators.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./nbs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="T705904_Evaluation_of_Multiple_Off_Policy_Estimators_on_Synthetic_Dataset.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Evaluation of Multiple Off-Policy Estimators on Synthetic Dataset</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="T792262_Optimal_Off_Policy_Evaluation_from_Multiple_Logging_Policies.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Optimal Off-Policy Evaluation from Multiple Logging Policies</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Sparsh A.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>