
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>OBP Library Workshop Tutorials &#8212; Reco Book</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="PyTorch Fundamentals Part 1" href="T632722_PyTorch_Fundamentals_Part_1.html" />
    <link rel="prev" title="Offline Policy Evaluation with VW Command Line" href="T167249_Offline_Policy_Evaluation_with_VW_Command_Line.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Reco Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Tutorials in Jupyter notebook format
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  User Stories
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="US780867_Transformer_based_Recommenders.html">
   Transformer-based Recommenders
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="T034923_BERT4Rec_on_ML1M_in_PyTorch.html">
     BERT4Rec on ML-1M in PyTorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T595874_BERT4Rec_on_ML25M_in_PyTorch_Lightning.html">
     BERT4Rec on ML-25M
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T088416_BST_Implementation_in_MXNet.html">
     BST Implementation in MXNet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T602245_BST_implementation_in_PyTorch.html">
     BST implementation in PyTorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T007665_BST_on_ML1M_in_Keras.html">
     A Transformer-based recommendation system
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T881207_BST_PTLightning_ML1M.html">
     Rating prediction using the Behavior Sequence Transformer (BST) model on ML-1M dataset in PyTorch Lightning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T025247_BST_using_Deepctr_library.html">
     BST using Deepctr library
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T757997_SASRec_PyTorch.html">
     SASRec implementation with PyTorch Library
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T225287_SASRec_PaddlePaddle.html">
     SASRec implementation with Paddle Library
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T701627_SR_SAN_Session_based_Model.html">
     SR-SAN Session-based Recommender
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T975104_SSEPT_ML1M_Tensorflow1x.html">
     SSE-PT Personalized Transformer Recommender on ML-1M in Tensorflow 1.x
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T472955_GCSAN_Session_based_Model.html">
     GCSAN Session-based Recommender
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T970274_Transformers4Rec_Session_based_Recommender_on_Yoochoose.html">
     End-to-end session-based recommendation with Transformers4Rec
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T382183_Transformers4Rec_XLNet_on_Synthetic_data.html">
     Transformers4Rec XLNet on Synthetic data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T793395_Session_based_recommendation_on_REES46_Dataset.html">
     End-to-end Session-based recommendation on REES46 Dataset
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Prototypes
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="T382881_DeepWalk_Karateclub.html">
   DeepWalk from scratch referencing Karateclub library
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T384270_DeepWalk_pure_python.html">
   DeepWalk in pure python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T677598_Jaccard_Cosine_SVD_DeepWalk_ML100K.html">
   Recommender System with DeepWalk Graph Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T815556_Node2vec_Karateclub.html">
   Node2vec from scratch referencing Karateclub library
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T894941_Node2vec_MovieLens_Keras.html">
   Graph representation learning with node2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T611050_Node2vec_PyG.html">
   Node2vec from scratch in PyTorch Geometric
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T186367_Node2vec_library.html">
   Node2vec from scratch referencing node2vec library
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T331379_bayesian_personalized_ranking.html">
   BPR from scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T081831_Data_Poisoning_Attacks_on_Factorization_Based_Collaborative_Filtering.html">
   Data Poisoning Attacks on Factorization-Based Collaborative Filtering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T102448_Adversarial_Learning_for_Recommendation.html">
   Adversarial Training (Regularization) on a Recommender System
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T865035_Simulating_Data_Poisoning_Attacks_against_Twitter_Recommender.html">
   Load and process dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T711285_Data_Poisoning_Attack_using_LFM_and_ItemAE_on_Synthetic_Dataset.html">
   Injection attack using LFM and ItemAE model trained on Toy dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T355514_Black_box_Attack_on_Sequential_Recs.html">
   Black-box Attack on Sequential Recs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T873451_Statistics_fundamentals.html">
   Statictics Fundamentals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T890478_Batch_Learning_from_Bandit_Feedback_%28BLBF%29.html">
   Imports
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T257798_Off_Policy_Learning_in_Two_stage_Recommender_Systems.html">
   Off-Policy Learning in Two-stage Recommender Systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T471827_Adaptive_Estimator_Selection_for_Off_Policy_Evaluation.html">
   Adaptive Estimator Selection for Off-Policy Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T902666_Evaluating_the_Robustness_of_Off_Policy_Evaluation.html">
   Evaluating the Robustness of Off-Policy Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T705904_Evaluation_of_Multiple_Off_Policy_Estimators_on_Synthetic_Dataset.html">
   Evaluation of Multiple Off-Policy Estimators on Synthetic Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T874693_Evaluating_Standard_Off_Policy_Estimators_with_Small_Sample_Open_Bandit_Dataset.html">
   Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T792262_Optimal_Off_Policy_Evaluation_from_Multiple_Logging_Policies.html">
   Optimal Off-Policy Evaluation from Multiple Logging Policies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T167249_Offline_Policy_Evaluation_with_VW_Command_Line.html">
   Offline Policy Evaluation with VW Command Line
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   OBP Library Workshop Tutorials
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T632722_PyTorch_Fundamentals_Part_1.html">
   PyTorch Fundamentals Part 1
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T472467_PyTorch_Fundamentals_Part_2.html">
   PyTorch Fundamentals Part 2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T206654_PyTorch_Fundamentals_Part_3.html">
   PyTorch Fundamentals Part 3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T536348_Attention_Mechanisms.html">
   Imports
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T500796_Agricultural_Satellite_Image_Segmentation.html">
   Agricultural Satellite Image Segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T611432_Image_Analysis_with_Tensorflow.html">
   Image Analysis with Tensorflow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T925716_MongoDB_to_CSV_Conversion.html">
   MongoDB to CSV conversion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T396469_PDF_to_Word_Cloud_via_Email.html">
   PDF to WordCloud via Email
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T030890_Job_Scraping_and_Clustering.html">
   Job scraping and clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T897054_Scene_Text_Recognition.html">
   Scene Text Recognition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T034809_Large_scale_Document_Retrieval_with_Elastic_Search.html">
   Large-scale Document Retrieval with ElasticSearch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T467251_vowpal_wabbit_contextual_recommender.html">
   Simulating a news personalization scenario using Contextual Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T686684_similar_product_recommender.html">
   Similar Product Recommender system using Deep Learning for an online e-commerce store
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T132203_Retail_Product_Recommendations_using_Word2vec.html">
   Retail Product Recommendations using word2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T501828_Recommender_Implicit_Negative_Feedback.html">
   Retail Product Recommendation with Negative Implicit Feedback
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T315965_Sequence_Aware_Recommenders_Music.html">
   Sequence Aware Recommender Systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T051777_image_similarity_recommendations.html">
   Similar Product Recommendations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990172_recobook_diversity_aware_book_recommender.html">
   Diversity Aware Book Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T488549_Goodreads_Diversity_Aware_Book_Recommender.html">
   Diversity Aware Book Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T023535_Kafka_MongoDB_Real_time_Streaming.html">
   Kafka MongoDB Real-time Streaming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T622304_Session_based_Recommender_Using_Word2vec.html">
   Session-based recommendation using word2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T416854_bandit_based_recommender_using_thompson_sampling_app.html">
   Bandit-based Online Learning using Thompson Sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T198578_Booking_dot_com_Trip_Recommendation.html">
   Booking.com Trip Recommendation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T519734_Vowpal_Wabbit_Contextual_Bandit.html">
   Vowpal Wabbit Contextual Bandit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T871537_Recommendation_Systems_using_Olist_Dataset.html">
   Recommendation systems using Olist dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T057885_Offline_Replayer_Evaluation.html">
   Offline Replayer Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T915054_method_for_effective_online_testing.html">
   Methods for effective online testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T513987_Recsys_2020_Feature_Engineering_Tutorial.html">
   Recsys’20 Feature Engineering Tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T227901_amazon_personalize_batch_job.html">
   Amazon Personalize Batch Job
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T022961_Amazon_Personalize_Workshop.html">
   Amazon Personalize Workshop
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T424437_Collaborative_Filtering_on_ML_latest_small.html">
   Collaborative Filtering on ML-latest-small
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T539160_Building_and_Deploying_ASOS_Fashion_Recommender.html">
   Building and deploying ASOS fashion recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T757697_Simple_Similarity_based_Recommender.html">
   Simple Similarity based Recommmendations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T051594_Analytics_Zoo.html">
   Analytics Zoo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T313645_A_B_Testing.html">
   A/B Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T475711_PinSage_Graph_based_Recommender.html">
   PinSage Graph-based Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T516490_Graph_Embeddings.html">
   Learn Embeddings using Graph Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T822164_movielens_milvus_redis_efficient_retrieval.html">
   Recommender with Redis and Milvus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T845186_Anime_Recommender.html">
   RekoNet Anime Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T855843_kafka_spark_streaming_colab.html">
   Kafka and Spark Streaming in Colab
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T460437_Building_Models_From_Scratch.html">
   Building Models from scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T855971_Conet_Model_for_Movie_Recommender.html">
   CoNet model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T996996_content_based_and_collaborative_movielens.html">
   Movie Recommendation with Content-Based and Collaborative Filtering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T239418_Simple_Movie_Recommenders.html">
   Simple Movie Recommenders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T138337_Simple_Movie_Recommender.html">
   Simple movie recommender in implicit, explicit, and cold-start settings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T935440_The_importance_of_Rating_Normalization.html">
   The importance of Rating Normalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T612622_cornac_examples.html">
   Cornac Examples
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T561435_Flower_Classification.html">
   Flower classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T680910_Trivago_Session_based_Recommender.html">
   Trivago Session-based Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_ads_selection_using_bandits.html">
   Best Ads detection using bandit methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_book_crossing_surprise_svd_nmf.html">
   Book-Crossing Recommendation System
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_book_recommender_kubeflow.html">
   Books recommendations with Kubeflow Pipelines on Scaleway Kapsule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_build_a_kubeflow_pipeline.html">
   Build a Kubeflow Pipeline
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_causal_inference.html">
   Causal Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_concept_embedding_nlp.html">
   Exploring Word Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_concept_neural_net.html">
   Neural Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_concept_nlp_basics.html">
   Natural Language Processing 101
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_concept_transformer_lm.html">
   TransformerLM Quick Start and Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_content_based_music_recommender_lyricsfreak.html">
   Content-based method for song recommendation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_evaluation_metrics_basics.html">
   Recommender System Evaluations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_implicit_synthetic.html">
   Comparing Implicit Models on Synthetic Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_movie_recommender_tensorflow.html">
   Recommendation Systems with TensorFlow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_movie_recommender_tensorflow_sagemaker.html">
   Movie recommender using Tensorflow in Sagemaker
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_movielens_eda_modeling.html">
   Movielens EDA and Modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_read_data_from_cassandra_into_pandas.html">
   Read Cassandra Data Snapshot as DataFrame
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_rl_in_action.html">
   Reinforcement Learning fundamentals in action
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_rnn_cnn_basics.html">
   Processing sequences using RNNs and CNNs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_tf_serving_in_action.html">
   TF Serving in action
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_training_indexing_movie_recommender.html">
   Training and indexing movie recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T207114_EduRec_MOOCCube_Course_Recommender.html">
   EduRec MOOCCube Course Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T273184_Multi_Task_Learning.html">
   Multi-task Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T661108_Book_Recommender_API.html">
   Book Recommender API
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T912764_Simple_Movie_Recommender_App.html">
   Simple Movie Recommender App
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T964554_Career_Village_Questions_Recommendation.html">
   CareerVillage Questions Recommendation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_amazon_women_apparel_tfidf_word2vec.html">
   Amazon Product Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_anime_recommender_graph_network.html">
   Anime Recommender with Bi-partite Graph Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_concept_self_attention.html">
   Self-Attention
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_course_recommender_svd_flask.html">
   Course Recommender with SVD based similarity
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_data_mining_similarity_measures.html">
   Concept - Data Mining Similarity Measures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_jaccard_recommender.html">
   Jaccard Similarity based Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_live_streamer_recommender.html">
   Live Streamer Recommender with Implicit feedback
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_songs_embedding_skipgram_recommender.html">
   Song Embeddings - Skipgram Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_toy_example_car_recommender_knn.html">
   Toy example - Car Recommender using KNN method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_wikirecs_recommender.html">
   WikiRecs
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/nbs/T966055_OBP_Library_Workshop_Tutorials.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sparsh-ai/reco-book/main?urlpath=tree/nbs/T966055_OBP_Library_Workshop_Tutorials.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#synthetic-data">
   Synthetic Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generate-synthetic-dataset">
     Generate Synthetic Dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-bandit-policies-opl">
     Train Bandit Policies (OPL)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#approximate-the-ground-truth-policy-value">
     Approximate the Ground-truth Policy Value
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#obtain-a-reward-estimator">
     Obtain a Reward Estimator
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#off-policy-evaluation-ope">
     Off-Policy Evaluation (OPE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluation-of-ope">
     Evaluation of OPE
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#off-policy-simulation-data">
   Off-Policy Simulation Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#imports">
     Imports
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluatin-of-ope-estimators-part-1-easy-setting">
     Evaluatin of OPE estimators (Part 1; easy setting)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualize-results">
     Visualize Results
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluation-of-ope-estimators-part-2-challenging-setting">
     Evaluation of OPE estimators (Part 2; challenging setting)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Visualize Results
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparameter-tuning-of-ope-estimators">
     Hyperparameter Tuning of OPE estimators
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Visualize Results
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-class-classificatoin-data">
   Multi-class Classificatoin Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bandit-reduction">
     Bandit Reduction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#off-policy-learning">
     Off-Policy Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Obtain a Reward Estimator
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     Off-Policy Evaluation (OPE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluation-of-ope-estimators">
     Evaluation of OPE estimators
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#obp-library-workshop-tutorial-on-zozotown-open-bandit-dataset">
   OBP Library Workshop Tutorial on ZOZOTOWN Open-Bandit Dataset
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-loading-and-preprocessing">
     Data Loading and Preprocessing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#let-s-see-some-properties-of-the-dataset-class">
     let’s see some properties of the dataset class
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#replicating-production-policy">
     Replicating Production Policy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#obtaining-a-reward-estimator">
     Obtaining a Reward Estimator
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     Off-Policy Evaluation (OPE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     Evaluation of OPE
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#real-world-example">
   Real-world Example
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     Data Loading and Preprocessing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#off-policy-learning-opl">
     Off-Policy Learning (OPL)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ipwlearner">
       IPWLearner
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#nnpolicylearner">
       NNPolicyLearner
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     Obtain a Reward Estimator
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#off-policy-evaluation">
     Off-Policy Evaluation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualize-the-ope-results">
     Visualize the OPE results
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#synthetic-slate-dataset">
   Synthetic Slate Dataset
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#synthetic-slate-data-generation">
     Synthetic Slate Data Generation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluation-policy-definition-off-policy-learning">
     Evaluation Policy Definition (Off-Policy Learning)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     Off-Policy Evaluation (OPE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     Evaluation of OPE estimators
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#off-policy-evaluation-of-online-bandit-algorithms">
   Off-Policy Evaluation of Online Bandit Algorithms
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#synthetic-data-generation">
     Synthetic Data Generation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     Off-Policy Evaluation (OPE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id12">
     Evaluation of OPE
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#off-policy-learners">
   Off-Policy Learners
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-synthetic-data">
     Generating Synthetic Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id13">
     Off-Policy Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluation-of-off-policy-learners">
     Evaluation of Off-Policy Learners
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><a href="https://colab.research.google.com/github/sparsh-ai/reco-book/blob/stage/nbs/T966055_OBP_Library_Workshop_Tutorials.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<div class="section" id="obp-library-workshop-tutorials">
<h1>OBP Library Workshop Tutorials<a class="headerlink" href="#obp-library-workshop-tutorials" title="Permalink to this headline">¶</a></h1>
<div class="section" id="synthetic-data">
<h2>Synthetic Data<a class="headerlink" href="#synthetic-data" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install obp==0.5.1
!pip install matplotlib==3.1.1
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="kn">import</span> <span class="nn">obp</span>
<span class="kn">from</span> <span class="nn">obp.dataset</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">SyntheticBanditDataset</span><span class="p">,</span>
    <span class="n">logistic_reward_function</span><span class="p">,</span>
    <span class="n">linear_behavior_policy</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">obp.policy</span> <span class="kn">import</span> <span class="n">IPWLearner</span>
<span class="kn">from</span> <span class="nn">obp.ope</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">OffPolicyEvaluation</span><span class="p">,</span> 
    <span class="n">RegressionModel</span><span class="p">,</span>
    <span class="n">InverseProbabilityWeighting</span> <span class="k">as</span> <span class="n">IPS</span><span class="p">,</span>
    <span class="n">DirectMethod</span> <span class="k">as</span> <span class="n">DM</span><span class="p">,</span>
    <span class="n">DoublyRobust</span> <span class="k">as</span> <span class="n">DR</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="generate-synthetic-dataset">
<h3>Generate Synthetic Dataset<a class="headerlink" href="#generate-synthetic-dataset" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">obp.dataset.SyntheticBanditDataset</span></code> is an easy-to-use synthetic data generator.</p>
<p>It takes</p>
<ul class="simple">
<li><p>number of actions (<code class="docutils literal notranslate"><span class="pre">n_actions</span></code>, <span class="math notranslate nohighlight">\(|\mathcal{A}|\)</span>)</p></li>
<li><p>dimension of context vectors (<code class="docutils literal notranslate"><span class="pre">dim_context</span></code>, <span class="math notranslate nohighlight">\(d\)</span>)</p></li>
<li><p>reward function (<code class="docutils literal notranslate"><span class="pre">reward_function</span></code>, <span class="math notranslate nohighlight">\(q(x,a)=\mathbb{E}[r \mid x,a]\)</span>)</p></li>
<li><p>behavior policy (<code class="docutils literal notranslate"><span class="pre">behavior_policy_function</span></code>, <span class="math notranslate nohighlight">\(\pi_b(a|x)\)</span>)</p></li>
</ul>
<p>as inputs and generates synthetic logged bandit data that can be used to evaluate the performance of decision making policies (obtained by <code class="docutils literal notranslate"><span class="pre">off-policy</span> <span class="pre">learning</span></code>) and OPE estimators.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate synthetic logged bandit data with 10 actions</span>
<span class="c1"># we use `logistic function` as the reward function and `linear_behavior_policy` as the behavior policy.</span>
<span class="c1"># one can define their own reward function and behavior policy such as nonlinear ones. </span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">SyntheticBanditDataset</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="c1"># number of actions; |A|</span>
    <span class="n">dim_context</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="c1"># number of dimensions of context vector</span>
    <span class="n">reward_function</span><span class="o">=</span><span class="n">logistic_reward_function</span><span class="p">,</span> <span class="c1"># mean reward function; q(x,a)</span>
    <span class="n">behavior_policy_function</span><span class="o">=</span><span class="n">linear_behavior_policy</span><span class="p">,</span> <span class="c1"># behavior policy; \pi_b</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>

<span class="p">)</span>
<span class="n">training_bandit_data</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="n">n_rounds</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">test_bandit_data</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="n">n_rounds</span><span class="o">=</span><span class="mi">1000000</span><span class="p">)</span>

<span class="n">training_bandit_data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;action&#39;: array([9, 2, 1, ..., 0, 2, 6]),
 &#39;action_context&#39;: array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]),
 &#39;context&#39;: array([[-0.20470766,  0.47894334, -0.51943872, -0.5557303 ,  1.96578057],
        [ 1.39340583,  0.09290788,  0.28174615,  0.76902257,  1.24643474],
        [ 1.00718936, -1.29622111,  0.27499163,  0.22891288,  1.35291684],
        ...,
        [-1.27028221,  0.80914602, -0.45084222,  0.47179511,  1.89401115],
        [-0.68890924,  0.08857502, -0.56359347, -0.41135069,  0.65157486],
        [ 0.51204121,  0.65384817, -1.98849253, -2.14429131, -0.34186901]]),
 &#39;expected_reward&#39;: array([[0.80210203, 0.73828559, 0.83199558, ..., 0.81190503, 0.70617705,
         0.68985306],
        [0.94119582, 0.93473317, 0.91345213, ..., 0.94140688, 0.93152449,
         0.90132868],
        [0.87248862, 0.67974991, 0.66965669, ..., 0.79229752, 0.82712978,
         0.74923536],
        ...,
        [0.66717573, 0.81583571, 0.77012708, ..., 0.87757008, 0.57652468,
         0.80629132],
        [0.52526986, 0.39952563, 0.61892038, ..., 0.53610389, 0.49392728,
         0.58408936],
        [0.55375831, 0.11662199, 0.807396  , ..., 0.22532856, 0.42629292,
         0.24120499]]),
 &#39;n_actions&#39;: 10,
 &#39;n_rounds&#39;: 10000,
 &#39;position&#39;: None,
 &#39;pscore&#39;: array([0.07250876, 0.10335615, 0.14110696, ..., 0.09756788, 0.10335615,
        0.14065505]),
 &#39;reward&#39;: array([1, 1, 1, ..., 0, 1, 1])}
</pre></div>
</div>
</div>
</div>
<p>the logged bandit feedback is collected by the behavior policy as follows.</p>
<p><span class="math notranslate nohighlight">\( \mathcal{D}_b := \{(x_i,a_i,r_i)\}\)</span>  where <span class="math notranslate nohighlight">\((x,a,r) \sim p(x)\pi_b(a \mid x)p(r \mid x,a) \)</span></p>
</div>
<div class="section" id="train-bandit-policies-opl">
<h3>Train Bandit Policies (OPL)<a class="headerlink" href="#train-bandit-policies-opl" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ipw_learner</span> <span class="o">=</span> <span class="n">IPWLearner</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="c1"># number of actions; |A|</span>
    <span class="n">base_classifier</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">)</span> <span class="c1"># any sklearn classifier</span>
<span class="p">)</span>

<span class="c1"># fit</span>
<span class="n">ipw_learner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">training_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span> <span class="c1"># context; x</span>
    <span class="n">action</span><span class="o">=</span><span class="n">training_bandit_data</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span> <span class="c1"># action; a</span>
    <span class="n">reward</span><span class="o">=</span><span class="n">training_bandit_data</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span> <span class="c1"># reward; r</span>
    <span class="n">pscore</span><span class="o">=</span><span class="n">training_bandit_data</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">],</span> <span class="c1"># propensity score; pi_b(a|x)</span>
<span class="p">)</span>

<span class="c1"># predict (action dist = action distribution)</span>
<span class="n">action_dist_ipw</span> <span class="o">=</span> <span class="n">ipw_learner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span> <span class="c1"># context in the test data</span>
<span class="p">)</span>

<span class="n">action_dist_ipw</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="c1"># which action to take for each context </span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0., 0., 0., ..., 1., 0., 0.],
       [0., 0., 0., ..., 0., 1., 0.],
       [1., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 1., 0., ..., 0., 0., 0.],
       [0., 1., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 1.]])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="approximate-the-ground-truth-policy-value">
<h3>Approximate the Ground-truth Policy Value<a class="headerlink" href="#approximate-the-ground-truth-policy-value" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[ V(\pi) \approx \frac{1}{|\mathcal{D}_{te}|} \sum_{i=1}^{|\mathcal{D}_{te}|} \mathbb{E}_{a \sim \pi(a|x_i)} [r(x_i, a)], \; \, where \; \, r(x,a) := \mathbb{E}_{r \sim p(r|x,a)} [r] \]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">policy_value_of_ipw</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">calc_ground_truth_policy_value</span><span class="p">(</span>
    <span class="n">expected_reward</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">[</span><span class="s2">&quot;expected_reward&quot;</span><span class="p">],</span> <span class="c1"># expected rewards; q(x,a)</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_ipw</span><span class="p">,</span> <span class="c1"># action distribution of IPWLearner</span>
<span class="p">)</span>

<span class="c1"># ground-truth policy value of `IPWLearner`</span>
<span class="n">policy_value_of_ipw</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7544002774485176
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="obtain-a-reward-estimator">
<h3>Obtain a Reward Estimator<a class="headerlink" href="#obtain-a-reward-estimator" title="Permalink to this headline">¶</a></h3>
<p>obp.ope.RegressionModel simplifies the process of reward modeling</p>
<p><span class="math notranslate nohighlight">\( r(x,a)=E[r∣x,a]≈r^(x,a) \)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># estimate the expected reward by using an ML model (Logistic Regression here)</span>
<span class="c1"># the estimated rewards are used by model-dependent estimators such as DM and DR</span>
<span class="n">regression_model</span> <span class="o">=</span> <span class="n">RegressionModel</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="c1"># number of actions; |A|</span>
    <span class="n">base_model</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">)</span> <span class="c1"># any sklearn classifier</span>
<span class="p">)</span>

<span class="n">estimated_rewards</span> <span class="o">=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span> <span class="c1"># context; x</span>
    <span class="n">action</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span> <span class="c1"># action; a</span>
    <span class="n">reward</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span> <span class="c1"># reward; r</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">estimated_rewards</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="c1"># \hat{q}(x,a)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.83287191, 0.77609002, 0.86301082, ..., 0.80705541, 0.87089962,
        0.88661944],
       [0.24064512, 0.18060694, 0.28603087, ..., 0.21010776, 0.30020355,
        0.33212285],
       [0.92681158, 0.89803854, 0.94120582, ..., 0.91400784, 0.94487921,
        0.95208655],
       ...,
       [0.95590514, 0.93780227, 0.96479479, ..., 0.94790501, 0.96704598,
        0.97144249],
       [0.9041431 , 0.8677301 , 0.92262342, ..., 0.88785354, 0.92736823,
        0.93671186],
       [0.19985698, 0.14801146, 0.23998121, ..., 0.1733142 , 0.25267968,
        0.28157917]])
</pre></div>
</div>
</div>
</div>
<p>please refer to <a class="reference external" href="https://arxiv.org/abs/2002.08536">https://arxiv.org/abs/2002.08536</a> about the details of the cross-fitting procedure.</p>
</div>
<div class="section" id="off-policy-evaluation-ope">
<h3>Off-Policy Evaluation (OPE)<a class="headerlink" href="#off-policy-evaluation-ope" title="Permalink to this headline">¶</a></h3>
<p>obp.ope.OffPolicyEvaluation simplifies the OPE process</p>
<p><span class="math notranslate nohighlight">\( V(πe)≈\hat{V}(πe;D0,θ) \)</span> using DM, IPS, and DR</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ope</span> <span class="o">=</span> <span class="n">OffPolicyEvaluation</span><span class="p">(</span>
    <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">,</span> <span class="c1"># test data</span>
    <span class="n">ope_estimators</span><span class="o">=</span><span class="p">[</span>
        <span class="n">IPS</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;IPS&quot;</span><span class="p">),</span> 
        <span class="n">DM</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;DM&quot;</span><span class="p">),</span> 
        <span class="n">DR</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;DR&quot;</span><span class="p">),</span>
    <span class="p">]</span> <span class="c1"># used estimators</span>
<span class="p">)</span>

<span class="n">estimated_policy_value</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">estimate_policy_values</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_ipw</span><span class="p">,</span> <span class="c1"># \pi_e(a|x)</span>
    <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards</span><span class="p">,</span> <span class="c1"># \hat{q}(x,a)</span>
<span class="p">)</span>

<span class="c1"># OPE results given by the three estimators</span>
<span class="n">estimated_policy_value</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;DM&#39;: 0.649636379816842, &#39;DR&#39;: 0.7515054724486265, &#39;IPS&#39;: 0.7514146726298303}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># estimate the policy value of IPWLearner with Logistic Regression</span>
<span class="n">estimated_policy_value_a</span><span class="p">,</span> <span class="n">estimated_interval_a</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">summarize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_ipw</span><span class="p">,</span>
    <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">estimated_interval_a</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># visualize policy values of IPWLearner with Logistic Regression estimated by the three OPE estimators</span>
<span class="n">ope</span><span class="o">.</span><span class="n">visualize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_ipw</span><span class="p">,</span>
    <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="c1"># number of resampling performed in the bootstrap procedure</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>         mean  95.0% CI (lower)  95.0% CI (upper)
IPS  0.751658          0.745758          0.756950
DM   0.649630          0.649152          0.650020
DR   0.751348          0.748093          0.754105 
</pre></div>
</div>
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_19_1.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_19_1.png" />
</div>
</div>
</div>
<div class="section" id="evaluation-of-ope">
<h3>Evaluation of OPE<a class="headerlink" href="#evaluation-of-ope" title="Permalink to this headline">¶</a></h3>
<p>Now, let’s evaluate the OPE performance (estimation accuracy) of the three estimators</p>
<p><span class="math notranslate nohighlight">\( V(\pi_e) \approx \hat{V} (\pi_e; \mathcal{D}_0, \theta) \)</span></p>
<p>We can then evaluate the estimation performance of OPE estimators by comparing the estimated policy values of the evaluation with its ground-truth as follows.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\textit{relative-ee} (\hat{V}; \mathcal{D}_b) := \left| \frac{V(\pi_e) - \hat{V} (\pi_e; \mathcal{D}_b)}{V(\pi_e)} \right|\)</span> (relative estimation error; relative-ee)</p></li>
<li><p><span class="math notranslate nohighlight">\(\textit{SE} (\hat{V}; \mathcal{D}_b) := \left( V(\pi_e) - \hat{V} (\pi_e; \mathcal{D}_b) \right)^2\)</span> (squared error; se)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">squared_errors</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">evaluate_performance_of_estimators</span><span class="p">(</span>
    <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">policy_value_of_ipw</span><span class="p">,</span> <span class="c1"># V(\pi_e)</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_ipw</span><span class="p">,</span> <span class="c1"># \pi_e(a|x)</span>
    <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards</span><span class="p">,</span> <span class="c1"># \hat{q}(x,a)</span>
    <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;se&quot;</span><span class="p">,</span> <span class="c1"># squared error</span>
<span class="p">)</span>

<span class="n">squared_errors</span> <span class="c1"># DR is the most accurate</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;DM&#39;: 0.010975474246980203,
 &#39;DR&#39;: 8.379895987394117e-06,
 &#39;IPS&#39;: 8.913836133368584e-06}
</pre></div>
</div>
</div>
</div>
<p>We can iterate the above process several times and calculate the following MSE</p>
<p><span class="math notranslate nohighlight">\( MSE (\hat{V}) := T^{-1} \sum_{t=1}^T SE (\hat{V}; \mathcal{D}_0^{(t)}) \)</span></p>
<p>where <span class="math notranslate nohighlight">\( \mathcal{D}_0^{(t)} \)</span> is the synthetic data in the t-th iteration</p>
</div>
</div>
<div class="section" id="off-policy-simulation-data">
<h2>Off-Policy Simulation Data<a class="headerlink" href="#off-policy-simulation-data" title="Permalink to this headline">¶</a></h2>
<div class="section" id="imports">
<h3>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">DataFrame</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="kn">import</span> <span class="nn">obp</span>
<span class="kn">from</span> <span class="nn">obp.dataset</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">SyntheticBanditDataset</span><span class="p">,</span>
    <span class="n">logistic_reward_function</span><span class="p">,</span>
    <span class="n">linear_behavior_policy</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">obp.policy</span> <span class="kn">import</span> <span class="n">IPWLearner</span>
<span class="kn">from</span> <span class="nn">obp.ope</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">OffPolicyEvaluation</span><span class="p">,</span> 
    <span class="n">RegressionModel</span><span class="p">,</span>
    <span class="n">InverseProbabilityWeighting</span> <span class="k">as</span> <span class="n">IPS</span><span class="p">,</span>
    <span class="n">SelfNormalizedInverseProbabilityWeighting</span> <span class="k">as</span> <span class="n">SNIPS</span><span class="p">,</span>
    <span class="n">DirectMethod</span> <span class="k">as</span> <span class="n">DM</span><span class="p">,</span>
    <span class="n">DoublyRobust</span> <span class="k">as</span> <span class="n">DR</span><span class="p">,</span>
    <span class="n">DoublyRobustWithShrinkage</span> <span class="k">as</span> <span class="n">DRos</span><span class="p">,</span>
    <span class="n">DoublyRobustWithShrinkageTuning</span> <span class="k">as</span> <span class="n">DRosTuning</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">obp.utils</span> <span class="kn">import</span> <span class="n">softmax</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="evaluatin-of-ope-estimators-part-1-easy-setting">
<h3>Evaluatin of OPE estimators (Part 1; easy setting)<a class="headerlink" href="#evaluatin-of-ope-estimators-part-1-easy-setting" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">### configurations</span>
<span class="n">num_runs</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">num_data_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">800</span><span class="p">,</span> <span class="mi">1600</span><span class="p">,</span> <span class="mi">3200</span><span class="p">,</span> <span class="mi">6400</span><span class="p">,</span> <span class="mi">12800</span><span class="p">,</span> <span class="mi">25600</span><span class="p">,</span> <span class="mi">51200</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">### define a dataset class</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">SyntheticBanditDataset</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
    <span class="n">dim_context</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">tau</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> 
    <span class="n">reward_function</span><span class="o">=</span><span class="n">logistic_reward_function</span><span class="p">,</span>
    <span class="n">behavior_policy_function</span><span class="o">=</span><span class="n">linear_behavior_policy</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1">#### training data is used to train an evaluation policy</span>
<span class="n">train_bandit_data</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="n">n_rounds</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="c1">#### test bandit data is used to approximate the ground-truth policy value</span>
<span class="n">test_bandit_data</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="n">n_rounds</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">### evaluation policy training</span>
<span class="n">ipw_learner</span> <span class="o">=</span> <span class="n">IPWLearner</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
    <span class="n">base_classifier</span><span class="o">=</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">ipw_learner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">train_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span> 
    <span class="n">action</span><span class="o">=</span><span class="n">train_bandit_data</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span> 
    <span class="n">reward</span><span class="o">=</span><span class="n">train_bandit_data</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span> 
    <span class="n">pscore</span><span class="o">=</span><span class="n">train_bandit_data</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">],</span> 
<span class="p">)</span>
<span class="n">action_dist_ipw_test</span> <span class="o">=</span> <span class="n">ipw_learner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">policy_value_of_ipw</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">calc_ground_truth_policy_value</span><span class="p">(</span>
    <span class="n">expected_reward</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">[</span><span class="s2">&quot;expected_reward&quot;</span><span class="p">],</span> 
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_ipw_test</span><span class="p">,</span> 
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">### evaluation of OPE estimators</span>
<span class="n">se_df_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">num_data</span> <span class="ow">in</span> <span class="n">num_data_list</span><span class="p">:</span>
    <span class="n">se_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;num_data=</span><span class="si">{</span><span class="n">num_data</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">):</span>
        <span class="c1">### generate validation data</span>
        <span class="n">validation_bandit_data</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span>
            <span class="n">n_rounds</span><span class="o">=</span><span class="n">num_data</span>
        <span class="p">)</span>

        <span class="c1">### make decisions on vlidation data</span>
        <span class="n">action_dist_ipw_val</span> <span class="o">=</span> <span class="n">ipw_learner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
            <span class="n">context</span><span class="o">=</span><span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="c1">### OPE using validation data</span>
        <span class="n">regression_model</span> <span class="o">=</span> <span class="n">RegressionModel</span><span class="p">(</span>
            <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> 
            <span class="n">base_model</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">estimated_rewards</span> <span class="o">=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span>
            <span class="n">context</span><span class="o">=</span><span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span> <span class="c1"># context; x</span>
            <span class="n">action</span><span class="o">=</span><span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span> <span class="c1"># action; a</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span> <span class="c1"># reward; r</span>
            <span class="n">n_folds</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># 2-fold cross fitting</span>
            <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">ope</span> <span class="o">=</span> <span class="n">OffPolicyEvaluation</span><span class="p">(</span>
            <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">validation_bandit_data</span><span class="p">,</span>
            <span class="n">ope_estimators</span><span class="o">=</span><span class="p">[</span>
                <span class="n">IPS</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;IPS&quot;</span><span class="p">),</span> 
                <span class="n">DM</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;DM&quot;</span><span class="p">),</span> 
                <span class="n">IPS</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;CIPS&quot;</span><span class="p">),</span> 
                <span class="n">SNIPS</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;SNIPS&quot;</span><span class="p">),</span>
                <span class="n">DR</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;DR&quot;</span><span class="p">),</span> 
                <span class="n">DRos</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;DRos&quot;</span><span class="p">),</span> 
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="n">squared_errors</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">evaluate_performance_of_estimators</span><span class="p">(</span>
            <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">policy_value_of_ipw</span><span class="p">,</span> <span class="c1"># V(\pi_e)</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_ipw_val</span><span class="p">,</span> <span class="c1"># \pi_e(a|x)</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards</span><span class="p">,</span> <span class="c1"># \hat{q}(x,a)</span>
            <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;se&quot;</span><span class="p">,</span> <span class="c1"># squared error</span>
        <span class="p">)</span>
        <span class="n">se_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">squared_errors</span><span class="p">)</span>
    <span class="c1">### maximum importance weight in the validation data</span>
    <span class="c1">#### a larger value indicates that the logging and evaluation policies are greatly different</span>
    <span class="n">max_iw</span> <span class="o">=</span> <span class="p">(</span><span class="n">action_dist_ipw_val</span><span class="p">[</span>
        <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;n_rounds&quot;</span><span class="p">]),</span> 
        <span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span> 
        <span class="mi">0</span>
    <span class="p">]</span> <span class="o">/</span> <span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">tqdm</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;maximum importance weight=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">max_iw</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1">### summarize results</span>
    <span class="n">se_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">se_list</span><span class="p">)</span><span class="o">.</span><span class="n">stack</span><span class="p">())</span>\
        <span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;level_1&quot;</span><span class="p">:</span> <span class="s2">&quot;est&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;se&quot;</span><span class="p">})</span>
    <span class="n">se_df</span><span class="p">[</span><span class="s2">&quot;num_data&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_data</span>
    <span class="n">se_df_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">se_df</span><span class="p">)</span>
    <span class="n">tqdm</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;=====&quot;</span> <span class="o">*</span> <span class="mi">15</span><span class="p">)</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># aggregate all results </span>
<span class="n">result_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">se_df_list</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=100...: 100%|██████████| 200/200 [00:10&lt;00:00, 18.22it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=6.32599

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=200...: 100%|██████████| 200/200 [00:09&lt;00:00, 20.21it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=6.32599

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=400...: 100%|██████████| 200/200 [00:10&lt;00:00, 18.76it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=8.6407

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=800...: 100%|██████████| 200/200 [00:15&lt;00:00, 12.92it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=18.60611

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=1600...: 100%|██████████| 200/200 [00:30&lt;00:00,  6.66it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=13.29143

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=3200...: 100%|██████████| 200/200 [00:40&lt;00:00,  4.96it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=13.29143

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=6400...: 100%|██████████| 200/200 [01:02&lt;00:00,  3.22it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=13.29143

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=12800...: 100%|██████████| 200/200 [01:42&lt;00:00,  1.95it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=18.60611

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=25600...: 100%|██████████| 200/200 [03:05&lt;00:00,  1.08it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=18.60611

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=51200...: 100%|██████████| 200/200 [05:49&lt;00:00,  1.75s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=18.60611

===========================================================================
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="visualize-results">
<h3>Visualize Results<a class="headerlink" href="#visualize-results" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># figure configs</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;(est == &#39;DM&#39; or est == &#39;IPS&#39;) and num_data &lt;= 6400&quot;</span>
<span class="n">xlabels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">1600</span><span class="p">,</span> <span class="mi">6400</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">dashes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;num_data&quot;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">&quot;se&quot;</span><span class="p">,</span>
    <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;est&quot;</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">result_df</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s2">&quot;(est == &#39;DM&#39; or est == &#39;IPS&#39;) and num_data &lt;= 12800&quot;</span><span class="p">),</span>
<span class="p">)</span>
<span class="c1"># title and legend</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;IPS&quot;</span><span class="p">,</span> <span class="s2">&quot;DM&quot;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="c1"># yaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;mean squared error (MSE)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="o">-</span><span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># xaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;number of samples in the log data&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">xlabels</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">xlabels</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># figure configs</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;(est == &#39;DM&#39; or est == &#39;CIPS&#39; or est == &#39;IPS&#39; or est == &#39;SNIPS&#39;)&quot;</span> 
<span class="n">query</span> <span class="o">+=</span> <span class="s2">&quot;and num_data &lt;= 6400&quot;</span>
<span class="n">xlabels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">1600</span><span class="p">,</span> <span class="mi">6400</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">dashes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;num_data&quot;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">&quot;se&quot;</span><span class="p">,</span>
    <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;est&quot;</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">result_df</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query</span><span class="p">),</span>
<span class="p">)</span>
<span class="c1"># title and legend</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;IPS&quot;</span><span class="p">,</span> <span class="s2">&quot;DM&quot;</span><span class="p">,</span> <span class="s2">&quot;CIPS&quot;</span><span class="p">,</span> <span class="s2">&quot;SNIPS&quot;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">)</span>
<span class="c1"># yaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;mean squared error (MSE)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="o">-</span><span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># xaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;number of samples in the log data&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">xlabels</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">xlabels</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_34_0.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_34_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># figure configs</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;(est == &#39;DM&#39; or est == &#39;IPS&#39; or est == &#39;SNIPS&#39; or est == &#39;CIPS&#39; or est == &#39;DR&#39;)&quot;</span>
<span class="n">query</span> <span class="o">+=</span> <span class="s2">&quot;and num_data &lt;= 6400&quot;</span>
<span class="n">xlabels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">1600</span><span class="p">,</span> <span class="mi">6400</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">dashes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;num_data&quot;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">&quot;se&quot;</span><span class="p">,</span>
    <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;est&quot;</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">result_df</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query</span><span class="p">),</span>
<span class="p">)</span>
<span class="c1"># title and legend</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;IPS&quot;</span><span class="p">,</span> <span class="s2">&quot;DM&quot;</span><span class="p">,</span> <span class="s2">&quot;CIPS&quot;</span><span class="p">,</span> <span class="s2">&quot;SNIPS&quot;</span><span class="p">,</span> <span class="s2">&quot;DR&quot;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">)</span>
<span class="c1"># yaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;mean squared error (MSE)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="o">-</span><span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># xaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;number of samples in the log data&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">xlabels</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">xlabels</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_35_0.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_35_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># figure configs</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;num_data &lt;= 6400&quot;</span>
<span class="n">xlabels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">1600</span><span class="p">,</span> <span class="mi">6400</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">dashes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;num_data&quot;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">&quot;se&quot;</span><span class="p">,</span>
    <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;est&quot;</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">result_df</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query</span><span class="p">),</span>
<span class="p">)</span>
<span class="c1"># title and legend</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;IPS&quot;</span><span class="p">,</span> <span class="s2">&quot;DM&quot;</span><span class="p">,</span> <span class="s2">&quot;CIPS&quot;</span><span class="p">,</span> <span class="s2">&quot;SNIPS&quot;</span><span class="p">,</span> <span class="s2">&quot;DR&quot;</span><span class="p">,</span> <span class="s2">&quot;DRos&quot;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="c1"># yaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;mean squared error (MSE)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="o">-</span><span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># xaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;number of samples in the log data&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">xlabels</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">xlabels</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_36_0.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_36_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">xlabels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">6400</span><span class="p">,</span> <span class="mi">25600</span><span class="p">,</span> <span class="mi">51200</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">dashes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;num_data&quot;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">&quot;se&quot;</span><span class="p">,</span>
    <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;est&quot;</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">result_df</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># title and legend</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;IPS&quot;</span><span class="p">,</span> <span class="s2">&quot;DM&quot;</span><span class="p">,</span> <span class="s2">&quot;CIPS&quot;</span><span class="p">,</span> <span class="s2">&quot;SNIPS&quot;</span><span class="p">,</span> <span class="s2">&quot;DR&quot;</span><span class="p">,</span> <span class="s2">&quot;DRos&quot;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="c1"># yaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;mean squared error (MSE)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="o">-</span><span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># xaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;number of samples in the log data&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">xlabels</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">xlabels</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_37_0.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_37_0.png" />
</div>
</div>
</div>
<div class="section" id="evaluation-of-ope-estimators-part-2-challenging-setting">
<h3>Evaluation of OPE estimators (Part 2; challenging setting)<a class="headerlink" href="#evaluation-of-ope-estimators-part-2-challenging-setting" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">### configurations</span>
<span class="n">num_runs</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">num_data_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">800</span><span class="p">,</span> <span class="mi">1600</span><span class="p">,</span> <span class="mi">3200</span><span class="p">,</span> <span class="mi">6400</span><span class="p">,</span> <span class="mi">12800</span><span class="p">,</span> <span class="mi">25600</span><span class="p">,</span> <span class="mi">51200</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">### define a dataset class</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">SyntheticBanditDataset</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
    <span class="n">dim_context</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">tau</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> 
    <span class="n">reward_function</span><span class="o">=</span><span class="n">logistic_reward_function</span><span class="p">,</span>
    <span class="n">behavior_policy_function</span><span class="o">=</span><span class="n">linear_behavior_policy</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1">#### training data is used to train an evaluation policy</span>
<span class="n">train_bandit_data</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="n">n_rounds</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="c1">#### test bandit data is used to approximate the ground-truth policy value</span>
<span class="n">test_bandit_data</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="n">n_rounds</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">### evaluation policy training</span>
<span class="n">ipw_learner</span> <span class="o">=</span> <span class="n">IPWLearner</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
    <span class="n">base_classifier</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">ipw_learner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">train_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span> 
    <span class="n">action</span><span class="o">=</span><span class="n">train_bandit_data</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span> 
    <span class="n">reward</span><span class="o">=</span><span class="n">train_bandit_data</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span> 
    <span class="n">pscore</span><span class="o">=</span><span class="n">train_bandit_data</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">],</span> 
<span class="p">)</span>
<span class="n">action_dist_ipw_test</span> <span class="o">=</span> <span class="n">ipw_learner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">policy_value_of_ipw</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">calc_ground_truth_policy_value</span><span class="p">(</span>
    <span class="n">expected_reward</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">[</span><span class="s2">&quot;expected_reward&quot;</span><span class="p">],</span> 
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_ipw_test</span><span class="p">,</span> 
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">### evaluation of OPE estimators</span>
<span class="n">se_df_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">num_data</span> <span class="ow">in</span> <span class="n">num_data_list</span><span class="p">:</span>
    <span class="n">se_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;num_data=</span><span class="si">{</span><span class="n">num_data</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">):</span>
        <span class="c1">### generate validation data</span>
        <span class="n">validation_bandit_data</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span>
            <span class="n">n_rounds</span><span class="o">=</span><span class="n">num_data</span>
        <span class="p">)</span>

        <span class="c1">### make decisions on vlidation data</span>
        <span class="n">action_dist_ipw_val</span> <span class="o">=</span> <span class="n">ipw_learner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
            <span class="n">context</span><span class="o">=</span><span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="c1">### OPE using validation data</span>
        <span class="n">regression_model</span> <span class="o">=</span> <span class="n">RegressionModel</span><span class="p">(</span>
            <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> 
            <span class="n">base_model</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">estimated_rewards</span> <span class="o">=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span>
            <span class="n">context</span><span class="o">=</span><span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span> <span class="c1"># context; x</span>
            <span class="n">action</span><span class="o">=</span><span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span> <span class="c1"># action; a</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span> <span class="c1"># reward; r</span>
            <span class="n">n_folds</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># 2-fold cross fitting</span>
            <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">ope</span> <span class="o">=</span> <span class="n">OffPolicyEvaluation</span><span class="p">(</span>
            <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">validation_bandit_data</span><span class="p">,</span>
            <span class="n">ope_estimators</span><span class="o">=</span><span class="p">[</span>
                <span class="n">IPS</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;IPS&quot;</span><span class="p">),</span> 
                <span class="n">DM</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;DM&quot;</span><span class="p">),</span> 
                <span class="n">IPS</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;CIPS&quot;</span><span class="p">),</span> 
                <span class="n">SNIPS</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;SNIPS&quot;</span><span class="p">),</span>
                <span class="n">DR</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;DR&quot;</span><span class="p">),</span> 
                <span class="n">DRos</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;DRos&quot;</span><span class="p">),</span> 
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="n">squared_errors</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">evaluate_performance_of_estimators</span><span class="p">(</span>
            <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">policy_value_of_ipw</span><span class="p">,</span> <span class="c1"># V(\pi_e)</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_ipw_val</span><span class="p">,</span> <span class="c1"># \pi_e(a|x)</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards</span><span class="p">,</span> <span class="c1"># \hat{q}(x,a)</span>
            <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;se&quot;</span><span class="p">,</span> <span class="c1"># squared error</span>
        <span class="p">)</span>
        <span class="n">se_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">squared_errors</span><span class="p">)</span>
    <span class="c1">### maximum importance weight in the validation data</span>
    <span class="c1">#### a larger value indicates that the logging and evaluation policies are greatly different</span>
    <span class="n">max_iw</span> <span class="o">=</span> <span class="p">(</span><span class="n">action_dist_ipw_val</span><span class="p">[</span>
        <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;n_rounds&quot;</span><span class="p">]),</span> 
        <span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span> 
        <span class="mi">0</span>
    <span class="p">]</span> <span class="o">/</span> <span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">tqdm</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;maximum importance weight=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">max_iw</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1">### summarize results</span>
    <span class="n">se_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">se_list</span><span class="p">)</span><span class="o">.</span><span class="n">stack</span><span class="p">())</span>\
        <span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;level_1&quot;</span><span class="p">:</span> <span class="s2">&quot;est&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;se&quot;</span><span class="p">})</span>
    <span class="n">se_df</span><span class="p">[</span><span class="s2">&quot;num_data&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_data</span>
    <span class="n">se_df_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">se_df</span><span class="p">)</span>
    <span class="n">tqdm</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;=====&quot;</span> <span class="o">*</span> <span class="mi">15</span><span class="p">)</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># aggregate all results </span>
<span class="n">result_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">se_df_list</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=100...: 100%|██████████| 200/200 [00:08&lt;00:00, 24.35it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=18.60611

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=200...: 100%|██████████| 200/200 [00:06&lt;00:00, 30.96it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=18.60611

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=400...: 100%|██████████| 200/200 [00:05&lt;00:00, 35.05it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=291.00022

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=800...: 100%|██████████| 200/200 [00:08&lt;00:00, 23.55it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=475.13758

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=1600...: 100%|██████████| 200/200 [00:16&lt;00:00, 12.32it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=475.13758

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=3200...: 100%|██████████| 200/200 [00:18&lt;00:00, 10.67it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=475.13758

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=6400...: 100%|██████████| 200/200 [00:27&lt;00:00,  7.28it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=475.13758

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=12800...: 100%|██████████| 200/200 [00:43&lt;00:00,  4.63it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=475.13758

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=25600...: 100%|██████████| 200/200 [01:16&lt;00:00,  2.62it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=475.13758

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=51200...: 100%|██████████| 200/200 [02:17&lt;00:00,  1.46it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=475.13758

===========================================================================
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id1">
<h3>Visualize Results<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># figure configs</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;(est == &#39;DM&#39; or est == &#39;IPS&#39; or est == &#39;SNIPS&#39; or est == &#39;CIPS&#39; or est == &#39;DR&#39;)&quot;</span>
<span class="n">query</span> <span class="o">+=</span> <span class="s2">&quot;and num_data &lt;= 6400&quot;</span>
<span class="n">xlabels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">1600</span><span class="p">,</span> <span class="mi">6400</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">dashes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;num_data&quot;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">&quot;se&quot;</span><span class="p">,</span>
    <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;est&quot;</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">result_df</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query</span><span class="p">),</span>
<span class="p">)</span>
<span class="c1"># title and legend</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;IPS&quot;</span><span class="p">,</span> <span class="s2">&quot;DM&quot;</span><span class="p">,</span> <span class="s2">&quot;CIPS&quot;</span><span class="p">,</span> <span class="s2">&quot;SNIPS&quot;</span><span class="p">,</span> <span class="s2">&quot;DR&quot;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">)</span>
<span class="c1"># yaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;mean squared error (MSE)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="o">-</span><span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># xaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;number of samples in the log data&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">xlabels</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">xlabels</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;num_data &lt;= 6400&quot;</span>
<span class="n">xlabels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">1600</span><span class="p">,</span> <span class="mi">6400</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">dashes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;num_data&quot;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">&quot;se&quot;</span><span class="p">,</span>
    <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;est&quot;</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">result_df</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query</span><span class="p">),</span>
<span class="p">)</span>
<span class="c1"># title and legend</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;IPS&quot;</span><span class="p">,</span> <span class="s2">&quot;DM&quot;</span><span class="p">,</span> <span class="s2">&quot;CIPS&quot;</span><span class="p">,</span> <span class="s2">&quot;SNIPS&quot;</span><span class="p">,</span> <span class="s2">&quot;DR&quot;</span><span class="p">,</span> <span class="s2">&quot;DRos&quot;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="c1"># yaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;mean squared error (MSE)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="o">-</span><span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># xaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;number of samples in the log data&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">xlabels</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">xlabels</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_45_0.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_45_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">xlabels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">6400</span><span class="p">,</span> <span class="mi">25600</span><span class="p">,</span> <span class="mi">51200</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">dashes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;num_data&quot;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">&quot;se&quot;</span><span class="p">,</span>
    <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;est&quot;</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">result_df</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># title and legend</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;IPS&quot;</span><span class="p">,</span> <span class="s2">&quot;DM&quot;</span><span class="p">,</span> <span class="s2">&quot;CIPS&quot;</span><span class="p">,</span> <span class="s2">&quot;SNIPS&quot;</span><span class="p">,</span> <span class="s2">&quot;DR&quot;</span><span class="p">,</span> <span class="s2">&quot;DRos&quot;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="c1"># yaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;mean squared error (MSE)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="o">-</span><span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># xaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;number of samples in the log data&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">xlabels</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">xlabels</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_46_0.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_46_0.png" />
</div>
</div>
</div>
<div class="section" id="hyperparameter-tuning-of-ope-estimators">
<h3>Hyperparameter Tuning of OPE estimators<a class="headerlink" href="#hyperparameter-tuning-of-ope-estimators" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">### configurations</span>
<span class="n">num_runs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">num_data_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">800</span><span class="p">,</span> <span class="mi">1600</span><span class="p">,</span> <span class="mi">3200</span><span class="p">,</span> <span class="mi">6400</span><span class="p">,</span> <span class="mi">12800</span><span class="p">,</span> <span class="mi">25600</span><span class="p">,</span> <span class="mi">51200</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">### define a dataset class</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">SyntheticBanditDataset</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
    <span class="n">dim_context</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">tau</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> 
    <span class="n">reward_function</span><span class="o">=</span><span class="n">logistic_reward_function</span><span class="p">,</span>
    <span class="n">behavior_policy_function</span><span class="o">=</span><span class="n">linear_behavior_policy</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1">#### training data is used to train an evaluation policy</span>
<span class="n">train_bandit_data</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="n">n_rounds</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="c1">#### test bandit data is used to approximate the ground-truth policy value</span>
<span class="n">test_bandit_data</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="n">n_rounds</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">### evaluation policy training</span>
<span class="n">ipw_learner</span> <span class="o">=</span> <span class="n">IPWLearner</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
    <span class="n">base_classifier</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">ipw_learner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">train_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span> 
    <span class="n">action</span><span class="o">=</span><span class="n">train_bandit_data</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span> 
    <span class="n">reward</span><span class="o">=</span><span class="n">train_bandit_data</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span> 
    <span class="n">pscore</span><span class="o">=</span><span class="n">train_bandit_data</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">],</span> 
<span class="p">)</span>
<span class="n">action_dist_ipw_test</span> <span class="o">=</span> <span class="n">ipw_learner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">policy_value_of_ipw</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">calc_ground_truth_policy_value</span><span class="p">(</span>
    <span class="n">expected_reward</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">[</span><span class="s2">&quot;expected_reward&quot;</span><span class="p">],</span> 
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_ipw_test</span><span class="p">,</span> 
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">### evaluation of OPE estimators</span>
<span class="n">se_df_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">num_data</span> <span class="ow">in</span> <span class="n">num_data_list</span><span class="p">:</span>
    <span class="n">se_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;num_data=</span><span class="si">{</span><span class="n">num_data</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">):</span>
        <span class="c1">### generate validation data</span>
        <span class="n">validation_bandit_data</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span>
            <span class="n">n_rounds</span><span class="o">=</span><span class="n">num_data</span>
        <span class="p">)</span>

        <span class="c1">### make decisions on vlidation data</span>
        <span class="n">action_dist_ipw_val</span> <span class="o">=</span> <span class="n">ipw_learner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
            <span class="n">context</span><span class="o">=</span><span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="c1">### OPE using validation data</span>
        <span class="n">regression_model</span> <span class="o">=</span> <span class="n">RegressionModel</span><span class="p">(</span>
            <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> 
            <span class="n">base_model</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">estimated_rewards</span> <span class="o">=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span>
            <span class="n">context</span><span class="o">=</span><span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span> <span class="c1"># context; x</span>
            <span class="n">action</span><span class="o">=</span><span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span> <span class="c1"># action; a</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span> <span class="c1"># reward; r</span>
            <span class="n">n_folds</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># 2-fold cross fitting</span>
            <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">ope</span> <span class="o">=</span> <span class="n">OffPolicyEvaluation</span><span class="p">(</span>
            <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">validation_bandit_data</span><span class="p">,</span>
            <span class="n">ope_estimators</span><span class="o">=</span><span class="p">[</span>
                <span class="n">DRos</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;DRos (1)&quot;</span><span class="p">),</span> 
                <span class="n">DRos</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;DRos (100)&quot;</span><span class="p">),</span> 
                <span class="n">DRos</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;DRos (10000)&quot;</span><span class="p">),</span>
                <span class="n">DRosTuning</span><span class="p">(</span>
                    <span class="n">use_bias_upper_bound</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">lambdas</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10002</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> 
                    <span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;DRos (tuning)&quot;</span>
                <span class="p">),</span> 
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="n">squared_errors</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">evaluate_performance_of_estimators</span><span class="p">(</span>
            <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">policy_value_of_ipw</span><span class="p">,</span> <span class="c1"># V(\pi_e)</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_ipw_val</span><span class="p">,</span> <span class="c1"># \pi_e(a|x)</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards</span><span class="p">,</span> <span class="c1"># \hat{q}(x,a)</span>
            <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;se&quot;</span><span class="p">,</span> <span class="c1"># squared error</span>
        <span class="p">)</span>
        <span class="n">se_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">squared_errors</span><span class="p">)</span>
    <span class="c1">### maximum importance weight in the validation data</span>
    <span class="c1">#### a larger value indicates that the logging and evaluation policies are greatly different</span>
    <span class="n">max_iw</span> <span class="o">=</span> <span class="p">(</span><span class="n">action_dist_ipw_val</span><span class="p">[</span>
        <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;n_rounds&quot;</span><span class="p">]),</span> 
        <span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span> 
        <span class="mi">0</span>
    <span class="p">]</span> <span class="o">/</span> <span class="n">validation_bandit_data</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">tqdm</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;maximum importance weight=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">max_iw</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1">### summarize results</span>
    <span class="n">se_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">se_list</span><span class="p">)</span><span class="o">.</span><span class="n">stack</span><span class="p">())</span>\
        <span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;level_1&quot;</span><span class="p">:</span> <span class="s2">&quot;est&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;se&quot;</span><span class="p">})</span>
    <span class="n">se_df</span><span class="p">[</span><span class="s2">&quot;num_data&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_data</span>
    <span class="n">se_df_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">se_df</span><span class="p">)</span>
    <span class="n">tqdm</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;=====&quot;</span> <span class="o">*</span> <span class="mi">15</span><span class="p">)</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># aggregate all results </span>
<span class="n">result_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">se_df_list</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=100...: 100%|██████████| 100/100 [00:04&lt;00:00, 23.82it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=18.60611

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=200...: 100%|██████████| 100/100 [00:04&lt;00:00, 24.78it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=18.60611

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=400...: 100%|██████████| 100/100 [00:04&lt;00:00, 24.94it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=18.60611

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=800...: 100%|██████████| 100/100 [00:05&lt;00:00, 18.17it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=18.60611

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=1600...: 100%|██████████| 100/100 [00:09&lt;00:00, 10.02it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=475.13758

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=3200...: 100%|██████████| 100/100 [00:13&lt;00:00,  7.33it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=475.13758

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=6400...: 100%|██████████| 100/100 [00:18&lt;00:00,  5.29it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=475.13758

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=12800...: 100%|██████████| 100/100 [00:30&lt;00:00,  3.23it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=475.13758

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=25600...: 100%|██████████| 100/100 [01:03&lt;00:00,  1.57it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=475.13758

===========================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_data=51200...: 100%|██████████| 100/100 [02:06&lt;00:00,  1.27s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>maximum importance weight=475.13758

===========================================================================
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id2">
<h3>Visualize Results<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;est == &#39;DRos (1)&#39; or est == &#39;DRos (100)&#39; or est == &#39;DRos (10000)&#39;&quot;</span>
<span class="n">xlabels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">6400</span><span class="p">,</span> <span class="mi">25600</span><span class="p">,</span> <span class="mi">51200</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">dashes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;num_data&quot;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">&quot;se&quot;</span><span class="p">,</span>
    <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;est&quot;</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">result_df</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query</span><span class="p">),</span>
<span class="p">)</span>
<span class="c1"># title and legend</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span>
    <span class="p">[</span><span class="s2">&quot;DRos (1)&quot;</span><span class="p">,</span> <span class="s2">&quot;DRos (100)&quot;</span><span class="p">,</span> <span class="s2">&quot;DRos (10000)&quot;</span><span class="p">],</span> 
    <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># yaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">3e-4</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;mean squared error (MSE)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># xaxis</span>
<span class="c1"># ax.set_xscale(&quot;log&quot;)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;number of samples in the log data&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">xlabels</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">xlabels</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_53_0.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_53_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">xlabels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">6400</span><span class="p">,</span> <span class="mi">25600</span><span class="p">,</span> <span class="mi">51200</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">dashes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;num_data&quot;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">&quot;se&quot;</span><span class="p">,</span>
    <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;est&quot;</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">result_df</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># title and legend</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span>
    <span class="p">[</span><span class="s2">&quot;DRos (1)&quot;</span><span class="p">,</span> <span class="s2">&quot;DRos (100)&quot;</span><span class="p">,</span> <span class="s2">&quot;DRos (10000)&quot;</span><span class="p">,</span> <span class="s2">&quot;DRos (tuning)&quot;</span><span class="p">],</span> 
    <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># yaxis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">3e-4</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;mean squared error (MSE)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># xaxis</span>
<span class="c1"># ax.set_xscale(&quot;log&quot;)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;number of samples in the log data&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">xlabels</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">xlabels</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_54_0.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_54_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="multi-class-classificatoin-data">
<h2>Multi-class Classificatoin Data<a class="headerlink" href="#multi-class-classificatoin-data" title="Permalink to this headline">¶</a></h2>
<p>This section provides an example of conducting OPE of an evaluation policy using classification data as logged bandit data.
It is quite common to conduct OPE experiments using classification data. Appendix G of <a class="reference external" href="https://arxiv.org/abs/1802.03493">Farajtabar et al.(2018)</a> describes how to conduct OPE experiments with classification data in detail.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="kn">import</span> <span class="nn">obp</span>
<span class="kn">from</span> <span class="nn">obp.dataset</span> <span class="kn">import</span> <span class="n">MultiClassToBanditReduction</span>
<span class="kn">from</span> <span class="nn">obp.ope</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">OffPolicyEvaluation</span><span class="p">,</span> 
    <span class="n">RegressionModel</span><span class="p">,</span>
    <span class="n">InverseProbabilityWeighting</span> <span class="k">as</span> <span class="n">IPS</span><span class="p">,</span>
    <span class="n">DirectMethod</span> <span class="k">as</span> <span class="n">DM</span><span class="p">,</span>
    <span class="n">DoublyRobust</span> <span class="k">as</span> <span class="n">DR</span><span class="p">,</span> 
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="bandit-reduction">
<h3>Bandit Reduction<a class="headerlink" href="#bandit-reduction" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">obp.dataset.MultiClassToBanditReduction</span></code> is an easy-to-use for transforming classification data to bandit data.
It takes</p>
<ul class="simple">
<li><p>feature vectors (<code class="docutils literal notranslate"><span class="pre">X</span></code>)</p></li>
<li><p>class labels (<code class="docutils literal notranslate"><span class="pre">y</span></code>)</p></li>
<li><p>classifier to construct behavior policy (<code class="docutils literal notranslate"><span class="pre">base_classifier_b</span></code>)</p></li>
<li><p>paramter of behavior policy (<code class="docutils literal notranslate"><span class="pre">alpha_b</span></code>)</p></li>
</ul>
<p>as its inputs and generates a bandit data that can be used to evaluate the performance of decision making policies (obtained by <code class="docutils literal notranslate"><span class="pre">off-policy</span> <span class="pre">learning</span></code>) and OPE estimators.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># load raw digits data</span>
<span class="c1"># `return_X_y` splits feature vectors and labels, instead of returning a Bunch object</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># convert the raw classification data into a logged bandit dataset</span>
<span class="c1"># we construct a behavior policy using Logistic Regression and parameter alpha_b</span>
<span class="c1"># given a pair of a feature vector and a label (x, c), create a pair of a context vector and reward (x, r)</span>
<span class="c1"># where r = 1 if the output of the behavior policy is equal to c and r = 0 otherwise</span>
<span class="c1"># please refer to https://zr-obp.readthedocs.io/en/latest/_autosummary/obp.dataset.multiclass.html for the details</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">MultiClassToBanditReduction</span><span class="p">(</span>
    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
    <span class="n">base_classifier_b</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">),</span>
    <span class="n">alpha_b</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">dataset_name</span><span class="o">=</span><span class="s2">&quot;digits&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># split the original data into training and evaluation sets</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">split_train_eval</span><span class="p">(</span><span class="n">eval_size</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># obtain logged bandit data generated by behavior policy</span>
<span class="n">bandit_data</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">)</span>

<span class="c1"># `bandit_data` is a dictionary storing logged bandit feedback</span>
<span class="n">bandit_data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;action&#39;: array([6, 8, 5, ..., 2, 5, 9]),
 &#39;context&#39;: array([[ 0.,  0.,  0., ..., 16.,  1.,  0.],
        [ 0.,  0.,  7., ..., 16.,  3.,  0.],
        [ 0.,  0., 12., ...,  8.,  0.,  0.],
        ...,
        [ 0.,  1., 13., ...,  8., 11.,  1.],
        [ 0.,  0., 15., ...,  0.,  0.,  0.],
        [ 0.,  0.,  4., ..., 15.,  3.,  0.]]),
 &#39;n_actions&#39;: 10,
 &#39;n_rounds&#39;: 1258,
 &#39;position&#39;: None,
 &#39;pscore&#39;: array([0.82, 0.82, 0.82, ..., 0.82, 0.82, 0.82]),
 &#39;reward&#39;: array([1., 1., 1., ..., 1., 1., 1.])}
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="off-policy-learning">
<h3>Off-Policy Learning<a class="headerlink" href="#off-policy-learning" title="Permalink to this headline">¶</a></h3>
<p>After generating logged bandit data, we now obtain an evaluation policy using the training set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># obtain action choice probabilities by an evaluation policy</span>
<span class="c1"># we construct an evaluation policy using Random Forest and parameter alpha_e</span>
<span class="n">action_dist</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_action_dist_by_eval_policy</span><span class="p">(</span>
    <span class="n">base_classifier_e</span><span class="o">=</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">),</span>
    <span class="n">alpha_e</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># which action to take for each context (a probability distribution over actions)</span>
<span class="n">action_dist</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],
       [0.01, 0.01, 0.01, ..., 0.01, 0.91, 0.01],
       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],
       ...,
       [0.01, 0.01, 0.91, ..., 0.01, 0.01, 0.01],
       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],
       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.91]])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id3">
<h3>Obtain a Reward Estimator<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">obp.ope.RegressionModel</span></code> simplifies the process of reward modeling</p>
<p><span class="math notranslate nohighlight">\(r(x,a) = \mathbb{E} [r \mid x, a] \approx \hat{r}(x,a)\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">regression_model</span> <span class="o">=</span> <span class="n">RegressionModel</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="c1"># number of actions; |A|</span>
    <span class="n">base_model</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">),</span> <span class="c1"># any sklearn classifier</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">estimated_rewards</span> <span class="o">=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
    <span class="n">action</span><span class="o">=</span><span class="n">bandit_data</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span>
    <span class="n">reward</span><span class="o">=</span><span class="n">bandit_data</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span>
    <span class="n">position</span><span class="o">=</span><span class="n">bandit_data</span><span class="p">[</span><span class="s2">&quot;position&quot;</span><span class="p">],</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">estimated_rewards</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="c1"># \hat{q}(x,a)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.91377194, 0.87015455, 0.91602424, ..., 0.81593598, 0.89834451,
        0.93694249],
       [0.88793388, 0.8336263 , 0.8907804 , ..., 0.76821779, 0.86854866,
        0.91741997],
       [0.74709488, 0.65133633, 0.75252172, ..., 0.55271454, 0.71126924,
        0.80552132],
       ...,
       [0.81387138, 0.7344083 , 0.81821398, ..., 0.64653235, 0.78478007,
        0.85976671],
       [0.96945004, 0.95253372, 0.97029528, ..., 0.92994418, 0.96358725,
        0.97801909],
       [0.58369616, 0.46996265, 0.59070835, ..., 0.36968511, 0.53900653,
        0.66283518]])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id4">
<h3>Off-Policy Evaluation (OPE)<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>OPE attempts to estimate the performance of evaluation policies using their action choice probabilities.</p>
<p>Here, we evaluate/compare the OPE performance (estimation accuracy) of</p>
<ul class="simple">
<li><p><strong>Inverse Propensity Score (IPS)</strong></p></li>
<li><p><strong>DirectMethod (DM)</strong></p></li>
<li><p><strong>Doubly Robust (DR)</strong></p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">obp.ope.OffPolicyEvaluation</span></code> simplifies the OPE process</p>
<p><span class="math notranslate nohighlight">\(V(\pi_e) \approx \hat{V} (\pi_e; \mathcal{D}_0, \theta)\)</span> using DM, IPS, and DR</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ope</span> <span class="o">=</span> <span class="n">OffPolicyEvaluation</span><span class="p">(</span>
    <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">bandit_data</span><span class="p">,</span> <span class="c1"># bandit data</span>
    <span class="n">ope_estimators</span><span class="o">=</span><span class="p">[</span>
        <span class="n">IPS</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;IPS&quot;</span><span class="p">),</span> 
        <span class="n">DM</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;DM&quot;</span><span class="p">),</span> 
        <span class="n">DR</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;DR&quot;</span><span class="p">),</span>
    <span class="p">]</span> <span class="c1"># used estimators</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">estimated_policy_value</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">estimate_policy_values</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span> <span class="c1"># \pi_e(a|x)</span>
    <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards</span><span class="p">,</span> <span class="c1"># \hat{q}</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># OPE results given by the three estimators</span>
<span class="n">estimated_policy_value</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;DM&#39;: 0.7881820246208123, &#39;DR&#39;: 0.8750052230298789, &#39;IPS&#39;: 0.8902729846058397}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># estimate the policy value of IPWLearner with Logistic Regression</span>
<span class="n">estimated_policy_value</span><span class="p">,</span> <span class="n">estimated_interval</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">summarize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
    <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">estimated_interval</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>         mean  95.0% CI (lower)  95.0% CI (upper)
IPS  0.892168          0.835530          0.990578
DM   0.788279          0.782205          0.794191
DR   0.872916          0.816735          0.913747 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># visualize policy values of the evaluation policy estimated by the three OPE estimators</span>
<span class="n">ope</span><span class="o">.</span><span class="n">visualize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
    <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="c1"># number of resampling performed in the bootstrap procedure</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_76_0.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_76_0.png" />
</div>
</div>
</div>
<div class="section" id="evaluation-of-ope-estimators">
<h3>Evaluation of OPE estimators<a class="headerlink" href="#evaluation-of-ope-estimators" title="Permalink to this headline">¶</a></h3>
<p>Our final step is <strong>the evaluation of OPE</strong>, which evaluates and compares the estimation accuracy of OPE estimators.</p>
<p>With the multi-class classification data, we can calculate the ground-truth policy value of the evaluation policy.
Therefore, we can compare the policy values estimated by OPE estimators with the ground-turth to evaluate OPE estimators.</p>
<p><strong>Approximate the Ground-truth Policy Value</strong></p>
<p><span class="math notranslate nohighlight">\(V(\pi) \approx \frac{1}{|\mathcal{D}_{te}|} \sum_{i=1}^{|\mathcal{D}_{te}|} \mathbb{E}_{a \sim \pi(a|x_i)} [r(x_i, a)], \; \, where \; \, r(x,a) := \mathbb{E}_{r \sim p(r|x,a)} [r]\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># calculate the ground-truth performance of the evaluation policy</span>
<span class="n">true_policy_value</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">calc_ground_truth_policy_value</span><span class="p">(</span><span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">)</span>

<span class="n">true_policy_value</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8770906200317964
</pre></div>
</div>
</div>
</div>
<p><strong>Evaluation of OPE</strong></p>
<p>Now, let’s evaluate the OPE performance (estimation accuracy) of the three estimators</p>
<p><span class="math notranslate nohighlight">\(SE (\hat{V}; \mathcal{D}_0) := \left( V(\pi_e) - \hat{V} (\pi_e; \mathcal{D}_0, \theta) \right)^2\)</span>,     (squared error of <span class="math notranslate nohighlight">\(\hat{V}\)</span>)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">squared_errors</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">evaluate_performance_of_estimators</span><span class="p">(</span>
    <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">true_policy_value</span><span class="p">,</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
    <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards</span><span class="p">,</span>
    <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;se&quot;</span><span class="p">,</span> <span class="c1"># squared error</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">squared_errors</span> <span class="c1"># DR is the most accurate </span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;DM&#39;: 0.007904738337954055,
 &#39;DR&#39;: 4.34888065560631e-06,
 &#39;IPS&#39;: 0.0001737747357629922}
</pre></div>
</div>
</div>
</div>
<p>We can iterate the above process several times and calculate the following MSE</p>
<p><span class="math notranslate nohighlight">\(MSE (\hat{V}) := T^{-1} \sum_{t=1}^T SE (\hat{V}; \mathcal{D}_0^{(t)}) \)</span></p>
<p>where <span class="math notranslate nohighlight">\(\mathcal{D}_0^{(t)}\)</span> is the synthetic data in the <span class="math notranslate nohighlight">\(t\)</span>-th iteration</p>
</div>
</div>
<div class="section" id="obp-library-workshop-tutorial-on-zozotown-open-bandit-dataset">
<h2>OBP Library Workshop Tutorial on ZOZOTOWN Open-Bandit Dataset<a class="headerlink" href="#obp-library-workshop-tutorial-on-zozotown-open-bandit-dataset" title="Permalink to this headline">¶</a></h2>
<p>This section demonstrates an example of conducting OPE of Bernoulli Thompson Sampling (BernoulliTS) as an evaluation policy. We use some OPE estimators and logged bandit feedback generated by running the Random policy (behavior policy) on the ZOZOTOWN platform. We also evaluate and compare the OPE performance (accuracy) of several estimators.</p>
<p>The example consists of the follwoing four major steps:</p>
<ul class="simple">
<li><p>(1) Data Loading and Preprocessing</p></li>
<li><p>(2) Replicating Production Policy</p></li>
<li><p>(3) Off-Policy Evaluation (OPE)</p></li>
<li><p>(4) Evaluation of OPE</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="kn">import</span> <span class="nn">obp</span>
<span class="kn">from</span> <span class="nn">obp.dataset</span> <span class="kn">import</span> <span class="n">OpenBanditDataset</span>
<span class="kn">from</span> <span class="nn">obp.policy</span> <span class="kn">import</span> <span class="n">BernoulliTS</span>
<span class="kn">from</span> <span class="nn">obp.ope</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">OffPolicyEvaluation</span><span class="p">,</span> 
    <span class="n">RegressionModel</span><span class="p">,</span>
    <span class="n">DirectMethod</span><span class="p">,</span>
    <span class="n">InverseProbabilityWeighting</span><span class="p">,</span>
    <span class="n">DoublyRobust</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="data-loading-and-preprocessing">
<h3>Data Loading and Preprocessing<a class="headerlink" href="#data-loading-and-preprocessing" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">obp.dataset.OpenBanditDataset</span></code> is an easy-to-use data loader for Open Bandit Dataset.</p>
<p>It takes behavior policy (‘bts’ or ‘random’) and campaign (‘all’, ‘men’, or ‘women’) as inputs and provides dataset preprocessing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># load and preprocess raw data in &quot;All&quot; campaign collected by the Random policy (behavior policy here)</span>
<span class="c1"># When `data_path` is not given, this class downloads the small-sized version of the Open Bandit Dataset.</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">OpenBanditDataset</span><span class="p">(</span><span class="n">behavior_policy</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="n">campaign</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>

<span class="c1"># obtain logged bandit feedback generated by behavior policy</span>
<span class="n">bandit_feedback</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>the logged bandit feedback is collected by the behavior policy as follows.</p>
<p><span class="math notranslate nohighlight">\( \mathcal{D}_b := \{(x_i,a_i,r_i)\}\)</span>  where <span class="math notranslate nohighlight">\((x,a,r) \sim p(x)\pi_b(a \mid x)p(r \mid x,a) \)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># `bandit_feedback` is a dictionary storing logged bandit feedback</span>
<span class="n">bandit_feedback</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dict_keys([&#39;n_rounds&#39;, &#39;n_actions&#39;, &#39;action&#39;, &#39;position&#39;, &#39;reward&#39;, &#39;pscore&#39;, &#39;context&#39;, &#39;action_context&#39;])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="let-s-see-some-properties-of-the-dataset-class">
<h3>let’s see some properties of the dataset class<a class="headerlink" href="#let-s-see-some-properties-of-the-dataset-class" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># name of the dataset is &#39;obd&#39; (open bandit dataset)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">dataset_name</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;obd&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># number of actions of the &quot;All&quot; campaign is 80</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>80
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># small sample example data has 10,000 samples (or rounds)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">n_rounds</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10000
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># default context (feature) engineering creates context vector with 20 dimensions</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">dim_context</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>20
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ZOZOTOWN recommendation interface has three positions</span>
<span class="c1"># (please see https://github.com/st-tech/zr-obp/blob/master/images/recommended_fashion_items.png)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">len_list</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="replicating-production-policy">
<h3>Replicating Production Policy<a class="headerlink" href="#replicating-production-policy" title="Permalink to this headline">¶</a></h3>
<p>After preparing the dataset, we now replicate the BernoulliTS policy implemented on the ZOZOTOWN recommendation interface during the data collection period.</p>
<p>Here, we use <code class="docutils literal notranslate"><span class="pre">obp.policy.BernoulliTS</span></code> as an evaluation policy.
By activating its <code class="docutils literal notranslate"><span class="pre">is_zozotown_prior</span></code> argument, we can replicate (the policy parameters of) BernoulliTS used in the ZOZOTOWN production.</p>
<p>(When <code class="docutils literal notranslate"><span class="pre">is_zozotown_prior=False</span></code>, non-informative prior distribution is used.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># define BernoulliTS as an evaluation policy</span>
<span class="n">evaluation_policy</span> <span class="o">=</span> <span class="n">BernoulliTS</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> 
    <span class="n">len_list</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">len_list</span><span class="p">,</span> 
    <span class="n">is_zozotown_prior</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># replicate the BernoulliTS policy in the ZOZOTOWN production</span>
    <span class="n">campaign</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># compute the action choice probabilities of the evaluation policy using Monte Carlo simulation</span>
<span class="n">action_dist</span> <span class="o">=</span> <span class="n">evaluation_policy</span><span class="o">.</span><span class="n">compute_batch_action_dist</span><span class="p">(</span>
    <span class="n">n_sim</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">n_rounds</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;n_rounds&quot;</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># action_dist is an array of shape (n_rounds, n_actions, len_list) </span>
<span class="c1"># representing the distribution over actions by the evaluation policy</span>
<span class="n">action_dist</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[[0.01078, 0.00931, 0.00917],
        [0.00167, 0.00077, 0.00076],
        [0.0058 , 0.00614, 0.00631],
        ...,
        [0.0008 , 0.00087, 0.00071],
        [0.00689, 0.00724, 0.00755],
        [0.0582 , 0.07603, 0.07998]],

       [[0.01078, 0.00931, 0.00917],
        [0.00167, 0.00077, 0.00076],
        [0.0058 , 0.00614, 0.00631],
        ...,
        [0.0008 , 0.00087, 0.00071],
        [0.00689, 0.00724, 0.00755],
        [0.0582 , 0.07603, 0.07998]],

       [[0.01078, 0.00931, 0.00917],
        [0.00167, 0.00077, 0.00076],
        [0.0058 , 0.00614, 0.00631],
        ...,
        [0.0008 , 0.00087, 0.00071],
        [0.00689, 0.00724, 0.00755],
        [0.0582 , 0.07603, 0.07998]],

       ...,

       [[0.01078, 0.00931, 0.00917],
        [0.00167, 0.00077, 0.00076],
        [0.0058 , 0.00614, 0.00631],
        ...,
        [0.0008 , 0.00087, 0.00071],
        [0.00689, 0.00724, 0.00755],
        [0.0582 , 0.07603, 0.07998]],

       [[0.01078, 0.00931, 0.00917],
        [0.00167, 0.00077, 0.00076],
        [0.0058 , 0.00614, 0.00631],
        ...,
        [0.0008 , 0.00087, 0.00071],
        [0.00689, 0.00724, 0.00755],
        [0.0582 , 0.07603, 0.07998]],

       [[0.01078, 0.00931, 0.00917],
        [0.00167, 0.00077, 0.00076],
        [0.0058 , 0.00614, 0.00631],
        ...,
        [0.0008 , 0.00087, 0.00071],
        [0.00689, 0.00724, 0.00755],
        [0.0582 , 0.07603, 0.07998]]])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="obtaining-a-reward-estimator">
<h3>Obtaining a Reward Estimator<a class="headerlink" href="#obtaining-a-reward-estimator" title="Permalink to this headline">¶</a></h3>
<p>A reward estimator <span class="math notranslate nohighlight">\(\hat{q}(x,a)\)</span> is needed for model dependent estimators such as DM or DR.</p>
<p><span class="math notranslate nohighlight">\(\hat{q}(x,a) \approx \mathbb{E} [r \mid x,a]\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># estimate the expected reward by using an ML model (Logistic Regression here)</span>
<span class="c1"># the estimated rewards are used by model-dependent estimators such as DM and DR</span>
<span class="n">regression_model</span> <span class="o">=</span> <span class="n">RegressionModel</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
    <span class="n">len_list</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">len_list</span><span class="p">,</span>
    <span class="n">action_context</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
    <span class="n">base_model</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">estimated_rewards_by_reg_model</span> <span class="o">=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
    <span class="n">action</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span>
    <span class="n">reward</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span>
    <span class="n">position</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;position&quot;</span><span class="p">],</span>
    <span class="n">pscore</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">],</span>
    <span class="n">n_folds</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1"># use 3-fold cross-fitting</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>please refer to <a class="reference external" href="https://arxiv.org/abs/2002.08536">https://arxiv.org/abs/2002.08536</a> about the details of the cross-fitting procedure.</p>
</div>
<div class="section" id="id5">
<h3>Off-Policy Evaluation (OPE)<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>Our next step is OPE, which attempts to estimate the performance of evaluation policies using the logged bandit feedback and OPE estimators.</p>
<p>Here, we use</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">obp.ope.InverseProbabilityWeighting</span></code> (IPW)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">obp.ope.DirectMethod</span></code> (DM)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">obp.ope.DoublyRobust</span></code> (DR)</p></li>
</ul>
<p>as estimators and visualize the OPE results.</p>
<p><span class="math notranslate nohighlight">\(V(\pi_e) \approx \hat{V} (\pi_e; \mathcal{D}_b, \theta)\)</span> using DM, IPW, and DR</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># estimate the policy value of BernoulliTS based on its action choice probabilities</span>
<span class="c1"># it is possible to set multiple OPE estimators to the `ope_estimators` argument</span>
<span class="n">ope</span> <span class="o">=</span> <span class="n">OffPolicyEvaluation</span><span class="p">(</span>
    <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">,</span>
    <span class="n">ope_estimators</span><span class="o">=</span><span class="p">[</span><span class="n">InverseProbabilityWeighting</span><span class="p">(),</span> <span class="n">DirectMethod</span><span class="p">(),</span> <span class="n">DoublyRobust</span><span class="p">()]</span>
<span class="p">)</span>

<span class="c1"># `summarize_off_policy_estimates` returns pandas dataframes including the OPE results</span>
<span class="n">estimated_policy_value</span><span class="p">,</span> <span class="n">estimated_interval</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">summarize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span> 
    <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="c1"># number of resampling performed in the bootstrap procedure.</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># the estimated policy value of the evaluation policy (the BernoulliTS policy)</span>
<span class="c1"># relative_estimated_policy_value is the policy value of the evaluation policy </span>
<span class="c1"># relative to the ground-truth policy value of the behavior policy (the Random policy here)</span>
<span class="n">estimated_policy_value</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>estimated_policy_value</th>
      <th>relative_estimated_policy_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>ipw</th>
      <td>0.004553</td>
      <td>1.198126</td>
    </tr>
    <tr>
      <th>dm</th>
      <td>0.003385</td>
      <td>0.890756</td>
    </tr>
    <tr>
      <th>dr</th>
      <td>0.004648</td>
      <td>1.223157</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># confidence intervals of policy value of BernoulliTS estimated by OPE estimators</span>
<span class="c1"># (`mean` values in this dataframe is also estimated via the non-parametric bootstrap procedure </span>
<span class="c1"># and is a bit different from the above values in `estimated_policy_value`)</span>
<span class="n">estimated_interval</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>95.0% CI (lower)</th>
      <th>95.0% CI (upper)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>ipw</th>
      <td>0.004544</td>
      <td>0.001531</td>
      <td>0.009254</td>
    </tr>
    <tr>
      <th>dm</th>
      <td>0.003385</td>
      <td>0.003337</td>
      <td>0.003433</td>
    </tr>
    <tr>
      <th>dr</th>
      <td>0.004639</td>
      <td>0.001625</td>
      <td>0.009323</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># visualize the policy values of BernoulliTS estimated by the three OPE estimators</span>
<span class="c1"># and their 95% confidence intervals (estimated by nonparametric bootstrap method)</span>
<span class="n">ope</span><span class="o">.</span><span class="n">visualize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
    <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="c1"># number of resampling performed in the bootstrap procedure</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_109_0.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_109_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># by activating the `is_relative` option</span>
<span class="c1"># we can visualize the estimated policy value of the evaluation policy</span>
<span class="c1"># relative to the ground-truth policy value of the behavior policy</span>
<span class="n">ope</span><span class="o">.</span><span class="n">visualize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
    <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="c1"># number of resampling performed in the bootstrap procedure</span>
    <span class="n">is_relative</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_110_0.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_110_0.png" />
</div>
</div>
<p>Note that the OPE demonstration here is with the small size example version of our dataset.</p>
<p>Please use its full size version (<a class="reference external" href="https://research.zozo.com/data.html">https://research.zozo.com/data.html</a>) to produce more reasonable results.</p>
</div>
<div class="section" id="id6">
<h3>Evaluation of OPE<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>Our final step is the <strong>evaluation of OPE</strong>, which evaluates the estimation accuracy of OPE estimators.</p>
<p>Specifically, we asses the accuracy of the estimator such as DM, IPW, and DR by comparing its estimation with the ground-truth policy value estimated via the on-policy estimation from the Open Bandit Dataset.</p>
<p>This type of evaluation of OPE is possible, because Open Bandit Dataset contains a set of <em>multiple</em> different logged bandit feedback datasets collected by running different policies on the same platform at the same time.</p>
<p>Please refer to <a class="reference external" href="https://zr-obp.readthedocs.io/en/latest/evaluation_ope.html">the documentation</a> for the details about the evaluation of OPE protocol.</p>
<p><strong>Approximate the Ground-truth Policy Value</strong>
With Open Bandit Dataset, we can estimate the ground-truth policy value of the evaluation policy in an on-policy manner as follows.</p>
<p><span class="math notranslate nohighlight">\(V(\pi_e) \approx \frac{1}{|\mathcal{D}_{e}|} \sum_{i=1}^{|\mathcal{D}_{e}|} \mathbb{E}_{n} [r_i]\)</span></p>
<p><span class="math notranslate nohighlight">\( \mathcal{D}_e := \{(x_i,a_i,r_i)\} \)</span> (<span class="math notranslate nohighlight">\((x,a,r) \sim p(x)\pi_e(a \mid x)p(r \mid x,a) \)</span>) is the log data collected by the evaluation policy (, which is used only for approximating the ground-truth policy value).</p>
<p>We can compare the policy values estimated by OPE estimators with this on-policy estimate to evaluate the accuracy of OPE.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># we first calculate the ground-truth policy value of the evaluation policy</span>
<span class="c1"># , which is estimated by averaging the factual (observed) rewards contained in the dataset (on-policy estimation)</span>
<span class="n">policy_value_bts</span> <span class="o">=</span> <span class="n">OpenBanditDataset</span><span class="o">.</span><span class="n">calc_on_policy_policy_value_estimate</span><span class="p">(</span>
    <span class="n">behavior_policy</span><span class="o">=</span><span class="s1">&#39;bts&#39;</span><span class="p">,</span> <span class="n">campaign</span><span class="o">=</span><span class="s1">&#39;all&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Evaluation of OPE</strong></p>
<p>We can evaluate the estimation performance of OPE estimators by comparing the estimated policy values of the evaluation with its ground-truth as follows.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\textit{relative-ee} (\hat{V}; \mathcal{D}_b) := \left| \frac{V(\pi_e) - \hat{V} (\pi_e; \mathcal{D}_b)}{V(\pi_e)} \right|\)</span> (relative estimation error; relative-ee)</p></li>
<li><p><span class="math notranslate nohighlight">\(\textit{SE} (\hat{V}; \mathcal{D}_b) := \left( V(\pi_e) - \hat{V} (\pi_e; \mathcal{D}_b) \right)^2\)</span> (squared error; se)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># evaluate the estimation performance of OPE estimators </span>
<span class="c1"># `evaluate_performance_of_estimators` returns a dictionary containing estimation performance of given estimators </span>
<span class="n">relative_ee</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">summarize_estimators_comparison</span><span class="p">(</span>
    <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">policy_value_bts</span><span class="p">,</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
    <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
    <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;relative-ee&quot;</span><span class="p">,</span> <span class="c1"># &quot;relative-ee&quot; (relative estimation error) or &quot;se&quot; (squared error)</span>
<span class="p">)</span>

<span class="c1"># estimation performances of the three estimators (lower means accurate)</span>
<span class="n">relative_ee</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>relative-ee</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>ipw</th>
      <td>0.084019</td>
    </tr>
    <tr>
      <th>dm</th>
      <td>0.194078</td>
    </tr>
    <tr>
      <th>dr</th>
      <td>0.106666</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We can iterate the above process several times to get more relibale results.</p>
<p>Please see <span class="xref myst">examples/obd</span> for a more sophisticated example of the evaluation of OPE with the Open Bandit Dataset.</p>
</div>
</div>
<div class="section" id="real-world-example">
<h2>Real-world Example<a class="headerlink" href="#real-world-example" title="Permalink to this headline">¶</a></h2>
<p><strong>“What is the best new policy for the ZOZOTOWN recommendation interface?”</strong></p>
<ol class="simple">
<li><p>Data Loading and Preprocessing (Random Bucket of Open Bandit Dataset)</p></li>
<li><p>Off-Policy Learning (IPWLearner and NNPolicyLearner)</p></li>
<li><p>Off-Policy Evaluation (IPWLearner vs NNPolicyLearner)</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="kn">import</span> <span class="nn">obp</span>
<span class="kn">from</span> <span class="nn">obp.dataset</span> <span class="kn">import</span> <span class="n">OpenBanditDataset</span>
<span class="kn">from</span> <span class="nn">obp.policy</span> <span class="kn">import</span> <span class="n">IPWLearner</span><span class="p">,</span> <span class="n">NNPolicyLearner</span>
<span class="kn">from</span> <span class="nn">obp.ope</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">RegressionModel</span><span class="p">,</span>
    <span class="n">OffPolicyEvaluation</span><span class="p">,</span>
    <span class="n">SelfNormalizedInverseProbabilityWeighting</span> <span class="k">as</span> <span class="n">SNIPS</span><span class="p">,</span>
    <span class="n">DoublyRobust</span> <span class="k">as</span> <span class="n">DR</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id7">
<h3>Data Loading and Preprocessing<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>Here we use a random bucket of the Open Bandit Pipeline. We can download this by using <code class="docutils literal notranslate"><span class="pre">obp.dataset.OpenBanditDataset</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># define OpenBanditDataset class to handle the real bandit data</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">OpenBanditDataset</span><span class="p">(</span>
    <span class="n">behavior_policy</span><span class="o">=</span><span class="s2">&quot;random&quot;</span><span class="p">,</span> <span class="n">campaign</span><span class="o">=</span><span class="s2">&quot;all&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># logged bandit data collected by the uniform random policy</span>
<span class="n">training_bandit_data</span><span class="p">,</span> <span class="n">test_bandit_data</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span>
    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">is_timeseries_split</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ignore the position effect for a demo purpose</span>
<span class="n">training_bandit_data</span><span class="p">[</span><span class="s2">&quot;position&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span> 
<span class="n">test_bandit_data</span><span class="p">[</span><span class="s2">&quot;position&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># number of actions</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>80
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># sample size</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">n_rounds</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10000
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="off-policy-learning-opl">
<h3>Off-Policy Learning (OPL)<a class="headerlink" href="#off-policy-learning-opl" title="Permalink to this headline">¶</a></h3>
<p>Train two new policies: <code class="docutils literal notranslate"><span class="pre">obp.policy.IPWLearner</span></code> and <code class="docutils literal notranslate"><span class="pre">obp.policy.NNPolicyLearner</span></code>.</p>
<div class="section" id="ipwlearner">
<h4>IPWLearner<a class="headerlink" href="#ipwlearner" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ipw_learner</span> <span class="o">=</span> <span class="n">IPWLearner</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
    <span class="n">base_classifier</span><span class="o">=</span><span class="n">RandomForestClassifier</span><span class="p">(</span>
        <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit</span>
<span class="n">ipw_learner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">training_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span> <span class="c1"># context; x</span>
    <span class="n">action</span><span class="o">=</span><span class="n">training_bandit_data</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span> <span class="c1"># action; a</span>
    <span class="n">reward</span><span class="o">=</span><span class="n">training_bandit_data</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span> <span class="c1"># reward; r</span>
    <span class="n">pscore</span><span class="o">=</span><span class="n">training_bandit_data</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">],</span> <span class="c1"># propensity score; pi_b(a|x)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># predict (make new decisions)</span>
<span class="n">action_dist_ipw</span> <span class="o">=</span> <span class="n">ipw_learner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="nnpolicylearner">
<h4>NNPolicyLearner<a class="headerlink" href="#nnpolicylearner" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nn_learner</span> <span class="o">=</span> <span class="n">NNPolicyLearner</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
    <span class="n">dim_context</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">dim_context</span><span class="p">,</span>
    <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span>
    <span class="n">off_policy_objective</span><span class="o">=</span><span class="s2">&quot;ipw&quot;</span><span class="p">,</span> <span class="c1"># = ips</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> 
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit</span>
<span class="n">nn_learner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">training_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span> <span class="c1"># context; x</span>
    <span class="n">action</span><span class="o">=</span><span class="n">training_bandit_data</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span> <span class="c1"># action; a</span>
    <span class="n">reward</span><span class="o">=</span><span class="n">training_bandit_data</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span> <span class="c1"># reward; r</span>
    <span class="n">pscore</span><span class="o">=</span><span class="n">training_bandit_data</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">],</span> <span class="c1"># propensity score; pi_b(a|x)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>policy learning: 100%|██████████| 200/200 [00:00&lt;00:00, 203.25it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># predict (make new decisions)</span>
<span class="n">action_dist_nn</span> <span class="o">=</span> <span class="n">nn_learner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="id8">
<h3>Obtain a Reward Estimator<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">obp.ope.RegressionModel</span></code> simplifies the process of reward modeling</p>
<p><span class="math notranslate nohighlight">\(r(x,a) = \mathbb{E} [r \mid x, a] \approx \hat{r}(x,a)\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">regression_model</span> <span class="o">=</span> <span class="n">RegressionModel</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> 
    <span class="n">base_model</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">estimated_rewards</span> <span class="o">=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span> 
    <span class="n">action</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span> 
    <span class="n">reward</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span> 
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="off-policy-evaluation">
<h3>Off-Policy Evaluation<a class="headerlink" href="#off-policy-evaluation" title="Permalink to this headline">¶</a></h3>
<p>Estimating the performance of IPWLearner and NNPolicyLearner via OPE.</p>
<p><code class="docutils literal notranslate"><span class="pre">obp.ope.OffPolicyEvaluation</span></code> simplifies the OPE process</p>
<p><span class="math notranslate nohighlight">\(V(\pi_e) \approx \hat{V} (\pi_e; \mathcal{D}_0, \theta)\)</span> using DM, IPS, and DR</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ope</span> <span class="o">=</span> <span class="n">OffPolicyEvaluation</span><span class="p">(</span>
    <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">test_bandit_data</span><span class="p">,</span>
    <span class="n">ope_estimators</span><span class="o">=</span><span class="p">[</span>
        <span class="n">SNIPS</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;SNIPS&quot;</span><span class="p">),</span>
        <span class="n">DR</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;DR&quot;</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="visualize-the-ope-results">
<h3>Visualize the OPE results<a class="headerlink" href="#visualize-the-ope-results" title="Permalink to this headline">¶</a></h3>
<p>Output the relative performances of the trained policies compared to the logging policy (uniform random)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ope</span><span class="o">.</span><span class="n">visualize_off_policy_estimates_of_multiple_policies</span><span class="p">(</span>
    <span class="n">policy_name_list</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;IPWLearner&quot;</span><span class="p">,</span> <span class="s2">&quot;NNPolicyLearner&quot;</span><span class="p">],</span>
    <span class="n">action_dist_list</span><span class="o">=</span><span class="p">[</span><span class="n">action_dist_ipw</span><span class="p">,</span> <span class="n">action_dist_nn</span><span class="p">],</span>
    <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">is_relative</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_144_0.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_144_0.png" />
</div>
</div>
<p>Both policy learner outperforms the random baseline. In particular, NNPolicyLearner seems to be the best, improving the random baseline by about 70%. It also outperforms IPWLearner in both SNIPS and DR.</p>
</div>
</div>
<div class="section" id="synthetic-slate-dataset">
<h2>Synthetic Slate Dataset<a class="headerlink" href="#synthetic-slate-dataset" title="Permalink to this headline">¶</a></h2>
<p>This section provides an example of conducting OPE of several different evaluation policies with synthetic slate bandit feedback data.</p>
<p>Our example with synthetic bandit data contains the follwoing four major steps:</p>
<ul class="simple">
<li><p>(1) Synthetic Slate Data Generation</p></li>
<li><p>(2) Defining Evaluation Policy</p></li>
<li><p>(3) Off-Policy Evaluation</p></li>
<li><p>(4) Evaluation of OPE Estimators</p></li>
</ul>
<p>The second step could be replaced by some Off-Policy Learning (OPL) step, but obp still does not implement any OPL module for slate bandit data. Implementing OPL for slate bandit data is our future work.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">obp</span>

<span class="kn">from</span> <span class="nn">obp.ope</span> <span class="kn">import</span> <span class="n">SlateStandardIPS</span><span class="p">,</span> <span class="n">SlateIndependentIPS</span><span class="p">,</span> <span class="n">SlateRewardInteractionIPS</span><span class="p">,</span> <span class="n">SlateOffPolicyEvaluation</span>
<span class="kn">from</span> <span class="nn">obp.dataset</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">logistic_reward_function</span><span class="p">,</span>
    <span class="n">SyntheticSlateBanditDataset</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="synthetic-slate-data-generation">
<h3>Synthetic Slate Data Generation<a class="headerlink" href="#synthetic-slate-data-generation" title="Permalink to this headline">¶</a></h3>
<p>We prepare easy-to-use synthetic slate data generator: <code class="docutils literal notranslate"><span class="pre">SyntheticSlateBanditDataset</span></code> class in the dataset module.</p>
<p>It takes the following arguments as inputs and generates a synthetic bandit dataset that can be used to evaluate the performance of decision making policies (obtained by <code class="docutils literal notranslate"><span class="pre">off-policy</span> <span class="pre">learning</span></code>) and OPE estimators.</p>
<ul class="simple">
<li><p>length of a list of actions recommended in each slate. (<code class="docutils literal notranslate"><span class="pre">len_list</span></code>)</p></li>
<li><p>number of unique actions (<code class="docutils literal notranslate"><span class="pre">n_unique_actions</span></code>)</p></li>
<li><p>dimension of context vectors (<code class="docutils literal notranslate"><span class="pre">dim_context</span></code>)</p></li>
<li><p>reward type (<code class="docutils literal notranslate"><span class="pre">reward_type</span></code>)</p></li>
<li><p>reward structure (<code class="docutils literal notranslate"><span class="pre">reward_structure</span></code>)</p></li>
<li><p>click model (<code class="docutils literal notranslate"><span class="pre">click_model</span></code>)</p></li>
<li><p>base reward function (<code class="docutils literal notranslate"><span class="pre">base_reward_function</span></code>)</p></li>
<li><p>behavior policy (<code class="docutils literal notranslate"><span class="pre">behavior_policy_function</span></code>)</p></li>
</ul>
<p>We use a uniform random policy as a behavior policy here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate a synthetic bandit dataset with 10 actions</span>
<span class="c1"># we use `logistic_reward_function` as the reward function and `linear_behavior_policy_logit` as the behavior policy.</span>
<span class="c1"># one can define their own reward function and behavior policy such as nonlinear ones. </span>

<span class="n">n_unique_action</span><span class="o">=</span><span class="mi">10</span>
<span class="n">len_list</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">dim_context</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">reward_type</span> <span class="o">=</span> <span class="s2">&quot;binary&quot;</span>
<span class="n">reward_structure</span><span class="o">=</span><span class="s2">&quot;cascade_additive&quot;</span>
<span class="n">click_model</span><span class="o">=</span><span class="kc">None</span>
<span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span>
<span class="n">base_reward_function</span><span class="o">=</span><span class="n">logistic_reward_function</span>

<span class="c1"># obtain  test sets of synthetic logged bandit feedback</span>
<span class="n">n_rounds_test</span> <span class="o">=</span> <span class="mi">10000</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># define Uniform Random Policy as a baseline behavior policy</span>
<span class="n">dataset_with_random_behavior</span> <span class="o">=</span> <span class="n">SyntheticSlateBanditDataset</span><span class="p">(</span>
    <span class="n">n_unique_action</span><span class="o">=</span><span class="n">n_unique_action</span><span class="p">,</span>
    <span class="n">len_list</span><span class="o">=</span><span class="n">len_list</span><span class="p">,</span>
    <span class="n">dim_context</span><span class="o">=</span><span class="n">dim_context</span><span class="p">,</span>
    <span class="n">reward_type</span><span class="o">=</span><span class="n">reward_type</span><span class="p">,</span>
    <span class="n">reward_structure</span><span class="o">=</span><span class="n">reward_structure</span><span class="p">,</span>
    <span class="n">click_model</span><span class="o">=</span><span class="n">click_model</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
    <span class="n">behavior_policy_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># set to uniform random</span>
    <span class="n">base_reward_function</span><span class="o">=</span><span class="n">base_reward_function</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># compute the factual action choice probabililties for the test set of the synthetic logged bandit feedback</span>
<span class="n">bandit_feedback_with_random_behavior</span> <span class="o">=</span> <span class="n">dataset_with_random_behavior</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span>
    <span class="n">n_rounds</span><span class="o">=</span><span class="n">n_rounds_test</span><span class="p">,</span>
    <span class="n">return_pscore_item_position</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># print policy value</span>
<span class="n">random_policy_value</span> <span class="o">=</span> <span class="n">dataset_with_random_behavior</span><span class="o">.</span><span class="n">calc_on_policy_policy_value</span><span class="p">(</span>
    <span class="n">reward</span><span class="o">=</span><span class="n">bandit_feedback_with_random_behavior</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span>
    <span class="n">slate_id</span><span class="o">=</span><span class="n">bandit_feedback_with_random_behavior</span><span class="p">[</span><span class="s2">&quot;slate_id&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random_policy_value</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[sample_action_and_obtain_pscore]: 100%|██████████| 10000/10000 [00:04&lt;00:00, 2033.87it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.8366
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="evaluation-policy-definition-off-policy-learning">
<h3>Evaluation Policy Definition (Off-Policy Learning)<a class="headerlink" href="#evaluation-policy-definition-off-policy-learning" title="Permalink to this headline">¶</a></h3>
<p>After generating synthetic data, we now define the evaluation policy as follows:</p>
<ol class="simple">
<li><p>Generate logit values of three valuation policies (<code class="docutils literal notranslate"><span class="pre">random</span></code>, <code class="docutils literal notranslate"><span class="pre">optimal</span></code>, and <code class="docutils literal notranslate"><span class="pre">anti-optimal</span></code>).</p></li>
</ol>
<ul class="simple">
<li><p>A <code class="docutils literal notranslate"><span class="pre">optimal</span></code> policy is defined by a policy that samples actions using<code class="docutils literal notranslate"><span class="pre">3</span> <span class="pre">*</span> <span class="pre">base_expected_reward</span></code>.</p></li>
<li><p>An <code class="docutils literal notranslate"><span class="pre">anti-optimal</span></code> policy is defined by a policy that samples actions using the sign inversion of <code class="docutils literal notranslate"><span class="pre">-3</span> <span class="pre">*</span> <span class="pre">base_expected_reward</span></code>.</p></li>
</ul>
<ol class="simple">
<li><p>Obtain pscores of the evaluation policies by <code class="docutils literal notranslate"><span class="pre">obtain_pscore_given_evaluation_policy_logit</span></code> method.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">random_policy_logit_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_rounds_test</span><span class="p">,</span> <span class="n">n_unique_action</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">base_expected_reward</span> <span class="o">=</span> <span class="n">dataset_with_random_behavior</span><span class="o">.</span><span class="n">base_reward_function</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback_with_random_behavior</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
    <span class="n">action_context</span><span class="o">=</span><span class="n">dataset_with_random_behavior</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="n">dataset_with_random_behavior</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optimal_policy_logit_</span> <span class="o">=</span> <span class="n">base_expected_reward</span> <span class="o">*</span> <span class="mi">3</span>
<span class="n">anti_optimal_policy_logit_</span> <span class="o">=</span> <span class="o">-</span><span class="mi">3</span> <span class="o">*</span> <span class="n">base_expected_reward</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">random_policy_pscores</span> <span class="o">=</span> <span class="n">dataset_with_random_behavior</span><span class="o">.</span><span class="n">obtain_pscore_given_evaluation_policy_logit</span><span class="p">(</span>
    <span class="n">action</span><span class="o">=</span><span class="n">bandit_feedback_with_random_behavior</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span>
    <span class="n">evaluation_policy_logit_</span><span class="o">=</span><span class="n">random_policy_logit_</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[obtain_pscore_given_evaluation_policy_logit]: 100%|██████████| 10000/10000 [00:23&lt;00:00, 418.87it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optimal_policy_pscores</span> <span class="o">=</span> <span class="n">dataset_with_random_behavior</span><span class="o">.</span><span class="n">obtain_pscore_given_evaluation_policy_logit</span><span class="p">(</span>
    <span class="n">action</span><span class="o">=</span><span class="n">bandit_feedback_with_random_behavior</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span>
    <span class="n">evaluation_policy_logit_</span><span class="o">=</span><span class="n">optimal_policy_logit_</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[obtain_pscore_given_evaluation_policy_logit]: 100%|██████████| 10000/10000 [00:28&lt;00:00, 356.59it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">anti_optimal_policy_pscores</span> <span class="o">=</span> <span class="n">dataset_with_random_behavior</span><span class="o">.</span><span class="n">obtain_pscore_given_evaluation_policy_logit</span><span class="p">(</span>
    <span class="n">action</span><span class="o">=</span><span class="n">bandit_feedback_with_random_behavior</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span>
    <span class="n">evaluation_policy_logit_</span><span class="o">=</span><span class="n">anti_optimal_policy_logit_</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[obtain_pscore_given_evaluation_policy_logit]: 100%|██████████| 10000/10000 [00:28&lt;00:00, 355.28it/s]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id9">
<h3>Off-Policy Evaluation (OPE)<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>Our next step is OPE which attempts to estimate the performance of evaluation policies using the logged bandit feedback and OPE estimators.</p>
<p>Here, we use the <strong>SlateStandardIPS (SIPS)</strong>, <strong>SlateIndependentIPS (IIPS)</strong>, and <strong>SlateRewardInteractionIPS (RIPS)</strong> estimators and visualize the OPE results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># estimate the policy value of the evaluation policies based on their action choice probabilities</span>
<span class="c1"># it is possible to set multiple OPE estimators to the `ope_estimators` argument</span>

<span class="n">sips</span> <span class="o">=</span> <span class="n">SlateStandardIPS</span><span class="p">(</span><span class="n">len_list</span><span class="o">=</span><span class="n">len_list</span><span class="p">)</span>
<span class="n">iips</span> <span class="o">=</span> <span class="n">SlateIndependentIPS</span><span class="p">(</span><span class="n">len_list</span><span class="o">=</span><span class="n">len_list</span><span class="p">)</span>
<span class="n">rips</span> <span class="o">=</span> <span class="n">SlateRewardInteractionIPS</span><span class="p">(</span><span class="n">len_list</span><span class="o">=</span><span class="n">len_list</span><span class="p">)</span>

<span class="n">ope</span> <span class="o">=</span> <span class="n">SlateOffPolicyEvaluation</span><span class="p">(</span>
    <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">bandit_feedback_with_random_behavior</span><span class="p">,</span>
    <span class="n">ope_estimators</span><span class="o">=</span><span class="p">[</span><span class="n">sips</span><span class="p">,</span> <span class="n">iips</span><span class="p">,</span> <span class="n">rips</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">estimated_interval_random</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">summarize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">evaluation_policy_pscore</span><span class="o">=</span><span class="n">random_policy_pscores</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_item_position</span><span class="o">=</span><span class="n">random_policy_pscores</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_cascade</span><span class="o">=</span><span class="n">random_policy_pscores</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="n">dataset_with_random_behavior</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">estimated_interval_random</span><span class="p">[</span><span class="s2">&quot;policy_name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;random&quot;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">estimated_interval_random</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="c1"># visualize estimated policy values of Uniform Random by the three OPE estimators</span>
<span class="c1"># and their 95% confidence intervals (estimated by nonparametric bootstrap method)</span>
<span class="n">ope</span><span class="o">.</span><span class="n">visualize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">evaluation_policy_pscore</span><span class="o">=</span><span class="n">random_policy_pscores</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_item_position</span><span class="o">=</span><span class="n">random_policy_pscores</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_cascade</span><span class="o">=</span><span class="n">random_policy_pscores</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="c1"># number of resampling performed in the bootstrap procedure</span>
    <span class="n">random_state</span><span class="o">=</span><span class="n">dataset_with_random_behavior</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>          mean  95.0% CI (lower)  95.0% CI (upper) policy_name
sips  1.836816            1.8205          1.852505      random
iips  1.836816            1.8205          1.852505      random
rips  1.836816            1.8205          1.852505      random 
</pre></div>
</div>
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_164_1.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_164_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">estimated_interval_optimal</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">summarize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">evaluation_policy_pscore</span><span class="o">=</span><span class="n">optimal_policy_pscores</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_item_position</span><span class="o">=</span><span class="n">optimal_policy_pscores</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_cascade</span><span class="o">=</span><span class="n">optimal_policy_pscores</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="n">dataset_with_random_behavior</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">estimated_interval_optimal</span><span class="p">[</span><span class="s2">&quot;policy_name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;optimal&quot;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">estimated_interval_optimal</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="c1"># visualize estimated policy values of Optimal by the three OPE estimators</span>
<span class="c1"># and their 95% confidence intervals (estimated by nonparametric bootstrap method)</span>
<span class="n">ope</span><span class="o">.</span><span class="n">visualize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">evaluation_policy_pscore</span><span class="o">=</span><span class="n">optimal_policy_pscores</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_item_position</span><span class="o">=</span><span class="n">optimal_policy_pscores</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_cascade</span><span class="o">=</span><span class="n">optimal_policy_pscores</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="c1"># number of resampling performed in the bootstrap procedure</span>
    <span class="n">random_state</span><span class="o">=</span><span class="n">dataset_with_random_behavior</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>          mean  95.0% CI (lower)  95.0% CI (upper) policy_name
sips  1.830555          1.803695          1.860548     optimal
iips  1.843117          1.825576          1.859695     optimal
rips  1.838866          1.815574          1.862451     optimal 
</pre></div>
</div>
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_165_1.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_165_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">estimated_interval_anti_optimal</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">summarize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">evaluation_policy_pscore</span><span class="o">=</span><span class="n">anti_optimal_policy_pscores</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_item_position</span><span class="o">=</span><span class="n">anti_optimal_policy_pscores</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_cascade</span><span class="o">=</span><span class="n">anti_optimal_policy_pscores</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="n">dataset_with_random_behavior</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">estimated_interval_anti_optimal</span><span class="p">[</span><span class="s2">&quot;policy_name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;anti-optimal&quot;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">estimated_interval_anti_optimal</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="c1"># visualize estimated policy values of Anti-optimal by the three OPE estimators</span>
<span class="c1"># and their 95% confidence intervals (estimated by nonparametric bootstrap method)</span>
<span class="n">ope</span><span class="o">.</span><span class="n">visualize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">evaluation_policy_pscore</span><span class="o">=</span><span class="n">anti_optimal_policy_pscores</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_item_position</span><span class="o">=</span><span class="n">anti_optimal_policy_pscores</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_cascade</span><span class="o">=</span><span class="n">anti_optimal_policy_pscores</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="c1"># number of resampling performed in the bootstrap procedure</span>
    <span class="n">random_state</span><span class="o">=</span><span class="n">dataset_with_random_behavior</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>          mean  95.0% CI (lower)  95.0% CI (upper)   policy_name
sips  1.854516          1.829643          1.877320  anti-optimal
iips  1.832793          1.815842          1.848599  anti-optimal
rips  1.844397          1.824965          1.864795  anti-optimal 
</pre></div>
</div>
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_166_1.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_166_1.png" />
</div>
</div>
</div>
<div class="section" id="id10">
<h3>Evaluation of OPE estimators<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>Our final step is <strong>the evaluation of OPE</strong>, which evaluates and compares the estimation accuracy of OPE estimators.</p>
<p>With synthetic slate data, we can calculate the policy value of the evaluation policies.
Therefore, we can compare the policy values estimated by OPE estimators with the ground-turths to evaluate the accuracy of OPE.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ground_truth_policy_value_random</span> <span class="o">=</span> <span class="n">dataset_with_random_behavior</span><span class="o">.</span><span class="n">calc_ground_truth_policy_value</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback_with_random_behavior</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
    <span class="n">evaluation_policy_logit_</span><span class="o">=</span><span class="n">random_policy_logit_</span>
<span class="p">)</span>
<span class="n">ground_truth_policy_value_random</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[calc_ground_truth_policy_value (pscore)]: 100%|██████████| 10000/10000 [00:10&lt;00:00, 930.45it/s]
[calc_ground_truth_policy_value (expected reward), batch_size=3334]: 100%|██████████| 3/3 [00:03&lt;00:00,  1.22s/it]
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.837144428308276
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ground_truth_policy_value_optimal</span> <span class="o">=</span> <span class="n">dataset_with_random_behavior</span><span class="o">.</span><span class="n">calc_ground_truth_policy_value</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback_with_random_behavior</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
    <span class="n">evaluation_policy_logit_</span><span class="o">=</span><span class="n">optimal_policy_logit_</span>
<span class="p">)</span>
<span class="n">ground_truth_policy_value_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[calc_ground_truth_policy_value (pscore)]: 100%|██████████| 10000/10000 [00:12&lt;00:00, 774.03it/s]
[calc_ground_truth_policy_value (expected reward), batch_size=3334]: 100%|██████████| 3/3 [00:03&lt;00:00,  1.22s/it]
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.8474242800908984
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ground_truth_policy_value_anti_optimal</span> <span class="o">=</span> <span class="n">dataset_with_random_behavior</span><span class="o">.</span><span class="n">calc_ground_truth_policy_value</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback_with_random_behavior</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
    <span class="n">evaluation_policy_logit_</span><span class="o">=</span><span class="n">anti_optimal_policy_logit_</span>
<span class="p">)</span>
<span class="n">ground_truth_policy_value_anti_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[calc_ground_truth_policy_value (pscore)]: 100%|██████████| 10000/10000 [00:12&lt;00:00, 786.50it/s]
[calc_ground_truth_policy_value (expected reward), batch_size=3334]: 100%|██████████| 3/3 [00:03&lt;00:00,  1.22s/it]
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.8352871486686428
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">estimated_interval_random</span><span class="p">[</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ground_truth_policy_value_random</span>
<span class="n">estimated_interval_optimal</span><span class="p">[</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ground_truth_policy_value_optimal</span>
<span class="n">estimated_interval_anti_optimal</span><span class="p">[</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ground_truth_policy_value_anti_optimal</span>

<span class="n">estimated_intervals</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">estimated_interval_random</span><span class="p">,</span>
        <span class="n">estimated_interval_optimal</span><span class="p">,</span>
        <span class="n">estimated_interval_anti_optimal</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">estimated_intervals</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>95.0% CI (lower)</th>
      <th>95.0% CI (upper)</th>
      <th>policy_name</th>
      <th>ground_truth</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>sips</th>
      <td>1.836816</td>
      <td>1.820500</td>
      <td>1.852505</td>
      <td>random</td>
      <td>1.837144</td>
    </tr>
    <tr>
      <th>iips</th>
      <td>1.836816</td>
      <td>1.820500</td>
      <td>1.852505</td>
      <td>random</td>
      <td>1.837144</td>
    </tr>
    <tr>
      <th>rips</th>
      <td>1.836816</td>
      <td>1.820500</td>
      <td>1.852505</td>
      <td>random</td>
      <td>1.837144</td>
    </tr>
    <tr>
      <th>sips</th>
      <td>1.830555</td>
      <td>1.803695</td>
      <td>1.860548</td>
      <td>optimal</td>
      <td>1.847424</td>
    </tr>
    <tr>
      <th>iips</th>
      <td>1.843117</td>
      <td>1.825576</td>
      <td>1.859695</td>
      <td>optimal</td>
      <td>1.847424</td>
    </tr>
    <tr>
      <th>rips</th>
      <td>1.838866</td>
      <td>1.815574</td>
      <td>1.862451</td>
      <td>optimal</td>
      <td>1.847424</td>
    </tr>
    <tr>
      <th>sips</th>
      <td>1.854516</td>
      <td>1.829643</td>
      <td>1.877320</td>
      <td>anti-optimal</td>
      <td>1.835287</td>
    </tr>
    <tr>
      <th>iips</th>
      <td>1.832793</td>
      <td>1.815842</td>
      <td>1.848599</td>
      <td>anti-optimal</td>
      <td>1.835287</td>
    </tr>
    <tr>
      <th>rips</th>
      <td>1.844397</td>
      <td>1.824965</td>
      <td>1.864795</td>
      <td>anti-optimal</td>
      <td>1.835287</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We can confirm that the three OPE estimators return the same results when the behavior policy and the evaluation policy is the same, and the estimates are quite similar to the <code class="docutils literal notranslate"><span class="pre">random_policy_value</span></code> calcurated above.</p>
<p>We can also observe that the performance of OPE estimators are as follows in this simulation: <code class="docutils literal notranslate"><span class="pre">IIPS</span> <span class="pre">&gt;</span> <span class="pre">RIPS</span> <span class="pre">&gt;</span> <span class="pre">SIPS</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># evaluate the estimation performances of OPE estimators </span>
<span class="c1"># by comparing the estimated policy values and its ground-truth.</span>
<span class="c1"># `summarize_estimators_comparison` returns a pandas dataframe containing estimation performances of given estimators </span>

<span class="n">relative_ee_for_random_evaluation_policy</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">summarize_estimators_comparison</span><span class="p">(</span>
    <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">ground_truth_policy_value_random</span><span class="p">,</span>
    <span class="n">evaluation_policy_pscore</span><span class="o">=</span><span class="n">random_policy_pscores</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_item_position</span><span class="o">=</span><span class="n">random_policy_pscores</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_cascade</span><span class="o">=</span><span class="n">random_policy_pscores</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">relative_ee_for_random_evaluation_policy</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>relative-ee</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>sips</th>
      <td>0.000296</td>
    </tr>
    <tr>
      <th>iips</th>
      <td>0.000296</td>
    </tr>
    <tr>
      <th>rips</th>
      <td>0.000296</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># evaluate the estimation performances of OPE estimators </span>
<span class="c1"># by comparing the estimated policy values and its ground-truth.</span>
<span class="c1"># `summarize_estimators_comparison` returns a pandas dataframe containing estimation performances of given estimators </span>

<span class="n">relative_ee_for_optimal_evaluation_policy</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">summarize_estimators_comparison</span><span class="p">(</span>
    <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">ground_truth_policy_value_optimal</span><span class="p">,</span>
    <span class="n">evaluation_policy_pscore</span><span class="o">=</span><span class="n">optimal_policy_pscores</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_item_position</span><span class="o">=</span><span class="n">optimal_policy_pscores</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_cascade</span><span class="o">=</span><span class="n">optimal_policy_pscores</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">relative_ee_for_optimal_evaluation_policy</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>relative-ee</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>sips</th>
      <td>0.009303</td>
    </tr>
    <tr>
      <th>iips</th>
      <td>0.002470</td>
    </tr>
    <tr>
      <th>rips</th>
      <td>0.004732</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># evaluate the estimation performances of OPE estimators </span>
<span class="c1"># by comparing the estimated policy values and its ground-truth.</span>
<span class="c1"># `summarize_estimators_comparison` returns a pandas dataframe containing estimation performances of given estimators </span>

<span class="n">relative_ee_for_anti_optimal_evaluation_policy</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">summarize_estimators_comparison</span><span class="p">(</span>
    <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">ground_truth_policy_value_anti_optimal</span><span class="p">,</span>
    <span class="n">evaluation_policy_pscore</span><span class="o">=</span><span class="n">anti_optimal_policy_pscores</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_item_position</span><span class="o">=</span><span class="n">anti_optimal_policy_pscores</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">evaluation_policy_pscore_cascade</span><span class="o">=</span><span class="n">anti_optimal_policy_pscores</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">relative_ee_for_anti_optimal_evaluation_policy</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>relative-ee</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>sips</th>
      <td>0.010281</td>
    </tr>
    <tr>
      <th>iips</th>
      <td>0.001506</td>
    </tr>
    <tr>
      <th>rips</th>
      <td>0.004751</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The variance of OPE estimators is as follows: <code class="docutils literal notranslate"><span class="pre">SIPS</span> <span class="pre">&gt;</span> <span class="pre">RIPS</span> <span class="pre">&gt;</span> <span class="pre">IIPS</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">estimated_intervals</span><span class="p">[</span><span class="s2">&quot;errbar_length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">estimated_intervals</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;policy_name&quot;</span><span class="p">,</span> <span class="s2">&quot;ground_truth&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;ggplot&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">errplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">yerr</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">)</span>
    <span class="n">data</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">yerr</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;bar&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="c1">#     ax.set_xlabel(&quot;OPE estimator&quot;)</span>
    
<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span>
    <span class="n">estimated_intervals</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;index&quot;</span><span class="p">:</span> <span class="s2">&quot;OPE estimator&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="s2">&quot;Policy value&quot;</span><span class="p">}),</span>
    <span class="n">col</span><span class="o">=</span><span class="s2">&quot;policy_name&quot;</span>
<span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">map_dataframe</span><span class="p">(</span><span class="n">errplot</span><span class="p">,</span> <span class="s2">&quot;OPE estimator&quot;</span><span class="p">,</span> <span class="s2">&quot;Policy value&quot;</span><span class="p">,</span> <span class="s2">&quot;errbar_length&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mf">1.7</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1.7, 1.9)
</pre></div>
</div>
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_179_1.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_179_1.png" />
</div>
</div>
<p>It is surprising that <code class="docutils literal notranslate"><span class="pre">RIPS</span></code> estimator does not achieve the best performance even if the reward structure is not independent. If we run a simulation where the reward of each position depends heavily on those of other positions, <code class="docutils literal notranslate"><span class="pre">RIPS</span></code>estimator could achieve the best performance.</p>
</div>
</div>
<div class="section" id="off-policy-evaluation-of-online-bandit-algorithms">
<h2>Off-Policy Evaluation of Online Bandit Algorithms<a class="headerlink" href="#off-policy-evaluation-of-online-bandit-algorithms" title="Permalink to this headline">¶</a></h2>
<p>This section provides an example of conducting OPE of online bandit algorithms using Replay Method (RM) with synthetic bandit feedback data.
RM uses a subset of the logged bandit feedback data where actions selected by the behavior policy are the same as that of the evaluation policy.
Theoretically, RM is unbiased when the behavior policy is uniformly random and the evaluation policy is fixed.
However, empirically, RM works well when evaluation policies are learning algorithms.
Please refer to <a class="reference external" href="https://arxiv.org/abs/1003.5956">https://arxiv.org/abs/1003.5956</a> about the details of RM.</p>
<p>Our example with online bandit algorithms contains the follwoing three major steps:</p>
<ul class="simple">
<li><p>(1) Synthetic Data Generation</p></li>
<li><p>(2) Off-Policy Evaluation (OPE)</p></li>
<li><p>(3) Evaluation of OPE</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">obp</span>
<span class="kn">from</span> <span class="nn">obp.dataset</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">SyntheticBanditDataset</span><span class="p">,</span>
    <span class="n">logistic_reward_function</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">obp.policy</span> <span class="kn">import</span> <span class="n">EpsilonGreedy</span><span class="p">,</span> <span class="n">LinTS</span><span class="p">,</span> <span class="n">LinUCB</span>
<span class="kn">from</span> <span class="nn">obp.ope</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">OffPolicyEvaluation</span><span class="p">,</span> 
    <span class="n">ReplayMethod</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">obp.simulator</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">calc_ground_truth_policy_value</span><span class="p">,</span>
    <span class="n">run_bandit_simulation</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="synthetic-data-generation">
<h3>Synthetic Data Generation<a class="headerlink" href="#synthetic-data-generation" title="Permalink to this headline">¶</a></h3>
<p>We prepare easy-to-use synthetic data generator: <code class="docutils literal notranslate"><span class="pre">SyntheticBanditDataset</span></code> class in the dataset module.</p>
<p>It takes number of actions (<code class="docutils literal notranslate"><span class="pre">n_actions</span></code>), dimension of context vectors (<code class="docutils literal notranslate"><span class="pre">dim_context</span></code>), reward function (<code class="docutils literal notranslate"><span class="pre">reward_function</span></code>), and behavior policy (<code class="docutils literal notranslate"><span class="pre">behavior_policy_function</span></code>) as inputs and generates a synthetic bandit dataset that can be used to evaluate the performance of decision making policies (obtained by <code class="docutils literal notranslate"><span class="pre">off-policy</span> <span class="pre">learning</span></code>) and OPE estimators.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate a synthetic bandit dataset with 10 actions</span>
<span class="c1"># we use `logistic function` as the reward function</span>
<span class="c1"># we use the uniformly random behavior policy because it is desriable for RM</span>
<span class="c1"># one can define their own reward function and behavior policy such as nonlinear ones. </span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">SyntheticBanditDataset</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">dim_context</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">reward_type</span><span class="o">=</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="c1"># &quot;binary&quot; or &quot;continuous&quot;</span>
    <span class="n">reward_function</span><span class="o">=</span><span class="n">logistic_reward_function</span><span class="p">,</span>
    <span class="n">behavior_policy_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># uniformly random</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># obtain a set of synthetic logged bandit feedback</span>
<span class="n">n_rounds</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">bandit_feedback</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="n">n_rounds</span><span class="o">=</span><span class="n">n_rounds</span><span class="p">)</span>

<span class="c1"># `bandit_feedback` is a dictionary storing synthetic logged bandit feedback</span>
<span class="n">bandit_feedback</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;action&#39;: array([6, 4, 2, ..., 9, 4, 7]),
 &#39;action_context&#39;: array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]),
 &#39;context&#39;: array([[-0.20470766,  0.47894334, -0.51943872, -0.5557303 ,  1.96578057],
        [ 1.39340583,  0.09290788,  0.28174615,  0.76902257,  1.24643474],
        [ 1.00718936, -1.29622111,  0.27499163,  0.22891288,  1.35291684],
        ...,
        [-1.27028221,  0.80914602, -0.45084222,  0.47179511,  1.89401115],
        [-0.68890924,  0.08857502, -0.56359347, -0.41135069,  0.65157486],
        [ 0.51204121,  0.65384817, -1.98849253, -2.14429131, -0.34186901]]),
 &#39;expected_reward&#39;: array([[0.80210203, 0.73828559, 0.83199558, ..., 0.81190503, 0.70617705,
         0.68985306],
        [0.94119582, 0.93473317, 0.91345213, ..., 0.94140688, 0.93152449,
         0.90132868],
        [0.87248862, 0.67974991, 0.66965669, ..., 0.79229752, 0.82712978,
         0.74923536],
        ...,
        [0.66717573, 0.81583571, 0.77012708, ..., 0.87757008, 0.57652468,
         0.80629132],
        [0.52526986, 0.39952563, 0.61892038, ..., 0.53610389, 0.49392728,
         0.58408936],
        [0.55375831, 0.11662199, 0.807396  , ..., 0.22532856, 0.42629292,
         0.24120499]]),
 &#39;n_actions&#39;: 10,
 &#39;n_rounds&#39;: 10000,
 &#39;position&#39;: None,
 &#39;pscore&#39;: array([0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]),
 &#39;reward&#39;: array([1, 1, 1, ..., 0, 1, 0])}
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id11">
<h3>Off-Policy Evaluation (OPE)<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>Our next step is OPE which attempts to estimate the performance of online bandit algorithms using the logged bandit feedback and RM.</p>
<p>Here, we visualize the OPE results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># simulations of online bandit algorithms</span>
<span class="c1"># obtain a deterministic action distribution representing which action is selected at each round in the simulation</span>
<span class="c1"># policies are updated only when the selected action is the same as that of the logged data</span>
<span class="n">epsilon_greedy</span> <span class="o">=</span> <span class="n">EpsilonGreedy</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
    <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span>
<span class="p">)</span>
<span class="n">action_dist_epsilon_greedy</span> <span class="o">=</span> <span class="n">run_bandit_simulation</span><span class="p">(</span>
    <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">,</span>
    <span class="n">policy</span><span class="o">=</span><span class="n">epsilon_greedy</span>
<span class="p">)</span>

<span class="n">lin_ts</span> <span class="o">=</span> <span class="n">LinTS</span><span class="p">(</span>
    <span class="n">dim</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">dim_context</span><span class="p">,</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span>
<span class="p">)</span>
<span class="n">action_dist_lin_ts</span> <span class="o">=</span> <span class="n">run_bandit_simulation</span><span class="p">(</span>
    <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">,</span>
    <span class="n">policy</span><span class="o">=</span><span class="n">lin_ts</span>
<span class="p">)</span>

<span class="n">lin_ucb</span> <span class="o">=</span> <span class="n">LinUCB</span><span class="p">(</span>
    <span class="n">dim</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">dim_context</span><span class="p">,</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span>
<span class="p">)</span>
<span class="n">action_dist_lin_ucb</span> <span class="o">=</span> <span class="n">run_bandit_simulation</span><span class="p">(</span>
    <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">,</span>
    <span class="n">policy</span><span class="o">=</span><span class="n">lin_ucb</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 10000/10000 [00:00&lt;00:00, 63812.95it/s]
100%|██████████| 10000/10000 [00:17&lt;00:00, 569.92it/s]
100%|██████████| 10000/10000 [00:01&lt;00:00, 6985.13it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># estimate the policy value of the online bandit algorithms using RM</span>
<span class="n">ope</span> <span class="o">=</span> <span class="n">OffPolicyEvaluation</span><span class="p">(</span>
    <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">,</span>
    <span class="n">ope_estimators</span><span class="o">=</span><span class="p">[</span><span class="n">ReplayMethod</span><span class="p">()]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># estimate the policy value of EpsilonGreedy</span>
<span class="n">estimated_policy_value_epsilon_greedy</span><span class="p">,</span> <span class="n">estimated_interval_epsilon_greedy</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">summarize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_epsilon_greedy</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">estimated_interval_epsilon_greedy</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># visualize estimated policy values of EpsilonGreedy by the three OPE estimators</span>
<span class="c1"># and their 95% confidence intervals (estimated by nonparametric bootstrap method)</span>
<span class="n">ope</span><span class="o">.</span><span class="n">visualize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_epsilon_greedy</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="c1"># number of resampling performed in the bootstrap procedure</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>    95.0% CI (lower)  95.0% CI (upper)      mean
rm            0.5702          0.658162  0.613388 
</pre></div>
</div>
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_189_1.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_189_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># estimate the policy value of LinTS</span>
<span class="n">estimated_policy_value_lin_ts</span><span class="p">,</span> <span class="n">estimated_interval_lin_ts</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">summarize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_lin_ts</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">estimated_interval_lin_ts</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># visualize estimated policy values of LinTS by the three OPE estimators</span>
<span class="c1"># and their 95% confidence intervals (estimated by nonparametric bootstrap method)</span>
<span class="n">ope</span><span class="o">.</span><span class="n">visualize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_lin_ts</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="c1"># number of resampling performed in the bootstrap procedure</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>    95.0% CI (lower)  95.0% CI (upper)     mean
rm          0.611872          0.713379  0.66498 
</pre></div>
</div>
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_190_1.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_190_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># estimate the policy value of LinUCB</span>
<span class="n">estimated_policy_value_lin_ucb</span><span class="p">,</span> <span class="n">estimated_interval_lin_ucb</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">summarize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_lin_ucb</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">estimated_interval_lin_ucb</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># visualize estimated policy values of LinUCB by the three OPE estimators</span>
<span class="c1"># and their 95% confidence intervals (estimated by nonparametric bootstrap method)</span>
<span class="n">ope</span><span class="o">.</span><span class="n">visualize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_lin_ucb</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="c1"># number of resampling performed in the bootstrap procedure</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>    95.0% CI (lower)  95.0% CI (upper)     mean
rm          0.597293          0.677215  0.64061 
</pre></div>
</div>
<img alt="../_images/T966055_OBP_Library_Workshop_Tutorials_191_1.png" src="../_images/T966055_OBP_Library_Workshop_Tutorials_191_1.png" />
</div>
</div>
<p>RM estimates that LinTS is the best policy.</p>
</div>
<div class="section" id="id12">
<h3>Evaluation of OPE<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<p>Our final step is <strong>the evaluation of OPE</strong>, which evaluates and compares the estimation accuracy of OPE estimators.</p>
<p>With synthetic data, we can calculate the policy value of the evaluation policies.
Therefore, we can compare the policy values estimated by RM with the ground-turths to evaluate the accuracy of OPE.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># we first calculate the policy values of the three evaluation policies</span>
<span class="c1"># in synthetic data, we know p(r|x,a), the reward distribution, so we can perform simulations</span>
<span class="c1"># here, a policy is updated at each round according to actions and rewards sampled from the policy and p(r|x,a)</span>
<span class="c1"># the policy values are averaged over `n_sim` simulations</span>
<span class="n">policy_value_epsilon_greedy</span> <span class="o">=</span> <span class="n">calc_ground_truth_policy_value</span><span class="p">(</span>
    <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">,</span>
    <span class="n">reward_sampler</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">sample_reward</span><span class="p">,</span> <span class="c1"># p(r|x,a)</span>
    <span class="n">policy</span><span class="o">=</span><span class="n">epsilon_greedy</span><span class="p">,</span>
    <span class="n">n_sim</span><span class="o">=</span><span class="mi">3</span> <span class="c1"># the number of simulations</span>
<span class="p">)</span>
<span class="n">policy_value_lin_ts</span> <span class="o">=</span> <span class="n">calc_ground_truth_policy_value</span><span class="p">(</span>
    <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">,</span>
    <span class="n">reward_sampler</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">sample_reward</span><span class="p">,</span> <span class="c1"># p(r|x,a)</span>
    <span class="n">policy</span><span class="o">=</span><span class="n">lin_ts</span><span class="p">,</span>
    <span class="n">n_sim</span><span class="o">=</span><span class="mi">3</span> <span class="c1"># the number of simulations</span>
<span class="p">)</span>
<span class="n">policy_value_lin_ucb</span> <span class="o">=</span> <span class="n">calc_ground_truth_policy_value</span><span class="p">(</span>
    <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">,</span>
    <span class="n">reward_sampler</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">sample_reward</span><span class="p">,</span> <span class="c1"># p(r|x,a)</span>
    <span class="n">policy</span><span class="o">=</span><span class="n">lin_ucb</span><span class="p">,</span>
    <span class="n">n_sim</span><span class="o">=</span><span class="mi">3</span> <span class="c1"># the number of simulations</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;policy value of EpsilonGreedy: </span><span class="si">{</span><span class="n">policy_value_epsilon_greedy</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;policy value of LinTS: </span><span class="si">{</span><span class="n">policy_value_lin_ts</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;policy value of LinUCB: </span><span class="si">{</span><span class="n">policy_value_lin_ucb</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 3/3 [00:15&lt;00:00,  5.12s/it]
100%|██████████| 3/3 [01:08&lt;00:00, 22.84s/it]
100%|██████████| 3/3 [00:20&lt;00:00,  6.93s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>policy value of EpsilonGreedy: 0.655486184283327
policy value of LinTS: 0.7323584842875861
policy value of LinUCB: 0.7098538447648373
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<p>In fact, LinTS reveals the best performance among the three evaluation policies.</p>
<p>Using the above policy values, we evaluate the estimation accuracy of the OPE estimators.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># evaluate the estimation performances of OPE estimators </span>
<span class="c1"># by comparing the estimated policy values of EpsilonGreedy and its ground-truth.</span>
<span class="c1"># `summarize_estimators_comparison` returns a pandas dataframe containing estimation performances of given estimators </span>
<span class="n">relative_ee_epsilon_greedy</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">summarize_estimators_comparison</span><span class="p">(</span>
    <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">policy_value_epsilon_greedy</span><span class="p">,</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_epsilon_greedy</span><span class="p">,</span>
    <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;relative-ee&quot;</span><span class="p">,</span> <span class="c1"># &quot;relative-ee&quot; (relative estimation error) or &quot;se&quot; (squared error)</span>
<span class="p">)</span>

<span class="c1"># estimation performances of the three estimators (lower means accurate)</span>
<span class="n">relative_ee_epsilon_greedy</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>relative-ee</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>rm</th>
      <td>0.068046</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># evaluate the estimation performance of OPE estimators </span>
<span class="c1"># by comparing the estimated policy values of LinTS t and its ground-truth.</span>
<span class="c1"># `summarize_estimators_comparison` returns a pandas dataframe containing estimation performances of given estimators </span>
<span class="n">relative_ee_lin_ts</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">summarize_estimators_comparison</span><span class="p">(</span>
    <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">policy_value_lin_ts</span><span class="p">,</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_lin_ts</span><span class="p">,</span>
    <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;relative-ee&quot;</span><span class="p">,</span> <span class="c1"># &quot;relative-ee&quot; (relative estimation error) or &quot;se&quot; (squared error)</span>
<span class="p">)</span>

<span class="c1"># estimation performances of the three estimators (lower means accurate)</span>
<span class="n">relative_ee_lin_ts</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>relative-ee</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>rm</th>
      <td>0.096554</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># evaluate the estimation performance of OPE estimators </span>
<span class="c1"># by comparing the estimated policy values of LinUCB and its ground-truth.</span>
<span class="c1"># `summarize_estimators_comparison` returns a pandas dataframe containing estimation performances of given estimators </span>
<span class="n">relative_ee_lin_ucb</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">summarize_estimators_comparison</span><span class="p">(</span>
    <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">policy_value_lin_ucb</span><span class="p">,</span>
    <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_lin_ucb</span><span class="p">,</span>
    <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;relative-ee&quot;</span><span class="p">,</span> <span class="c1"># &quot;relative-ee&quot; (relative estimation error) or &quot;se&quot; (squared error)</span>
<span class="p">)</span>

<span class="c1"># estimation performances of the three estimators (lower means accurate)</span>
<span class="n">relative_ee_lin_ucb</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>relative-ee</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>rm</th>
      <td>0.097352</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
</div>
<div class="section" id="off-policy-learners">
<h2>Off-Policy Learners<a class="headerlink" href="#off-policy-learners" title="Permalink to this headline">¶</a></h2>
<p>This section provides an example of implementing several off-policy learning methods with synthetic logged bandit data.</p>
<p>The example consists of the follwoing four major steps:</p>
<ul class="simple">
<li><p>(1) Generating Synthetic Data</p></li>
<li><p>(2) Off-Policy Learning</p></li>
<li><p>(3) Evaluation of Off-Policy Learners</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span> <span class="k">as</span> <span class="n">RandomForest</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="kn">import</span> <span class="nn">obp</span>
<span class="kn">from</span> <span class="nn">obp.dataset</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">SyntheticBanditDataset</span><span class="p">,</span>
    <span class="n">logistic_reward_function</span><span class="p">,</span>
    <span class="n">linear_reward_function</span><span class="p">,</span>
    <span class="n">linear_behavior_policy</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">obp.policy</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">IPWLearner</span><span class="p">,</span> 
    <span class="n">NNPolicyLearner</span><span class="p">,</span> 
    <span class="n">Random</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="generating-synthetic-data">
<h3>Generating Synthetic Data<a class="headerlink" href="#generating-synthetic-data" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">obp.dataset.SyntheticBanditDataset</span></code> is an easy-to-use synthetic data generator.</p>
<p>It takes</p>
<ul class="simple">
<li><p>number of actions (<code class="docutils literal notranslate"><span class="pre">n_actions</span></code>, <span class="math notranslate nohighlight">\(|\mathcal{A}|\)</span>)</p></li>
<li><p>dimension of context vectors (<code class="docutils literal notranslate"><span class="pre">dim_context</span></code>, <span class="math notranslate nohighlight">\(d\)</span>)</p></li>
<li><p>reward function (<code class="docutils literal notranslate"><span class="pre">reward_function</span></code>, <span class="math notranslate nohighlight">\(q(x,a)=\mathbb{E}[r \mid x,a]\)</span>)</p></li>
<li><p>behavior policy (<code class="docutils literal notranslate"><span class="pre">behavior_policy_function</span></code>, <span class="math notranslate nohighlight">\(\pi_b(a|x)\)</span>)</p></li>
</ul>
<p>as inputs and generates a synthetic logged bandit data that can be used to evaluate the performance of decision making policies (obtained by <code class="docutils literal notranslate"><span class="pre">off-policy</span> <span class="pre">learning</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate a synthetic bandit dataset with 10 actions</span>
<span class="c1"># we use `logistic function` as the reward function and `linear_behavior_policy` as the behavior policy.</span>
<span class="c1"># one can define their own reward function and behavior policy such as nonlinear ones. </span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">SyntheticBanditDataset</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">dim_context</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">tau</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="c1"># temperature hyperparameter to control the entropy of the behavior policy</span>
    <span class="n">reward_type</span><span class="o">=</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="c1"># &quot;binary&quot; or &quot;continuous&quot;</span>
    <span class="n">reward_function</span><span class="o">=</span><span class="n">logistic_reward_function</span><span class="p">,</span>
    <span class="n">behavior_policy_function</span><span class="o">=</span><span class="n">linear_behavior_policy</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># obtain training and test sets of synthetic logged bandit data</span>
<span class="n">n_rounds_train</span><span class="p">,</span> <span class="n">n_rounds_test</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">10000</span>
<span class="n">bandit_feedback_train</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="n">n_rounds</span><span class="o">=</span><span class="n">n_rounds_train</span><span class="p">)</span>
<span class="n">bandit_feedback_test</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="n">n_rounds</span><span class="o">=</span><span class="n">n_rounds_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>the logged bandit data is collected by the behavior policy as follows.</p>
<p><span class="math notranslate nohighlight">\( \mathcal{D}_b := \{(x_i,a_i,r_i)\}_{i=1}^n\)</span>  where <span class="math notranslate nohighlight">\((x,a,r) \sim p(x)\pi_b(a \mid x)p(r \mid x,a) \)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># `bandit_feedback` is a dictionary storing synthetic logged bandit feedback</span>
<span class="n">bandit_feedback_train</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;action&#39;: array([6, 1, 1, ..., 0, 1, 6]),
 &#39;action_context&#39;: array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]),
 &#39;context&#39;: array([[-0.20470766,  0.47894334, -0.51943872, -0.5557303 ,  1.96578057],
        [ 1.39340583,  0.09290788,  0.28174615,  0.76902257,  1.24643474],
        [ 1.00718936, -1.29622111,  0.27499163,  0.22891288,  1.35291684],
        ...,
        [-1.27028221,  0.80914602, -0.45084222,  0.47179511,  1.89401115],
        [-0.68890924,  0.08857502, -0.56359347, -0.41135069,  0.65157486],
        [ 0.51204121,  0.65384817, -1.98849253, -2.14429131, -0.34186901]]),
 &#39;expected_reward&#39;: array([[0.80210203, 0.73828559, 0.83199558, ..., 0.81190503, 0.70617705,
         0.68985306],
        [0.94119582, 0.93473317, 0.91345213, ..., 0.94140688, 0.93152449,
         0.90132868],
        [0.87248862, 0.67974991, 0.66965669, ..., 0.79229752, 0.82712978,
         0.74923536],
        ...,
        [0.66717573, 0.81583571, 0.77012708, ..., 0.87757008, 0.57652468,
         0.80629132],
        [0.52526986, 0.39952563, 0.61892038, ..., 0.53610389, 0.49392728,
         0.58408936],
        [0.55375831, 0.11662199, 0.807396  , ..., 0.22532856, 0.42629292,
         0.24120499]]),
 &#39;n_actions&#39;: 10,
 &#39;n_rounds&#39;: 10000,
 &#39;position&#39;: None,
 &#39;pscore&#39;: array([0.29815101, 0.30297159, 0.30297159, ..., 0.04788441, 0.30297159,
        0.29815101]),
 &#39;reward&#39;: array([1, 1, 1, ..., 0, 0, 1])}
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id13">
<h3>Off-Policy Learning<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<p>After generating synthetic data, we now train some decision making policies.</p>
<p>To train policies, we use</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">obp.policy.NNPolicyLearner</span></code> (Neural Network Policy Learner)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">obp.policy.IPWLearner</span></code></p></li>
</ul>
<p>For NN Learner, we use</p>
<ul class="simple">
<li><p>Direct Method (“dm”)</p></li>
<li><p>InverseProbabilityWeighting (“ipw”)</p></li>
<li><p>DoublyRobust (“dr”)</p></li>
</ul>
<p>as its objective functions (<code class="docutils literal notranslate"><span class="pre">off_policy_objective</span></code>).</p>
<p>For IPW Learner, we use <em>RandomForestClassifier</em> and <em>LogisticRegression</em> implemented in scikit-learn for base machine learning methods.</p>
<p>A policy is trained by maximizing an OPE estimator as an objective function as follows.</p>
<div class="math notranslate nohighlight">
\[ \hat{\pi} \in \arg \max_{\pi \in \Pi} \hat{V} (\pi; \mathcal{D}_{tr}) - \lambda \cdot \Omega (\pi)  \]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{V}(\cdot; \mathcal{D})\)</span> is an off-policy objective and <span class="math notranslate nohighlight">\(\mathcal{D}_{tr}\)</span> is a training bandit dataset. <span class="math notranslate nohighlight">\(\Omega (\cdot)\)</span> is a regularization term.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># define NNPolicyLearner with DM as its objective function</span>
<span class="n">nn_dm</span> <span class="o">=</span> <span class="n">NNPolicyLearner</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
    <span class="n">dim_context</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">dim_context</span><span class="p">,</span>
    <span class="n">off_policy_objective</span><span class="o">=</span><span class="s2">&quot;dm&quot;</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># train NNPolicyLearner on the training set of logged bandit data</span>
<span class="n">nn_dm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
    <span class="n">action</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span>
    <span class="n">reward</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="c1"># obtains action choice probabilities for the test set</span>
<span class="n">action_dist_nn_dm</span> <span class="o">=</span> <span class="n">nn_dm</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback_test</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>q-func learning: 100%|██████████| 200/200 [00:24&lt;00:00,  8.10it/s]
policy learning: 100%|██████████| 200/200 [01:06&lt;00:00,  3.01it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># define NNPolicyLearner with IPW as its objective function</span>
<span class="n">nn_ipw</span> <span class="o">=</span> <span class="n">NNPolicyLearner</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
    <span class="n">dim_context</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">dim_context</span><span class="p">,</span>
    <span class="n">off_policy_objective</span><span class="o">=</span><span class="s2">&quot;ipw&quot;</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># train NNPolicyLearner on the training set of logged bandit data</span>
<span class="n">nn_ipw</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
    <span class="n">action</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span>
    <span class="n">reward</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span>
    <span class="n">pscore</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="c1"># obtains action choice probabilities for the test set</span>
<span class="n">action_dist_nn_ipw</span> <span class="o">=</span> <span class="n">nn_ipw</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback_test</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>policy learning: 100%|██████████| 200/200 [00:58&lt;00:00,  3.40it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># define NNPolicyLearner with DR as its objective function</span>
<span class="n">nn_dr</span> <span class="o">=</span> <span class="n">NNPolicyLearner</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
    <span class="n">dim_context</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">dim_context</span><span class="p">,</span>
    <span class="n">off_policy_objective</span><span class="o">=</span><span class="s2">&quot;dr&quot;</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># train NNPolicyLearner on the training set of logged bandit data</span>
<span class="n">nn_dr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
    <span class="n">action</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span>
    <span class="n">reward</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span>
    <span class="n">pscore</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="c1"># obtains action choice probabilities for the test set</span>
<span class="n">action_dist_nn_dr</span> <span class="o">=</span> <span class="n">nn_dr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback_test</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>q-func learning: 100%|██████████| 200/200 [00:25&lt;00:00,  7.99it/s]
policy learning: 100%|██████████| 200/200 [01:21&lt;00:00,  2.46it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># define IPWLearner with Logistic Regression as its base ML model</span>
<span class="n">ipw_lr</span> <span class="o">=</span> <span class="n">IPWLearner</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
    <span class="n">base_classifier</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># train IPWLearner on the training set of logged bandit data</span>
<span class="n">ipw_lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
    <span class="n">action</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span>
    <span class="n">reward</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span>
    <span class="n">pscore</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># obtains action choice probabilities for the test set</span>
<span class="n">action_dist_ipw_lr</span> <span class="o">=</span> <span class="n">ipw_lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback_test</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># define IPWLearner with Random Forest as its base ML model</span>
<span class="n">ipw_rf</span> <span class="o">=</span> <span class="n">IPWLearner</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
    <span class="n">base_classifier</span><span class="o">=</span><span class="n">RandomForest</span><span class="p">(</span>
        <span class="n">n_estimators</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="c1"># train IPWLearner on the training set of logged bandit data</span>
<span class="n">ipw_rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
    <span class="n">action</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span>
    <span class="n">reward</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span>
    <span class="n">pscore</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># obtains action choice probabilities for the test set</span>
<span class="n">action_dist_ipw_rf</span> <span class="o">=</span> <span class="n">ipw_rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback_test</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># define Uniform Random Policy as a baseline evaluation policy</span>
<span class="n">random</span> <span class="o">=</span> <span class="n">Random</span><span class="p">(</span><span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,)</span>

<span class="c1"># compute the action choice probabilities for the test set</span>
<span class="n">action_dist_random</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">compute_batch_action_dist</span><span class="p">(</span>
    <span class="n">n_rounds</span><span class="o">=</span><span class="n">bandit_feedback_test</span><span class="p">[</span><span class="s2">&quot;n_rounds&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># action_dist is a probability distribution over actions (can be deterministic)</span>
<span class="n">action_dist_ipw_lr</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0., 0., 0., ..., 1., 0., 0.],
       [0., 0., 1., ..., 0., 0., 0.],
       [0., 0., 1., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 1., 0.],
       [0., 0., 0., ..., 0., 0., 0.]])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="evaluation-of-off-policy-learners">
<h3>Evaluation of Off-Policy Learners<a class="headerlink" href="#evaluation-of-off-policy-learners" title="Permalink to this headline">¶</a></h3>
<p>Our final step is the evaluation and comparison of the off-policy learnres.</p>
<p>With synthetic data, we can calculate the policy value of the off-policy learners as follows.</p>
<div class="math notranslate nohighlight">
\[V(\pi_e) \approx \frac{1}{|\mathcal{D}_{te}|} \sum_{i=1}^{|\mathcal{D}_{te}|} \mathbb{E}_{a \sim \pi_e(a|x_i)} [q(x_i, a)], \; \, where \; \, q(x,a) := \mathbb{E}_{r \sim p(r|x,a)} [r]\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{D}_{te}\)</span> is the test set of logged bandit data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># we calculate the policy values of the trained policies based on the expected rewards of the test data</span>
<span class="n">policy_names</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;NN Policy Learner with DM&quot;</span><span class="p">,</span>
    <span class="s2">&quot;NN Policy Learner with IPW&quot;</span><span class="p">,</span>
    <span class="s2">&quot;NN Policy Learner with DR&quot;</span><span class="p">,</span>
    <span class="s2">&quot;IPW Learner with Logistic Regression&quot;</span><span class="p">,</span>
    <span class="s2">&quot;IPW Learner with Random Forest&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Unifrom Random&quot;</span>
<span class="p">]</span>
<span class="n">action_dist_list</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">action_dist_nn_dm</span><span class="p">,</span>
    <span class="n">action_dist_nn_ipw</span><span class="p">,</span>
    <span class="n">action_dist_nn_dr</span><span class="p">,</span>
    <span class="n">action_dist_ipw_lr</span><span class="p">,</span>
    <span class="n">action_dist_ipw_rf</span><span class="p">,</span>
    <span class="n">action_dist_random</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">action_dist</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">policy_names</span><span class="p">,</span> <span class="n">action_dist_list</span><span class="p">):</span>
    <span class="n">true_policy_value</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">calc_ground_truth_policy_value</span><span class="p">(</span>
        <span class="n">expected_reward</span><span class="o">=</span><span class="n">bandit_feedback_test</span><span class="p">[</span><span class="s2">&quot;expected_reward&quot;</span><span class="p">],</span>
        <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;policy value of </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">true_policy_value</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>policy value of NN Policy Learner with DM: 0.7404426113352143
policy value of NN Policy Learner with IPW: 0.7212314908365143
policy value of NN Policy Learner with DR: 0.7236078671760903
policy value of IPW Learner with Logistic Regression: 0.7225216225722526
policy value of IPW Learner with Random Forest: 0.6826465969408197
policy value of Unifrom Random: 0.6056038101021686
</pre></div>
</div>
</div>
</div>
<p>In fact, NNPolicyLearner maximizing the DM estimator seems the best in this simple setting.</p>
<p>We can iterate the above process several times to get more relibale results.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./nbs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="T167249_Offline_Policy_Evaluation_with_VW_Command_Line.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Offline Policy Evaluation with VW Command Line</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="T632722_PyTorch_Fundamentals_Part_1.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">PyTorch Fundamentals Part 1</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Sparsh A.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>