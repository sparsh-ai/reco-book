!pip install git+https://github.com/maciejkula/spotlight.git@master#egg=spotlight
!git clone https://github.com/microsoft/recommenders.git
!pip install cornac
!pip install pandas==0.25.0

import os
import sys
import math
import random
import datetime
import itertools
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
from scipy.sparse import csr_matrix, dok_matrix
from sklearn.model_selection import ParameterGrid

from fastai.collab import *
from fastai.tabular import *
from fastai.text import *

import cornac

from spotlight.interactions import Interactions
from spotlight.interactions import SequenceInteractions
from spotlight.cross_validation import random_train_test_split
from spotlight.cross_validation import user_based_train_test_split
from spotlight.factorization.implicit import ImplicitFactorizationModel
from spotlight.evaluation import mrr_score
from spotlight.evaluation import precision_recall_score

from spotlight.interactions import Interactions
from spotlight.cross_validation import random_train_test_split
from spotlight.cross_validation import user_based_train_test_split
from spotlight.factorization.implicit import ImplicitFactorizationModel
from spotlight.evaluation import mrr_score
from spotlight.evaluation import precision_recall_score

from spotlight.interactions import SequenceInteractions
from spotlight.sequence.implicit import ImplicitSequenceModel
from spotlight.evaluation import sequence_mrr_score
from spotlight.evaluation import sequence_precision_recall_score

import warnings
warnings.filterwarnings("ignore")

sys.path.append('/content/recommenders/')
from reco_utils.dataset.python_splitters import python_chrono_split
from reco_utils.evaluation.python_evaluation import map_at_k
from reco_utils.evaluation.python_evaluation import precision_at_k
from reco_utils.evaluation.python_evaluation import ndcg_at_k 
from reco_utils.evaluation.python_evaluation import recall_at_k
from reco_utils.evaluation.python_evaluation import get_top_k_items
from reco_utils.recommender.cornac.cornac_utils import predict_ranking

# loading data
df = pd.read_csv('rawdata.csv', header = 0,
                 names = ['event','userid','itemid','timestamp'],
                 dtype={0:'category', 1:'category', 2:'category'},
                 parse_dates=['timestamp'])

df.head()

df.info()

# dropping exact duplicates
df = df.drop_duplicates()

# userid normalization
userid_encoder = preprocessing.LabelEncoder()
df.userid = userid_encoder.fit_transform(df.userid)

# itemid normalization
itemid_encoder = preprocessing.LabelEncoder()
df.itemid = itemid_encoder.fit_transform(df.itemid)

df.describe().T

df.describe(exclude='int').T

df.timestamp.max() - df.timestamp.min()

df.event.value_counts()

df.event.value_counts()/df.userid.nunique()

# User events
user_activity_count = dict()
for row in df.itertuples():
    if row.userid not in user_activity_count:
        user_activity_count[row.userid] = {'view_item':0, 
                                           'add_to_cart':0,
                                           'begin_checkout':0,
                                           'remove_from_cart':0, 
                                           'purchase':0}
    if row.event == 'view_item':
        user_activity_count[row.userid]['view_item'] += 1
    elif row.event == 'add_to_cart':
        user_activity_count[row.userid]['add_to_cart'] += 1
    elif row.event == 'begin_checkout':
        user_activity_count[row.userid]['begin_checkout'] += 1
    elif row.event == 'remove_from_cart':
        user_activity_count[row.userid]['remove_from_cart'] += 1
    elif row.event == 'purchase':
        user_activity_count[row.userid]['purchase'] += 1

user_activity = pd.DataFrame(user_activity_count)
user_activity = user_activity.transpose()
user_activity['activity'] = user_activity.sum(axis=1)

tempDF = pd.DataFrame(user_activity.activity.value_counts()).reset_index()
tempDF.columns = ['#Interactions','#Users']
sns.scatterplot(x='#Interactions', y='#Users', data=tempDF);

#hide
df_activity = user_activity.copy()
event = df_activity.columns.astype('str')
sns.countplot(df_activity.loc[df_activity[event[0]]>0,event[0]]);

sns.countplot(df_activity.loc[df_activity[event[1]]>0,event[1]])
plt.show()

sns.countplot(df_activity.loc[df_activity[event[4]]>0,event[4]])
plt.show()

# item events
item_activity_count = dict()
for row in df.itertuples():
    if row.itemid not in item_activity_count:
        item_activity_count[row.itemid] = {'view_item':0, 
                                           'add_to_cart':0,
                                           'begin_checkout':0,
                                           'remove_from_cart':0, 
                                           'purchase':0}
    if row.event == 'view_item':
        item_activity_count[row.itemid]['view_item'] += 1
    elif row.event == 'add_to_cart':
        item_activity_count[row.itemid]['add_to_cart'] += 1
    elif row.event == 'begin_checkout':
        item_activity_count[row.itemid]['begin_checkout'] += 1
    elif row.event == 'remove_from_cart':
        item_activity_count[row.itemid]['remove_from_cart'] += 1
    elif row.event == 'purchase':
        item_activity_count[row.itemid]['purchase'] += 1

item_activity = pd.DataFrame(item_activity_count)
item_activity = item_activity.transpose()
item_activity['activity'] = item_activity.sum(axis=1)

tempDF = pd.DataFrame(item_activity.activity.value_counts()).reset_index()
tempDF.columns = ['#Interactions','#Items']
sns.scatterplot(x='#Interactions', y='#Items', data=tempDF);

plt.rcParams['figure.figsize'] = 15,3
data = pd.DataFrame(pd.to_datetime(df['timestamp'], infer_datetime_format=True))
data['Count'] = 1
data.set_index('timestamp', inplace=True)
data = data.resample('D').apply({'Count':'count'})
ax = data['Count'].plot(marker='o', linestyle='-')

def top_trending(n, timeperiod, timestamp):
  start = str(timestamp.replace(microsecond=0) - pd.Timedelta(minutes=timeperiod))
  end = str(timestamp.replace(microsecond=0))
  trending_items = df.loc[(df.timestamp.between(start,end) & (df.event=='view_item')),:].sort_values('timestamp', ascending=False)
  return trending_items.itemid.value_counts().index[:n]

user_current_time = df.timestamp[100]
top_trending(5, 50, user_current_time)

def least_n_items(n=10):
  temp1 = df.loc[df.event=='view_item'].groupby(['itemid'])['event'].count().sort_values(ascending=True).reset_index()
  temp2 = df.groupby('itemid').timestamp.max().reset_index()
  item_ids = pd.merge(temp1,temp2,on='itemid').sort_values(['event', 'timestamp'], ascending=[True, False]).reset_index().loc[:n-1,'itemid']
  return itemid_encoder.inverse_transform(item_ids.values)

least_n_items(10)

data_count = df.groupby(['userid', 'itemid']).agg({'timestamp': 'count'}).reset_index()
data_count.columns = ['userid', 'itemid', 'affinity']
data_count.head()

data_w = df.loc[df.event!='remove_from_cart',:]

affinity_weights = {
    'view_item': 1,
    'add_to_cart': 3, 
    'begin_checkout': 5, 
    'purchase': 6,
    'remove_from_cart': 3
}

data_w['event'].apply(lambda x: affinity_weights[x])

data_w.head()

data_w['weight'] = data_w['event'].apply(lambda x: affinity_weights[x])
data_wcount = data_w.groupby(['userid', 'itemid'])['weight'].sum().reset_index()
data_wcount.columns = ['userid', 'itemid', 'affinity']
data_wcount.head()

T = 30
t_ref = datetime.datetime.utcnow()

data_w['timedecay'] = data_w.apply(
    lambda x: x['weight'] * math.exp(-math.log2((t_ref - pd.to_datetime(x['timestamp']).tz_convert(None)).days / T)), 
    axis=1
)

data_w.head()

data_wt = data_w.groupby(['userid', 'itemid'])['timedecay'].sum().reset_index()
data_wt.columns = ['userid', 'itemid', 'affinity']
data_wt.head()

data = data_w[['userid','itemid','timedecay','timestamp']]

col = {
  'col_user': 'userid',
  'col_item': 'itemid',
  'col_rating': 'timedecay',
  'col_timestamp': 'timestamp',
}

col3 = {
  'col_user': 'userid',
  'col_item': 'itemid',
  'col_timestamp': 'timestamp',
}

train, test = python_chrono_split(data, ratio=0.75, min_rating=10, 
                                  filter_by='user', **col3)

train.loc[train.userid==7,:]

test.loc[test.userid==7,:]

# Recommending the most popular items is intuitive and simple approach
item_counts = train['itemid'].value_counts().to_frame().reset_index()
item_counts.columns = ['itemid', 'count']
item_counts.head()

user_item_col = ['userid', 'itemid']

# Cross join users and items
test_users = test['userid'].unique()
user_item_list = list(itertools.product(test_users, item_counts['itemid']))
users_items = pd.DataFrame(user_item_list, columns=user_item_col)

print("Number of user-item pairs:", len(users_items))

# Remove seen items (items in the train set) as we will not recommend those again to the users
from reco_utils.dataset.pandas_df_utils import filter_by
users_items_remove_seen = filter_by(users_items, train, user_item_col)

print("After remove seen items:", len(users_items_remove_seen))

# Generate recommendations
baseline_recommendations = pd.merge(item_counts, users_items_remove_seen, 
                                    on=['itemid'], how='inner')
baseline_recommendations.head()

k = 10

cols = {
  'col_user': 'userid',
  'col_item': 'itemid',
  'col_rating': 'timedecay',
  'col_prediction': 'count',
}

eval_map = map_at_k(test, baseline_recommendations, k=k, **cols)
eval_ndcg = ndcg_at_k(test, baseline_recommendations, k=k, **cols)
eval_precision = precision_at_k(test, baseline_recommendations, k=k, **cols)
eval_recall = recall_at_k(test, baseline_recommendations, k=k, **cols)

print("MAP:\t%f" % eval_map,
      "NDCG@K:\t%f" % eval_ndcg,
      "Precision@K:\t%f" % eval_precision,
      "Recall@K:\t%f" % eval_recall, sep='\n')

TOP_K = 10
NUM_FACTORS = 200
NUM_EPOCHS = 100
SEED = 40

train_set = cornac.data.Dataset.from_uir(train.itertuples(index=False), seed=SEED)

bpr = cornac.models.BPR(
    k=NUM_FACTORS,
    max_iter=NUM_EPOCHS,
    learning_rate=0.01,
    lambda_reg=0.001,
    verbose=True,
    seed=SEED
)

from reco_utils.common.timer import Timer
with Timer() as t:
    bpr.fit(train_set)
print("Took {} seconds for training.".format(t))

with Timer() as t:
    all_predictions = predict_ranking(bpr, train, usercol='userid', itemcol='itemid', remove_seen=True)
print("Took {} seconds for prediction.".format(t))

all_predictions.head()

k = 10
cols = {
  'col_user': 'userid',
  'col_item': 'itemid',
  'col_rating': 'timedecay',
  'col_prediction': 'prediction',
}

eval_map = map_at_k(test, all_predictions, k=k, **cols)
eval_ndcg = ndcg_at_k(test, all_predictions, k=k, **cols)
eval_precision = precision_at_k(test, all_predictions, k=k, **cols)
eval_recall = recall_at_k(test, all_predictions, k=k, **cols)

print("MAP:\t%f" % eval_map,
      "NDCG:\t%f" % eval_ndcg,
      "Precision@K:\t%f" % eval_precision,
      "Recall@K:\t%f" % eval_recall, sep='\n')

from reco_utils.recommender.sar.sar_singlenode import SARSingleNode

TOP_K = 10

header = {
    "col_user": "userid",
    "col_item": "itemid",
    "col_rating": "timedecay",
    "col_timestamp": "timestamp",
    "col_prediction": "prediction",
}

model = SARSingleNode(
    similarity_type="jaccard", 
    time_decay_coefficient=0, 
    time_now=None, 
    timedecay_formula=False, 
    **header
)

model.fit(train)

top_k = model.recommend_k_items(test, remove_seen=True)

# all ranking metrics have the same arguments
args = [test, top_k]
kwargs = dict(col_user='userid', 
              col_item='itemid', 
              col_rating='timedecay', 
              col_prediction='prediction', 
              relevancy_method='top_k', 
              k=TOP_K)

eval_map = map_at_k(*args, **kwargs)
eval_ndcg = ndcg_at_k(*args, **kwargs)
eval_precision = precision_at_k(*args, **kwargs)
eval_recall = recall_at_k(*args, **kwargs)

print(f"Model:",
      f"Top K:\t\t {TOP_K}",
      f"MAP:\t\t {eval_map:f}",
      f"NDCG:\t\t {eval_ndcg:f}",
      f"Precision@K:\t {eval_precision:f}",
      f"Recall@K:\t {eval_recall:f}", sep='\n')

interactions = Interactions(user_ids = df.userid.astype('int32').values,
                            item_ids = df.itemid.astype('int32').values,
                            timestamps = df.timestamp.astype('int32'),
                            num_users = df.userid.nunique(),
                            num_items = df.itemid.nunique())

train_user, test_user = random_train_test_split(interactions, test_percentage=0.2)

model = ImplicitFactorizationModel(loss='bpr', embedding_dim=64, n_iter=10, 
                                   batch_size=256, l2=0.0, learning_rate=0.01, 
                                   optimizer_func=None, use_cuda=False, 
                                   representation=None, sparse=False, 
                                   num_negative_samples=10)

model.fit(train_user, verbose=1)

pr = precision_recall_score(model, test=test_user, train=train_user, k=10)
print('Pricison@10 is {:.3f} and Recall@10 is {:.3f}'.format(pr[0].mean(), pr[1].mean()))

interactions = Interactions(user_ids = df.userid.astype('int32').values,
                            item_ids = df.itemid.astype('int32').values,
                            timestamps = df.timestamp.astype('int32'),
                            num_users = df.userid.nunique(),
                            num_items = df.itemid.nunique())

train_user, test_user = random_train_test_split(interactions, test_percentage=0.2)

params_grid = {'loss':['bpr', 'hinge'],
               'embedding_dim':[32, 64],
               'learning_rate': [0.01, 0.05, 0.1],
               'num_negative_samples': [5,10,50]
               }
grid = ParameterGrid(params_grid)

for p in grid:
  model = ImplicitFactorizationModel(**p, n_iter=10, batch_size=256, l2=0.0, 
                                    optimizer_func=None, use_cuda=False, 
                                    representation=None, sparse=False)
  model.fit(train_user, verbose=1)
  pr = precision_recall_score(model, test=test_user, train=train_user, k=10)
  print('Pricison@10 is {:.3f} and Recall@10 is {:.3f}'.format(pr[0].mean(), pr[1].mean()))

interactions = Interactions(user_ids = df.userid.astype('int32').values,
                            item_ids = df.itemid.astype('int32').values+1,
                            timestamps = df.timestamp.astype('int32'))

train, test = random_train_test_split(interactions, test_percentage=0.2)
train_seq = train.to_sequence(max_sequence_length=10)
test_seq = test.to_sequence(max_sequence_length=10)

model = ImplicitSequenceModel(loss='bpr', representation='pooling', 
                              embedding_dim=32, n_iter=10, batch_size=256, 
                              l2=0.0, learning_rate=0.01, optimizer_func=None, 
                              use_cuda=False, sparse=False, num_negative_samples=5)

model.fit(train_seq, verbose=1)

mrr_seq = sequence_mrr_score(model, test_seq)
mrr_seq.mean()

df['rating'] = df['event'].map({'view_item': 1,
                                'add_to_cart': 2,
                                'begin_checkout': 3, 
                                'purchase': 5,
                               'remove_from_cart': 0,
                                })

ratings = df[["userid", 'itemid', "rating", 'timestamp']].copy()

data = CollabDataBunch.from_df(ratings, seed=42)
data

learn = collab_learner(data, n_factors=50, y_range=[0,5.5])
learn.lr_find()
learn.recorder.plot(skip_end=15)

learn.fit_one_cycle(1, 5e-6)

learn.summary()

learn.fit(10, 1e-3)
