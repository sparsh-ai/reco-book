{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwoWv1vAW4HP"
   },
   "source": [
    "# Sequence Aware Recommender Systems\n",
    "> A tutorial on Sequence-Aware Recommender Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ZKDe2jqyM0s"
   },
   "outputs": [],
   "source": [
    "!pip install pymining\n",
    "!pip install treelib==1.5.3\n",
    "!pip install networkx==1.11\n",
    "!pip install theano==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsvfrPtTY5Mi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 14:00:03,787 - INFO - 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "import random\n",
    "from scipy.sparse import find\n",
    "import logging\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from functools import reduce\n",
    "import networkx as nx\n",
    "import uuid\n",
    "import treelib\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from collections import OrderedDict\n",
    "from theano import function\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import subprocess\n",
    "\n",
    "import csv\n",
    "import math\n",
    "from numba import jit\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0kziAt6V91T"
   },
   "outputs": [],
   "source": [
    "# WnB Remote Logging Setup\n",
    "!pip install -q wandb\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIaE8u_YgqyT"
   },
   "source": [
    "```text\n",
    "wandb: You can find your API key in your browser here: https://wandb.ai/authorize\n",
    "wandb: Paste an API key from your profile and hit enter: ··········\n",
    "wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
    "True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oP9H0Ouiafqy"
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_test_sequences(test_data, given_k):\n",
    "  # we can run evaluation only over sequences longer than abs(LAST_K)\n",
    "  test_sequences = test_data.loc[test_data['sequence'].map(len) > abs(given_k), 'sequence'].values\n",
    "  return test_sequences\n",
    "\n",
    "  def get_test_sequences_and_users(test_data, given_k, train_users):\n",
    "    # we can run evaluation only over sequences longer than abs(LAST_K)\n",
    "    mask = test_data['sequence'].map(len) > abs(given_k)\n",
    "    mask &= test_data['user_id'].isin(train_users)\n",
    "    test_sequences = test_data.loc[mask, 'sequence'].values\n",
    "    test_users = test_data.loc[mask, 'user_id'].values\n",
    "    return test_sequences, test_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-fIXG7AQWLs"
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4ruV0ZBa7Ud"
   },
   "outputs": [],
   "source": [
    "!mkdir datasets && \\\n",
    "cd datasets && \\\n",
    "wget https://raw.githubusercontent.com/mquad/sars_tutorial/master/datasets/sessions.zip && \\\n",
    "unzip sessions.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFqhpSU0cF9Q"
   },
   "outputs": [],
   "source": [
    "dataset_path = 'datasets/sessions.csv'\n",
    "# load this sample if you experience a severe slowdown with the previous dataset\n",
    "dataset_path = 'datasets/sessions_sample_10.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nal4L4GvdzkW"
   },
   "outputs": [],
   "source": [
    "def load_and_adapt(path, last_months=0):\n",
    "    file_ext = os.path.splitext(path)[-1]\n",
    "    if file_ext == '.csv':\n",
    "        data = pd.read_csv(path, header=0)\n",
    "    elif file_ext == '.hdf':\n",
    "        data = pd.read_hdf(path)\n",
    "    else:\n",
    "        raise ValueError('Unsupported file {} having extension {}'.format(path, file_ext))\n",
    "\n",
    "    col_names = ['session_id', 'user_id', 'item_id', 'ts'] + data.columns.values.tolist()[4:]\n",
    "    data.columns = col_names\n",
    "\n",
    "    if last_months > 0:\n",
    "        def add_months(sourcedate, months):\n",
    "            month = sourcedate.month - 1 + months\n",
    "            year = int(sourcedate.year + month / 12)\n",
    "            month = month % 12 + 1\n",
    "            day = min(sourcedate.day, calendar.monthrange(year, month)[1])\n",
    "            return datetime.date(year, month, day)\n",
    "\n",
    "        lastdate = datetime.datetime.fromtimestamp(data.ts.max())\n",
    "        firstdate = add_months(lastdate, -last_months)\n",
    "        initial_unix = time.mktime(firstdate.timetuple())\n",
    "\n",
    "        # filter out older interactions\n",
    "        data = data[data['ts'] >= initial_unix]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8JJjpyIcU2W"
   },
   "outputs": [],
   "source": [
    "def create_seq_db_filter_top_k(path, topk=0, last_months=0):\n",
    "    file = load_and_adapt(path, last_months=last_months)\n",
    "\n",
    "    c = Counter(list(file['item_id']))\n",
    "\n",
    "    if topk > 1:\n",
    "        keeper = set([x[0] for x in c.most_common(topk)])\n",
    "        file = file[file['item_id'].isin(keeper)]\n",
    "\n",
    "    # group by session id and concat song_id\n",
    "    groups = file.groupby('session_id')\n",
    "\n",
    "    # convert item ids to string, then aggregate them to lists\n",
    "    aggregated = groups['item_id'].agg(sequence = lambda x: list(map(str, x)))\n",
    "    init_ts = groups['ts'].min()\n",
    "    users = groups['user_id'].min()  # it's just fast, min doesn't actually make sense\n",
    "\n",
    "    result = aggregated.join(init_ts).join(users)\n",
    "    result.reset_index(inplace=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mOlVf2Rcrtr"
   },
   "outputs": [],
   "source": [
    "# for the sake of speed, let's keep only the top-1k most popular items in the last month\n",
    "dataset = create_seq_db_filter_top_k(path=dataset_path, topk=1000, last_months=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuI4vEF-dXuR"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>ts</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>357</td>\n",
       "      <td>[793, 3489]</td>\n",
       "      <td>1421003874</td>\n",
       "      <td>4296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>359</td>\n",
       "      <td>[1762]</td>\n",
       "      <td>1421018535</td>\n",
       "      <td>4296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>394</td>\n",
       "      <td>[1256]</td>\n",
       "      <td>1421007470</td>\n",
       "      <td>30980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4127</td>\n",
       "      <td>[1948, 1364, 2060, 1115, 6488, 2060]</td>\n",
       "      <td>1421416896</td>\n",
       "      <td>28117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6400</td>\n",
       "      <td>[687, 1394]</td>\n",
       "      <td>1420807778</td>\n",
       "      <td>35247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id                              sequence          ts  user_id\n",
       "0         357                           [793, 3489]  1421003874     4296\n",
       "1         359                                [1762]  1421018535     4296\n",
       "2         394                                [1256]  1421007470    30980\n",
       "3        4127  [1948, 1364, 2060, 1115, 6488, 2060]  1421416896    28117\n",
       "4        6400                           [687, 1394]  1420807778    35247"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kB_9Z50VQPvG"
   },
   "source": [
    "### Data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MeokKkSne6VL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items: 1000\n",
      "Number of users: 4165\n",
      "Number of sessions: 6765\n",
      "\n",
      "Session length:\n",
      "\tAverage: 4.29\n",
      "\tMedian: 3.0\n",
      "\tMin: 1\n",
      "\tMax: 148\n",
      "Sessions per user:\n",
      "\tAverage: 1.62\n",
      "\tMedian: 1.0\n",
      "\tMin: 1\n",
      "\tMax: 13\n",
      "Most popular items: [('443', 207), ('1065', 155), ('67', 146), ('2308', 138), ('658', 131)]\n"
     ]
    }
   ],
   "source": [
    "cnt = Counter()\n",
    "dataset.sequence.map(cnt.update);\n",
    "\n",
    "sequence_length = dataset.sequence.map(len).values\n",
    "n_sessions_per_user = dataset.groupby('user_id').size()\n",
    "\n",
    "print('Number of items: {}'.format(len(cnt)))\n",
    "print('Number of users: {}'.format(dataset.user_id.nunique()))\n",
    "print('Number of sessions: {}'.format(len(dataset)) )\n",
    "\n",
    "print('\\nSession length:\\n\\tAverage: {:.2f}\\n\\tMedian: {}\\n\\tMin: {}\\n\\tMax: {}'.format(\n",
    "    sequence_length.mean(), \n",
    "    np.quantile(sequence_length, 0.5), \n",
    "    sequence_length.min(), \n",
    "    sequence_length.max()))\n",
    "\n",
    "print('Sessions per user:\\n\\tAverage: {:.2f}\\n\\tMedian: {}\\n\\tMin: {}\\n\\tMax: {}'.format(\n",
    "    n_sessions_per_user.mean(), \n",
    "    np.quantile(n_sessions_per_user, 0.5), \n",
    "    n_sessions_per_user.min(), \n",
    "    n_sessions_per_user.max()))\n",
    "\n",
    "print('Most popular items: {}'.format(cnt.most_common(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9ou3sHJfQDU"
   },
   "source": [
    "### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcmr4OhMfoim"
   },
   "outputs": [],
   "source": [
    "def random_holdout(dataset, perc=0.8, seed=1234):\n",
    "    \"\"\"\n",
    "    Split sequence dataset randomly\n",
    "    :param dataset: the sequence dataset\n",
    "    :param perc: the training percentange\n",
    "    :param seed: the random seed\n",
    "    :return: the training and test splits\n",
    "    \"\"\"\n",
    "    dataset = dataset.sample(frac=1, random_state=seed)\n",
    "    nseqs = len(dataset)\n",
    "    train_size = int(nseqs * perc)\n",
    "    # split data according to the shuffled index and the holdout size\n",
    "    train_split = dataset[:train_size]\n",
    "    test_split = dataset[train_size:]\n",
    "\n",
    "    return train_split, test_split\n",
    "\n",
    "\n",
    "def temporal_holdout(dataset, ts_threshold):\n",
    "    \"\"\"\n",
    "    Split sequence dataset using timestamps\n",
    "    :param dataset: the sequence dataset\n",
    "    :param ts_threshold: the timestamp from which test sequences will start\n",
    "    :return: the training and test splits\n",
    "    \"\"\"\n",
    "    train = dataset.loc[dataset['ts'] < ts_threshold]\n",
    "    test = dataset.loc[dataset['ts'] >= ts_threshold]\n",
    "    train, test = clean_split(train, test)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def last_session_out_split(data,\n",
    "                           user_key='user_id',\n",
    "                           session_key='session_id',\n",
    "                           time_key='ts'):\n",
    "    \"\"\"\n",
    "    Assign the last session of every user to the test set and the remaining ones to the training set\n",
    "    \"\"\"\n",
    "    sessions = data.sort_values(by=[user_key, time_key]).groupby(user_key)[session_key]\n",
    "    last_session = sessions.last()\n",
    "    train = data[~data.session_id.isin(last_session.values)].copy()\n",
    "    test = data[data.session_id.isin(last_session.values)].copy()\n",
    "    train, test = clean_split(train, test)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def clean_split(train, test):\n",
    "    \"\"\"\n",
    "    Remove new items from the test set.\n",
    "    :param train: The training set.\n",
    "    :param test: The test set.\n",
    "    :return: The cleaned training and test sets.\n",
    "    \"\"\"\n",
    "    train_items = set()\n",
    "    train['sequence'].apply(lambda seq: train_items.update(set(seq)))\n",
    "    test['sequence'] = test['sequence'].apply(lambda seq: [it for it in seq if it in train_items])\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def balance_dataset(x, y):\n",
    "    number_of_elements = y.shape[0]\n",
    "    nnz = set(find(y)[0])\n",
    "    zero = set(range(number_of_elements)).difference(nnz)\n",
    "\n",
    "    max_samples = min(len(zero), len(nnz))\n",
    "\n",
    "    nnz_indices = random.sample(nnz, max_samples)\n",
    "    zero_indeces = random.sample(zero, max_samples)\n",
    "    indeces = nnz_indices + zero_indeces\n",
    "\n",
    "    return x[indeces, :], y[indeces, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2TBhZR_fTZL"
   },
   "source": [
    "For simplicity, let's split the dataset by assigning the last session of every user to the test set, and all the previous ones to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCUfC4B5fH7z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sessions: 2600 - Test sessions: 4165\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = last_session_out_split(dataset)\n",
    "print(\"Train sessions: {} - Test sessions: {}\".format(len(train_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXY9pNOvgFKJ"
   },
   "source": [
    "## Fitting the recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-pVMWfHSxpc"
   },
   "source": [
    "### Algorithm summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkpD2FBug-d8"
   },
   "source": [
    "<img src='https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fec4cd24b-d212-4225-93b8-f021d2dc5fc1%2FUntitled.png?table=block&id=6e90aea9-6ab5-4074-a76f-08a5ddf0d124&spaceId=63b72b1f-0e90-4ab8-a6df-a060a6545a56&width=2000&userId=21ec183f-f0be-4b6b-9b3e-6f0d4e5c5469&cache=v2'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Qwsz2Aigy_V"
   },
   "outputs": [],
   "source": [
    "class ISeqRecommender(object):\n",
    "    \"\"\"Abstract Recommender class\"\"\"\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ISeqRecommender, self).__init__()\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        pass\n",
    "\n",
    "    def recommend(self, user_profile, user_id=None):\n",
    "        \"\"\"\n",
    "        Given the user profile return a list of recommendation\n",
    "        :param user_profile: the user profile as a list of item identifiers\n",
    "        :param user_id: (optional) the user id\n",
    "        :return: list of recommendations e.g. [([2], 0.875), ([6], 1.0)]\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def get_recommendation_list(recommendation):\n",
    "        return list(map(lambda x: x[0], recommendation))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_recommendation_confidence_list(recommendation):\n",
    "        return list(map(lambda x: x[1], recommendation))\n",
    "\n",
    "    def activate_debug_print(self):\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    def deactivate_debug_print(self):\n",
    "        self.logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5r-10UO6gHLV"
   },
   "source": [
    "### Popularity recommender\n",
    "\n",
    "```PopularityRecommender``` simply recommends items ordered by their popularity in the training set.\n",
    "\n",
    "It doesn't have any hyper-parameter, so we can move on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9ONhJ9eg_wm"
   },
   "outputs": [],
   "source": [
    "class PopularityRecommender(ISeqRecommender):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PopularityRecommender, self).__init__()\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        sequences = train_data['sequence'].values\n",
    "\n",
    "        count_dict = {}\n",
    "        for s in sequences:\n",
    "            for item in s:\n",
    "                if item not in count_dict:\n",
    "                    count_dict[item] = 1\n",
    "                else:\n",
    "                    count_dict[item] += 1\n",
    "\n",
    "        self.top = sorted(count_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        self.top = [([x[0]], x[1]) for x in self.top]\n",
    "\n",
    "    def recommend(self, user_profile, user_id=None):\n",
    "        return self.top\n",
    "\n",
    "    def get_popular_list(self):\n",
    "        return self.top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUGpGUUFfzgE"
   },
   "outputs": [],
   "source": [
    "poprecommender = PopularityRecommender()\n",
    "poprecommender.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hW4LxkbwuQ5"
   },
   "source": [
    "### Frequent Sequential Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgCNHoiDitgA"
   },
   "source": [
    "<img src='https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fdf7f4468-d71e-4855-9d06-087f3e89bbab%2FUntitled.png?table=block&id=222f2d0b-e4da-4e38-af6c-fa462a830817&spaceId=63b72b1f-0e90-4ab8-a6df-a060a6545a56&width=2000&userId=21ec183f-f0be-4b6b-9b3e-6f0d4e5c5469&cache=v2'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEpfCUXVwxzT"
   },
   "source": [
    "This algorithm extract Frequent Sequential Patterns from all the training sequences. Patterns are having support lower than `minsup` are discarded (support = # occurrences of a pattern in the traning data).  \n",
    "Recommendations are then generated by looking for patterns having a _prefix_ corresponding to the last `[max_context, min_context]` elements in the user profile, taken in order. Matches are then sorted by decreasing _confidence_ score (ratio between the support of the matched rule and the support of the context). Matches having confidence below `minconf` are discarded.\n",
    "\n",
    "The class `FSMRecommender` has the following initialization hyper-parameters:\n",
    "* `minsup`: the minimum support threshold. It is interpreted as relative count if in \\[0-1\\], otherwise as an absolute count. NOTE: Relative count required for training with SPFM (faster).\n",
    "* `minconf`: the minimum confidence threshold. Use to filter irrelevent recommendations.\n",
    "* `max_context`: the maximum number of items in the user profile (starting from the last) that will be used for lookup in the database of frequent sequences.\n",
    "* `min_context`: the minimum number of items in the user profile (starting from the last) that will be used for lookup in the database of frequent sequences.\n",
    "* `spmf_path`: path to SPMF jar file. If provided, SPFM library will be used for pattern extraction (algorithm: Prefix Span). Otherwise, use pymining, which can be significantly slower depending on the sequence database size.\n",
    "* `db_path`: path to the sequence database file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZ__fSCIygSb"
   },
   "outputs": [],
   "source": [
    "def callSPMF(spmfPath, command):\n",
    "    # java -jar spmf.jar run PrefixSpan contextPrefixSpan.txt output.txt 50%\n",
    "    comm = ' '.join(['java -jar', spmfPath, 'run', command])\n",
    "    print(comm)\n",
    "    p = subprocess.Popen(comm,\n",
    "                         stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.STDOUT,\n",
    "                         shell=True)\n",
    "    p.communicate()  # wait for completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGHAmOEVyzLY"
   },
   "outputs": [],
   "source": [
    "class SmartTree(treelib.Tree):\n",
    "    _PATH_NOT_FOUND = -1\n",
    "\n",
    "    def find_path(self, origin, path):\n",
    "        \"\"\"\n",
    "        Takes the nodeId where to start the path search and the path to look for,\n",
    "        :returns -1 if path not found, nodeId of the last node if path found\n",
    "        \"\"\"\n",
    "\n",
    "        if not path:\n",
    "            # path found\n",
    "            return origin\n",
    "\n",
    "        res = self._PATH_NOT_FOUND\n",
    "        # note: fpointer getting deprecation warning, would improve in next ver.\n",
    "        for nodeId in self[origin].fpointer:\n",
    "            node = self[nodeId]\n",
    "            if node.tag == path[0]:\n",
    "                res = self.find_path(nodeId, path[1:])\n",
    "                break\n",
    "\n",
    "        if res is None:\n",
    "            # path not found\n",
    "            return self._PATH_NOT_FOUND\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "    def longest_subpath(self, origin, path):\n",
    "        \"\"\"\n",
    "        Takes the nodeId where to start the path search and the path to look for,\n",
    "        :returns the nodeId of the node where the path is broken and the number of missing element for the complete path\n",
    "        \"\"\"\n",
    "\n",
    "        if not path:  # path empty, all nodes matched\n",
    "            # path found\n",
    "            return origin, 0\n",
    "\n",
    "        res = ()\n",
    "\n",
    "        for nodeId in self[origin].fpointer:\n",
    "            node = self[nodeId]\n",
    "            if node.tag == path[0]:\n",
    "                res = self.longest_subpath(nodeId, path[1:])\n",
    "                break\n",
    "\n",
    "        if res == ():\n",
    "            # path not found\n",
    "            return origin, len(path)\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "    def add_path(self, origin, path, support=None):\n",
    "        \"\"\"add a path, starting from origin\"\"\"\n",
    "        sub = self.longest_subpath(origin, path)\n",
    "        if sub[1] == 0:\n",
    "            # path already exists, updating support\n",
    "            self[sub[0]].data = {'support': support}\n",
    "\n",
    "        else:\n",
    "            # add what's missing\n",
    "            missingPath = path[-sub[1]:]\n",
    "\n",
    "            par = sub[0]\n",
    "            for item in missingPath:\n",
    "                itemId = uuid.uuid4()\n",
    "                self.create_node(item, itemId, parent=par, data={'support': support})\n",
    "                par = itemId\n",
    "\n",
    "    def path_is_valid(self, path):\n",
    "        return path != self._PATH_NOT_FOUND\n",
    "\n",
    "    def create_node(self, tag=None, identifier=None, parent=None, data=None):\n",
    "        \"\"\"override to get a random id if none provided\"\"\"\n",
    "        id = uuid.uuid4() if identifier is None else identifier\n",
    "        if id == self._PATH_NOT_FOUND:\n",
    "            raise NameError(\"Cannot create a node with special id \" + str(self._PATH_NOT_FOUND))\n",
    "        super(SmartTree, self).create_node(tag, id, parent, data)\n",
    "\n",
    "    def set_root(self, root_tag=None, root_id=None):\n",
    "        id = uuid.uuid4()\n",
    "        root_id = root_id if root_id is not None else id\n",
    "        root_tag = root_tag if root_tag is not None else 'root'\n",
    "        self.create_node(root_tag, root_id)\n",
    "        self.root = root_id\n",
    "        return root_id\n",
    "\n",
    "    def get_root(self):\n",
    "        try:\n",
    "            return self.root\n",
    "        except AttributeError:\n",
    "            return None\n",
    "\n",
    "    def find_n_length_paths(self, origin, length, exclude_origin=True):\n",
    "\n",
    "        if length == 0:\n",
    "            return [[]] if exclude_origin else [[origin]]\n",
    "\n",
    "        else:\n",
    "            children = self[origin].fpointer\n",
    "            paths = []\n",
    "            for c in children:\n",
    "                children_paths = self.find_n_length_paths(c, length - 1, False)\n",
    "                # this line is magic, if there are no children the all path gets lost,\n",
    "                # that's how i get paths of exactly length wanted\n",
    "                l = list(map(lambda x: [] + x, children_paths)) if exclude_origin else list(\n",
    "                    map(lambda x: [origin] + x, children_paths))\n",
    "                for el in l:\n",
    "                    paths.append(el)\n",
    "            return paths\n",
    "\n",
    "    def get_paths_tag(self, list_of_paths):\n",
    "        return list(map(lambda x: self.get_nodes_tag(x), list_of_paths))\n",
    "\n",
    "    def get_nodes_tag(self, list_of_nids):\n",
    "        return list(map(lambda y: self[y].tag, list_of_nids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_TCVWY3y1zv"
   },
   "outputs": [],
   "source": [
    "class FSMRecommender(ISeqRecommender):\n",
    "    \"\"\"Frequent Sequence Mining recommender\"\"\"\n",
    "\n",
    "    def __init__(self, minsup, minconf, max_context=1, min_context=1, spmf_path=None, db_path=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param minsup: the minimum support threshold. It is interpreted as relative count if in [0-1],\n",
    "                otherwise as an absolute count. NOTE: Relative count required for training with SPFM (faster).\n",
    "        :param minconf: the minimum confidence threshold.\n",
    "        :param max_context: (optional) the maximum number of items in the user profile (starting from the last) that will be used\n",
    "                for lookup in the database of frequent sequences.\n",
    "        :param min_context: (optional) the minimum number of items in the user profile (starting from the last) that will be used\n",
    "                for lookup in the database of frequent sequences.\n",
    "        :param spmf_path: (optional) path to SPMF jar file. If provided, SPFM library will be used for pattern extraction (algorithm: Prefix Span).\n",
    "                Otherwise, use pymining, which can be significantly slower depending on the sequence database size.\n",
    "        :param db_path: (optional) path to the sequence database file\n",
    "        \"\"\"\n",
    "\n",
    "        super(FSMRecommender, self).__init__()\n",
    "        self.minsup = minsup\n",
    "        self.minconf = minconf\n",
    "        self.max_context = max_context\n",
    "        self.min_context = min_context\n",
    "        self.recommendation_length = 1\n",
    "        self.db_path = db_path\n",
    "        self.spmf_path = spmf_path\n",
    "        self.spmf_algorithm = \"PrefixSpan\"\n",
    "        self.output_path = \"tmp/tmp_output.txt\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'FreqSeqMiningRecommender: ' \\\n",
    "               'minsup={minsup}, ' \\\n",
    "               'minconf={minconf}, ' \\\n",
    "               'max_context={max_context}, ' \\\n",
    "               'min_context={min_context}, ' \\\n",
    "               'spmf_path={spmf_path}, ' \\\n",
    "               'db_path={db_path}'.format(**self.__dict__)\n",
    "\n",
    "    def fit(self, train_data=None):\n",
    "        \"\"\"\n",
    "        Fit the model\n",
    "        :param train_data: (optional) DataFrame with the training sequences, which must be assigned to column \"sequence\".\n",
    "            If None, run FSM using SPFM over the sequence database stored in `self.db_path`.\n",
    "            Otherwise, run FSM using `pymining.seqmining` (slower).\n",
    "        \"\"\"\n",
    "\n",
    "        if train_data is None:\n",
    "            if self.spmf_path is None or self.db_path is None:\n",
    "                raise ValueError(\"You should set db_path and spfm_path before calling fit() without arguments.\")\n",
    "\n",
    "            self.logger.info('Using SPFM (Java) for Frequent Sequence Mining')\n",
    "            if 0 <= self.minsup <= 1:\n",
    "                percentage_min_sup = self.minsup * 100\n",
    "            else:\n",
    "                raise NameError(\"SPMF only accepts 0<=minsup<=1\")\n",
    "\n",
    "            # call spmf\n",
    "            command = ' '.join([self.spmf_algorithm, self.db_path, self.output_path, str(percentage_min_sup) + '%'])\n",
    "            callSPMF(self.spmf_path, command)\n",
    "\n",
    "            # parse back output from text file\n",
    "            self._parse_spfm_output()\n",
    "        else:\n",
    "            # use pymining\n",
    "            self.logger.info('Using pymining.seqmining (python) for Frequent Sequence Mining')\n",
    "            sequences = train_data['sequence'].values\n",
    "            msup = int(self.minsup * len(sequences)) if 0 <= self.minsup <= 1 else self.minsup\n",
    "            self.logger.info('Mining frequent sequences (minsup={})'.format(msup))\n",
    "            self.freq_seqs = seqmining.freq_seq_enum(sequences, msup)\n",
    "\n",
    "        self.logger.info('{} frequent sequences found'.format(len(self.freq_seqs)))\n",
    "        self.logger.info('Building the prefix tree')\n",
    "        self.tree = SmartTree()\n",
    "        self.root_node = self.tree.set_root()\n",
    "        for pattern, support in self.freq_seqs:\n",
    "            if len(pattern) == 1:\n",
    "                # add node to root\n",
    "                self.tree.create_node(pattern[0], parent=self.root_node, data={\"support\": support})\n",
    "            elif len(pattern) > 1:\n",
    "                # add entire path starting from root\n",
    "                self.tree.add_path(self.root_node, pattern, support)\n",
    "            else:\n",
    "                raise ValueError('Frequent sequence of length 0')\n",
    "        self.logger.info('Training completed')\n",
    "\n",
    "    def recommend(self, user_profile, user_id=None):\n",
    "        n = len(user_profile)\n",
    "        c = min(n, self.max_context)\n",
    "        match = []\n",
    "        # iterate over decreasing context lengths until a match with sufficient confidence is found\n",
    "        while not match and c >= self.min_context:\n",
    "            q = user_profile[n - c:n]\n",
    "            match = self._find_match(q, self.recommendation_length)\n",
    "            c -= 1\n",
    "        return match\n",
    "\n",
    "    def _find_match(self, context, recommendation_length):\n",
    "        # search context\n",
    "        lastNode = self.tree.find_path(self.root_node, context)\n",
    "\n",
    "        if lastNode == -1:\n",
    "            return []\n",
    "        else:  # context matched\n",
    "            context_support = self.tree[lastNode].data['support']\n",
    "            children = self.tree[lastNode].fpointer\n",
    "\n",
    "            if not children:\n",
    "                return []\n",
    "\n",
    "            # find all path of length recommendation_length from match\n",
    "            paths = self.tree.find_n_length_paths(lastNode, recommendation_length)\n",
    "            return sorted(self._filter_confidence(context_support, paths), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    def _filter_confidence(self, context_support, path_list):\n",
    "        goodPaths = []\n",
    "        for p in path_list:\n",
    "            confidence = self.tree[p[len(p) - 1]].data['support'] / float(context_support)\n",
    "            if confidence >= self.minconf:\n",
    "                goodPaths.append((self.tree.get_nodes_tag(p), confidence))\n",
    "        return goodPaths\n",
    "\n",
    "    def _set_tree_debug_only(self, tree):\n",
    "        self.tree = tree\n",
    "        self.root_node = tree.get_root()\n",
    "\n",
    "    def get_freq_seqs(self):\n",
    "        return self.freq_seqs\n",
    "\n",
    "    def get_sequence_tree(self):\n",
    "        return self.tree\n",
    "\n",
    "    def show_tree(self):\n",
    "        self.tree.show()\n",
    "\n",
    "    def get_confidence_list(self, recommendation):\n",
    "        return list(map(lambda x: x[1], recommendation))\n",
    "\n",
    "    def _parse_spfm_output(self):\n",
    "        with open(self.output_path, 'r') as fin:\n",
    "            self.freq_seqs = []\n",
    "            for line in fin:\n",
    "                pieces = line.split('#SUP: ')\n",
    "                support = pieces[1].strip()\n",
    "                items = pieces[0].split(' ')\n",
    "                seq = tuple(x for x in items if x != '' and x != '-1')\n",
    "                seq_and_support = (seq, int(support))\n",
    "                self.freq_seqs.append(seq_and_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZUNvNtjxIuo"
   },
   "outputs": [],
   "source": [
    "def sequences_to_spfm_format(sequences, tmp_path='tmp/sequences.txt'):\n",
    "    \"\"\"\n",
    "    Convert a list of sequences to SPFM format and write them to `tmp_path`\n",
    "    :param sequences: the list of sequences\n",
    "    :param tmp_path: the path where sequences will be written in the SPFM format\n",
    "    \"\"\"\n",
    "    basedir = os.path.split(tmp_path)[0]\n",
    "    os.makedirs(basedir, exist_ok=True)\n",
    "    with open(tmp_path, 'w') as fout:\n",
    "        for s in sequences:\n",
    "            fout.write(' -1 '.join(map(str, s)))\n",
    "            fout.write(' -2\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yglt9PhRxgmX"
   },
   "outputs": [],
   "source": [
    "!mkdir spmf && \\\n",
    "cd spmf && \\\n",
    "wget https://raw.githubusercontent.com/mquad/sars_tutorial/master/spmf/spmf.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hgir3w1Xw9WZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 13:57:15,144 - INFO - Using SPFM (Java) for Frequent Sequence Mining\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java -jar spmf/spmf.jar run PrefixSpan tmp/sequences.txt tmp/tmp_output.txt 0.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 13:57:16,823 - INFO - 66730 frequent sequences found\n",
      "2021-04-25 13:57:16,827 - INFO - Building the prefix tree\n",
      "2021-04-25 13:57:29,086 - INFO - Training completed\n"
     ]
    }
   ],
   "source": [
    "# convert the training sequences to SPFM format first\n",
    "db_path = 'tmp/sequences.txt'\n",
    "sequences_to_spfm_format(train_data['sequence'], tmp_path=db_path)\n",
    "\n",
    "# then we instantiate and fit the recommender\n",
    "fsmrecommender = FSMRecommender(minsup=0.002, \n",
    "                             minconf=0.1, \n",
    "                             min_context=1, \n",
    "                             max_context=10,\n",
    "                             spmf_path='spmf/spmf.jar',\n",
    "                             db_path=db_path)\n",
    "\n",
    "# calling fit() without arguments to use SPFM and the sequences stored in db_path\n",
    "fsmrecommender.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56IH21NA23o1"
   },
   "source": [
    "### Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ane7x4hSiy0s"
   },
   "source": [
    "<img src='https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F35b572f7-a65c-458f-922a-8e59a2b9edc5%2FUntitled.png?table=block&id=3c851d27-3aad-40e0-aca5-da3ec7a88662&spaceId=63b72b1f-0e90-4ab8-a6df-a060a6545a56&width=2000&userId=21ec183f-f0be-4b6b-9b3e-6f0d4e5c5469&cache=v2'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAwp3_K57vXQ"
   },
   "source": [
    "Here we fit the recommedation algorithm over the sessions in the training set.  \n",
    "This recommender is based on the `MarkovChainRecommender` implemented from:\n",
    "\n",
    "_Shani, Guy, David Heckerman, and Ronen I. Brafman. \"An MDP-based recommender system.\" Journal of Machine Learning Research 6, no. Sep (2005): 1265-1295. Chapter 3-4_\n",
    "\n",
    "This recommender computes the item transition matrices for any Markov Chain having order in `[min_order, max_order]`. Each individual Markov Chain model employes some heristics like skipping or clustering to deal better with data sparsity. Recommendations are generated by sorting items by their transition probability to being next, given the user profile. The scores coming from different MC are weighted _inversely_ wrt to their order.\n",
    "\n",
    "The class `MixedMarkovChainRecommender` has the following initialization hyper-parameters:\n",
    "* `min_order`: the minimum order of the Mixed Markov Chain\n",
    "* `max_order`: the maximum order of the Mixed Markov Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D06Xt--z8zP6"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jK3o16-Z6_yS"
   },
   "outputs": [],
   "source": [
    "def add_nodes_to_graph(seqs, last_k):\n",
    "    t = SmartTree()\n",
    "    rootNode = t.set_root()\n",
    "\n",
    "    countDict = {}\n",
    "    G = nx.DiGraph()\n",
    "    for s in seqs:\n",
    "        nearHistory = tuple(s[-(last_k):])\n",
    "        if nearHistory in countDict:\n",
    "            # increment count\n",
    "            countDict[nearHistory] += 1\n",
    "        else:\n",
    "            # init count\n",
    "            countDict[nearHistory] = 1\n",
    "            # add seq to sequence tree\n",
    "            t.add_path(rootNode, list(nearHistory))\n",
    "            # add node to graph\n",
    "            G.add_node(nearHistory)\n",
    "\n",
    "        ## i also have to save the sequence of length k+1 because otherwise I cannot calculate the count\n",
    "        ## from state x to state y. So the seqeunces of length k+1 are in the tree but not in the states\n",
    "        nearHistoryLong = tuple(\n",
    "            s[-(last_k + 1):])  # +1 because I need one more element to calculate the transition prob\n",
    "        if nearHistory != nearHistoryLong:  # otherwise short seq are counted double\n",
    "            if nearHistoryLong in countDict:\n",
    "                # increment count\n",
    "                countDict[nearHistoryLong] += 1\n",
    "            else:\n",
    "                # init count\n",
    "                countDict[nearHistoryLong] = 1\n",
    "    return (t, countDict, G)\n",
    "\n",
    "\n",
    "def add_edges(t, countDict, G, last_k):\n",
    "    \"\"\"\n",
    "    :param t: Tree of the sequnces available as states\n",
    "    :param countDict: dicionary counting the occurence for each sequence\n",
    "    :param G: the graph containing the states (each one is a sequence)\n",
    "    :param last_k: the number of recent item considered\n",
    "    :return: the same graph G, with edges connecting states\n",
    "    \"\"\"\n",
    "    # add links\n",
    "    rootNode = t.get_root()\n",
    "    for node in G.nodes_iter():\n",
    "        # if the sequence is shorter than states's len, the next state has all the sequence as prefix\n",
    "        next_state_prefix = node[1:] if len(node) == last_k else node\n",
    "        p = t.find_path(rootNode, next_state_prefix)\n",
    "        if t.path_is_valid(p):\n",
    "            children = t.get_nodes_tag(t[p].fpointer)\n",
    "            for c in children:\n",
    "                # the tree may suggest a children which is not a state of the graph, because it was part of a longer\n",
    "                # sequence, in that case no edge has to be added\n",
    "                if next_state_prefix + (c,) in G.nodes():\n",
    "                    if countDict.get(node + (c,), 0) != 0:  # do not add edge if count is 0\n",
    "                        G.add_edge(node, next_state_prefix + (c,), {'count': countDict.get(node + (c,), 0)})\n",
    "    return G\n",
    "\n",
    "\n",
    "def apply_skipping(G, last_k, seqs):\n",
    "    # iterate over seqs to add skipping count\n",
    "    window = last_k\n",
    "\n",
    "    for us in seqs:\n",
    "        s = tuple(us)\n",
    "        for i in range(len(s) - window):\n",
    "            previous_state = s[i:i + window]\n",
    "            next_state_prefix = previous_state[1:]\n",
    "            for j in range(i + window + 1, len(s)):\n",
    "                fractional_count = 1 / (2 ** (j - (i + window)))\n",
    "                next_state = next_state_prefix + (s[j],)\n",
    "                # update count\n",
    "                old_count = G.get_edge_data(previous_state, next_state, {}).get('count', 0)\n",
    "                if G.has_edge(previous_state, next_state):\n",
    "                    G[previous_state][next_state]['count'] = old_count + fractional_count\n",
    "                else:\n",
    "                    G.add_edge(previous_state, next_state, {'count': fractional_count})\n",
    "                # print('updating '+str(previous_state)+'->'+str(next_state)+' from '+str(old_count)+' to '+str(old_count+fractional_count))\n",
    "\n",
    "    # normalize\n",
    "    for n in G.nodes_iter():\n",
    "        edges = G.out_edges(n)\n",
    "        countSum = reduce(lambda x, y: x + y, [G[x[0]][x[1]]['count'] for x in edges], 0)\n",
    "        for e in edges:\n",
    "            G[e[0]][e[1]]['count'] = G[e[0]][e[1]]['count'] / float(countSum) if countSum else 0\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def apply_clustering(G):\n",
    "    ##clustering\n",
    "    def sequence_similarity(s, t):\n",
    "        sum = 0\n",
    "        for i in range(min(len(s), len(t))):\n",
    "            sum += 0 if s[i] != t[i] else (i + 2)\n",
    "        return sum\n",
    "\n",
    "    similarity_dict = {}\n",
    "    # for each state in the graph, calculate similarity\n",
    "    for node in G.nodes_iter():\n",
    "        for deno in G.nodes_iter():\n",
    "            if node == deno or (node, deno) in similarity_dict:\n",
    "                continue  # skip if same or already done\n",
    "            else:\n",
    "                sim = sequence_similarity(node, deno)\n",
    "                if sim:  # save only if different from zero\n",
    "                    similarity_dict[node, deno] = similarity_dict[deno, node] = sim\n",
    "\n",
    "    similarity_count_dict = {}\n",
    "\n",
    "    for node in G.nodes_iter():\n",
    "        for deno in G.nodes_iter():\n",
    "            if node == deno: continue\n",
    "            sum = 0\n",
    "            for in_edge in G.in_edges_iter([deno]):\n",
    "                intermediate_node = in_edge[0]\n",
    "                if intermediate_node != node:  # I want to count the effect of going through Other nodes\n",
    "                    sum += similarity_dict.get((node, intermediate_node), 0) * G[intermediate_node][deno]['count']\n",
    "            if sum:\n",
    "                similarity_count_dict[node, deno] = sum\n",
    "\n",
    "    def compute_normalization_similarity_count(G, node):\n",
    "        normalization_sum = 0\n",
    "        for other_state in G.nodes_iter():\n",
    "            # skip similarity with myself is 0 because of how similarity_dict is calculated\n",
    "            normalization_sum += similarity_count_dict.get((node, other_state), 0)\n",
    "        return normalization_sum\n",
    "\n",
    "    ##update transition probability\n",
    "    ### this can be made faster(?) if I store the adjancency matrix where node are connected if\n",
    "    # there is a probability due to the clustering (i.e. there is an entry in similarity_count_dict\n",
    "    # in this way I only have to check those edges. now it's already pretty optimized anyway\n",
    "    ALPHA = 0.5\n",
    "    for node in G.nodes_iter():\n",
    "        normalization_sum = compute_normalization_similarity_count(G, node)\n",
    "\n",
    "        # first half the original transition prob\n",
    "        for u, v in G.out_edges_iter([node]):\n",
    "            G[u][v]['count'] *= ALPHA\n",
    "\n",
    "        # if there is similarity probability somewhere\n",
    "        if normalization_sum:\n",
    "            # add similarity probability\n",
    "            for deno in G.nodes_iter():\n",
    "                # skip if same node or there is nothing that can be added to that node\n",
    "                if node == deno or similarity_count_dict.get((node, deno), 0) == 0: continue\n",
    "\n",
    "                partial_prob = (1 - ALPHA) * similarity_count_dict.get((node, deno), 0) / normalization_sum\n",
    "\n",
    "                if G.has_edge(node, deno):\n",
    "                    G[node][deno]['count'] += partial_prob\n",
    "                elif partial_prob:  # there wasn't an edge but now there is partial prob from other nodes\n",
    "                    G.add_edge(node, deno, {'count': partial_prob})\n",
    "\n",
    "    return G, similarity_dict, similarity_count_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MN418XCb7eGU"
   },
   "outputs": [],
   "source": [
    "class MarkovChainRecommender(ISeqRecommender):\n",
    "    \"\"\"\n",
    "    Implementation from Shani, Guy, David Heckerman, and Ronen I. Brafman. \"An MDP-based recommender system.\"\n",
    "    Journal of Machine Learning Research 6, no. Sep (2005): 1265-1295. Chapter 3-4\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    def __init__(self, order):\n",
    "        \"\"\"\n",
    "        :param order: the order of the Markov Chain\n",
    "        \"\"\"\n",
    "        super(MarkovChainRecommender, self).__init__()\n",
    "        self.order = order\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        sequences = train_data['sequence'].values\n",
    "\n",
    "        logging.info('Building Markov Chain model with k = ' + str(self.order))\n",
    "        logging.info('Adding nodes')\n",
    "        self.tree, self.count_dict, self.G = add_nodes_to_graph(sequences, self.order)\n",
    "        logging.info('Adding edges')\n",
    "        self.G = add_edges(self.tree, self.count_dict, self.G, self.order)\n",
    "        logging.info('Applying skipping')\n",
    "        self.G = apply_skipping(self.G, self.order, sequences)\n",
    "        logging.info('Applying clustering')\n",
    "        logging.info('{} states in the graph'.format(len(self.G.nodes())))\n",
    "        self.G, _, _ = apply_clustering(self.G)\n",
    "        # drop not useful resources\n",
    "        self.tree = None\n",
    "        self.count_dict = None\n",
    "        gc.collect()\n",
    "\n",
    "    def recommend(self, user_profile, user_id=None):\n",
    "\n",
    "        # if the user profile is longer than the markov order, chop it keeping recent history\n",
    "        state = tuple(user_profile[-self.order:])\n",
    "        # see if graph has that state\n",
    "        recommendations = []\n",
    "        if self.G.has_node(state):\n",
    "            # search for recommendations in the forward star\n",
    "            rec_dict = {}\n",
    "            for u, v in self.G.out_edges_iter([state]):\n",
    "                lastElement = tuple(v[-1:])\n",
    "                if lastElement in rec_dict:\n",
    "                    rec_dict[lastElement] += self.G[u][v]['count']\n",
    "                else:\n",
    "                    rec_dict[lastElement] = self.G[u][v]['count']\n",
    "            for k, v in rec_dict.items():\n",
    "                recommendations.append((list(k), v))\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def _set_graph_debug(self, G):\n",
    "        self.G = G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGDAMOY47ijO"
   },
   "outputs": [],
   "source": [
    "class MixedMarkovChainRecommender(ISeqRecommender):\n",
    "    \"\"\"\n",
    "    Creates markov models with different values of k, and return recommendation by weighting the list of\n",
    "    recommendation of each model.\n",
    "\n",
    "    Reference: Shani, Guy, David Heckerman, and Ronen I. Brafman. \"An MDP-based recommender system.\"\n",
    "    Journal of Machine Learning Research 6, no. Sep (2005): 1265-1295. Chapter 3-4\n",
    "    \"\"\"\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    recommenders = {}\n",
    "\n",
    "    def __init__(self, min_order=1, max_order=1):\n",
    "        \"\"\"\n",
    "        :param min_order: the minimum order of the Mixed Markov Chain\n",
    "        :param max_order: the maximum order of the Mixed Markov Chain\n",
    "        \"\"\"\n",
    "        super(MixedMarkovChainRecommender, self).__init__()\n",
    "        self.min_order = min_order\n",
    "        self.max_order = max_order\n",
    "        # define the models\n",
    "        for i in range(self.min_order, self.max_order + 1):\n",
    "            self.recommenders[i] = MarkovChainRecommender(i)\n",
    "\n",
    "    def fit(self, user_profile):\n",
    "        for order in self.recommenders:\n",
    "            self.recommenders[order].fit(user_profile)\n",
    "\n",
    "    def recommend(self, user_profile, user_id=None):\n",
    "        rec_dict = {}\n",
    "        recommendations = []\n",
    "        sum_of_weights = 0\n",
    "        for order, r in self.recommenders.items():\n",
    "            rec_list = r.recommend(user_profile)\n",
    "            sum_of_weights += 1 / order\n",
    "            for i in rec_list:\n",
    "                if tuple(i[0]) in rec_dict:\n",
    "                    rec_dict[tuple(i[0])] += 1 / order * i[1]\n",
    "                else:\n",
    "                    rec_dict[tuple(i[0])] = 1 / order * i[1]\n",
    "        for k, v in rec_dict.items():\n",
    "            recommendations.append((list(k), v / sum_of_weights))\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def _set_model_debug(self, recommender, order):\n",
    "        self.recommenders[order] = recommender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldqbw7_Y7kli"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 13:57:40,210 - INFO - Building Markov Chain model with k = 1\n",
      "2021-04-25 13:57:40,213 - INFO - Adding nodes\n",
      "2021-04-25 13:57:40,499 - INFO - Adding edges\n",
      "2021-04-25 13:57:58,752 - INFO - Applying skipping\n",
      "2021-04-25 13:57:58,973 - INFO - Applying clustering\n",
      "2021-04-25 13:57:58,974 - INFO - 999 states in the graph\n"
     ]
    }
   ],
   "source": [
    "# You can try with max_order=2 or higher too, but it will take some time to complete though due to slow heristic computations\n",
    "mmcrecommender = MixedMarkovChainRecommender(min_order=1, max_order=1)\n",
    "mmcrecommender.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwExXJvX_qZg"
   },
   "source": [
    "### FPMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdjrJOJzhtzL"
   },
   "source": [
    "Here we fit the recommedation algorithm over the sessions in the training set.  \n",
    "This recommender is based on the following paper:\n",
    "\n",
    "_Rendle, S., Freudenthaler, C., & Schmidt-Thieme, L. (2010). Factorizing personalized Markov chains for next-basket recommendation. Proceedings of the 19th International Conference on World Wide Web - WWW ’10, 811_\n",
    "\n",
    "In short, FPMC factorizes a personalized order-1 transition tensor using Tensor Factorization with pairwise loss function akin to BPR (Bayesian Pairwise Ranking).\n",
    "\n",
    "TF allows to impute values for the missing transitions between items for each user. For this reason, FPMC can be used for generating _personalized_ recommendations in session-aware recommenders as well.\n",
    "\n",
    "In this notebook, you will be able to change the number of latent factors and a few other learning hyper-parameters and see the impact on the recommendation quality.\n",
    "\n",
    "The class `FPMCRecommender` has the following initialization hyper-parameters:\n",
    "* `n_factor`: (optional) the number of latent factors\n",
    "* `learn_rate`: (optional) the learning rate\n",
    "* `regular`: (optional) the L2 regularization coefficient\n",
    "* `n_epoch`: (optional) the number of training epochs\n",
    "* `n_neg`: (optional) the number of negative samples used in BPR learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acHmXF9qAUIj"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    if x >= 0:\n",
    "        return math.exp(-np.logaddexp(0, -x))\n",
    "    else:\n",
    "        return math.exp(x - np.logaddexp(x, 0))\n",
    "\n",
    "\n",
    "def load_data_from_dir(dirname):\n",
    "    fname_user_idxseq = dirname + '/' + 'idxseq.txt'\n",
    "    fname_user_list = dirname + '/' + 'user_idx_list.txt'\n",
    "    fname_item_list = dirname + '/' + 'item_idx_list.txt'\n",
    "    user_set = load_idx_list_file(fname_user_list)\n",
    "    item_set = load_idx_list_file(fname_item_list)\n",
    "\n",
    "    data_list = []\n",
    "    with open(fname_user_idxseq, 'r') as f:\n",
    "        for l in f:\n",
    "            l = [int(s) for s in l.strip().split()]\n",
    "            user = l[0]\n",
    "            b_tm1 = list(set(l[1:-1]))\n",
    "            label = l[-1]\n",
    "\n",
    "            data_list.append((user, label, b_tm1))\n",
    "\n",
    "    return data_list, user_set, item_set\n",
    "\n",
    "\n",
    "def load_idx_list_file(fname, delimiter=','):\n",
    "    idx_set = set()\n",
    "    with open(fname, 'r') as f:\n",
    "        # dicard header\n",
    "        f.readline()\n",
    "\n",
    "        for l in csv.reader(f, delimiter=delimiter, quotechar='\"'):\n",
    "            idx = int(l[0])\n",
    "            idx_set.add(idx)\n",
    "    return idx_set\n",
    "\n",
    "\n",
    "def data_to_3_list(data_list):\n",
    "    u_list = []\n",
    "    i_list = []\n",
    "    b_tm1_list = []\n",
    "    max_l = 0\n",
    "    for d in data_list:\n",
    "        u_list.append(d[0])\n",
    "        i_list.append(d[1])\n",
    "        b_tm1_list.append(d[2])\n",
    "        if len(d[2]) > max_l:\n",
    "            max_l = len(d[2])\n",
    "    for b_tm1 in b_tm1_list:\n",
    "        b_tm1.extend([-1 for i in range(max_l - len(b_tm1))])\n",
    "    b_tm1_list = np.array(b_tm1_list)\n",
    "\n",
    "    return (u_list, i_list, b_tm1_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbpqaPLlArhx"
   },
   "outputs": [],
   "source": [
    "class FPMC_basic:\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    def __init__(self, n_user, n_item, n_factor, learn_rate, regular):\n",
    "        self.user_set = set()\n",
    "        self.item_set = set()\n",
    "\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "\n",
    "        self.n_factor = n_factor\n",
    "        self.learn_rate = learn_rate\n",
    "        self.regular = regular\n",
    "\n",
    "    @staticmethod\n",
    "    def dump(fpmcObj, fname):\n",
    "        pickle.dump(fpmcObj, open(fname, 'wb'))\n",
    "\n",
    "    @staticmethod\n",
    "    def load(fname):\n",
    "        return pickle.load(open(fname, 'rb'))\n",
    "\n",
    "    def init_model(self, std=0.01):\n",
    "        self.VUI = np.random.normal(0, std, size=(self.n_user, self.n_factor))\n",
    "        self.VIU = np.random.normal(0, std, size=(self.n_item, self.n_factor))\n",
    "        self.VIL = np.random.normal(0, std, size=(self.n_item, self.n_factor))\n",
    "        self.VLI = np.random.normal(0, std, size=(self.n_item, self.n_factor))\n",
    "        self.VUI_m_VIU = np.dot(self.VUI, self.VIU.T)\n",
    "        self.VIL_m_VLI = np.dot(self.VIL, self.VLI.T)\n",
    "\n",
    "    def compute_x(self, u, i, b_tm1):\n",
    "        acc_val = 0.0\n",
    "        for l in b_tm1:\n",
    "            acc_val += np.dot(self.VIL[i], self.VLI[l])\n",
    "        return (np.dot(self.VUI[u], self.VIU[i]) + (acc_val / len(b_tm1)))\n",
    "\n",
    "    def compute_x_batch(self, u, b_tm1):\n",
    "        former = self.VUI_m_VIU[u]\n",
    "        latter = np.mean(self.VIL_m_VLI[:, b_tm1], axis=1).T\n",
    "        return (former + latter)\n",
    "\n",
    "    def evaluation(self, data_list):\n",
    "        np.dot(self.VUI, self.VIU.T, out=self.VUI_m_VIU)\n",
    "        np.dot(self.VIL, self.VLI.T, out=self.VIL_m_VLI)\n",
    "\n",
    "        correct_count = 0\n",
    "        rr_list = []\n",
    "        for (u, i, b_tm1) in data_list:\n",
    "            scores = self.compute_x_batch(u, b_tm1)\n",
    "\n",
    "            if i == scores.argmax():\n",
    "                correct_count += 1\n",
    "\n",
    "            rank = len(np.where(scores > scores[i])[0]) + 1\n",
    "            rr = 1.0 / rank\n",
    "            rr_list.append(rr)\n",
    "\n",
    "        try:\n",
    "            acc = correct_count / len(rr_list)\n",
    "            mrr = (sum(rr_list) / len(rr_list))\n",
    "            return (acc, mrr)\n",
    "        except:\n",
    "            return (0.0, 0.0)\n",
    "\n",
    "    def learn_epoch(self, tr_data, neg_batch_size):\n",
    "        for iter_idx in range(len(tr_data)):\n",
    "            (u, i, b_tm1) = random.choice(tr_data)\n",
    "\n",
    "            exclu_set = self.item_set - set([i])\n",
    "            j_list = random.sample(exclu_set, neg_batch_size)\n",
    "\n",
    "            z1 = self.compute_x(u, i, b_tm1)\n",
    "            for j in j_list:\n",
    "                z2 = self.compute_x(u, j, b_tm1)\n",
    "                delta = 1 - sigmoid(z1 - z2)\n",
    "\n",
    "                VUI_update = self.learn_rate * (delta * (self.VIU[i] - self.VIU[j]) - self.regular * self.VUI[u])\n",
    "                VIUi_update = self.learn_rate * (delta * self.VUI[u] - self.regular * self.VIU[i])\n",
    "                VIUj_update = self.learn_rate * (-delta * self.VUI[u] - self.regular * self.VIU[j])\n",
    "\n",
    "                self.VUI[u] += VUI_update\n",
    "                self.VIU[i] += VIUi_update\n",
    "                self.VIU[j] += VIUj_update\n",
    "\n",
    "                eta = np.mean(self.VLI[b_tm1], axis=0)\n",
    "                VILi_update = self.learn_rate * (delta * eta - self.regular * self.VIL[i])\n",
    "                VILj_update = self.learn_rate * (-delta * eta - self.regular * self.VIL[j])\n",
    "                VLI_update = self.learn_rate * (\n",
    "                        (delta * (self.VIL[i] - self.VIL[j]) / len(b_tm1)) - self.regular * self.VLI[b_tm1])\n",
    "\n",
    "                self.VIL[i] += VILi_update\n",
    "                self.VIL[j] += VILj_update\n",
    "                self.VLI[b_tm1] += VLI_update\n",
    "\n",
    "    def learnSBPR_FPMC(self, tr_data, n_epoch=10, neg_batch_size=10):\n",
    "        for epoch in range(n_epoch):\n",
    "            self.learn_epoch(tr_data, neg_batch_size=neg_batch_size)\n",
    "            self.logger.info('epoch %d done' % epoch)\n",
    "            # if eval_per_epoch == True:\n",
    "            #     acc_in, mrr_in = self.evaluation(tr_data)\n",
    "            #     if te_data != None:\n",
    "            #         acc_out, mrr_out = self.evaluation(te_data)\n",
    "            #         self.logger.info ('In sample:%.4f\\t%.4f \\t Out sample:%.4f\\t%.4f' % (acc_in, mrr_in, acc_out, mrr_out))\n",
    "            #     else:\n",
    "            #         self.logger.info ('In sample:%.4f\\t%.4f' % (acc_in, mrr_in))\n",
    "            # else:\n",
    "            #\n",
    "\n",
    "        # if eval_per_epoch == False:\n",
    "        #     acc_in, mrr_in = self.evaluation(tr_data)\n",
    "        #     if te_data != None:\n",
    "        #         acc_out, mrr_out = self.evaluation(te_data)\n",
    "        #         print ('In sample:%.4f\\t%.4f \\t Out sample:%.4f\\t%.4f' % (acc_in, mrr_in, acc_out, mrr_out))\n",
    "        #     else:\n",
    "        #         print ('In sample:%.4f\\t%.4f' % (acc_in, mrr_in))\n",
    "        #\n",
    "        # if te_data != None:\n",
    "        #     return (acc_out, mrr_out)\n",
    "        # else:\n",
    "        #     return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39xmlWh1AUFb"
   },
   "outputs": [],
   "source": [
    "class FPMC(FPMC_basic):\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    def __init__(self, n_user, n_item, n_factor, learn_rate, regular):\n",
    "        super(FPMC, self).__init__(n_user, n_item, n_factor, learn_rate, regular)\n",
    "\n",
    "    def evaluation(self, data_3_list):\n",
    "        np.dot(self.VUI, self.VIU.T, out=self.VUI_m_VIU)\n",
    "        np.dot(self.VIL, self.VLI.T, out=self.VIL_m_VLI)\n",
    "        acc, mrr = evaluation_jit(data_3_list[0], data_3_list[1], data_3_list[2], self.VUI_m_VIU, self.VIL_m_VLI)\n",
    "\n",
    "        return acc, mrr\n",
    "\n",
    "    def evaluation_recommender(self, user, user_profile):\n",
    "        np.dot(self.VUI, self.VIU.T, out=self.VUI_m_VIU)\n",
    "        np.dot(self.VIL, self.VLI.T, out=self.VIL_m_VLI)\n",
    "        scores = evaluation_jit_recommender(user, user_profile, self.VUI_m_VIU, self.VIL_m_VLI)\n",
    "        return sorted(range(len(scores)), key=lambda x: -scores[x]), sorted(scores, reverse=True)\n",
    "\n",
    "    def learn_epoch(self, data_3_list, neg_batch_size):\n",
    "        VUI, VIU, VLI, VIL = learn_epoch_jit(data_3_list[0], data_3_list[1], data_3_list[2], neg_batch_size,\n",
    "                                             np.array(list(self.item_set)), self.VUI, self.VIU, self.VLI, self.VIL,\n",
    "                                             self.learn_rate, self.regular)\n",
    "        self.VUI = VUI\n",
    "        self.VIU = VIU\n",
    "        self.VLI = VLI\n",
    "        self.VIL = VIL\n",
    "\n",
    "    def learnSBPR_FPMC(self, tr_data, n_epoch=10, neg_batch_size=10):\n",
    "        tr_3_list = data_to_3_list(tr_data)\n",
    "\n",
    "        for epoch in range(n_epoch):\n",
    "            self.learn_epoch(tr_3_list, neg_batch_size)\n",
    "            self.logger.info('epoch %d done' % epoch)\n",
    "\n",
    "        # if eval_per_epoch == False:\n",
    "        #     acc_in, mrr_in = self.evaluation(tr_3_list)\n",
    "        #     if te_data != None:\n",
    "        #         acc_out, mrr_out = self.evaluation(te_3_list)\n",
    "        #         print ('In sample:%.4f\\t%.4f \\t Out sample:%.4f\\t%.4f' % (acc_in, mrr_in, acc_out, mrr_out))\n",
    "        #     else:\n",
    "        #         print ('In sample:%.4f\\t%.4f' % (acc_in, mrr_in))\n",
    "        #\n",
    "        #\n",
    "        # if te_data != None:\n",
    "        #     if ret_in_score:\n",
    "        #         return (acc_in, mrr_in, acc_out, mrr_out)\n",
    "        #     else:\n",
    "        #         return (acc_out, mrr_out)\n",
    "        # else:\n",
    "        #     return None\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def compute_x_jit(u, i, b_tm1, VUI, VIU, VLI, VIL):\n",
    "    acc_val = 0.0\n",
    "    for l in b_tm1:\n",
    "        acc_val += np.dot(VIL[i], VLI[l])\n",
    "    return (np.dot(VUI[u], VIU[i]) + (acc_val / len(b_tm1)))\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def learn_epoch_jit(u_list, i_list, b_tm1_list, neg_batch_size, item_set, VUI, VIU, VLI, VIL, learn_rate, regular):\n",
    "    for iter_idx in range(len(u_list)):\n",
    "        d_idx = np.random.randint(0, len(u_list))\n",
    "        u = u_list[d_idx]\n",
    "        i = i_list[d_idx]\n",
    "        b_tm1 = b_tm1_list[d_idx][b_tm1_list[d_idx] != -1]\n",
    "\n",
    "        j_list = np.random.choice(item_set, size=neg_batch_size, replace=False)\n",
    "        z1 = compute_x_jit(u, i, b_tm1, VUI, VIU, VLI, VIL)\n",
    "        for j in j_list:\n",
    "            z2 = compute_x_jit(u, j, b_tm1, VUI, VIU, VLI, VIL)\n",
    "            delta = 1 - sigmoid_jit(z1 - z2)\n",
    "\n",
    "            VUI_update = learn_rate * (delta * (VIU[i] - VIU[j]) - regular * VUI[u])\n",
    "            VIUi_update = learn_rate * (delta * VUI[u] - regular * VIU[i])\n",
    "            VIUj_update = learn_rate * (-delta * VUI[u] - regular * VIU[j])\n",
    "\n",
    "            VUI[u] += VUI_update\n",
    "            VIU[i] += VIUi_update\n",
    "            VIU[j] += VIUj_update\n",
    "\n",
    "            eta = np.zeros(VLI.shape[1])\n",
    "            for l in b_tm1:\n",
    "                eta += VLI[l]\n",
    "            eta = eta / len(b_tm1)\n",
    "\n",
    "            VILi_update = learn_rate * (delta * eta - regular * VIL[i])\n",
    "            VILj_update = learn_rate * (-delta * eta - regular * VIL[j])\n",
    "            VLI_updates = np.zeros((len(b_tm1), VLI.shape[1]))\n",
    "            for idx, l in enumerate(b_tm1):\n",
    "                VLI_updates[idx] = learn_rate * ((delta * (VIL[i] - VIL[j]) / len(b_tm1)) - regular * VLI[l])\n",
    "\n",
    "            VIL[i] += VILi_update\n",
    "            VIL[j] += VILj_update\n",
    "            for idx, l in enumerate(b_tm1):\n",
    "                VLI[l] += VLI_updates[idx]\n",
    "\n",
    "    return VUI, VIU, VLI, VIL\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def sigmoid_jit(x):\n",
    "    if x >= 0:\n",
    "        return math.exp(-np.logaddexp(0, -x))\n",
    "    else:\n",
    "        return math.exp(x - np.logaddexp(x, 0))\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def compute_x_batch_jit(u, b_tm1, VUI_m_VIU, VIL_m_VLI):\n",
    "    former = VUI_m_VIU[u]\n",
    "    latter = np.zeros(VIL_m_VLI.shape[0])\n",
    "    for idx in range(VIL_m_VLI.shape[0]):\n",
    "        for l in b_tm1:\n",
    "            latter[idx] += VIL_m_VLI[idx, l]\n",
    "    latter = latter / len(b_tm1)\n",
    "\n",
    "    return (former + latter)\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def evaluation_jit(u_list, i_list, b_tm1_list, VUI_m_VIU, VIL_m_VLI):\n",
    "    correct_count = 0\n",
    "    acc_rr = 0\n",
    "    for d_idx in range(len(u_list)):\n",
    "        u = u_list[d_idx]\n",
    "        i = i_list[d_idx]\n",
    "        b_tm1 = b_tm1_list[d_idx][b_tm1_list[d_idx] != -1]\n",
    "        scores = compute_x_batch_jit(u, b_tm1, VUI_m_VIU, VIL_m_VLI)\n",
    "\n",
    "        if i == scores.argmax():\n",
    "            correct_count += 1\n",
    "\n",
    "        rank = len(np.where(scores > scores[i])[0]) + 1\n",
    "        rr = 1.0 / rank\n",
    "        acc_rr += rr\n",
    "\n",
    "    acc = correct_count / len(u_list)\n",
    "    mrr = acc_rr / len(u_list)\n",
    "    return (acc, mrr)\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def evaluation_jit_recommender(user, b_tm1_list, VUI_m_VIU, VIL_m_VLI):\n",
    "    u = user\n",
    "    # b_tm1 = [x for x in b_tm1_list if x!=-1]\n",
    "    b_tm1 = b_tm1_list\n",
    "    scores = compute_x_batch_jit(u, b_tm1, VUI_m_VIU, VIL_m_VLI)\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sq_W2Rt6_sHy"
   },
   "outputs": [],
   "source": [
    "class FPMCRecommender(ISeqRecommender):\n",
    "    \"\"\"\n",
    "    Implementation of\n",
    "    Rendle, S., Freudenthaler, C., & Schmidt-Thieme, L. (2010). Factorizing personalized Markov chains for next-basket recommendation.\n",
    "    Proceedings of the 19th International Conference on World Wide Web - WWW ’10, 811\n",
    "\n",
    "    Based on the implementation available at https://github.com/khesui/FPMC\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_factor=32, learn_rate=0.01, regular=0.001, n_epoch=15, n_neg=10):\n",
    "        \"\"\"\n",
    "        :param n_factor: (optional) the number of latent factors\n",
    "        :param learn_rate: (optional) the learning rate\n",
    "        :param regular: (optional) the L2 regularization coefficient\n",
    "        :param n_epoch: (optional) the number of training epochs\n",
    "        :param n_neg: (optional) the number of negative samples used in BPR learning\n",
    "        \"\"\"\n",
    "        super(FPMCRecommender, self).__init__()\n",
    "        self.n_epoch = n_epoch\n",
    "        self.n_neg = n_neg\n",
    "        self.n_factor = n_factor\n",
    "        self.learn_rate = learn_rate\n",
    "        self.regular = regular\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'FPMCRecommender(n_epoch={n_epoch}, ' \\\n",
    "               'n_neg={n_neg}, ' \\\n",
    "               'n_factor={n_factor}, ' \\\n",
    "               'learn_rate={learn_rate}, ' \\\n",
    "               'regular={regular})'.format(**self.__dict__)\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        self._declare(train_data)\n",
    "\n",
    "        train_data_supervised = []\n",
    "\n",
    "        for i, row in train_data.iterrows():\n",
    "            u = self.user_mapping[row['user_id']]\n",
    "\n",
    "            seq = []\n",
    "            if len(row['sequence']) > 1:  # cannot use sequences with length 1 for supervised learning\n",
    "                for item in row['sequence']:\n",
    "                    i = self.item_mapping[item]\n",
    "                    seq.append(i)\n",
    "\n",
    "                train_data_supervised.append((u, seq[len(seq) - 1], seq[:len(seq) - 1]))\n",
    "\n",
    "        self.fpmc = FPMC(n_user=len(self.user_mapping), n_item=len(self.item_mapping),\n",
    "                         n_factor=self.n_factor, learn_rate=self.learn_rate, regular=self.regular)\n",
    "\n",
    "        self.fpmc.user_set = set(self.user_mapping.values())\n",
    "        self.fpmc.item_set = set(self.item_mapping.values())\n",
    "        self.fpmc.init_model()\n",
    "\n",
    "        self.fpmc.learnSBPR_FPMC(train_data_supervised, n_epoch=self.n_epoch, neg_batch_size=self.n_neg)\n",
    "\n",
    "    def recommend(self, user_profile, user_id=None):\n",
    "        context = []\n",
    "        for item in user_profile:\n",
    "            context.append(self.item_mapping[item])\n",
    "\n",
    "        items, scores = self.fpmc.evaluation_recommender(self.user_mapping[user_id], context)\n",
    "        recommendations = []\n",
    "\n",
    "        for i, it in enumerate(items):\n",
    "            recommendations.append(([self.reverse_item_mapping[it]], scores[i]))\n",
    "        return recommendations\n",
    "\n",
    "    def _declare(self, data):\n",
    "        self.user_mapping = {}\n",
    "        self.item_mapping = {}\n",
    "        self.reverse_item_mapping = {}\n",
    "\n",
    "        user_counter = 0\n",
    "        item_counter = 0\n",
    "        for i, row in data.iterrows():\n",
    "            if row['user_id'] not in self.user_mapping:\n",
    "                self.user_mapping[row['user_id']] = user_counter\n",
    "                user_counter += 1\n",
    "\n",
    "            for item in row['sequence']:\n",
    "                if item not in self.item_mapping:\n",
    "                    self.item_mapping[item] = item_counter\n",
    "                    self.reverse_item_mapping[item_counter] = item\n",
    "                    item_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIlLlnIf_s2H"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 13:59:28,886 - INFO - epoch 0 done\n",
      "2021-04-25 13:59:28,992 - INFO - epoch 1 done\n",
      "2021-04-25 13:59:29,107 - INFO - epoch 2 done\n",
      "2021-04-25 13:59:29,217 - INFO - epoch 3 done\n",
      "2021-04-25 13:59:29,330 - INFO - epoch 4 done\n"
     ]
    }
   ],
   "source": [
    "fpmcrecommender = FPMCRecommender(n_factor=16, n_epoch=5)\n",
    "fpmcrecommender.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_2Oyd5LEMCu"
   },
   "source": [
    "### Prod2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNB-Fot6i9Ec"
   },
   "source": [
    "<img src='https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4fb989da-3eee-495e-b562-92ef930df86d%2FUntitled.png?table=block&id=44191bff-f7b6-43a9-8cad-4431fe776d83&spaceId=63b72b1f-0e90-4ab8-a6df-a060a6545a56&width=2000&userId=21ec183f-f0be-4b6b-9b3e-6f0d4e5c5469&cache=v2'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ho6bLOgifP4d"
   },
   "source": [
    "Here we fit the recommedation algorithm over the sessions in the training set.  \n",
    "\n",
    "This is simplified implementation of the following:\n",
    "\n",
    "_Grbovic, Mihajlo, Vladan Radosavljevic, Nemanja Djuric, Narayan Bhamidipati,\n",
    "Jaikit Savla, Varun Bhagwan, and Doug Sharp. \"E-commerce in your inbox: Product recommendations at scale.\"\n",
    " In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1809-1818. ACM, 2015._\n",
    " \n",
    "This implementation uses the `gensim` implementation of Word2Vec to compute item embeddings using the skip-gram model.\n",
    "\n",
    "Recommendations are generated by returning the k-nearest neighbors of the last items in the user profile, whose relevance is weighted using a simple *exponential decay* (the *last item* in the user profile is the *most relevant* one, and the *first item* the *least relevant*).\n",
    "\n",
    "The original paper contains other variants of this algorithm (namely bagged-prod2vec and prod2vec-cluster) which are not subject of this tutorial. \n",
    "\n",
    "The class `Prod2VecRecommender` has the following initialization hyper-parameters:\n",
    "* `min_count`: the minimum item frequency. Items less frequent that min_count will be pruned\n",
    "* `size`: the size of the embeddings\n",
    "* `window`: the size of the context window\n",
    "* `decay_alpha`: the exponential decay factor used to discount the similarity scores for items  back in the user profile. Lower values mean higher discounting of past user interactions. Allows values in [0-1]\n",
    "* `workers`: the number of threads used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J94KBIMMEq3j"
   },
   "outputs": [],
   "source": [
    "class Prod2VecRecommender(ISeqRecommender):\n",
    "    \"\"\"\n",
    "    Implementation of the Prod2Vec skipgram model from\n",
    "    Grbovic Mihajlo, Vladan Radosavljevic, Nemanja Djuric, Narayan Bhamidipati, Jaikit Savla, Varun Bhagwan, and Doug Sharp.\n",
    "    \"E-commerce in your inbox: Product recommendations at scale.\"\n",
    "    In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,\n",
    "    pp. 1809-1818. ACM, 2015.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    def __init__(self, min_count=2, size=100, window=5, decay_alpha=0.9, workers=4):\n",
    "        \"\"\"\n",
    "        :param min_count: (optional) the minimum item frequency. Items less frequent that min_count will be pruned\n",
    "        :param size: (optional) the size of the embeddings\n",
    "        :param window: (optional) the size of the context window\n",
    "        :param decay_alpha: (optional) the exponential decay factor used to discount the similarity scores for items\n",
    "                back in the user profile. Lower values mean higher discounting of past user interactions. Allows values in [0-1].\n",
    "        :param workers: (optional) the number of threads used for training\n",
    "        \"\"\"\n",
    "        super(Prod2VecRecommender, self).__init__()\n",
    "        self.min_count = min_count\n",
    "        self.size = size\n",
    "        self.window = window\n",
    "        self.decay_alpha = decay_alpha\n",
    "        self.workers = workers\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Prod2VecRecommender(min_count={min_count}, ' \\\n",
    "               'size={size}, ' \\\n",
    "               'window={window}, ' \\\n",
    "               'decay_alpha={decay_alpha}, ' \\\n",
    "               'workers={workers})'.format(**self.__dict__)\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        sequences = train_data['sequence'].values\n",
    "        self.model = gensim.models.Word2Vec(sequences,\n",
    "                                            min_count=self.min_count,\n",
    "                                            window=self.window,\n",
    "                                            hs=1,\n",
    "                                            size=self.size,\n",
    "                                            sg=1,\n",
    "                                            workers=self.workers)\n",
    "\n",
    "    def recommend(self, user_profile, user_id=None):\n",
    "        user_profile = list(map(str, user_profile))\n",
    "        rec = []\n",
    "        try:\n",
    "            # iterate the user profile backwards\n",
    "            for i, item in enumerate(user_profile[::-1]):\n",
    "                ms = self.model.most_similar(positive=item)\n",
    "                # apply exponential decay to the similarity scores\n",
    "                decay = self.decay_alpha ** i\n",
    "                ms = [(x[0], decay * x[1]) for x in ms]\n",
    "                rec.extend(ms)\n",
    "            # sort items by similarity score\n",
    "            rec = sorted(rec, key=lambda x: -x[1])\n",
    "        except KeyError:\n",
    "            rec = []\n",
    "        return [([x[0]], x[1]) for x in rec]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCZDntVWEk2Y"
   },
   "outputs": [],
   "source": [
    "p2vrecommender = Prod2VecRecommender(min_count=2, \n",
    "                                  size=50, \n",
    "                                  window=5,\n",
    "                                  decay_alpha=0.9,\n",
    "                                  workers=4)\n",
    "p2vrecommender.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GT94MmEFPoF"
   },
   "source": [
    "### Session based RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbS5EBEXFVW1"
   },
   "source": [
    "Here we fit the recommedation algorithm over the sessions in the training set.  \n",
    "\n",
    "This is a **simplified** interface to Recurrent Neural Network models for Session-based recommendation.\n",
    "Based on the following two papers:\n",
    "\n",
    "* Recurrent Neural Networks with Top-k Gains for Session-based Recommendations, Hidasi and Karatzoglou, CIKM 2018\n",
    "* Personalizing Session-based Recommendation with Hierarchical Recurrent Neural Networks, Quadrana et al, Recsys 2017\n",
    "\n",
    "In this notebook, we will consider the session-based (**non-personalized**) version of the algorithm. \n",
    "Here's a schematic representation of the model:\n",
    "\n",
    "<img src='https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ffe88d088-1384-4c25-ae8b-626e148099c7%2FUntitled.png?table=block&id=f3bdbe38-a195-4525-bbb5-cb4c84499aee&spaceId=63b72b1f-0e90-4ab8-a6df-a060a6545a56&width=2000&userId=21ec183f-f0be-4b6b-9b3e-6f0d4e5c5469&cache=v2'>\n",
    "\n",
    "Each item in the current user session is first encoded either using _1-hot encoding_ or a _dense embedding vector_. The item representation is then forwarded to one or more Gated Reucurrent Unit (GRU) layers, which \"mix\" the information coming from the past steps of the sequence with the representation of the current item. The last hidden state of the network is finally use to compute the likelihood scores for the next items by using one out of several loss functions (e.g. cross-entropy, BPR, TOP1, BPR-max, TOP1-max, etc.).\n",
    "\n",
    "For simplicity, we only support _1-hot encoded_ inputs and the _BPR-max_ loss function here.\n",
    "\n",
    "The hyper-parameters of the model are:\n",
    "\n",
    "* `session_layers`: number of units per layer used at session level.\n",
    "    It has to be a list of integers for multi-layer networks, or a integer value for single-layer networks.\n",
    "* `user_layers`: number of units per layer used at user level. Required only by personalized models. (`None` in this case)\n",
    "* `batch_size`: the mini-batch size used in training\n",
    "* `learning_rate`: the learning rate used in training (Adagrad optimized)\n",
    "* `momentum`: the momentum coefficient used in training\n",
    "* `dropout`: it's a float value for the hidden-layer(s) dropout.\n",
    "* `epochs`: number of training epochs\n",
    "* `personalized`: whether to train a personalized model using the HRNN model (`False` in this case).\n",
    "\n",
    "\n",
    "**NOTE: GRU4Rec originally has many more hyper-parameters. Going through all of them is out from the scope of this tutorial, but we suggest to check-out the original source code [here](https://github.com/hidasib/GRU4Rec) if you are interested.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_ubpvFMF0Bz"
   },
   "outputs": [],
   "source": [
    "def dataset_to_gru4rec_format(dataset):\n",
    "    \"\"\"\n",
    "    Convert a list of sequences to GRU4Rec format.\n",
    "    Based on this StackOverflow answer: https://stackoverflow.com/a/48532692\n",
    "\n",
    "    :param dataset: the dataset to be transformed\n",
    "    \"\"\"\n",
    "\n",
    "    lst_col = 'sequence'\n",
    "    df = dataset.reset_index()\n",
    "    unstacked = pd.DataFrame({\n",
    "        col: np.repeat(df[col].values, df[lst_col].str.len()) for col in df.columns.drop(lst_col)}\n",
    "    ).assign(**{lst_col: np.concatenate(df[lst_col].values)})[df.columns]\n",
    "    # ensure that events in the session have increasing timestamps\n",
    "    unstacked['ts'] = unstacked['ts'] + unstacked.groupby('user_id').cumcount()\n",
    "    unstacked.rename(columns={'sequence': 'item_id'}, inplace=True)\n",
    "    return unstacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OkI_zUZGvqS"
   },
   "outputs": [],
   "source": [
    "def gpu_diag_wide(X):\n",
    "    E = T.eye(*X.shape)\n",
    "    return T.sum(X*E, axis=1)\n",
    "\n",
    "def gpu_diag_tall(X):\n",
    "    E = T.eye(*X.shape)\n",
    "    return T.sum(X*E, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EqSFZg1G-u-"
   },
   "outputs": [],
   "source": [
    "mrng = MRG_RandomStreams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLJqfowcG3QI"
   },
   "outputs": [],
   "source": [
    "class GRU4Rec:\n",
    "    '''\n",
    "    GRU4Rec(loss='bpr-max', final_act='elu-1', hidden_act='tanh', layers=[100],\n",
    "                 n_epochs=10, batch_size=32, dropout_p_hidden=0.0, dropout_p_embed=0.0, learning_rate=0.1, momentum=0.0, lmbd=0.0, embedding=0, n_sample=2048, sample_alpha=0.75, smoothing=0.0, constrained_embedding=False,\n",
    "                 adapt='adagrad', adapt_params=[], grad_cap=0.0, bpreg=1.0,\n",
    "                 sigma=0.0, init_as_normal=False, train_random_order=False, time_sort=True,\n",
    "                 session_key='SessionId', item_key='ItemId', time_key='Time')\n",
    "    Initializes the network.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    loss : 'top1', 'bpr', 'cross-entropy', 'xe_logit', 'top1-max', 'bpr-max'\n",
    "        selects the loss function (default : 'bpr-max')\n",
    "    final_act : 'softmax', 'linear', 'relu', 'tanh', 'softmax_logit', 'leaky-<X>', 'elu-<X>', 'selu-<X>-<Y>'\n",
    "        selects the activation function of the final layer, <X> and <Y> are the parameters of the activation function (default : 'elu-1')\n",
    "    hidden_act : 'linear', 'relu', 'tanh', 'leaky-<X>', 'elu-<X>', 'selu-<X>-<Y>'\n",
    "        selects the activation function on the hidden states, <X> and <Y> are the parameters of the activation function (default : 'tanh')\n",
    "    layers : list of int values\n",
    "        list of the number of GRU units in the layers (default : [100])\n",
    "    n_epochs : int\n",
    "        number of training epochs (default: 10)\n",
    "    batch_size : int\n",
    "        size of the minibacth, also effect the number of negative samples through minibatch based sampling (default: 32)\n",
    "    dropout_p_hidden : float\n",
    "        probability of dropout of hidden units (default: 0.0)\n",
    "    dropout_p_embed : float\n",
    "        probability of dropout of the input units, applicable only if embeddings are used (default: 0.0)\n",
    "    learning_rate : float\n",
    "        learning rate (default: 0.05)\n",
    "    momentum : float\n",
    "        if not zero, Nesterov momentum will be applied during training with the given strength (default: 0.0)\n",
    "    lmbd : float\n",
    "        coefficient of the L2 regularization (default: 0.0)\n",
    "    embedding : int\n",
    "        size of the embedding used, 0 means not to use embedding (default: 0)\n",
    "    n_sample : int\n",
    "        number of additional negative samples to be used (besides the other examples of the minibatch) (default: 2048)\n",
    "    sample_alpha : float\n",
    "        the probability of an item used as an additional negative sample is supp^sample_alpha (default: 0.75)\n",
    "        (e.g.: sample_alpha=1 --> popularity based sampling; sample_alpha=0 --> uniform sampling)\n",
    "    smoothing : float\n",
    "        (only works with cross-entropy and xe_logit losses) if set to non-zero class labels are smoothed with this value, i.e. the expected utput is (e/N, ..., e/N, 1-e+e/N, e/N, ..., e/N) instead of (0, ..., 0, 1, 0, ..., 0), where N is the number of outputs and e is the smoothing value (default: 0.0)\n",
    "    constrained_embedding : bool\n",
    "        if True, the output weight matrix is also used as input embedding (default: False)\n",
    "    adapt : None, 'adagrad', 'rmsprop', 'adam', 'adadelta'\n",
    "        sets the appropriate learning rate adaptation strategy, use None for standard SGD (default: 'adagrad')\n",
    "    adapt_params : list\n",
    "        parameters for the adaptive learning methods (default: [])\n",
    "    grad_cap : float\n",
    "        clip gradients that exceede this value to this value, 0 means no clipping (default: 0.0)\n",
    "    bpreg : float\n",
    "        score regularization coefficient for the BPR-max loss function (default: 1.0)\n",
    "    sigma : float\n",
    "        \"width\" of initialization; either the standard deviation or the min/max of the init interval (with normal and uniform initializations respectively); 0 means adaptive normalization (sigma depends on the size of the weight matrix); (default: 0.0)\n",
    "    init_as_normal : boolean\n",
    "        False: init from uniform distribution on [-sigma,sigma]; True: init from normal distribution N(0,sigma); (default: False)\n",
    "    train_random_order : boolean\n",
    "        whether to randomize the order of sessions in each epoch (default: False)\n",
    "    time_sort : boolean\n",
    "        whether to ensure the the order of sessions is chronological (default: True)\n",
    "    session_key : string\n",
    "        header of the session ID column in the input file (default: 'SessionId')\n",
    "    item_key : string\n",
    "        header of the item ID column in the input file (default: 'ItemId')\n",
    "    time_key : string\n",
    "        header of the timestamp column in the input file (default: 'Time')\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, loss='bpr-max', final_act='linear', hidden_act='tanh', layers=[100],\n",
    "                 n_epochs=10, batch_size=32, dropout_p_hidden=0.0, dropout_p_embed=0.0, learning_rate=0.1, momentum=0.0,\n",
    "                 lmbd=0.0, embedding=0, n_sample=2048, sample_alpha=0.75, smoothing=0.0, constrained_embedding=False,\n",
    "                 adapt='adagrad', adapt_params=[], grad_cap=0.0, bpreg=1.0,\n",
    "                 sigma=0.0, init_as_normal=False, train_random_order=False, time_sort=True,\n",
    "                 session_key='SessionId', item_key='ItemId', time_key='Time'):\n",
    "        self.layers = layers\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout_p_hidden = dropout_p_hidden\n",
    "        self.dropout_p_embed = dropout_p_embed\n",
    "        self.learning_rate = learning_rate\n",
    "        self.adapt_params = adapt_params\n",
    "        self.momentum = momentum\n",
    "        self.sigma = sigma\n",
    "        self.init_as_normal = init_as_normal\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.grad_cap = grad_cap\n",
    "        self.bpreg = bpreg\n",
    "        self.train_random_order = train_random_order\n",
    "        self.lmbd = lmbd\n",
    "        self.embedding = embedding\n",
    "        self.constrained_embedding = constrained_embedding\n",
    "        self.time_sort = time_sort\n",
    "        self.adapt = adapt\n",
    "        self.loss = loss\n",
    "        self.set_loss_function(self.loss)\n",
    "        self.final_act = final_act\n",
    "        self.set_final_activation(self.final_act)\n",
    "        self.hidden_act = hidden_act\n",
    "        self.set_hidden_activation(self.hidden_act)\n",
    "        self.n_sample = n_sample\n",
    "        self.sample_alpha = sample_alpha\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def set_loss_function(self, loss):\n",
    "        if loss == 'cross-entropy':\n",
    "            self.loss_function = self.cross_entropy\n",
    "        elif loss == 'bpr':\n",
    "            self.loss_function = self.bpr\n",
    "        elif loss == 'bpr-max':\n",
    "            self.loss_function = self.bpr_max\n",
    "        elif loss == 'top1':\n",
    "            self.loss_function = self.top1\n",
    "        elif loss == 'top1-max':\n",
    "            self.loss_function = self.top1_max\n",
    "        elif loss == 'xe_logit':\n",
    "            self.loss_function = self.cross_entropy_logits\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def set_final_activation(self, final_act):\n",
    "        if final_act == 'linear':\n",
    "            self.final_activation = self.linear\n",
    "        elif final_act == 'relu':\n",
    "            self.final_activation = self.relu\n",
    "        elif final_act == 'softmax':\n",
    "            self.final_activation = self.softmax\n",
    "        elif final_act == 'tanh':\n",
    "            self.final_activation = self.tanh\n",
    "        elif final_act == 'softmax_logit':\n",
    "            self.final_activation = self.softmax_logit\n",
    "        elif final_act.startswith('leaky-'):\n",
    "            self.final_activation = self.LeakyReLU(float(final_act.split('-')[1])).execute\n",
    "        elif final_act.startswith('elu-'):\n",
    "            self.final_activation = self.Elu(float(final_act.split('-')[1])).execute\n",
    "        elif final_act.startswith('selu-'):\n",
    "            self.final_activation = self.Selu(*[float(x) for x in final_act.split('-')[1:]]).execute\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def set_hidden_activation(self, hidden_act):\n",
    "        if hidden_act == 'relu':\n",
    "            self.hidden_activation = self.relu\n",
    "        elif hidden_act == 'tanh':\n",
    "            self.hidden_activation = self.tanh\n",
    "        elif hidden_act == 'linear':\n",
    "            self.hidden_activation = self.linear\n",
    "        elif hidden_act.startswith('leaky-'):\n",
    "            self.hidden_activation = self.LeakyReLU(float(hidden_act.split('-')[1])).execute\n",
    "        elif hidden_act.startswith('elu-'):\n",
    "            self.hidden_activation = self.Elu(float(hidden_act.split('-')[1])).execute\n",
    "        elif hidden_act.startswith('selu-'):\n",
    "            self.hidden_activation = self.Selu(*[float(x) for x in hidden_act.split('-')[1:]]).execute\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def set_params(self, **kvargs):\n",
    "        maxk_len = np.max([len(x) for x in kvargs.keys()])\n",
    "        maxv_len = np.max([len(x) for x in kvargs.values()])\n",
    "        for k, v in kvargs.items():\n",
    "            if not hasattr(self, k):\n",
    "                print('Unkown attribute: {}'.format(k))\n",
    "                raise NotImplementedError\n",
    "            else:\n",
    "                if k == 'adapt_params':\n",
    "                    v = [float(l) for l in v.split('/')]\n",
    "                elif type(getattr(self, k)) == list:\n",
    "                    v = [int(l) for l in v.split('/')]\n",
    "                if type(getattr(self, k)) == bool:\n",
    "                    if v == 'True' or v == '1':\n",
    "                        v = True\n",
    "                    elif v == 'False' or v == '0':\n",
    "                        v = False\n",
    "                    else:\n",
    "                        print('Invalid value for boolean parameter: {}'.format(v))\n",
    "                        raise NotImplementedError\n",
    "                setattr(self, k, type(getattr(self, k))(v))\n",
    "                if k == 'loss': self.set_loss_function(self.loss)\n",
    "                if k == 'final_act': self.set_final_activation(self.final_act)\n",
    "                if k == 'hidden_act': self.set_hidden_activation(self.hidden_act)\n",
    "                print('SET   {}{}TO   {}{}(type: {})'.format(k, ' ' * (maxk_len - len(k) + 3), getattr(self, k),\n",
    "                                                             ' ' * (maxv_len - len(str(getattr(self, k))) + 3),\n",
    "                                                             type(getattr(self, k))))\n",
    "\n",
    "    ######################ACTIVATION FUNCTIONS#####################\n",
    "    def linear(self, X):\n",
    "        return X\n",
    "\n",
    "    def tanh(self, X):\n",
    "        return T.tanh(X)\n",
    "\n",
    "    def softmax(self, X):\n",
    "        e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "        return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "    def softmax_logit(self, X):\n",
    "        X = X - X.max(axis=1).dimshuffle(0, 'x')\n",
    "        return T.log(T.exp(X).sum(axis=1).dimshuffle(0, 'x')) - X\n",
    "\n",
    "    def softmax_neg(self, X):\n",
    "        hm = 1.0 - T.eye(*X.shape)\n",
    "        X = X * hm\n",
    "        e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x')) * hm\n",
    "        return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "    def relu(self, X):\n",
    "        return T.maximum(X, 0)\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        return T.nnet.sigmoid(X)\n",
    "\n",
    "    class Selu:\n",
    "        def __init__(self, lmbd, alpha):\n",
    "            self.lmbd = lmbd\n",
    "            self.alpha = alpha\n",
    "\n",
    "        def execute(self, X):\n",
    "            return self.lmbd * T.switch(T.ge(X, 0), X, self.alpha * (T.exp(X) - 1))\n",
    "\n",
    "    class Elu:\n",
    "        def __init__(self, alpha):\n",
    "            self.alpha = alpha\n",
    "\n",
    "        def execute(self, X):\n",
    "            return T.switch(T.ge(X, 0), X, self.alpha * (T.exp(X) - 1))\n",
    "\n",
    "    class LeakyReLU:\n",
    "        def __init__(self, leak):\n",
    "            self.leak = leak\n",
    "\n",
    "        def execute(self, X):\n",
    "            return T.switch(T.ge(X, 0), X, self.leak * X)\n",
    "\n",
    "    #################################LOSS FUNCTIONS################################\n",
    "    def cross_entropy(self, yhat, M):\n",
    "        if self.smoothing:\n",
    "            n_out = M + self.n_sample\n",
    "            return T.cast(T.mean(\n",
    "                (1.0 - (n_out / (n_out - 1)) * self.smoothing) * (-T.log(gpu_diag_wide(yhat) + 1e-24)) + (\n",
    "                            self.smoothing / (n_out - 1)) * T.sum(-T.log(yhat + 1e-24), axis=1)), theano.config.floatX)\n",
    "        else:\n",
    "            return T.cast(T.mean(-T.log(gpu_diag_wide(yhat) + 1e-24)), theano.config.floatX)\n",
    "\n",
    "    def cross_entropy_logits(self, yhat, M):\n",
    "        if self.smoothing:\n",
    "            n_out = M + self.n_sample\n",
    "            return T.cast(T.mean((1.0 - (n_out / (n_out - 1)) * self.smoothing) * gpu_diag_wide(yhat) + (\n",
    "                        self.smoothing / (n_out - 1)) * T.sum(yhat, axis=1)), theano.config.floatX)\n",
    "        else:\n",
    "            return T.cast(T.mean(gpu_diag_wide(yhat)), theano.config.floatX)\n",
    "\n",
    "    def bpr(self, yhat, M):\n",
    "        return T.cast(T.mean(-T.log(T.nnet.sigmoid(gpu_diag_wide(yhat).dimshuffle((0, 'x')) - yhat))),\n",
    "                      theano.config.floatX)\n",
    "\n",
    "    def bpr_max(self, yhat, M):\n",
    "        softmax_scores = self.softmax_neg(yhat)\n",
    "        return T.cast(T.mean(-T.log(\n",
    "            T.sum(T.nnet.sigmoid(gpu_diag_wide(yhat).dimshuffle((0, 'x')) - yhat) * softmax_scores,\n",
    "                  axis=1) + 1e-24) + self.bpreg * T.sum((yhat ** 2) * softmax_scores, axis=1)), theano.config.floatX)\n",
    "\n",
    "    def top1(self, yhat, M):\n",
    "        ydiag = gpu_diag_wide(yhat).dimshuffle((0, 'x'))\n",
    "        return T.cast(T.mean(\n",
    "            T.mean(T.nnet.sigmoid(-ydiag + yhat) + T.nnet.sigmoid(yhat ** 2), axis=1) - T.nnet.sigmoid(ydiag ** 2) / (\n",
    "                        M + self.n_sample)), theano.config.floatX)\n",
    "\n",
    "    def top1_max(self, yhat, M):\n",
    "        softmax_scores = self.softmax_neg(yhat)\n",
    "        y = softmax_scores * (\n",
    "                    T.nnet.sigmoid(-gpu_diag_wide(yhat).dimshuffle((0, 'x')) + yhat) + T.nnet.sigmoid(yhat ** 2))\n",
    "        return T.cast(T.mean(T.sum(y, axis=1)), theano.config.floatX)\n",
    "\n",
    "    ###############################################################################\n",
    "    def floatX(self, X):\n",
    "        return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "    def init_weights(self, shape, name=None):\n",
    "        return theano.shared(self.init_matrix(shape), borrow=True, name=name)\n",
    "\n",
    "    def init_matrix(self, shape):\n",
    "        if self.sigma != 0:\n",
    "            sigma = self.sigma\n",
    "        else:\n",
    "            sigma = np.sqrt(6.0 / (shape[0] + shape[1]))\n",
    "        if self.init_as_normal:\n",
    "            return self.floatX(np.random.randn(*shape) * sigma)\n",
    "        else:\n",
    "            return self.floatX(np.random.rand(*shape) * sigma * 2 - sigma)\n",
    "\n",
    "    def extend_weights(self, W, n_new):\n",
    "        matrix = W.get_value()\n",
    "        sigma = self.sigma if self.sigma != 0 else np.sqrt(6.0 / (matrix.shape[0] + matrix.shape[1] + n_new))\n",
    "        if self.init_as_normal:\n",
    "            new_rows = self.floatX(np.random.randn(n_new, matrix.shape[1]) * sigma)\n",
    "        else:\n",
    "            new_rows = self.floatX(np.random.rand(n_new, matrix.shape[1]) * sigma * 2 - sigma)\n",
    "        W.set_value(np.vstack([matrix, new_rows]))\n",
    "\n",
    "    def init(self, data):\n",
    "        data.sort_values([self.session_key, self.time_key], inplace=True)\n",
    "        offset_sessions = np.zeros(data[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        offset_sessions[1:] = data.groupby(self.session_key).size().cumsum()\n",
    "        np.random.seed(42)\n",
    "        self.Wx, self.Wh, self.Wrz, self.Bh, self.H = [], [], [], [], []\n",
    "        if self.constrained_embedding:\n",
    "            n_features = self.layers[-1]\n",
    "        elif self.embedding:\n",
    "            self.E = self.init_weights((self.n_items, self.embedding), name='E')\n",
    "            n_features = self.embedding\n",
    "        else:\n",
    "            n_features = self.n_items\n",
    "        for i in range(len(self.layers)):\n",
    "            m = []\n",
    "            m.append(self.init_matrix((self.layers[i - 1] if i > 0 else n_features, self.layers[i])))\n",
    "            m.append(self.init_matrix((self.layers[i - 1] if i > 0 else n_features, self.layers[i])))\n",
    "            m.append(self.init_matrix((self.layers[i - 1] if i > 0 else n_features, self.layers[i])))\n",
    "            self.Wx.append(\n",
    "                theano.shared(value=np.hstack(m), borrow=True, name='Wx{}'.format(i)))  # For compatibility's sake\n",
    "            self.Wh.append(self.init_weights((self.layers[i], self.layers[i]), name='Wh{}'.format(i)))\n",
    "            m2 = []\n",
    "            m2.append(self.init_matrix((self.layers[i], self.layers[i])))\n",
    "            m2.append(self.init_matrix((self.layers[i], self.layers[i])))\n",
    "            self.Wrz.append(\n",
    "                theano.shared(value=np.hstack(m2), borrow=True, name='Wrz{}'.format(i)))  # For compatibility's sake\n",
    "            self.Bh.append(theano.shared(value=np.zeros((self.layers[i] * 3,), dtype=theano.config.floatX), borrow=True,\n",
    "                                         name='Bh{}'.format(i)))\n",
    "            self.H.append(theano.shared(value=np.zeros((self.batch_size, self.layers[i]), dtype=theano.config.floatX),\n",
    "                                        borrow=True, name='H{}'.format(i)))\n",
    "        self.Wy = self.init_weights((self.n_items, self.layers[-1]), name='Wy')\n",
    "        self.By = theano.shared(value=np.zeros((self.n_items, 1), dtype=theano.config.floatX), borrow=True, name='By')\n",
    "        return offset_sessions\n",
    "\n",
    "    def dropout(self, X, drop_p):\n",
    "        if drop_p > 0:\n",
    "            retain_prob = 1 - drop_p\n",
    "            X *= mrng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX) / retain_prob\n",
    "        return X\n",
    "\n",
    "    def adam(self, param, grad, updates, sample_idx=None, epsilon=1e-6):\n",
    "        v1 = np.float32(self.adapt_params[0])\n",
    "        v2 = np.float32(1.0 - self.adapt_params[0])\n",
    "        v3 = np.float32(self.adapt_params[1])\n",
    "        v4 = np.float32(1.0 - self.adapt_params[1])\n",
    "        acc = theano.shared(param.get_value(borrow=False) * 0., borrow=True)\n",
    "        meang = theano.shared(param.get_value(borrow=False) * 0., borrow=True)\n",
    "        countt = theano.shared(param.get_value(borrow=False) * 0., borrow=True)\n",
    "        if sample_idx is None:\n",
    "            acc_new = v3 * acc + v4 * (grad ** 2)\n",
    "            meang_new = v1 * meang + v2 * grad\n",
    "            countt_new = countt + 1\n",
    "            updates[acc] = acc_new\n",
    "            updates[meang] = meang_new\n",
    "            updates[countt] = countt_new\n",
    "        else:\n",
    "            acc_s = acc[sample_idx]\n",
    "            meang_s = meang[sample_idx]\n",
    "            countt_s = countt[sample_idx]\n",
    "            #            acc_new = v3 * acc_s + v4 * (grad**2) #Faster, but inaccurate when an index occurs multiple times\n",
    "            #            updates[acc] = T.set_subtensor(acc_s, acc_new) #Faster, but inaccurate when an index occurs multiple times\n",
    "            updates[acc] = T.inc_subtensor(T.set_subtensor(acc_s, acc_s * v3)[sample_idx],\n",
    "                                           v4 * (grad ** 2))  # Slower, but accurate when an index occurs multiple times\n",
    "            acc_new = updates[acc][sample_idx]  # Slower, but accurate when an index occurs multiple times\n",
    "            #            meang_new = v1 * meang_s + v2 * grad\n",
    "            #            updates[meang] = T.set_subtensor(meang_s, meang_new) #Faster, but inaccurate when an index occurs multiple times\n",
    "            updates[meang] = T.inc_subtensor(T.set_subtensor(meang_s, meang_s * v1)[sample_idx], v2 * (\n",
    "                        grad ** 2))  # Slower, but accurate when an index occurs multiple times\n",
    "            meang_new = updates[meang][sample_idx]  # Slower, but accurate when an index occurs multiple times\n",
    "            countt_new = countt_s + 1.0\n",
    "            updates[countt] = T.set_subtensor(countt_s, countt_new)\n",
    "        return (meang_new / (1 - v1 ** countt_new)) / (T.sqrt(acc_new / (1 - v1 ** countt_new)) + epsilon)\n",
    "\n",
    "    def adagrad(self, param, grad, updates, sample_idx=None, epsilon=1e-6):\n",
    "        acc = theano.shared(param.get_value(borrow=False) * 0., borrow=True)\n",
    "        if sample_idx is None:\n",
    "            acc_new = acc + grad ** 2\n",
    "            updates[acc] = acc_new\n",
    "        else:\n",
    "            acc_s = acc[sample_idx]\n",
    "            acc_new = acc_s + grad ** 2\n",
    "            updates[acc] = T.set_subtensor(acc_s, acc_new)\n",
    "        gradient_scaling = T.cast(T.sqrt(acc_new + epsilon), theano.config.floatX)\n",
    "        return grad / gradient_scaling\n",
    "\n",
    "    def adadelta(self, param, grad, updates, sample_idx=None, epsilon=1e-6):\n",
    "        v1 = np.float32(self.adapt_params[0])\n",
    "        v2 = np.float32(1.0 - self.adapt_params[0])\n",
    "        acc = theano.shared(param.get_value(borrow=False) * 0., borrow=True)\n",
    "        upd = theano.shared(param.get_value(borrow=False) * 0., borrow=True)\n",
    "        if sample_idx is None:\n",
    "            acc_new = v1 * acc + v2 * (grad ** 2)\n",
    "            updates[acc] = acc_new\n",
    "            grad_scaling = (upd + epsilon) / (acc_new + epsilon)\n",
    "            upd_new = v1 * upd + v2 * grad_scaling * (grad ** 2)\n",
    "            updates[upd] = upd_new\n",
    "        else:\n",
    "            acc_s = acc[sample_idx]\n",
    "            #            acc_new = v1 * acc_s + v2 * (grad**2) #Faster, but inaccurate when an index occurs multiple times\n",
    "            #            updates[acc] = T.set_subtensor(acc_s, acc_new) #Faster, but inaccurate when an index occurs multiple times\n",
    "            updates[acc] = T.inc_subtensor(T.set_subtensor(acc_s, acc_s * v1)[sample_idx],\n",
    "                                           v2 * (grad ** 2))  # Slower, but accurate when an index occurs multiple times\n",
    "            acc_new = updates[acc][sample_idx]  # Slower, but accurate when an index occurs multiple times\n",
    "            upd_s = upd[sample_idx]\n",
    "            grad_scaling = (upd_s + epsilon) / (acc_new + epsilon)\n",
    "            #            updates[upd] = T.set_subtensor(upd_s, v1 * upd_s + v2 * grad_scaling * (grad**2)) #Faster, but inaccurate when an index occurs multiple times\n",
    "            updates[upd] = T.inc_subtensor(T.set_subtensor(upd_s, upd_s * v1)[sample_idx], v2 * grad_scaling * (\n",
    "                        grad ** 2))  # Slower, but accurate when an index occurs multiple times\n",
    "        gradient_scaling = T.cast(T.sqrt(grad_scaling), theano.config.floatX)\n",
    "        if self.learning_rate != 1.0:\n",
    "            print('Warn: learning_rate is not 1.0 while using adadelta. Setting learning_rate to 1.0')\n",
    "            self.learning_rate = 1.0\n",
    "        return grad * gradient_scaling  # Ok, checked\n",
    "\n",
    "    def rmsprop(self, param, grad, updates, sample_idx=None, epsilon=1e-6):\n",
    "        v1 = np.float32(self.adapt_params[0])\n",
    "        v2 = np.float32(1.0 - self.adapt_params[0])\n",
    "        acc = theano.shared(param.get_value(borrow=False) * 0., borrow=True)\n",
    "        if sample_idx is None:\n",
    "            acc_new = v1 * acc + v2 * grad ** 2\n",
    "            updates[acc] = acc_new\n",
    "        else:\n",
    "            acc_s = acc[sample_idx]\n",
    "            #            acc_new = v1 * acc_s + v2 * grad ** 2 #Faster, but inaccurate when an index occurs multiple times\n",
    "            #            updates[acc] = T.set_subtensor(acc_s, acc_new) #Faster, but inaccurate when an index occurs multiple times\n",
    "            updates[acc] = T.inc_subtensor(T.set_subtensor(acc_s, acc_s * v1)[sample_idx],\n",
    "                                           v2 * grad ** 2)  # Slower, but accurate when an index occurs multiple times\n",
    "            acc_new = updates[acc][sample_idx]  # Slower, but accurate when an index occurs multiple times\n",
    "        gradient_scaling = T.cast(T.sqrt(acc_new + epsilon), theano.config.floatX)\n",
    "        return grad / gradient_scaling\n",
    "\n",
    "    def RMSprop(self, cost, params, full_params, sampled_params, sidxs, epsilon=1e-6):\n",
    "        grads = [T.grad(cost=cost, wrt=param) for param in params]\n",
    "        sgrads = [T.grad(cost=cost, wrt=sparam) for sparam in sampled_params]\n",
    "        updates = OrderedDict()\n",
    "        if self.grad_cap > 0:\n",
    "            norm = T.cast(T.sqrt(T.sum([T.sum([T.sum(g ** 2) for g in g_list]) for g_list in grads]) + T.sum(\n",
    "                [T.sum(g ** 2) for g in sgrads])), theano.config.floatX)\n",
    "            grads = [[T.switch(T.ge(norm, self.grad_cap), g * self.grad_cap / norm, g) for g in g_list] for g_list in\n",
    "                     grads]\n",
    "            sgrads = [T.switch(T.ge(norm, self.grad_cap), g * self.grad_cap / norm, g) for g in sgrads]\n",
    "        for p_list, g_list in zip(params, grads):\n",
    "            for p, g in zip(p_list, g_list):\n",
    "                if self.adapt == 'adagrad':\n",
    "                    g = self.adagrad(p, g, updates)\n",
    "                elif self.adapt == 'rmsprop':\n",
    "                    g = self.rmsprop(p, g, updates)\n",
    "                elif self.adapt == 'adadelta':\n",
    "                    g = self.adadelta(p, g, updates)\n",
    "                elif self.adapt == 'adam':\n",
    "                    g = self.adam(p, g, updates)\n",
    "                if self.momentum > 0:\n",
    "                    velocity = theano.shared(p.get_value(borrow=False) * 0., borrow=True)\n",
    "                    velocity2 = self.momentum * velocity - np.float32(self.learning_rate) * (g + self.lmbd * p)\n",
    "                    updates[velocity] = velocity2\n",
    "                    updates[p] = p + velocity2\n",
    "                else:\n",
    "                    updates[p] = p * np.float32(1.0 - self.learning_rate * self.lmbd) - np.float32(\n",
    "                        self.learning_rate) * g\n",
    "        for i in range(len(sgrads)):\n",
    "            g = sgrads[i]\n",
    "            fullP = full_params[i]\n",
    "            sample_idx = sidxs[i]\n",
    "            sparam = sampled_params[i]\n",
    "            if self.adapt == 'adagrad':\n",
    "                g = self.adagrad(fullP, g, updates, sample_idx)\n",
    "            elif self.adapt == 'rmsprop':\n",
    "                g = self.rmsprop(fullP, g, updates, sample_idx)\n",
    "            elif self.adapt == 'adadelta':\n",
    "                g = self.adadelta(fullP, g, updates, sample_idx)\n",
    "            elif self.adapt == 'adam':\n",
    "                g = self.adam(fullP, g, updates, sample_idx)\n",
    "            if self.lmbd > 0:\n",
    "                delta = np.float32(self.learning_rate) * (g + self.lmbd * sparam)\n",
    "            else:\n",
    "                delta = np.float32(self.learning_rate) * g\n",
    "            if self.momentum > 0:\n",
    "                velocity = theano.shared(fullP.get_value(borrow=False) * 0., borrow=True)\n",
    "                vs = velocity[sample_idx]\n",
    "                velocity2 = self.momentum * vs - delta\n",
    "                updates[velocity] = T.set_subtensor(vs, velocity2)\n",
    "                updates[fullP] = T.inc_subtensor(sparam, velocity2)\n",
    "            else:\n",
    "                updates[fullP] = T.inc_subtensor(sparam, - delta)\n",
    "        return updates\n",
    "\n",
    "    def model(self, X, H, M, R=None, Y=None, drop_p_hidden=0.0, drop_p_embed=0.0, predict=False):\n",
    "        sparams, full_params, sidxs = [], [], []\n",
    "        if self.constrained_embedding:\n",
    "            if Y is not None: X = T.concatenate([X, Y], axis=0)\n",
    "            S = self.Wy[X]\n",
    "            Sx = S[:M]\n",
    "            Sy = S[M:]\n",
    "            y = self.dropout(Sx, drop_p_embed)\n",
    "            H_new = []\n",
    "            start = 0\n",
    "            sparams.append(S)\n",
    "            full_params.append(self.Wy)\n",
    "            sidxs.append(X)\n",
    "        elif self.embedding:\n",
    "            Sx = self.E[X]\n",
    "            y = self.dropout(Sx, drop_p_embed)\n",
    "            H_new = []\n",
    "            start = 0\n",
    "            sparams.append(Sx)\n",
    "            full_params.append(self.E)\n",
    "            sidxs.append(X)\n",
    "        else:\n",
    "            Sx = self.Wx[0][X]\n",
    "            vec = Sx + self.Bh[0]\n",
    "            rz = T.nnet.sigmoid(vec[:, self.layers[0]:] + T.dot(H[0], self.Wrz[0]))\n",
    "            h = self.hidden_activation(T.dot(H[0] * rz[:, :self.layers[0]], self.Wh[0]) + vec[:, :self.layers[0]])\n",
    "            z = rz[:, self.layers[0]:]\n",
    "            h = (1.0 - z) * H[0] + z * h\n",
    "            h = self.dropout(h, drop_p_hidden)\n",
    "            y = h\n",
    "            H_new = [T.switch(R.dimshuffle((0, 'x')), 0, h) if not predict else h]\n",
    "            start = 1\n",
    "            sparams.append(Sx)\n",
    "            full_params.append(self.Wx[0])\n",
    "            sidxs.append(X)\n",
    "        for i in range(start, len(self.layers)):\n",
    "            vec = T.dot(y, self.Wx[i]) + self.Bh[i]\n",
    "            rz = T.nnet.sigmoid(vec[:, self.layers[i]:] + T.dot(H[i], self.Wrz[i]))\n",
    "            h = self.hidden_activation(T.dot(H[i] * rz[:, :self.layers[i]], self.Wh[i]) + vec[:, :self.layers[i]])\n",
    "            z = rz[:, self.layers[i]:]\n",
    "            h = (1.0 - z) * H[i] + z * h\n",
    "            h = self.dropout(h, drop_p_hidden)\n",
    "            y = h\n",
    "            H_new.append(T.switch(R.dimshuffle((0, 'x')), 0, h) if not predict else h)\n",
    "        if Y is not None:\n",
    "            if (not self.constrained_embedding) or predict:\n",
    "                Sy = self.Wy[Y]\n",
    "                sparams.append(Sy)\n",
    "                full_params.append(self.Wy)\n",
    "                sidxs.append(Y)\n",
    "            SBy = self.By[Y]\n",
    "            sparams.append(SBy)\n",
    "            full_params.append(self.By)\n",
    "            sidxs.append(Y)\n",
    "            if predict and self.final_act == 'softmax_logit':\n",
    "                y = self.softmax(T.dot(y, Sy.T) + SBy.flatten())\n",
    "            else:\n",
    "                y = self.final_activation(T.dot(y, Sy.T) + SBy.flatten())\n",
    "            return H_new, y, sparams, full_params, sidxs\n",
    "        else:\n",
    "            if predict and self.final_act == 'softmax_logit':\n",
    "                y = self.softmax(T.dot(y, self.Wy.T) + self.By.flatten())\n",
    "            else:\n",
    "                y = self.final_activation(T.dot(y, self.Wy.T) + self.By.flatten())\n",
    "            return H_new, y, sparams, full_params, sidxs\n",
    "\n",
    "    def generate_neg_samples(self, pop, length):\n",
    "        if self.sample_alpha:\n",
    "            sample = np.searchsorted(pop, np.random.rand(self.n_sample * length))\n",
    "        else:\n",
    "            sample = np.random.choice(self.n_items, size=self.n_sample * length)\n",
    "        if length > 1:\n",
    "            sample = sample.reshape((length, self.n_sample))\n",
    "        return sample\n",
    "\n",
    "    def fit(self, data, sample_store=10000000):\n",
    "        '''\n",
    "        Trains the network.\n",
    "\n",
    "        Parameters\n",
    "        --------\n",
    "        data : pandas.DataFrame\n",
    "            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n",
    "            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n",
    "        sample_store : int\n",
    "            If additional negative samples are used (n_sample > 0), the efficiency of GPU utilization can be sped up, by precomputing a large batch of negative samples (and recomputing when necessary).\n",
    "            This parameter regulizes the size of this precomputed ID set. Its value is the maximum number of int values (IDs) to be stored. Precomputed IDs are stored in the RAM.\n",
    "            For the most efficient computation, a balance must be found between storing few examples and constantly interrupting GPU computations for a short time vs. computing many examples and interrupting GPU computations for a long time (but rarely).\n",
    "\n",
    "        '''\n",
    "        self.predict = None\n",
    "        self.error_during_train = False\n",
    "        itemids = data[self.item_key].unique()\n",
    "        self.n_items = len(itemids)\n",
    "        self.itemidmap = pd.Series(data=np.arange(self.n_items), index=itemids)\n",
    "        data = pd.merge(data, pd.DataFrame({self.item_key: itemids, 'ItemIdx': self.itemidmap[itemids].values}),\n",
    "                        on=self.item_key, how='inner')\n",
    "        offset_sessions = self.init(data)\n",
    "        if self.n_sample:\n",
    "            pop = data.groupby(self.item_key).size()\n",
    "            pop = pop[self.itemidmap.index.values].values ** self.sample_alpha\n",
    "            pop = pop.cumsum() / pop.sum()\n",
    "            pop[-1] = 1\n",
    "            if sample_store:\n",
    "                generate_length = sample_store // self.n_sample\n",
    "                if generate_length <= 1:\n",
    "                    sample_store = 0\n",
    "                    print('No example store was used')\n",
    "                else:\n",
    "                    neg_samples = self.generate_neg_samples(pop, generate_length)\n",
    "                    sample_pointer = 0\n",
    "            else:\n",
    "                print('No example store was used')\n",
    "        X = T.ivector()\n",
    "        Y = T.ivector()\n",
    "        M = T.iscalar()\n",
    "        R = T.bvector()\n",
    "        H_new, Y_pred, sparams, full_params, sidxs = self.model(X, self.H, M, R, Y, self.dropout_p_hidden,\n",
    "                                                                self.dropout_p_embed)\n",
    "        cost = (M / self.batch_size) * self.loss_function(Y_pred, M)\n",
    "        params = [self.Wx if self.embedding or self.constrained_embedding else self.Wx[1:], self.Wh, self.Wrz, self.Bh]\n",
    "        updates = self.RMSprop(cost, params, full_params, sparams, sidxs)\n",
    "        for i in range(len(self.H)):\n",
    "            updates[self.H[i]] = H_new[i]\n",
    "        train_function = function(inputs=[X, Y, M, R], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "        base_order = np.argsort(\n",
    "            data.groupby(self.session_key)[self.time_key].min().values) if self.time_sort else np.arange(\n",
    "            len(offset_sessions) - 1)\n",
    "        data_items = data.ItemIdx.values\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for i in range(len(self.layers)):\n",
    "                self.H[i].set_value(np.zeros((self.batch_size, self.layers[i]), dtype=theano.config.floatX),\n",
    "                                    borrow=True)\n",
    "            c = []\n",
    "            cc = []\n",
    "            session_idx_arr = np.random.permutation(len(offset_sessions) - 1) if self.train_random_order else base_order\n",
    "            iters = np.arange(self.batch_size)\n",
    "            maxiter = iters.max()\n",
    "            start = offset_sessions[session_idx_arr[iters]]\n",
    "            end = offset_sessions[session_idx_arr[iters] + 1]\n",
    "            finished = False\n",
    "            while not finished:\n",
    "                minlen = (end - start).min()\n",
    "                out_idx = data_items[start]\n",
    "                for i in range(minlen - 1):\n",
    "                    in_idx = out_idx\n",
    "                    out_idx = data_items[start + i + 1]\n",
    "                    if self.n_sample:\n",
    "                        if sample_store:\n",
    "                            if sample_pointer == generate_length:\n",
    "                                neg_samples = self.generate_neg_samples(pop, generate_length)\n",
    "                                sample_pointer = 0\n",
    "                            sample = neg_samples[sample_pointer]\n",
    "                            sample_pointer += 1\n",
    "                        else:\n",
    "                            sample = self.generate_neg_samples(pop, 1)\n",
    "                        y = np.hstack([out_idx, sample])\n",
    "                    else:\n",
    "                        y = out_idx\n",
    "                        if self.n_sample:\n",
    "                            if sample_pointer == generate_length:\n",
    "                                generate_samples()\n",
    "                                sample_pointer = 0\n",
    "                            sample_pointer += 1\n",
    "                    reset = (start + i + 1 == end - 1)\n",
    "                    cost = train_function(in_idx, y, len(iters), reset)\n",
    "                    c.append(cost)\n",
    "                    cc.append(len(iters))\n",
    "                    if np.isnan(cost):\n",
    "                        print(str(epoch) + ': NaN error!')\n",
    "                        self.error_during_train = True\n",
    "                        return\n",
    "                start = start + minlen - 1\n",
    "                finished_mask = (end - start <= 1)\n",
    "                n_finished = finished_mask.sum()\n",
    "                iters[finished_mask] = maxiter + np.arange(1, n_finished + 1)\n",
    "                maxiter += n_finished\n",
    "                valid_mask = (iters < len(offset_sessions) - 1)\n",
    "                n_valid = valid_mask.sum()\n",
    "                if (n_valid == 0) or (n_valid < 2 and self.n_sample == 0):\n",
    "                    finished = True\n",
    "                    break\n",
    "                mask = finished_mask & valid_mask\n",
    "                sessions = session_idx_arr[iters[mask]]\n",
    "                start[mask] = offset_sessions[sessions]\n",
    "                end[mask] = offset_sessions[sessions + 1]\n",
    "                iters = iters[valid_mask]\n",
    "                start = start[valid_mask]\n",
    "                end = end[valid_mask]\n",
    "                if n_valid < len(valid_mask):\n",
    "                    for i in range(len(self.H)):\n",
    "                        tmp = self.H[i].get_value(borrow=True)\n",
    "                        tmp = tmp[valid_mask]\n",
    "                        self.H[i].set_value(tmp, borrow=True)\n",
    "            c = np.array(c)\n",
    "            cc = np.array(cc)\n",
    "            avgc = np.sum(c * cc) / np.sum(cc)\n",
    "            if np.isnan(avgc):\n",
    "                print('Epoch {}: NaN error!'.format(str(epoch)))\n",
    "                self.error_during_train = True\n",
    "                return\n",
    "            print('Epoch{}\\tloss: {:.6f}'.format(epoch, avgc))\n",
    "\n",
    "    def predict_next_batch(self, session_ids, input_item_ids, predict_for_item_ids=None, batch=100):\n",
    "        '''\n",
    "        Gives predicton scores for a selected set of items. Can be used in batch mode to predict for multiple independent events (i.e. events of different sessions) at once and thus speed up evaluation.\n",
    "\n",
    "        If the session ID at a given coordinate of the session_ids parameter remains the same during subsequent calls of the function, the corresponding hidden state of the network will be kept intact (i.e. that's how one can predict an item to a session).\n",
    "        If it changes, the hidden state of the network is reset to zeros.\n",
    "\n",
    "        Parameters\n",
    "        --------\n",
    "        session_ids : 1D array\n",
    "            Contains the session IDs of the events of the batch. Its length must equal to the prediction batch size (batch param).\n",
    "        input_item_ids : 1D array\n",
    "            Contains the item IDs of the events of the batch. Every item ID must be must be in the training data of the network. Its length must equal to the prediction batch size (batch param).\n",
    "        predict_for_item_ids : 1D array (optional)\n",
    "            IDs of items for which the network should give prediction scores. Every ID must be in the training set. The default value is None, which means that the network gives prediction on its every output (i.e. for all items in the training set).\n",
    "        batch : int\n",
    "            Prediction batch size.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        out : pandas.DataFrame\n",
    "            Prediction scores for selected items for every event of the batch.\n",
    "            Columns: events of the batch; rows: items. Rows are indexed by the item IDs.\n",
    "\n",
    "        '''\n",
    "        if self.error_during_train: raise Exception\n",
    "        if self.predict is None or self.predict_batch != batch:\n",
    "            self.predict_batch = batch\n",
    "            X = T.ivector()\n",
    "            Y = T.ivector()\n",
    "            M = T.iscalar() if self.constrained_embedding or (predict_for_item_ids is not None) else None\n",
    "            for i in range(len(self.layers)):\n",
    "                self.H[i].set_value(np.zeros((batch, self.layers[i]), dtype=theano.config.floatX), borrow=True)\n",
    "            if predict_for_item_ids is not None:\n",
    "                H_new, yhat, _, _, _ = self.model(X, self.H, M, Y=Y, predict=True)\n",
    "            else:\n",
    "                H_new, yhat, _, _, _ = self.model(X, self.H, M, predict=True)\n",
    "            updatesH = OrderedDict()\n",
    "            for i in range(len(self.H)):\n",
    "                updatesH[self.H[i]] = H_new[i]\n",
    "            if predict_for_item_ids is not None:\n",
    "                if self.constrained_embedding:\n",
    "                    self.predict = function(inputs=[X, Y, M], outputs=yhat, updates=updatesH, allow_input_downcast=True)\n",
    "                else:\n",
    "                    self.predict = function(inputs=[X, Y], outputs=yhat, updates=updatesH, allow_input_downcast=True)\n",
    "            else:\n",
    "                if self.constrained_embedding:\n",
    "                    self.predict = function(inputs=[X, M], outputs=yhat, updates=updatesH, allow_input_downcast=True)\n",
    "                else:\n",
    "                    self.predict = function(inputs=[X], outputs=yhat, updates=updatesH, allow_input_downcast=True)\n",
    "            self.current_session = np.ones(batch) * -1\n",
    "        session_change = np.arange(batch)[session_ids != self.current_session]\n",
    "        if len(session_change) > 0:\n",
    "            for i in range(len(self.H)):\n",
    "                tmp = self.H[i].get_value(borrow=True)\n",
    "                tmp[session_change] = 0\n",
    "                self.H[i].set_value(tmp, borrow=True)\n",
    "            self.current_session = session_ids.copy()\n",
    "        in_idxs = self.itemidmap[input_item_ids]\n",
    "        if np.any(np.isnan(in_idxs)):\n",
    "            preds = np.random.randn(len(self.itemidmap), len(in_idxs))\n",
    "            return pd.DataFrame(data=preds, index=self.itemidmap.index)\n",
    "\n",
    "        if predict_for_item_ids is not None:\n",
    "            iIdxs = self.itemidmap[predict_for_item_ids]\n",
    "            if self.constrained_embedding:\n",
    "                preds = np.asarray(self.predict(in_idxs, iIdxs, batch)).T\n",
    "            else:\n",
    "                preds = np.asarray(self.predict(in_idxs, iIdxs)).T\n",
    "            return pd.DataFrame(data=preds, index=predict_for_item_ids)\n",
    "        else:\n",
    "            if self.constrained_embedding:\n",
    "                preds = np.asarray(self.predict(in_idxs, batch)).T\n",
    "            else:\n",
    "                preds = np.asarray(self.predict(in_idxs)).T\n",
    "            return pd.DataFrame(data=preds, index=self.itemidmap.index)\n",
    "\n",
    "    def symbolic_predict(self, X, Y, M, items, batch_size):\n",
    "        if not self.constrained_embedding: M = None\n",
    "        H = []\n",
    "        for i in range(len(self.layers)):\n",
    "            H.append(theano.shared(np.zeros((batch_size, self.layers[i]), dtype=theano.config.floatX)))\n",
    "        if items is not None:\n",
    "            H_new, yhat, _, _, _ = self.model(X, H, M, Y=Y, predict=True)\n",
    "        else:\n",
    "            H_new, yhat, _, _, _ = self.model(X, H, M, predict=True)\n",
    "        updatesH = OrderedDict()\n",
    "        for i in range(len(H)):\n",
    "            updatesH[H[i]] = H_new[i]\n",
    "        return yhat, H, updatesH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXW0TsCGZ-hC"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s: %(name)s: %(levelname)s: %(message)s\")\n",
    "\n",
    "srng = MRG_RandomStreams()\n",
    "\n",
    "\n",
    "def inspect(tvar):\n",
    "    return tvar.get_value(borrow=True)\n",
    "\n",
    "\n",
    "def print_norm(tvar, name='var'):\n",
    "    logger.info('{}: {:.4f}'.format(name, np.linalg.norm(inspect(tvar))))\n",
    "\n",
    "\n",
    "class Sampler:\n",
    "    def __init__(self, data, n_sample, rng=None, item_key='item_id', sample_alpha=0.75, sample_store=10000000):\n",
    "        self.sample_alpha = sample_alpha\n",
    "        self.sample_store = sample_store\n",
    "        self.n_sample = n_sample\n",
    "        if rng is None:\n",
    "            self.rng = np.random.RandomState(1234)\n",
    "        else:\n",
    "            self.rng = rng\n",
    "\n",
    "        self.pop = data[item_key].value_counts() ** sample_alpha\n",
    "        self.pop = self.pop.cumsum() / self.pop.sum()\n",
    "        if self.sample_store:\n",
    "            self.generate_length = self.sample_store // self.n_sample\n",
    "            if self.generate_length <= 1:\n",
    "                self.sample_store = 0\n",
    "                logger.info('No example store was used')\n",
    "            else:\n",
    "                self.neg_samples = self._generate_neg_samples(self.pop, self.generate_length)\n",
    "                self.sample_pointer = 0\n",
    "                logger.info('Created sample store with {} batches of samples'.format(self.generate_length))\n",
    "        else:\n",
    "            logger.info('No example store was used')\n",
    "\n",
    "    def next_sample(self):\n",
    "        if self.sample_store:\n",
    "            if self.sample_pointer == self.generate_length:\n",
    "                self.neg_samples = self._generate_neg_samples(self.pop, self.generate_length)\n",
    "                self.sample_pointer = 0\n",
    "            sample = self.neg_samples[self.sample_pointer]\n",
    "            self.sample_pointer += 1\n",
    "        else:\n",
    "            sample = self._generate_neg_samples(self.pop, 1)\n",
    "        return sample\n",
    "\n",
    "    def _generate_neg_samples(self, pop, length):\n",
    "        n_items = pop.shape[0]\n",
    "        if self.sample_alpha:\n",
    "            sample = np.searchsorted(pop, self.rng.rand(self.n_sample * length))\n",
    "        else:\n",
    "            sample = self.rng.choice(n_items, size=self.n_sample * length)\n",
    "        if length > 1:\n",
    "            sample = sample.reshape((length, self.n_sample))\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBOeAjX9HFZk"
   },
   "outputs": [],
   "source": [
    "class HGRU4Rec:\n",
    "    \"\"\"\n",
    "    HGRU4Rec(session_layers, user_layers, n_epochs=10, batch_size=50,\n",
    "             learning_rate=0.05, momentum=0.0,\n",
    "             adapt='adagrad', decay=0.9, grad_cap=0, sigma=0,\n",
    "             dropout_p_hidden_usr=0.0,\n",
    "             dropout_p_hidden_ses=0.0, dropout_p_init=0.0,\n",
    "             init_as_normal=False, reset_after_session=True, loss='top1', hidden_act='tanh', final_act=None,\n",
    "             train_random_order=False, lmbd=0.0,\n",
    "             session_key='SessionId', item_key='ItemId', time_key='Time', user_key='UserId', n_sample=0,\n",
    "             sample_alpha=0.75,\n",
    "             item_embedding=None, init_item_embeddings=None,\n",
    "             user_hidden_bias_mode='init', user_output_bias=False,\n",
    "             user_to_session_act='tanh', seed=42)\n",
    "    Initializes the network.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    session_layers : 1D array\n",
    "        list of the number of GRU units in the session layers\n",
    "    user_layers : 1D array\n",
    "        list of the number of GRU units in the user layers\n",
    "    n_epochs : int\n",
    "        number of training epochs (default: 10)\n",
    "    batch_size : int\n",
    "        size of the minibatch, also effect the number of negative samples through minibatch based sampling (default: 50)\n",
    "    dropout_p_hidden_usr : float\n",
    "        probability of dropout of hidden units for the user layers (default: 0.0)\n",
    "    dropout_p_hidden_ses : float\n",
    "        probability of dropout of hidden units for the session layers (default: 0.0)\n",
    "    dropout_p_init : float\n",
    "        probability of dropout of the session-level initialization (default: 0.0)\n",
    "    learning_rate : float\n",
    "        learning rate (default: 0.05)\n",
    "    momentum : float\n",
    "        if not zero, Nesterov momentum will be applied during training with the given strength (default: 0.0)\n",
    "    adapt : None, 'adagrad', 'rmsprop', 'adam', 'adadelta'\n",
    "        sets the appropriate learning rate adaptation strategy, use None for standard SGD (default: 'adagrad')\n",
    "    decay : float\n",
    "        decay parameter for RMSProp, has no effect in other modes (default: 0.9)\n",
    "    grad_cap : float\n",
    "        clip gradients that exceede this value to this value, 0 means no clipping (default: 0.0)\n",
    "    sigma : float\n",
    "        \"width\" of initialization; either the standard deviation or the min/max of the init interval (with normal and uniform initializations respectively); 0 means adaptive normalization (sigma depends on the size of the weight matrix); (default: 0)\n",
    "    init_as_normal : boolean\n",
    "        False: init from uniform distribution on [-sigma,sigma]; True: init from normal distribution N(0,sigma); (default: False)\n",
    "    reset_after_session : boolean\n",
    "        whether the hidden state is set to zero after a session finished (default: True)\n",
    "    loss : 'top1', 'bpr' or 'cross-entropy'\n",
    "        selects the loss function (default: 'top1')\n",
    "    hidden_act : 'tanh' or 'relu'\n",
    "        selects the activation function on the hidden states (default: 'tanh')\n",
    "    final_act : None, 'linear', 'relu' or 'tanh'\n",
    "        selects the activation function of the final layer where appropriate, None means default (tanh if the loss is brp or top1; softmax for cross-entropy),\n",
    "        cross-entropy is only affeted by 'tanh' where the softmax layers is preceeded by a tanh nonlinearity (default: None)\n",
    "    train_random_order : boolean\n",
    "        whether to randomize the order of sessions in each epoch (default: False)\n",
    "    lmbd : float\n",
    "        coefficient of the L2 regularization (default: 0.0)\n",
    "    session_key : string\n",
    "        header of the session ID column in the input file (default: 'SessionId')\n",
    "    item_key : string\n",
    "        header of the item ID column in the input file (default: 'ItemId')\n",
    "    time_key : string\n",
    "        header of the timestamp column in the input file (default: 'Time')\n",
    "    user_key : string\n",
    "        header of the user column in the input file (default: 'UserId')\n",
    "    n_sample : int\n",
    "        number of additional negative samples to be used (besides the other examples of the minibatch) (default: 0)\n",
    "    sample_alpha : float\n",
    "        the probability of an item used as an additional negative sample is supp^sample_alpha (default: 0.75)\n",
    "        (e.g.: sample_alpha=1 --> popularity based sampling; sample_alpha=0 --> uniform sampling)\n",
    "    item_embedding: int\n",
    "        size of the item embedding vector (default: None)\n",
    "    init_item_embeddings: 2D array or dict\n",
    "        array with the initial values of the embeddings vector of every item,\n",
    "        or dict that maps each item id to its embedding vector (default: None)\n",
    "    user_propagation_mode: string\n",
    "        'init' to use the (last) user hidden state to initialize the (first) session hidden state;\n",
    "        'all' to propagate the user hidden also in input the the (first) session layers. (default: 'init')\n",
    "    user_to_output: boolean\n",
    "        True to propagate the (last) user hidden state in input to the final output layer, False otherwise (default: False)\n",
    "    user_to_session_act: string\n",
    "        activation of the user-to-session initialization network (default: 'tanh')\n",
    "    seed: int\n",
    "        random seed (default: 42)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, session_layers, user_layers, n_epochs=10, batch_size=50, learning_rate=0.05, momentum=0.0,\n",
    "                 adapt='adagrad', decay=0.9, grad_cap=0, sigma=0, dropout_p_hidden_usr=0.0,\n",
    "                 dropout_p_hidden_ses=0.0, dropout_p_init=0.0, init_as_normal=False,\n",
    "                 reset_after_session=True, loss='top1', hidden_act='tanh', final_act=None, train_random_order=False,\n",
    "                 lmbd=0.0, session_key='SessionId', item_key='ItemId', time_key='Time', user_key='UserId', n_sample=0,\n",
    "                 sample_alpha=0.75, item_embedding=None, init_item_embeddings=None, user_propagation_mode='init',\n",
    "                 user_to_output=False, user_to_session_act='tanh', seed=42):\n",
    "        self.session_layers = session_layers\n",
    "        self.user_layers = user_layers\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout_p_hidden_usr = dropout_p_hidden_usr\n",
    "        self.dropout_p_hidden_ses = dropout_p_hidden_ses\n",
    "        self.dropout_p_init = dropout_p_init\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.momentum = momentum\n",
    "        self.sigma = sigma\n",
    "        self.init_as_normal = init_as_normal\n",
    "        self.reset_after_session = reset_after_session\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.user_key = user_key\n",
    "        self.grad_cap = grad_cap\n",
    "        self.train_random_order = train_random_order\n",
    "        self.lmbd = lmbd\n",
    "\n",
    "        self.user_propagation_mode = user_propagation_mode\n",
    "        self.user_to_output = user_to_output\n",
    "\n",
    "        self.item_embedding = item_embedding\n",
    "        self.init_item_embeddings = init_item_embeddings\n",
    "\n",
    "        self.rng = np.random.RandomState(seed=seed)\n",
    "\n",
    "        if adapt == 'rmsprop':\n",
    "            self.adapt = 'rmsprop'\n",
    "        elif adapt == 'adagrad':\n",
    "            self.adapt = 'adagrad'\n",
    "        elif adapt == 'adadelta':\n",
    "            self.adapt = 'adadelta'\n",
    "        elif adapt == 'adam':\n",
    "            self.adapt = 'adam'\n",
    "        else:\n",
    "            self.adapt = False\n",
    "        if loss == 'cross-entropy':\n",
    "            if final_act == 'tanh':\n",
    "                self.final_activation = self.softmaxth\n",
    "            else:\n",
    "                self.final_activation = self.softmax\n",
    "            self.loss_function = self.cross_entropy\n",
    "        elif loss == 'bpr':\n",
    "            if final_act == 'linear':\n",
    "                self.final_activation = self.linear\n",
    "            elif final_act == 'relu':\n",
    "                self.final_activation = self.relu\n",
    "            else:\n",
    "                self.final_activation = self.tanh\n",
    "            self.loss_function = self.bpr\n",
    "        elif loss == 'top1':\n",
    "            if final_act == 'linear':\n",
    "                self.final_activation = self.linear\n",
    "            elif final_act == 'relu':\n",
    "                self.final_activation = self.relu\n",
    "            else:\n",
    "                self.final_activation = self.tanh\n",
    "            self.loss_function = self.top1\n",
    "        else:\n",
    "            raise NotImplementedError('loss {} not implemented'.format(loss))\n",
    "        if hidden_act == 'relu':\n",
    "            self.hidden_activation = self.relu\n",
    "        elif hidden_act == 'tanh':\n",
    "            self.hidden_activation = self.tanh\n",
    "        else:\n",
    "            raise NotImplementedError('hidden activation {} not implemented'.format(hidden_act))\n",
    "        if user_to_session_act == 'relu':\n",
    "            self.s_init_act = self.relu\n",
    "        elif user_to_session_act == 'tanh':\n",
    "            self.s_init_act = self.tanh\n",
    "        else:\n",
    "            raise NotImplementedError('user-to-session activation {} not implemented'.format(hidden_act))\n",
    "\n",
    "        self.n_sample = n_sample\n",
    "        self.sample_alpha = sample_alpha\n",
    "\n",
    "    ######################ACTIVATION FUNCTIONS#####################\n",
    "    def linear(self, X):\n",
    "        return X\n",
    "\n",
    "    def tanh(self, X):\n",
    "        return T.tanh(X)\n",
    "\n",
    "    def softmax(self, X):\n",
    "        e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "        return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "    def softmaxth(self, X):\n",
    "        X = self.tanh(X)\n",
    "        e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "        return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "    def relu(self, X):\n",
    "        return T.maximum(X, 0)\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        return T.nnet.sigmoid(X)\n",
    "\n",
    "    #################################LOSS FUNCTIONS################################\n",
    "    def cross_entropy(self, yhat):\n",
    "        return T.cast(T.mean(-T.log(T.diag(yhat) + 1e-24)), theano.config.floatX)\n",
    "\n",
    "    def bpr(self, yhat):\n",
    "        return T.cast(T.mean(-T.log(T.nnet.sigmoid(T.diag(yhat) - yhat.T))), theano.config.floatX)\n",
    "\n",
    "    def top1(self, yhat):\n",
    "        yhatT = yhat.T\n",
    "        return T.cast(T.mean(\n",
    "            T.mean(T.nnet.sigmoid(-T.diag(yhat) + yhatT) + T.nnet.sigmoid(yhatT ** 2), axis=0) - T.nnet.sigmoid(\n",
    "                T.diag(yhat) ** 2) / self.batch_size), theano.config.floatX)\n",
    "\n",
    "    ###############################################################################\n",
    "    def floatX(self, X):\n",
    "        return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "    def init_weights(self, shape):\n",
    "        sigma = self.sigma if self.sigma != 0 else np.sqrt(6.0 / (shape[0] + shape[1]))\n",
    "        if self.init_as_normal:\n",
    "            return theano.shared(self.floatX(self.rng.randn(*shape) * sigma), borrow=True)\n",
    "        else:\n",
    "            return theano.shared(self.floatX(self.rng.rand(*shape) * sigma * 2 - sigma), borrow=True)\n",
    "\n",
    "    def init_matrix(self, shape):\n",
    "        sigma = self.sigma if self.sigma != 0 else np.sqrt(6.0 / (shape[0] + shape[1]))\n",
    "        if self.init_as_normal:\n",
    "            return self.floatX(self.rng.randn(*shape) * sigma)\n",
    "        else:\n",
    "            return self.floatX(self.rng.rand(*shape) * sigma * 2 - sigma)\n",
    "\n",
    "    def extend_weights(self, W, n_new):\n",
    "        matrix = W.get_value()\n",
    "        sigma = self.sigma if self.sigma != 0 else np.sqrt(6.0 / (matrix.shape[0] + matrix.shape[1] + n_new))\n",
    "        if self.init_as_normal:\n",
    "            new_rows = self.floatX(self.rng.randn(n_new, matrix.shape[1]) * sigma)\n",
    "        else:\n",
    "            new_rows = self.floatX(self.rng.rand(n_new, matrix.shape[1]) * sigma * 2 - sigma)\n",
    "        W.set_value(np.vstack([matrix, new_rows]))\n",
    "\n",
    "    def set_item_embeddings(self, E, values):\n",
    "        if isinstance(values, dict):\n",
    "            keys, values = values.keys(), np.vstack(list(values.values()))\n",
    "        elif isinstance(values, np.ndarray):\n",
    "            # use item ids ranging from 0 to the number of rows in values\n",
    "            keys, values = np.arange(values.shape[0]), values\n",
    "        else:\n",
    "            raise NotImplementedError('Unsupported type')\n",
    "        # map item ids to the internal indices\n",
    "        mask = np.in1d(keys, self.itemidmap.index, assume_unique=True)\n",
    "        idx = self.itemidmap[keys].dropna().values.astype(np.int)\n",
    "        emb = E.get_value()\n",
    "        emb[idx] = values[mask]\n",
    "        E.set_value(emb)\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        # sort by user and time key in order\n",
    "        data.sort_values([self.user_key, self.session_key, self.time_key], inplace=True)\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "        offset_session = np.r_[0, data.groupby([self.user_key, self.session_key], sort=False).size().cumsum()[:-1]]\n",
    "        user_indptr = np.r_[0, data.groupby(self.user_key, sort=False)[self.session_key].nunique().cumsum()[:-1]]\n",
    "        return user_indptr, offset_session\n",
    "\n",
    "    def save_state(self):\n",
    "        state = OrderedDict()\n",
    "        for i in range(len(self.session_layers)):\n",
    "            state['Ws_in_' + str(i)] = self.Ws_in[i].get_value()\n",
    "            state['Ws_hh_' + str(i)] = self.Ws_hh[i].get_value()\n",
    "            state['Ws_rz_' + str(i)] = self.Ws_rz[i].get_value()\n",
    "            state['Bs_h_' + str(i)] = self.Bs_h[i].get_value()\n",
    "            state['Hs_' + str(i)] = self.Hs[i].get_value()\n",
    "        state['Wsy'] = self.Wsy.get_value()\n",
    "        state['By'] = self.By.get_value()\n",
    "        for i in range(len(self.user_layers)):\n",
    "            state['Wu_in_' + str(i)] = self.Wu_in[i].get_value()\n",
    "            state['Wu_hh_' + str(i)] = self.Wu_hh[i].get_value()\n",
    "            state['Wu_rz_' + str(i)] = self.Wu_rz[i].get_value()\n",
    "            state['Bu_h_' + str(i)] = self.Bu_h[i].get_value()\n",
    "            state['Hu_' + str(i)] = self.Hu[i].get_value()\n",
    "        if self.user_to_output:\n",
    "            state['Wuy'] = self.Wuy.get_value()\n",
    "        state['Wu_to_s_init'] = self.Ws_init[0].get_value()\n",
    "        state['Bu_to_s_init'] = self.Bs_init[0].get_value()\n",
    "        if self.user_propagation_mode == 'all':\n",
    "            state['Wu_to_s'] = self.Wu_to_s[0].get_value()\n",
    "        return state\n",
    "\n",
    "    def load_state(self, state):\n",
    "        for i in range(len(self.session_layers)):\n",
    "            self.Ws_in[i].set_value(state['Ws_in_' + str(i)], borrow=True)\n",
    "            self.Ws_hh[i].set_value(state['Ws_hh_' + str(i)], borrow=True)\n",
    "            self.Ws_rz[i].set_value(state['Ws_rz_' + str(i)], borrow=True)\n",
    "            self.Bs_h[i].set_value(state['Bs_h_' + str(i)], borrow=True)\n",
    "            self.Hs[i].set_value(state['Hs_' + str(i)], borrow=True)\n",
    "        self.Wsy.set_value(state['Wsy'], borrow=True)\n",
    "        self.By.set_value(state['By'], borrow=True)\n",
    "        for i in range(len(self.user_layers)):\n",
    "            self.Wu_in[i].set_value(state['Wu_in_' + str(i)], borrow=True)\n",
    "            self.Wu_hh[i].set_value(state['Wu_hh_' + str(i)], borrow=True)\n",
    "            self.Wu_rz[i].set_value(state['Wu_rz_' + str(i)], borrow=True)\n",
    "            self.Bu_h[i].set_value(state['Bu_h_' + str(i)], borrow=True)\n",
    "            self.Hu[i].set_value(state['Hu_' + str(i)], borrow=True)\n",
    "        if self.user_to_output:\n",
    "            self.Wuy.set_value(state['Wuy'], borrow=True)\n",
    "        self.Ws_init[0].set_value(state['Wu_to_s_init'], borrow=True)\n",
    "        self.Bs_init[0].set_value(state['Bu_to_s_init'], borrow=True)\n",
    "        if self.user_propagation_mode == 'all':\n",
    "            self.Wu_to_s[0].set_value(state['Wu_to_s'], borrow=True)\n",
    "\n",
    "    def print_state(self):\n",
    "        for i in range(len(self.session_layers)):\n",
    "            print_norm(self.Ws_in[i], 'Ws_in_' + str(i))\n",
    "            print_norm(self.Ws_hh[i], 'Ws_hh_' + str(i))\n",
    "            print_norm(self.Ws_rz[i], 'Ws_rz_' + str(i))\n",
    "            print_norm(self.Bs_h[i], 'Bs_h_' + str(i))\n",
    "            print_norm(self.Hs[i], 'Hs_' + str(i))\n",
    "        print_norm(self.Wsy, 'Wsy')\n",
    "        print_norm(self.By, 'By')\n",
    "        for i in range(len(self.user_layers)):\n",
    "            print_norm(self.Wu_in[i], 'Wu_in_' + str(i))\n",
    "            print_norm(self.Wu_hh[i], 'Wu_hh_' + str(i))\n",
    "            print_norm(self.Wu_rz[i], 'Wu_rz_' + str(i))\n",
    "            print_norm(self.Bu_h[i], 'Bu_h_' + str(i))\n",
    "            print_norm(self.Hu[i], 'Hu_' + str(i))\n",
    "        if self.user_to_output:\n",
    "            print_norm(self.Wuy, 'Wuy')\n",
    "        print_norm(self.Ws_init[0], 'Wu_to_s_init')\n",
    "        print_norm(self.Bs_init[0], 'Bu_to_s_init')\n",
    "        if self.user_propagation_mode == 'all':\n",
    "            print_norm(self.Wu_to_s[0], 'Wu_to_s')\n",
    "\n",
    "    def init(self):\n",
    "        rnn_input_size = self.n_items\n",
    "        if self.item_embedding is not None:\n",
    "            self.E_item = self.init_weights((self.n_items, self.item_embedding))\n",
    "            if self.init_item_embeddings is not None:\n",
    "                self.set_item_embeddings(self.E_item, self.init_item_embeddings)\n",
    "            rnn_input_size = self.item_embedding\n",
    "\n",
    "        # Initialize the session parameters\n",
    "        self.Ws_in, self.Ws_hh, self.Ws_rz, self.Bs_h, self.Hs = [], [], [], [], []\n",
    "        for i in range(len(self.session_layers)):\n",
    "            m = []\n",
    "            m.append(\n",
    "                self.init_matrix((self.session_layers[i - 1] if i > 0 else rnn_input_size, self.session_layers[i])))\n",
    "            m.append(\n",
    "                self.init_matrix((self.session_layers[i - 1] if i > 0 else rnn_input_size, self.session_layers[i])))\n",
    "            m.append(\n",
    "                self.init_matrix((self.session_layers[i - 1] if i > 0 else rnn_input_size, self.session_layers[i])))\n",
    "            self.Ws_in.append(theano.shared(value=np.hstack(m), borrow=True))\n",
    "            self.Ws_hh.append(self.init_weights((self.session_layers[i], self.session_layers[i])))\n",
    "            m2 = []\n",
    "            m2.append(self.init_matrix((self.session_layers[i], self.session_layers[i])))\n",
    "            m2.append(self.init_matrix((self.session_layers[i], self.session_layers[i])))\n",
    "            self.Ws_rz.append(theano.shared(value=np.hstack(m2), borrow=True))\n",
    "            self.Bs_h.append(\n",
    "                theano.shared(value=np.zeros((self.session_layers[i] * 3,), dtype=theano.config.floatX), borrow=True))\n",
    "            self.Hs.append(\n",
    "                theano.shared(value=np.zeros((self.batch_size, self.session_layers[i]), dtype=theano.config.floatX),\n",
    "                              borrow=True))\n",
    "        # Session to output weights\n",
    "        self.Wsy = self.init_weights((self.n_items, self.session_layers[-1]))\n",
    "        # Global output bias\n",
    "        self.By = theano.shared(value=np.zeros((self.n_items, 1), dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "        # Initialize the user parameters\n",
    "        self.Wu_in, self.Wu_hh, self.Wu_rz, self.Bu_h, self.Hu = [], [], [], [], []\n",
    "        for i in range(len(self.user_layers)):\n",
    "            m = []\n",
    "            m.append(self.init_matrix(\n",
    "                (self.user_layers[i - 1] if i > 0 else self.session_layers[-1], self.user_layers[i])))\n",
    "            m.append(self.init_matrix(\n",
    "                (self.user_layers[i - 1] if i > 0 else self.session_layers[-1], self.user_layers[i])))\n",
    "            m.append(self.init_matrix(\n",
    "                (self.user_layers[i - 1] if i > 0 else self.session_layers[-1], self.user_layers[i])))\n",
    "            self.Wu_in.append(theano.shared(value=np.hstack(m), borrow=True))\n",
    "            self.Wu_hh.append(self.init_weights((self.user_layers[i], self.user_layers[i])))\n",
    "            m2 = []\n",
    "            m2.append(self.init_matrix((self.user_layers[i], self.user_layers[i])))\n",
    "            m2.append(self.init_matrix((self.user_layers[i], self.user_layers[i])))\n",
    "            self.Wu_rz.append(theano.shared(value=np.hstack(m2), borrow=True))\n",
    "            self.Bu_h.append(\n",
    "                theano.shared(value=np.zeros((self.user_layers[i] * 3,), dtype=theano.config.floatX), borrow=True))\n",
    "            self.Hu.append(\n",
    "                theano.shared(value=np.zeros((self.batch_size, self.user_layers[i]), dtype=theano.config.floatX),\n",
    "                              borrow=True))\n",
    "        if self.user_to_output:\n",
    "            # User to output weights\n",
    "            self.Wuy = self.init_weights((self.n_items, self.user_layers[-1]))\n",
    "\n",
    "        # User-to-Session parameters\n",
    "        self.Ws_init, self.Bs_init = [], []\n",
    "        self.Ws_init.append(self.init_weights((self.user_layers[-1], self.session_layers[0])))\n",
    "        self.Bs_init.append(\n",
    "            theano.shared(value=np.zeros((self.session_layers[0],), dtype=theano.config.floatX), borrow=True))\n",
    "        if self.user_propagation_mode == 'all':\n",
    "            m = []\n",
    "            m.append(self.init_matrix((self.user_layers[-1], self.session_layers[0])))\n",
    "            m.append(self.init_matrix((self.user_layers[-1], self.session_layers[0])))\n",
    "            m.append(self.init_matrix((self.user_layers[-1], self.session_layers[0])))\n",
    "            self.Wu_to_s = [theano.shared(value=np.hstack(m), borrow=True)]\n",
    "\n",
    "    def dropout(self, X, drop_p):\n",
    "        if drop_p > 0:\n",
    "            retain_prob = 1 - drop_p\n",
    "            X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX) / retain_prob\n",
    "        return X\n",
    "\n",
    "    def adam(self, param, grad, updates, sample_idx=None, epsilon=1e-6):\n",
    "        v1 = np.float32(self.decay)\n",
    "        v2 = np.float32(1.0 - self.decay)\n",
    "        acc = theano.shared(param.get_value(borrow=False) * 0., borrow=True)\n",
    "        meang = theano.shared(param.get_value(borrow=False) * 0., borrow=True)\n",
    "        countt = theano.shared(param.get_value(borrow=False) * 0., borrow=True)\n",
    "        if sample_idx is None:\n",
    "            acc_new = v1 * acc + v2 * grad ** 2\n",
    "            meang_new = v1 * meang + v2 * grad\n",
    "            countt_new = countt + 1\n",
    "            updates[acc] = acc_new\n",
    "            updates[meang] = meang_new\n",
    "            updates[countt] = countt_new\n",
    "        else:\n",
    "            acc_s = acc[sample_idx]\n",
    "            meang_s = meang[sample_idx]\n",
    "            countt_s = countt[sample_idx]\n",
    "            acc_new = v1 * acc_s + v2 * grad ** 2\n",
    "            meang_new = v1 * meang_s + v2 * grad\n",
    "            countt_new = countt_s + 1.0\n",
    "            updates[acc] = T.set_subtensor(acc_s, acc_new)\n",
    "            updates[meang] = T.set_subtensor(meang_s, meang_new)\n",
    "            updates[countt] = T.set_subtensor(countt_s, countt_new)\n",
    "        return (meang_new / (1 - v1 ** countt_new)) / (T.sqrt(acc_new / (1 - v1 ** countt_new)) + epsilon)\n",
    "\n",
    "    def adagrad(self, param, grad, updates, sample_idx=None, epsilon=1e-6):\n",
    "        acc = theano.shared(param.get_value(borrow=False) * 0., borrow=True)\n",
    "        if sample_idx is None:\n",
    "            acc_new = acc + grad ** 2\n",
    "            updates[acc] = acc_new\n",
    "        else:\n",
    "            acc_s = acc[sample_idx]\n",
    "            acc_new = acc_s + grad ** 2\n",
    "            updates[acc] = T.set_subtensor(acc_s, acc_new)\n",
    "        gradient_scaling = T.cast(T.sqrt(acc_new + epsilon), theano.config.floatX)\n",
    "        return grad / gradient_scaling\n",
    "\n",
    "    def adadelta(self, param, grad, updates, sample_idx=None, epsilon=1e-6):\n",
    "        v1 = np.float32(self.decay)\n",
    "        v2 = np.float32(1.0 - self.decay)\n",
    "        acc = theano.shared(param.get_value(borrow=False) * 0., borrow=True)\n",
    "        upd = theano.shared(param.get_value(borrow=False) * 0., borrow=True)\n",
    "        if sample_idx is None:\n",
    "            acc_new = acc + grad ** 2\n",
    "            updates[acc] = acc_new\n",
    "            grad = T.sqrt(upd + epsilon) * grad\n",
    "            upd_new = v1 * upd + v2 * grad ** 2\n",
    "            updates[upd] = upd_new\n",
    "        else:\n",
    "            acc_s = acc[sample_idx]\n",
    "            acc_new = acc_s + grad ** 2\n",
    "            updates[acc] = T.set_subtensor(acc_s, acc_new)\n",
    "            upd_s = upd[sample_idx]\n",
    "            upd_new = v1 * upd_s + v2 * grad ** 2\n",
    "            updates[upd] = T.set_subtensor(upd_s, upd_new)\n",
    "            grad = T.sqrt(upd_s + epsilon) * grad\n",
    "        gradient_scaling = T.cast(T.sqrt(acc_new + epsilon), theano.config.floatX)\n",
    "        return grad / gradient_scaling\n",
    "\n",
    "    def rmsprop(self, param, grad, updates, sample_idx=None, epsilon=1e-6):\n",
    "        v1 = np.float32(self.decay)\n",
    "        v2 = np.float32(1.0 - self.decay)\n",
    "        acc = theano.shared(param.get_value(borrow=False) * 0., borrow=True)\n",
    "        if sample_idx is None:\n",
    "            acc_new = v1 * acc + v2 * grad ** 2\n",
    "            updates[acc] = acc_new\n",
    "        else:\n",
    "            acc_s = acc[sample_idx]\n",
    "            acc_new = v1 * acc_s + v2 * grad ** 2\n",
    "            updates[acc] = T.set_subtensor(acc_s, acc_new)\n",
    "        gradient_scaling = T.cast(T.sqrt(acc_new + epsilon), theano.config.floatX)\n",
    "        return grad / gradient_scaling\n",
    "\n",
    "    def RMSprop(self, cost, params, full_params, sampled_params, sidxs, epsilon=1e-6):\n",
    "        grads = [T.grad(cost=cost, wrt=param) for param in params]\n",
    "        sgrads = [T.grad(cost=cost, wrt=sparam) for sparam in sampled_params]\n",
    "        updates = OrderedDict()\n",
    "        if self.grad_cap > 0:\n",
    "            norm = T.cast(T.sqrt(T.sum([T.sum([T.sum(g ** 2) for g in g_list]) for g_list in grads]) + T.sum(\n",
    "                [T.sum(g ** 2) for g in sgrads])), theano.config.floatX)\n",
    "            grads = [[T.switch(T.ge(norm, self.grad_cap), g * self.grad_cap / norm, g) for g in g_list] for g_list in\n",
    "                     grads]\n",
    "            sgrads = [T.switch(T.ge(norm, self.grad_cap), g * self.grad_cap / norm, g) for g in sgrads]\n",
    "        for p_list, g_list in zip(params, grads):\n",
    "            for p, g in zip(p_list, g_list):\n",
    "                if self.adapt:\n",
    "                    if self.adapt == 'adagrad':\n",
    "                        g = self.adagrad(p, g, updates)\n",
    "                    if self.adapt == 'rmsprop':\n",
    "                        g = self.rmsprop(p, g, updates)\n",
    "                    if self.adapt == 'adadelta':\n",
    "                        g = self.adadelta(p, g, updates)\n",
    "                    if self.adapt == 'adam':\n",
    "                        g = self.adam(p, g, updates)\n",
    "                if self.momentum > 0:\n",
    "                    velocity = theano.shared(p.get_value(borrow=False) * 0., borrow=True)\n",
    "                    velocity2 = self.momentum * velocity - np.float32(self.learning_rate) * (g + self.lmbd * p)\n",
    "                    updates[velocity] = velocity2\n",
    "                    updates[p] = p + velocity2\n",
    "                else:\n",
    "                    updates[p] = p * np.float32(1.0 - self.learning_rate * self.lmbd) - np.float32(\n",
    "                        self.learning_rate) * g\n",
    "        for i in range(len(sgrads)):\n",
    "            g = sgrads[i]\n",
    "            fullP = full_params[i]\n",
    "            sample_idx = sidxs[i]\n",
    "            sparam = sampled_params[i]\n",
    "            if self.adapt:\n",
    "                if self.adapt == 'adagrad':\n",
    "                    g = self.adagrad(fullP, g, updates, sample_idx)\n",
    "                if self.adapt == 'rmsprop':\n",
    "                    g = self.rmsprop(fullP, g, updates, sample_idx)\n",
    "                if self.adapt == 'adadelta':\n",
    "                    g = self.adadelta(fullP, g, updates, sample_idx)\n",
    "                if self.adapt == 'adam':\n",
    "                    g = self.adam(fullP, g, updates, sample_idx)\n",
    "            if self.lmbd > 0:\n",
    "                delta = np.float32(self.learning_rate) * (g + self.lmbd * sparam)\n",
    "            else:\n",
    "                delta = np.float32(self.learning_rate) * g\n",
    "            if self.momentum > 0:\n",
    "                velocity = theano.shared(fullP.get_value(borrow=False) * 0., borrow=True)\n",
    "                vs = velocity[sample_idx]\n",
    "                velocity2 = self.momentum * vs - delta\n",
    "                updates[velocity] = T.set_subtensor(vs, velocity2)\n",
    "                updates[fullP] = T.inc_subtensor(sparam, velocity2)\n",
    "            else:\n",
    "                updates[fullP] = T.inc_subtensor(sparam, - delta)\n",
    "        return updates\n",
    "\n",
    "    def model(self, X, Sstart, Ustart, Hs, Hu, Y=None,\n",
    "              drop_p_hidden_usr=0.0,\n",
    "              drop_p_hidden_ses=0.0,\n",
    "              drop_p_init=0.0):\n",
    "        #\n",
    "        # USER GRU\n",
    "        #\n",
    "        # update the User GRU with the last hidden state of the Session GRU\n",
    "        # NOTE: the User GRU gets actually updated only when a new session starts\n",
    "        user_in = T.dot(Hs[-1], self.Wu_in[0]) + self.Bu_h[0]\n",
    "        user_in = user_in.T\n",
    "        # ^ 3 * user_layers[0] x batch_size\n",
    "\n",
    "        rz_u = T.nnet.sigmoid(user_in[self.user_layers[0]:]\n",
    "                              + T.dot(Hu[0], self.Wu_rz[0]).T)\n",
    "        # ^ 2 * user_layers[0] x batch_size\n",
    "\n",
    "        h_u = self.hidden_activation(T.dot(Hu[0] * rz_u[:self.user_layers[0]].T, self.Wu_hh[0]).T\n",
    "                                     + user_in[:self.user_layers[0]])\n",
    "        # ^ user_layers[0] x batch_size\n",
    "\n",
    "        z = rz_u[self.user_layers[0]:].T\n",
    "        # batch_size x user_layers[0]\n",
    "        h_u = (1.0 - z) * Hu[0] + z * h_u.T\n",
    "        h_u = self.dropout(h_u, drop_p_hidden_usr)\n",
    "        # ^ batch_size x user_layers[0]\n",
    "\n",
    "        # update the User GRU only when a new session starts\n",
    "        # Hu contains the state of the previous session\n",
    "        h_u = Hu[0] * (1 - Sstart[:, None]) + h_u * Sstart[:, None]\n",
    "        # ^ batch_size x user_layers[0]\n",
    "\n",
    "        # reset the user network state for new users\n",
    "        h_u = T.zeros_like(h_u) * Ustart[:, None] + h_u * (1 - Ustart[:, None])\n",
    "\n",
    "        Hu_new = [h_u]\n",
    "        for i in range(1, len(self.user_layers)):\n",
    "            user_in = T.dot(h_u, self.Wu_in[i]) + self.Bu_h[i]\n",
    "            user_in = user_in.T\n",
    "            rz_u = T.nnet.sigmoid(user_in[self.user_layers[i]:]\n",
    "                                  + T.dot(Hu[i], self.Wu_rz[i]).T)\n",
    "\n",
    "            h_u = self.hidden_activation(T.dot(Hu[i] * rz_u[:self.user_layers[i]].T, self.Wu_hh[i]).T\n",
    "                                         + user_in[:self.user_layers[i]])\n",
    "\n",
    "            z = rz_u[self.user_layers[i]:].T\n",
    "            h_u = (1.0 - z) * Hu[i] + z * h_u.T\n",
    "            h_u = self.dropout(h_u, drop_p_hidden_usr)\n",
    "            h_u = Hu[i] * (1 - Sstart[:, None]) + h_u * Sstart[:, None]\n",
    "            h_u = T.zeros_like(h_u) * Ustart[:, None] + h_u * (1 - Ustart[:, None])\n",
    "            Hu_new.append(h_u)\n",
    "\n",
    "        #\n",
    "        # SESSION GRU\n",
    "        #\n",
    "        # Process the input items\n",
    "        if self.item_embedding is not None:\n",
    "            # get the item embedding\n",
    "            SE_item = self.E_item[X]  # sampled item embedding\n",
    "            vec = T.dot(SE_item, self.Ws_in[0]) + self.Bs_h[0]\n",
    "            Sin = SE_item\n",
    "        else:\n",
    "            Sx = self.Ws_in[0][X]\n",
    "            vec = Sx + self.Bs_h[0]\n",
    "            Sin = Sx\n",
    "        session_in = vec.T\n",
    "        # ^ session_layers[0] x batch_size\n",
    "\n",
    "        # initialize the h_s with h_c only for starting sessions\n",
    "        h_s_init = self.dropout(self.s_init_act(T.dot(h_u, self.Ws_init[0]) + self.Bs_init), drop_p_init)\n",
    "        h_s = Hs[0] * (1 - Sstart[:, None]) + h_s_init * Sstart[:, None]\n",
    "        # reset h_s for starting users\n",
    "        h_s = h_s * (1 - Ustart[:, None]) + T.zeros_like(h_s) * Ustart[:, None]\n",
    "        self.h_s_init = h_s\n",
    "\n",
    "        if self.user_propagation_mode == 'all':\n",
    "            # this propagates the bias throughout all the session\n",
    "            user_bias = T.dot(h_u, self.Wu_to_s[0]).T\n",
    "            # ^ 3*session_layers[0] x batch_size\n",
    "\n",
    "            # update the Session GRU\n",
    "            rz_s = T.nnet.sigmoid(user_bias[self.session_layers[0]:]\n",
    "                                  + session_in[self.session_layers[0]:]\n",
    "                                  + T.dot(h_s, self.Ws_rz[0]).T)\n",
    "            # ^ 2*session_layers[0] x batch_size\n",
    "\n",
    "            h_s = self.hidden_activation(T.dot(h_s * rz_s[:self.session_layers[0]].T, self.Ws_hh[0]).T\n",
    "                                         + session_in[:self.session_layers[0]])\n",
    "            # ^ session_layers[0] x batch_size\n",
    "        else:\n",
    "            rz_s = T.nnet.sigmoid(session_in[self.session_layers[0]:]\n",
    "                                  + T.dot(h_s, self.Ws_rz[0]).T)\n",
    "            h_s = self.hidden_activation(T.dot(h_s * rz_s[:self.session_layers[0]].T, self.Ws_hh[0]).T\n",
    "                                         + session_in[:self.session_layers[0]])\n",
    "\n",
    "        z = rz_s[self.session_layers[0]:].T\n",
    "        # ^ batch_size x session_layers[0]\n",
    "        h_s = (1.0 - z) * Hs[0] + z * h_s.T\n",
    "        h_s = self.dropout(h_s, drop_p_hidden_ses)\n",
    "        # ^ batch_size x session_layers[0]\n",
    "        Hs_new = [h_s]\n",
    "        for i in range(1, len(self.session_layers)):\n",
    "            session_in = T.dot(h_s, self.Ws_in[i]) + self.Bs_h[i]\n",
    "            session_in = session_in.T\n",
    "            rz_s = T.nnet.sigmoid(session_in[self.session_layers[i]:]\n",
    "                                  + T.dot(Hs[i], self.Ws_rz[i]).T)\n",
    "            h_s = self.hidden_activation(T.dot(Hs[i] * rz_s[:self.session_layers[i]].T, self.Ws_hh[i]).T\n",
    "                                         + session_in[:self.session_layers[i]])\n",
    "            z = rz_s[self.session_layers[i]:].T\n",
    "            h_s = (1.0 - z) * Hs[i] + z * h_s.T\n",
    "            h_s = self.dropout(h_s, drop_p_hidden_ses)\n",
    "            Hs_new.append(h_s)\n",
    "\n",
    "        self.h_s_new = h_s\n",
    "\n",
    "        if Y is not None:\n",
    "            Ssy = self.Wsy[Y]\n",
    "            SBy = self.By[Y]\n",
    "            preact = T.dot(h_s, Ssy.T) + SBy.flatten()\n",
    "            sampled_params = [Sin, Ssy, SBy]\n",
    "            if self.user_to_output:\n",
    "                Scy = self.Wuy[Y]\n",
    "                preact += T.dot(h_u, Scy.T)\n",
    "                sampled_params.append(Scy)\n",
    "            y = self.final_activation(preact)\n",
    "            return Hs_new, Hu_new, y, sampled_params\n",
    "        else:\n",
    "            preact = T.dot(h_s, self.Wsy.T) + self.By.flatten()\n",
    "            if self.user_to_output:\n",
    "                preact += T.dot(h_u, self.Wuy.T)\n",
    "            y = self.final_activation(preact)\n",
    "            return Hs_new, Hu_new, y, [Sin]\n",
    "\n",
    "    def fit(self, train_data, valid_data=None, retrain=False, sample_store=10000000, patience=3, margin=1.003,\n",
    "            save_to=None, load_from=None):\n",
    "        '''\n",
    "        Trains the network.\n",
    "\n",
    "        Parameters\n",
    "        --------\n",
    "        train_data : pandas.DataFrame\n",
    "            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n",
    "            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n",
    "        valid_data: pandas.DataFrame\n",
    "            Validation data. If not none, it enables early stopping.\n",
    "             Contains the transactions in the same format as in train_data, and it is used exclusively to compute the loss after each training iteration over train_data.\n",
    "        retrain : boolean\n",
    "            If False, do normal train. If True, do additional train (weights from previous trainings are kept as the initial network) (default: False)\n",
    "        sample_store : int\n",
    "            If additional negative samples are used (n_sample > 0), the efficiency of GPU utilization can be sped up, by precomputing a large batch of negative samples (and recomputing when necessary).\n",
    "            This parameter regulizes the size of this precomputed ID set. Its value is the maximum number of int values (IDs) to be stored. Precomputed IDs are stored in the RAM.\n",
    "            For the most efficient computation, a balance must be found between storing few examples and constantly interrupting GPU computations for a short time vs. computing many examples and interrupting GPU computations for a long time (but rarely).\n",
    "        patience: int\n",
    "            Patience of the early stopping procedure. Number of iterations with not decreasing validation loss before terminating the training procedure\n",
    "        margin: float\n",
    "            Margin of early stopping. Percentage improvement over the current best validation loss to do not incur into a patience penalty\n",
    "        save_to: string\n",
    "            Path where to save the state of the best model resulting from training.\n",
    "            If early stopping is enabled, saves the model with the lowest validation loss. Otherwise, saves the model corresponding to the last iteration.\n",
    "        load_from: string\n",
    "            Path from where to load the state of a previously saved model.\n",
    "        '''\n",
    "        self.predict = None\n",
    "        self.update = None\n",
    "        self.error_during_train = False\n",
    "        itemids = train_data[self.item_key].unique()\n",
    "        self.n_items = len(itemids)\n",
    "        self.init()  # initialize the network\n",
    "        if load_from:\n",
    "            logger.info('Resuming from state: {}'.format(load_from))\n",
    "            self.load_state(pickle.load(open(load_from, 'rb')))\n",
    "\n",
    "        if not retrain:\n",
    "            self.itemidmap = pd.Series(data=np.arange(self.n_items), index=itemids)\n",
    "            train_data = pd.merge(train_data,\n",
    "                                  pd.DataFrame({self.item_key: itemids, 'ItemIdx': self.itemidmap[itemids].values}),\n",
    "                                  on=self.item_key, how='inner')\n",
    "            user_indptr, offset_sessions = self.preprocess_data(train_data)\n",
    "        else:\n",
    "            raise Exception('Not supported yet!')\n",
    "\n",
    "        if valid_data is not None:\n",
    "            valid_data = pd.merge(valid_data,\n",
    "                                  pd.DataFrame({self.item_key: itemids, 'ItemIdx': self.itemidmap[itemids].values}),\n",
    "                                  on=self.item_key, how='inner')\n",
    "            user_indptr_valid, offset_sessions_valid = self.preprocess_data(valid_data)\n",
    "\n",
    "        X, Y = T.ivectors(2)\n",
    "        Sstart, Ustart = T.fvectors(2)\n",
    "        Hs_new, Hu_new, Y_pred, sampled_params = self.model(X, Sstart, Ustart, self.Hs, self.Hu, Y,\n",
    "                                                            drop_p_hidden_usr=self.dropout_p_hidden_usr,\n",
    "                                                            drop_p_hidden_ses=self.dropout_p_hidden_ses,\n",
    "                                                            drop_p_init=self.dropout_p_init)\n",
    "        cost = self.loss_function(Y_pred)\n",
    "        # set up the parameter and sampled parameter vectors\n",
    "        if self.item_embedding is None:\n",
    "            params = [self.Ws_in[1:], self.Ws_hh, self.Ws_rz, self.Bs_h, self.Ws_init, self.Bs_init,\n",
    "                      self.Wu_in, self.Wu_hh, self.Wu_rz, self.Bu_h]\n",
    "            full_params = [self.Ws_in[0], self.Wsy, self.By]\n",
    "        else:\n",
    "            params = [self.Ws_in, self.Ws_hh, self.Ws_rz, self.Bs_h, self.Ws_init, self.Bs_init,\n",
    "                      self.Wu_in, self.Wu_hh, self.Wu_rz, self.Bu_h]\n",
    "            full_params = [self.E_item, self.Wsy, self.By]\n",
    "\n",
    "        if self.user_propagation_mode == 'all':\n",
    "            params.append(self.Wu_to_s)\n",
    "        sidxs = [X, Y, Y]\n",
    "        if self.user_to_output:\n",
    "            full_params.append(self.Wuy)\n",
    "            sidxs.append(Y)\n",
    "\n",
    "        updates = self.RMSprop(cost, params, full_params, sampled_params, sidxs)\n",
    "        eval_updates = OrderedDict()\n",
    "        # Update the hidden states of the Session GRU\n",
    "        for i in range(len(self.Hs)):\n",
    "            updates[self.Hs[i]] = Hs_new[i]\n",
    "            eval_updates[self.Hs[i]] = Hs_new[i]\n",
    "        # Update the hidden states of the User GRU\n",
    "        for i in range(len(self.Hu)):\n",
    "            updates[self.Hu[i]] = Hu_new[i]\n",
    "            eval_updates[self.Hu[i]] = Hu_new[i]\n",
    "\n",
    "        # Compile the training and evaluation functions\n",
    "        self.train_function = function(inputs=[X, Sstart, Ustart, Y], outputs=cost, updates=updates,\n",
    "                                       allow_input_downcast=True,\n",
    "                                       on_unused_input='warn')\n",
    "        self.eval_function = function(inputs=[X, Sstart, Ustart, Y], outputs=cost, updates=eval_updates,\n",
    "                                      allow_input_downcast=True,\n",
    "                                      on_unused_input='warn')\n",
    "        # Negative item sampling\n",
    "        if self.n_sample:\n",
    "            self.neg_sampler = Sampler(train_data,\n",
    "                                       self.n_sample,\n",
    "                                       rng=self.rng,\n",
    "                                       item_key=self.item_key,\n",
    "                                       sample_alpha=self.sample_alpha,\n",
    "                                       sample_store=sample_store)\n",
    "        # Training starts here\n",
    "        best_valid, best_state = None, None\n",
    "        my_patience = patience\n",
    "        epoch = 0\n",
    "        while epoch < self.n_epochs and my_patience > 0:\n",
    "            train_cost = self.iterate(train_data, self.train_function, offset_sessions, user_indptr)\n",
    "            # self.print_state()\n",
    "            if np.isnan(train_cost):\n",
    "                return\n",
    "            if valid_data is not None:\n",
    "                valid_cost = self.iterate(valid_data, self.eval_function, offset_sessions_valid, user_indptr_valid)\n",
    "                if best_valid is None or valid_cost < best_valid:\n",
    "                    best_valid = valid_cost\n",
    "                    best_state = self.save_state()\n",
    "                    my_patience = patience\n",
    "                elif valid_cost >= best_valid * margin:\n",
    "                    my_patience -= 1\n",
    "                logger.info(\n",
    "                    'Epoch {} - train cost: {:.4f} - valid cost: {:.4f} (patience: {})'.format(epoch,\n",
    "                                                                                               train_cost,\n",
    "                                                                                               valid_cost,\n",
    "                                                                                               my_patience))\n",
    "            else:\n",
    "                logger.info('Epoch {} - train cost: {:.4f}'.format(epoch, train_cost))\n",
    "            epoch += 1\n",
    "        if my_patience == 0:\n",
    "            logger.info('Early stopping condition met!')\n",
    "        if best_state:\n",
    "            # always load the state associated with the best validation cost\n",
    "            self.load_state(best_state)\n",
    "        if save_to:\n",
    "            if best_state:\n",
    "                state = best_state\n",
    "            else:\n",
    "                state = self.save_state()\n",
    "            logger.info('Saving model to: {}'.format(save_to))\n",
    "            pickle.dump(state, open(save_to, 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def iterate(self, data, fun, offset_sessions, user_indptr, reset_state=True):\n",
    "        if reset_state:\n",
    "            # Reset session layers\n",
    "            for i in range(len(self.session_layers)):\n",
    "                self.Hs[i].set_value(np.zeros((self.batch_size, self.session_layers[i]), dtype=theano.config.floatX),\n",
    "                                     borrow=True)\n",
    "            # Reset user layers\n",
    "            for i in range(len(self.user_layers)):\n",
    "                self.Hu[i].set_value(np.zeros((self.batch_size, self.user_layers[i]), dtype=theano.config.floatX),\n",
    "                                     borrow=True)\n",
    "        # variables to manage iterations over users\n",
    "        n_users = len(user_indptr)\n",
    "        offset_users = offset_sessions[user_indptr]\n",
    "        user_idx_arr = np.arange(n_users - 1)\n",
    "        user_iters = np.arange(self.batch_size)\n",
    "        user_maxiter = user_iters.max()\n",
    "        user_start = offset_users[user_idx_arr[user_iters]]\n",
    "        user_end = offset_users[user_idx_arr[user_iters] + 1]\n",
    "\n",
    "        # variables to manage iterations over sessions\n",
    "        session_iters = user_indptr[user_iters]\n",
    "        session_start = offset_sessions[session_iters]\n",
    "        session_end = offset_sessions[session_iters + 1]\n",
    "\n",
    "        sstart = np.zeros((self.batch_size,), dtype=np.float32)\n",
    "        ustart = np.zeros((self.batch_size,), dtype=np.float32)\n",
    "        finished = False\n",
    "        n = 0\n",
    "        c = []\n",
    "        while not finished:\n",
    "            session_minlen = (session_end - session_start).min()\n",
    "            out_idx = data.ItemIdx.values[session_start]\n",
    "            for i in range(session_minlen - 1):\n",
    "                in_idx = out_idx\n",
    "                out_idx = data.ItemIdx.values[session_start + i + 1]\n",
    "                if self.n_sample:\n",
    "                    sample = self.neg_sampler.next_sample()\n",
    "                    y = np.hstack([out_idx, sample])\n",
    "                else:\n",
    "                    y = out_idx\n",
    "                cost = fun(in_idx, sstart, ustart, y)\n",
    "                n += 1\n",
    "                # reset sstart and ustart\n",
    "                sstart = np.zeros_like(sstart, dtype=np.float32)\n",
    "                ustart = np.zeros_like(ustart, dtype=np.float32)\n",
    "                c.append(cost)\n",
    "                if np.isnan(cost):\n",
    "                    logger.error('NaN error!')\n",
    "                    self.error_during_train = True\n",
    "                    return\n",
    "            session_start = session_start + session_minlen - 1\n",
    "            session_start_mask = np.arange(len(session_iters))[(session_end - session_start) <= 1]\n",
    "            sstart[session_start_mask] = 1\n",
    "            for idx in session_start_mask:\n",
    "                session_iters[idx] += 1\n",
    "                if session_iters[idx] + 1 >= len(offset_sessions):\n",
    "                    finished = True\n",
    "                    break\n",
    "                session_start[idx] = offset_sessions[session_iters[idx]]\n",
    "                session_end[idx] = offset_sessions[session_iters[idx] + 1]\n",
    "\n",
    "            # reset the User hidden state at user change\n",
    "            user_change_mask = np.arange(len(user_iters))[(user_end - session_start <= 0)]\n",
    "            ustart[user_change_mask] = 1\n",
    "            for idx in user_change_mask:\n",
    "                user_maxiter += 1\n",
    "                if user_maxiter + 1 >= len(offset_users):\n",
    "                    finished = True\n",
    "                    break\n",
    "                user_iters[idx] = user_maxiter\n",
    "                user_start[idx] = offset_users[user_maxiter]\n",
    "                user_end[idx] = offset_users[user_maxiter + 1]\n",
    "                session_iters[idx] = user_indptr[user_maxiter]\n",
    "                session_start[idx] = offset_sessions[session_iters[idx]]\n",
    "                session_end[idx] = offset_sessions[session_iters[idx] + 1]\n",
    "        avgc = np.mean(c)\n",
    "        return avgc\n",
    "\n",
    "    def predict_next_batch(self, session_ids, input_item_ids, input_user_ids,\n",
    "                           predict_for_item_ids=None, batch=100):\n",
    "        '''\n",
    "        Gives predicton scores for a selected set of items. Can be used in batch mode to predict for multiple independent events (i.e. events of different sessions) at once and thus speed up evaluation.\n",
    "\n",
    "        If the session ID at a given coordinate of the session_ids parameter remains the same during subsequent calls of the function, the corresponding hidden state of the network will be kept intact (i.e. that's how one can predict an item to a session).\n",
    "        If it changes, the hidden state of the network is reset to zeros.\n",
    "\n",
    "        Parameters\n",
    "        --------\n",
    "        session_ids : 1D array\n",
    "            Contains the session IDs of the events of the batch. Its length must equal to the prediction batch size (batch param).\n",
    "        input_item_ids : 1D array\n",
    "            Contains the item IDs of the events of the batch. Every item ID must be must be in the training data of the network. Its length must equal to the prediction batch size (batch param).\n",
    "        input_user_ids : 1D array\n",
    "            Contains the user IDs of the events of the batch. Every user ID must be must be in the training data of the network. Its length must equal to the prediction batch size (batch param).\n",
    "        predict_for_item_ids : 1D array (optional)\n",
    "            IDs of items for which the network should give prediction scores. Every ID must be in the training set. The default value is None, which means that the network gives prediction on its every output (i.e. for all items in the training set).\n",
    "        batch : int\n",
    "            Prediction batch size.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        out : pandas.DataFrame\n",
    "            Prediction scores for selected items for every event of the batch.\n",
    "            Columns: events of the batch; rows: items. Rows are indexed by the item IDs.\n",
    "\n",
    "        '''\n",
    "        if self.error_during_train: raise Exception\n",
    "        if self.predict is None or self.predict_batch != batch:\n",
    "            X, Y = T.ivectors(2)\n",
    "            Sstart, Ustart = T.fvectors(2)\n",
    "            for i in range(len(self.session_layers)):\n",
    "                self.Hs[i].set_value(np.zeros((batch, self.session_layers[i]), dtype=theano.config.floatX), borrow=True)\n",
    "            for i in range(len(self.user_layers)):\n",
    "                self.Hu[i].set_value(np.zeros((batch, self.user_layers[i]), dtype=theano.config.floatX), borrow=True)\n",
    "            if predict_for_item_ids is not None:\n",
    "                Hs_new, Hu_new, yhat, _ = self.model(X, Sstart, Ustart, self.Hs, self.Hu, Y)\n",
    "            else:\n",
    "                Hs_new, Hu_new, yhat, _ = self.model(X, Sstart, Ustart, self.Hs, self.Hu)\n",
    "            updatesH = OrderedDict()\n",
    "            for i in range(len(self.Hs)):\n",
    "                updatesH[self.Hs[i]] = Hs_new[i]\n",
    "            for i in range(len(self.Hu)):\n",
    "                updatesH[self.Hu[i]] = Hu_new[i]\n",
    "\n",
    "            if predict_for_item_ids is not None:\n",
    "                self.predict = function(inputs=[X, Sstart, Ustart, Y], outputs=yhat, updates=updatesH,\n",
    "                                        on_unused_input='warn', allow_input_downcast=True)\n",
    "            else:\n",
    "                self.predict = function(inputs=[X, Sstart, Ustart], outputs=yhat, updates=updatesH,\n",
    "                                        on_unused_input='warn', allow_input_downcast=True)\n",
    "            self.current_session = np.ones(batch) * -1\n",
    "            self.current_users = np.ones(batch) * -1\n",
    "            self.predict_batch = batch\n",
    "\n",
    "        session_change = session_ids != self.current_session\n",
    "        self.current_session = session_ids.copy()\n",
    "        user_change = input_user_ids != self.current_users\n",
    "        self.current_users = input_user_ids.copy()\n",
    "\n",
    "        in_idxs = self.itemidmap[input_item_ids]\n",
    "        if predict_for_item_ids is not None:\n",
    "            iIdxs = self.itemidmap[predict_for_item_ids]\n",
    "            preds = np.asarray(self.predict(in_idxs, session_change, user_change, iIdxs)).T\n",
    "            return pd.DataFrame(data=preds, index=predict_for_item_ids)\n",
    "        else:\n",
    "            preds = np.asarray(self.predict(in_idxs, session_change, user_change)).T\n",
    "            return pd.DataFrame(data=preds, index=self.itemidmap.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15xyIktWFfcR"
   },
   "outputs": [],
   "source": [
    "class RNNRecommender(ISeqRecommender):\n",
    "    \"\"\"\n",
    "    A **simplified** interface to Recurrent Neural Network models for Session-based recommendation.\n",
    "    Based on the following two papers:\n",
    "\n",
    "    * Recurrent Neural Networks with Top-k Gains for Session-based Recommendations, Hidasi and Karatzoglou, CIKM 2018\n",
    "    * Personalizing Session-based Recommendation with Hierarchical Recurrent Neural Networks, Quadrana et al, Recsys 2017\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 session_layers,\n",
    "                 user_layers=None,\n",
    "                 batch_size=32,\n",
    "                 learning_rate=0.1,\n",
    "                 momentum=0.0,\n",
    "                 dropout=None,\n",
    "                 epochs=10,\n",
    "                 personalized=False):\n",
    "        \"\"\"\n",
    "        :param session_layers: number of units per layer used at session level.\n",
    "            It has to be a list of integers for multi-layer networks, or a integer value for single-layer networks.\n",
    "        :param user_layers: number of units per layer used at user level. Required only by personalized models.\n",
    "            It has to be a list of integers for multi-layer networks, or a integer value for single-layer networks.\n",
    "        :param batch_size: the mini-batch size used in training\n",
    "        :param learning_rate: the learning rate used in training (Adagrad optimized)\n",
    "        :param momentum: the momentum coefficient used in training\n",
    "        :param dropout: dropout coefficients.\n",
    "            If personalized=False, it's a float value for the hidden-layer(s) dropout.\n",
    "            If personalized=True, it's a 3-tuple with the values for the dropout of (user hidden, session hidden, user-to-session hidden) layers.\n",
    "        :param epochs: number of training epochs\n",
    "        :param personalized: whether to train a personalized model using the HRNN model.\n",
    "            It will require user ids at prediction time.\n",
    "        \"\"\"\n",
    "        super(RNNRecommender).__init__()\n",
    "        if isinstance(session_layers, int):\n",
    "            session_layers = [session_layers]\n",
    "        if isinstance(user_layers, int):\n",
    "            user_layers = [user_layers]\n",
    "        self.session_layers = session_layers\n",
    "        self.user_layers = user_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        if dropout is None:\n",
    "            if not personalized:\n",
    "                dropout = 0.0\n",
    "            else:\n",
    "                dropout = (0.0, 0.0, 0.0)\n",
    "        self.dropout = dropout\n",
    "        self.epochs = epochs\n",
    "        self.personalized = personalized\n",
    "        self.pseudo_session_id = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'RNNRecommender(' \\\n",
    "               'session_layers={session_layers}, ' \\\n",
    "               'user_layers={user_layers}, ' \\\n",
    "               'batch_size={batch_size}, ' \\\n",
    "               'learning_rate={learning_rate}, ' \\\n",
    "               'momentum={momentum}, ' \\\n",
    "               'dropout={dropout}, ' \\\n",
    "               'epochs={epochs}, ' \\\n",
    "               'personalized={personalized}, ' \\\n",
    "               ')'.format(**self.__dict__)\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        self.logger.info('Converting training data to GRU4Rec format')\n",
    "        # parse training data to GRU4Rec format\n",
    "        train_data = dataset_to_gru4rec_format(dataset=train_data)\n",
    "\n",
    "        if not self.personalized:\n",
    "            # fit GRU4Rec\n",
    "            self.model = GRU4Rec(layers=self.session_layers,\n",
    "                                 n_epochs=self.epochs,\n",
    "                                 batch_size=self.batch_size,\n",
    "                                 learning_rate=self.learning_rate,\n",
    "                                 momentum=self.momentum,\n",
    "                                 dropout_p_hidden=self.dropout,\n",
    "                                 session_key='session_id',\n",
    "                                 item_key='item_id',\n",
    "                                 time_key='ts')\n",
    "        else:\n",
    "            if self.user_layers is None:\n",
    "                raise ValueError('You should set the value of user_layers before training the personalized model.')\n",
    "\n",
    "            if len(self.dropout) != 3:\n",
    "                raise ValueError('dropout should be a 3 tuple with '\n",
    "                                 '(user hidden, session hidden, user-to-session hidden) dropout values.')\n",
    "\n",
    "            self.model = HGRU4Rec(session_layers=self.session_layers,\n",
    "                                  user_layers=self.user_layers,\n",
    "                                  batch_size=self.batch_size,\n",
    "                                  n_epochs=self.epochs,\n",
    "                                  learning_rate=self.learning_rate,\n",
    "                                  momentum=self.momentum,\n",
    "                                  dropout_p_hidden_usr=self.dropout[0],\n",
    "                                  dropout_p_hidden_ses=self.dropout[1],\n",
    "                                  dropout_p_init=self.dropout[2],\n",
    "                                  session_key='session_id',\n",
    "                                  user_key='user_id',\n",
    "                                  item_key='item_id',\n",
    "                                  time_key='ts')\n",
    "        self.logger.info('Training started')\n",
    "        self.model.fit(train_data)\n",
    "        self.logger.info('Training completed')\n",
    "\n",
    "    def recommend(self, user_profile, user_id=None):\n",
    "        if not self.personalized:\n",
    "            for item in user_profile:\n",
    "                pred = self.model.predict_next_batch(np.array([self.pseudo_session_id]),\n",
    "                                                     np.array([item]),\n",
    "                                                     batch=1)\n",
    "        else:\n",
    "            if user_id is None:\n",
    "                raise ValueError('user_id required by personalized models')\n",
    "            for item in user_profile:\n",
    "                pred = self.model.predict_next_batch(np.array([self.pseudo_session_id]),\n",
    "                                                     np.array([item]),\n",
    "                                                     np.array([user_id]),\n",
    "                                                     batch=1)\n",
    "        # sort items by predicted score\n",
    "        pred.sort_values(0, ascending=False, inplace=True)\n",
    "        # increase the psuedo-session id so that future call to recommend() won't be connected\n",
    "        self.pseudo_session_id += 1\n",
    "        # convert to the required output format\n",
    "        return [([x.index], x._2) for x in pred.reset_index().itertuples()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pGWU1rvFRS0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 14:00:57,953 - INFO - Converting training data to GRU4Rec format\n",
      "2021-04-25 14:00:57,980 - INFO - Training started\n",
      "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
      "2021-04-25 14:01:11,615 - WARNING - We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0\tloss: 0.627941\n",
      "Epoch1\tloss: 0.533681\n",
      "Epoch2\tloss: 0.505859\n",
      "Epoch3\tloss: 0.491371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 14:02:15,525 - INFO - Training completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch4\tloss: 0.483965\n"
     ]
    }
   ],
   "source": [
    "rnnrecommender = RNNRecommender(session_layers=[20], \n",
    "                             batch_size=16,\n",
    "                             learning_rate=0.1,\n",
    "                             momentum=0.1,\n",
    "                             dropout=0.1,\n",
    "                             epochs=5,\n",
    "                             personalized=False)\n",
    "rnnrecommender.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7QpgKAfI0PN"
   },
   "source": [
    "### Personalized RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_nM3sqpI2Ms"
   },
   "source": [
    "Here we fit the recommedation algorithm over the sessions in the training set.  \n",
    "\n",
    "This is a **simplified** interface to Recurrent Neural Network models for Session-based recommendation.\n",
    "Based on the following two papers:\n",
    "\n",
    "* Recurrent Neural Networks with Top-k Gains for Session-based Recommendations, Hidasi and Karatzoglou, CIKM 2018\n",
    "* Personalizing Session-based Recommendation with Hierarchical Recurrent Neural Networks, Quadrana et al, Recsys 2017\n",
    "\n",
    "In this notebook, we will consider the session-aware (**personalized**) version of the algorithm.\n",
    "Here's a schematic representation of the model:\n",
    "\n",
    "<img src='https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb0715490-8128-45cf-b98e-8c5e557e8076%2FUntitled.png?table=block&id=e75f1410-aacc-4984-87f3-d042400d725a&spaceId=63b72b1f-0e90-4ab8-a6df-a060a6545a56&width=2000&userId=21ec183f-f0be-4b6b-9b3e-6f0d4e5c5469&cache=v2'>\n",
    "\n",
    "Each user session goes through a _session_ RNN, which models **short-term** user preferences. At the end of each session, the state of the _session_ RNN is used to update a _user_ RNN, which models more **long-term** user preferences. It's state is passed forward to the next _session_ RNN, which can now personalize recommendations depending on both short-term and long-term user interests.\n",
    "\n",
    "The hyper-parameters of the model are:\n",
    "\n",
    "* `session_layers`: number of units per layer used at session level.\n",
    "    It has to be a list of integers for multi-layer networks, or a integer value for single-layer networks.\n",
    "* `user_layers`: number of units per layer used at user level. Required only by personalized models.\n",
    "    It has to be a list of integers for multi-layer networks, or a integer value for single-layer networks.\n",
    "* `batch_size`: the mini-batch size used in training\n",
    "* `learning_rate`: the learning rate used in training (Adagrad optimized)\n",
    "* `momentum`: the momentum coefficient used in training\n",
    "* `dropout`: it's a 3-tuple with the values for the dropout of (user hidden, session hidden, user-to-session hidden) layers.\n",
    "* `epochs`: number of training epochs\n",
    "* `personalized`: whether to train a personalized model using the HRNN model (`True` in this case).\n",
    "\n",
    "**NOTE: HGRU4Rec originally has more hyper-parameters. Going through all of them is out from the scope of this tutorial, but we suggest to check-out the original source code [here](https://github.com/mquad/hgru4rec) in case you are interested.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BF2C7VjPIz6s"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 14:02:15,598 - INFO - Converting training data to GRU4Rec format\n",
      "2021-04-25 14:02:15,633 - INFO - Training started\n",
      "2021-04-25 14:02:51,671 - INFO - Epoch 0 - train cost: 1.0400\n",
      "2021-04-25 14:02:52,260 - INFO - Epoch 1 - train cost: 0.9588\n",
      "2021-04-25 14:02:52,854 - INFO - Epoch 2 - train cost: 0.9023\n",
      "2021-04-25 14:02:53,433 - INFO - Epoch 3 - train cost: 0.8703\n",
      "2021-04-25 14:02:54,036 - INFO - Epoch 4 - train cost: 0.8492\n",
      "2021-04-25 14:02:54,039 - INFO - Training completed\n"
     ]
    }
   ],
   "source": [
    "prnnrecommender = RNNRecommender(session_layers=[20], \n",
    "                             user_layers=[20],\n",
    "                             batch_size=16,\n",
    "                             learning_rate=0.5,\n",
    "                             momentum=0.1,\n",
    "                             dropout=(0.1,0.1,0.1),\n",
    "                             epochs=5,\n",
    "                             personalized=True)\n",
    "prnnrecommender.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "al43z8GbKFL3"
   },
   "source": [
    "### KNN recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21uCC5C6LmM7"
   },
   "source": [
    "The class `KNNRecommender` takes the following initialization hyper-parameters:\n",
    "- `model`: One among the following KNN models:\n",
    "    - `iknn`: ItemKNN, item-to-item KNN based on the *last* item in the session to determine the items to be recommended.\n",
    "    - `sknn`: SessionKNN, compares the *entire* current session with the past sessions in the training data to determine the items to be recommended.\n",
    "    - `v-sknn`: VMSessionKNN, use linearly decayed real-valued vectors to encode the current session, then compares the current session with the past sessions in the training data using the dot-product to determine the items to be recommended.\n",
    "    - `s-sknn`: SeqSessionKNN, this variant also puts more weight on elements that appear later in the session by using a custom scoring function (see the paper by Ludewng and Jannach).\n",
    "    - `sf-sknn`: SeqFilterSessionKNN, this variant also puts more weight on elements that appear later in the session in a more restrictive way by using a custom scoring function (see the paper by Ludewng and Jannach).\n",
    "\n",
    "- param `init_args`: The model initialization arguments. See the following initializations or check `util.knn` for more details on each model:\n",
    "    - `iknn`: ItemKNN(n_sims=100, lmbd=20, alpha=0.5)\n",
    "    - `sknn`: SessionKNN(k, sample_size=500, sampling='recent', similarity='jaccard', remind=False, pop_boost=0)\n",
    "    - `v-sknn`: VMSessionKNN(k, sample_size=1000, sampling='recent', similarity='cosine', weighting='div',\n",
    "         dwelling_time=False, last_n_days=None, last_n_clicks=None, extend=False, weighting_score='div_score',\n",
    "         weighting_time=False, normalize=True)\n",
    "    - `s-knn`: SeqSessionKNN(k, sample_size=1000, sampling='recent', similarity='jaccard', weighting='div',\n",
    "        remind=False, pop_boost=0, extend=False, normalize=True)\n",
    "    - `sf-sknn`: SeqFilterSessionKNN(k, sample_size=1000, sampling='recent', similarity='jaccard', remind=False, pop_boost=0,extend=False, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wm5CriCtKUtW"
   },
   "outputs": [],
   "source": [
    "class ItemKNN:\n",
    "    '''\n",
    "    ItemKNN(n_sims = 100, lmbd = 20, alpha = 0.5, session_key = 'SessionId', item_key = 'ItemId', time_key = 'Time')\n",
    "    \n",
    "    Item-to-item predictor that computes the the similarity to all items to the given item.\n",
    "    \n",
    "    Similarity of two items is given by:\n",
    "    \n",
    "    .. math::\n",
    "        s_{i,j}=\\sum_{s}I\\{(s,i)\\in D & (s,j)\\in D\\} / (supp_i+\\\\lambda)^{\\\\alpha}(supp_j+\\\\lambda)^{1-\\\\alpha}\n",
    "        \n",
    "    Parameters\n",
    "    --------\n",
    "    n_sims : int\n",
    "        Only give back non-zero scores to the N most similar items. Should be higher or equal than the cut-off of your evaluation. (Default value: 100)\n",
    "    lmbd : float\n",
    "        Regularization. Discounts the similarity of rare items (incidental co-occurrences). (Default value: 20)\n",
    "    alpha : float\n",
    "        Balance between normalizing with the supports of the two items. 0.5 gives cosine similarity, 1.0 gives confidence (as in association rules).\n",
    "    session_key : string\n",
    "        header of the session ID column in the input file (default: 'SessionId')\n",
    "    item_key : string\n",
    "        header of the item ID column in the input file (default: 'ItemId')\n",
    "    time_key : string\n",
    "        header of the timestamp column in the input file (default: 'Time')\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_sims=100, lmbd=20, alpha=0.5, session_key='SessionId', item_key='ItemId', time_key='Time'):\n",
    "        self.n_sims = n_sims\n",
    "        self.lmbd = lmbd\n",
    "        self.alpha = alpha\n",
    "        self.item_key = item_key\n",
    "        self.session_key = session_key\n",
    "        self.time_key = time_key\n",
    "\n",
    "    def fit(self, data):\n",
    "        '''\n",
    "        Trains the predictor.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        data: pandas.DataFrame\n",
    "            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n",
    "            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n",
    "            \n",
    "        '''\n",
    "        data.set_index(np.arange(len(data)), inplace=True)\n",
    "        self.itemids = data[self.item_key].unique()\n",
    "        n_items = len(self.itemids)\n",
    "        data = pd.merge(data, pd.DataFrame({self.item_key: self.itemids, 'ItemIdx': np.arange(len(self.itemids))}),\n",
    "                        on=self.item_key, how='inner')\n",
    "        sessionids = data[self.session_key].unique()\n",
    "        data = pd.merge(data, pd.DataFrame({self.session_key: sessionids, 'SessionIdx': np.arange(len(sessionids))}),\n",
    "                        on=self.session_key, how='inner')\n",
    "        supp = data.groupby('SessionIdx').size()\n",
    "        session_offsets = np.zeros(len(supp) + 1, dtype=np.int32)\n",
    "        session_offsets[1:] = supp.cumsum()\n",
    "        index_by_sessions = data.sort_values(['SessionIdx', self.time_key]).index.values\n",
    "        supp = data.groupby('ItemIdx').size()\n",
    "        item_offsets = np.zeros(n_items + 1, dtype=np.int32)\n",
    "        item_offsets[1:] = supp.cumsum()\n",
    "        index_by_items = data.sort_values(['ItemIdx', self.time_key]).index.values\n",
    "        self.sims = dict()\n",
    "        for i in range(n_items):\n",
    "            iarray = np.zeros(n_items)\n",
    "            start = item_offsets[i]\n",
    "            end = item_offsets[i + 1]\n",
    "            for e in index_by_items[start:end]:\n",
    "                uidx = data.SessionIdx.values[e]\n",
    "                ustart = session_offsets[uidx]\n",
    "                uend = session_offsets[uidx + 1]\n",
    "                user_events = index_by_sessions[ustart:uend]\n",
    "                iarray[data.ItemIdx.values[user_events]] += 1\n",
    "            iarray[i] = 0\n",
    "            norm = np.power((supp[i] + self.lmbd), self.alpha) * np.power((supp.values + self.lmbd), (1.0 - self.alpha))\n",
    "            norm[norm == 0] = 1\n",
    "            iarray = iarray / norm\n",
    "            indices = np.argsort(iarray)[-1:-1 - self.n_sims:-1]\n",
    "            self.sims[self.itemids[i]] = pd.Series(data=iarray[indices], index=self.itemids[indices])\n",
    "\n",
    "    def predict_next(self, session_id, input_item_id, predict_for_item_ids=None, skip=False, type='view', timestamp=0):\n",
    "        '''\n",
    "        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n",
    "                \n",
    "        Parameters\n",
    "        --------\n",
    "        session_id : int or string\n",
    "            The session IDs of the event.\n",
    "        input_item_id : int or string\n",
    "            The item ID of the event. Must be in the set of item IDs of the training set.\n",
    "        predict_for_item_ids : 1D array\n",
    "            IDs of items for which the network should give prediction scores. Every ID must be in the set of item IDs of the training set.\n",
    "            \n",
    "        Returns\n",
    "        --------\n",
    "        out : pandas.Series\n",
    "            Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs.\n",
    "        \n",
    "        '''\n",
    "        if predict_for_item_ids is None:\n",
    "            predict_for_item_ids = self.itemids\n",
    "        preds = np.zeros(len(predict_for_item_ids))\n",
    "        sim_list = self.sims[input_item_id]\n",
    "        mask = np.in1d(predict_for_item_ids, sim_list.index)\n",
    "        preds[mask] = sim_list[predict_for_item_ids[mask]]\n",
    "        return pd.Series(data=preds, index=predict_for_item_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWIHeeqNKpFd"
   },
   "outputs": [],
   "source": [
    "from _operator import itemgetter\n",
    "from math import sqrt\n",
    "import random\n",
    "import time\n",
    "from math import log10\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta as td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-M6MTzThKkJB"
   },
   "outputs": [],
   "source": [
    "class SeqFilterSessionKNN:\n",
    "    '''\n",
    "    SessionKNN( k, sample_size=500, sampling='recent',  similarity = 'jaccard', remind=False, pop_boost=0, session_key = 'SessionId', item_key= 'ItemId')\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    k : int\n",
    "        Number of neighboring session to calculate the item scores from. (Default value: 100)\n",
    "    sample_size : int\n",
    "        Defines the length of a subset of all training sessions to calculate the nearest neighbors from. (Default value: 500)\n",
    "    sampling : string\n",
    "        String to define the sampling method for sessions (recent, random). (default: recent)\n",
    "    similarity : string\n",
    "        String to define the method for the similarity calculation (jaccard, cosine, binary, tanimoto). (default: jaccard)\n",
    "    remind : bool\n",
    "        Should the last items of the current session be boosted to the top as reminders\n",
    "    pop_boost : int\n",
    "        Push popular items in the neighbor sessions by this factor. (default: 0 to leave out)\n",
    "    extend : bool\n",
    "        Add evaluated sessions to the maps\n",
    "    normalize : bool\n",
    "        Normalize the scores in the end\n",
    "    session_key : string\n",
    "        Header of the session ID column in the input file. (default: 'SessionId')\n",
    "    item_key : string\n",
    "        Header of the item ID column in the input file. (default: 'ItemId')\n",
    "    time_key : string\n",
    "        Header of the timestamp column in the input file. (default: 'Time')\n",
    "    '''\n",
    "\n",
    "    def __init__(self, k, sample_size=1000, sampling='recent', similarity='jaccard', remind=False, pop_boost=0,\n",
    "                 extend=False, normalize=True, session_key='SessionId', item_key='ItemId', time_key='Time'):\n",
    "\n",
    "        self.remind = remind\n",
    "        self.k = k\n",
    "        self.sample_size = sample_size\n",
    "        self.sampling = sampling\n",
    "        self.similarity = similarity\n",
    "        self.pop_boost = pop_boost\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.extend = extend\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # updated while recommending\n",
    "        self.session = -1\n",
    "        self.session_items = []\n",
    "        self.relevant_sessions = set()\n",
    "\n",
    "        # cache relations once at startup\n",
    "        self.session_item_map = dict()\n",
    "        self.item_session_map = dict()\n",
    "        self.session_time = dict()\n",
    "        self.followed_by = dict()\n",
    "\n",
    "        self.sim_time = 0\n",
    "\n",
    "    def fit(self, train, items=None):\n",
    "        '''\n",
    "        Trains the predictor.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        data: pandas.DataFrame\n",
    "            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n",
    "            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n",
    "            \n",
    "        '''\n",
    "\n",
    "        index_session = train.columns.get_loc(self.session_key)\n",
    "        index_item = train.columns.get_loc(self.item_key)\n",
    "        index_time = train.columns.get_loc(self.time_key)\n",
    "        self.itemids = train[self.item_key].unique()\n",
    "\n",
    "        session = -1\n",
    "        session_items = set()\n",
    "        last_item = -1\n",
    "        time = -1\n",
    "        # cnt = 0\n",
    "        for row in train.itertuples(index=False):\n",
    "            # cache items of sessions\n",
    "            if row[index_session] != session:\n",
    "                if len(session_items) > 0:\n",
    "                    self.session_item_map.update({session: session_items})\n",
    "                    # cache the last time stamp of the session\n",
    "                    self.session_time.update({session: time})\n",
    "                session = row[index_session]\n",
    "                session_items = set()\n",
    "            else:\n",
    "                if last_item != -1:  # fill followed by map for filtering of candidate items\n",
    "                    if not last_item in self.followed_by:\n",
    "                        self.followed_by[last_item] = set()\n",
    "                    self.followed_by[last_item].add(row[index_item])\n",
    "\n",
    "            time = row[index_time]\n",
    "            session_items.add(row[index_item])\n",
    "\n",
    "            # cache sessions involving an item\n",
    "            map_is = self.item_session_map.get(row[index_item])\n",
    "            if map_is is None:\n",
    "                map_is = set()\n",
    "                self.item_session_map.update({row[index_item]: map_is})\n",
    "            map_is.add(row[index_session])\n",
    "\n",
    "            last_item = row[index_item]\n",
    "\n",
    "        # Add the last tuple    \n",
    "        self.session_item_map.update({session: session_items})\n",
    "        self.session_time.update({session: time})\n",
    "\n",
    "    def predict_next(self, session_id, input_item_id, predict_for_item_ids=None, skip=False, type='view', timestamp=0):\n",
    "        '''\n",
    "        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n",
    "                \n",
    "        Parameters\n",
    "        --------\n",
    "        session_id : int or string\n",
    "            The session IDs of the event.\n",
    "        input_item_id : int or string\n",
    "            The item ID of the event. Must be in the set of item IDs of the training set.\n",
    "        predict_for_item_ids : 1D array\n",
    "            IDs of items for which the network should give prediction scores. Every ID must be in the set of item IDs of the training set.\n",
    "            \n",
    "        Returns\n",
    "        --------\n",
    "        out : pandas.Series\n",
    "            Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs.\n",
    "        \n",
    "        '''\n",
    "\n",
    "        #         gc.collect()\n",
    "        #         process = psutil.Process(os.getpid())\n",
    "        #         print( 'cknn.predict_next: ', process.memory_info().rss, ' memory used')\n",
    "\n",
    "        if (self.session != session_id):  # new session\n",
    "\n",
    "            if (self.extend):\n",
    "                item_set = set(self.session_items)\n",
    "                self.session_item_map[self.session] = item_set;\n",
    "                for item in item_set:\n",
    "                    map_is = self.item_session_map.get(item)\n",
    "                    if map_is is None:\n",
    "                        map_is = set()\n",
    "                        self.item_session_map.update({item: map_is})\n",
    "                    map_is.add(self.session)\n",
    "\n",
    "                ts = time.time()\n",
    "                self.session_time.update({self.session: ts})\n",
    "\n",
    "                last_item = -1\n",
    "                for item in self.session_items:\n",
    "                    if last_item != -1:\n",
    "                        if not last_item in self.followed_by:\n",
    "                            self.followed_by[last_item] = set()\n",
    "                        self.followed_by[last_item].add(item)\n",
    "                    last_item = item\n",
    "\n",
    "            self.session = session_id\n",
    "            self.session_items = list()\n",
    "            self.relevant_sessions = set()\n",
    "\n",
    "        if type == 'view':\n",
    "            self.session_items.append(input_item_id)\n",
    "\n",
    "        if skip:\n",
    "            return\n",
    "\n",
    "        neighbors = self.find_neighbors(set(self.session_items), input_item_id, session_id)\n",
    "        scores = self.score_items(neighbors, input_item_id)\n",
    "\n",
    "        # add some reminders\n",
    "        if self.remind:\n",
    "\n",
    "            reminderScore = 5\n",
    "            takeLastN = 3\n",
    "\n",
    "            cnt = 0\n",
    "            for elem in self.session_items[-takeLastN:]:\n",
    "                cnt = cnt + 1\n",
    "                # reminderScore = reminderScore + (cnt/100)\n",
    "\n",
    "                oldScore = scores.get(elem)\n",
    "                newScore = 0\n",
    "                if oldScore is None:\n",
    "                    newScore = reminderScore\n",
    "                else:\n",
    "                    newScore = oldScore + reminderScore\n",
    "                # print 'old score ', oldScore\n",
    "                # update the score and add a small number for the position \n",
    "                newScore = (newScore * reminderScore) + (cnt / 100)\n",
    "\n",
    "                scores.update({elem: newScore})\n",
    "\n",
    "        # push popular ones\n",
    "        if self.pop_boost > 0:\n",
    "\n",
    "            pop = self.item_pop(neighbors)\n",
    "            # Iterate over the item neighbors\n",
    "            # print itemScores\n",
    "            for key in scores:\n",
    "                item_pop = pop.get(key)\n",
    "                # Gives some minimal MRR boost?\n",
    "                scores.update({key: (scores[key] + (self.pop_boost * item_pop))})\n",
    "\n",
    "        # Create things in the format ..\n",
    "        if predict_for_item_ids is None:\n",
    "            predict_for_item_ids = self.itemids\n",
    "        predictions = np.zeros(len(predict_for_item_ids))\n",
    "        mask = np.in1d(predict_for_item_ids, list(scores.keys()))\n",
    "\n",
    "        items = predict_for_item_ids[mask]\n",
    "        values = [scores[x] for x in items]\n",
    "        predictions[mask] = values\n",
    "        series = pd.Series(data=predictions, index=predict_for_item_ids)\n",
    "\n",
    "        if self.normalize:\n",
    "            series = series / series.max()\n",
    "\n",
    "        return series\n",
    "\n",
    "    def item_pop(self, sessions):\n",
    "        '''\n",
    "        Returns a dict(item,score) of the item popularity for the given list of sessions (only a set of ids)\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        sessions: set\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        out : dict            \n",
    "        '''\n",
    "        result = dict()\n",
    "        max_pop = 0\n",
    "        for session, weight in sessions:\n",
    "            items = self.items_for_session(session)\n",
    "            for item in items:\n",
    "\n",
    "                count = result.get(item)\n",
    "                if count is None:\n",
    "                    result.update({item: 1})\n",
    "                else:\n",
    "                    result.update({item: count + 1})\n",
    "\n",
    "                if (result.get(item) > max_pop):\n",
    "                    max_pop = result.get(item)\n",
    "\n",
    "        for key in result:\n",
    "            result.update({key: (result[key] / max_pop)})\n",
    "\n",
    "        return result\n",
    "\n",
    "    def jaccard(self, first, second):\n",
    "        '''\n",
    "        Calculates the jaccard index for two sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        sc = time.clock()\n",
    "        intersection = len(first & second)\n",
    "        union = len(first | second)\n",
    "        res = intersection / union\n",
    "\n",
    "        self.sim_time += (time.clock() - sc)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def cosine(self, first, second):\n",
    "        '''\n",
    "        Calculates the cosine similarity for two sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        li = len(first & second)\n",
    "        la = len(first)\n",
    "        lb = len(second)\n",
    "        result = li / sqrt(la) * sqrt(lb)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def tanimoto(self, first, second):\n",
    "        '''\n",
    "        Calculates the cosine tanimoto similarity for two sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        li = len(first & second)\n",
    "        la = len(first)\n",
    "        lb = len(second)\n",
    "        result = li / (la + lb - li)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def binary(self, first, second):\n",
    "        '''\n",
    "        Calculates the ? for 2 sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        a = len(first & second)\n",
    "        b = len(first)\n",
    "        c = len(second)\n",
    "\n",
    "        result = (2 * a) / ((2 * a) + b + c)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def items_for_session(self, session):\n",
    "        '''\n",
    "        Returns all items in the session\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        session: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : set           \n",
    "        '''\n",
    "        return self.session_item_map.get(session);\n",
    "\n",
    "    def sessions_for_item(self, item_id):\n",
    "        '''\n",
    "        Returns all session for an item\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        item: Id of the item session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : set           \n",
    "        '''\n",
    "        return self.item_session_map.get(item_id)\n",
    "\n",
    "    def most_recent_sessions(self, sessions, number):\n",
    "        '''\n",
    "        Find the most recent sessions in the given set\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        sessions: set of session ids\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : set           \n",
    "        '''\n",
    "        sample = set()\n",
    "\n",
    "        tuples = list()\n",
    "        for session in sessions:\n",
    "            time = self.session_time.get(session)\n",
    "            if time is None:\n",
    "                print(' EMPTY TIMESTAMP!! ', session)\n",
    "            tuples.append((session, time))\n",
    "\n",
    "        tuples = sorted(tuples, key=itemgetter(1), reverse=True)\n",
    "        # print 'sorted list ', sortedList\n",
    "        cnt = 0\n",
    "        for element in tuples:\n",
    "            cnt = cnt + 1\n",
    "            if cnt > number:\n",
    "                break\n",
    "            sample.add(element[0])\n",
    "        # print 'returning sample of size ', len(sample)\n",
    "        return sample\n",
    "\n",
    "    def possible_neighbor_sessions(self, session_items, input_item_id, session_id):\n",
    "        '''\n",
    "        Find a set of session to later on find neighbors in.\n",
    "        A self.sample_size of 0 uses all sessions in which any item of the current session appears.\n",
    "        self.sampling can be performed with the options \"recent\" or \"random\".\n",
    "        \"recent\" selects the self.sample_size most recent sessions while \"random\" just choses randomly. \n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        sessions: set of session ids\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : set           \n",
    "        '''\n",
    "\n",
    "        self.relevant_sessions = self.relevant_sessions | self.sessions_for_item(input_item_id);\n",
    "\n",
    "        if self.sample_size == 0:  # use all session as possible neighbors\n",
    "\n",
    "            print('!!!!! runnig KNN without a sample size (check config)')\n",
    "            return self.relevant_sessions\n",
    "\n",
    "        else:  # sample some sessions\n",
    "\n",
    "            self.relevant_sessions = self.relevant_sessions | self.sessions_for_item(input_item_id);\n",
    "\n",
    "            if len(self.relevant_sessions) > self.sample_size:\n",
    "\n",
    "                if self.sampling == 'recent':\n",
    "                    sample = self.most_recent_sessions(self.relevant_sessions, self.sample_size)\n",
    "                elif self.sampling == 'random':\n",
    "                    sample = random.sample(self.relevant_sessions, self.sample_size)\n",
    "                else:\n",
    "                    sample = self.relevant_sessions[:self.sample_size]\n",
    "\n",
    "                return sample\n",
    "            else:\n",
    "                return self.relevant_sessions\n",
    "\n",
    "    def calc_similarity(self, session_items, sessions):\n",
    "        '''\n",
    "        Calculates the configured similarity for the items in session_items and each session in sessions.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        session_items: set of item ids\n",
    "        sessions: list of session ids\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : list of tuple (session_id,similarity)           \n",
    "        '''\n",
    "\n",
    "        # print 'nb of sessions to test ', len(sessionsToTest), ' metric: ', self.metric\n",
    "        neighbors = []\n",
    "        cnt = 0\n",
    "        for session in sessions:\n",
    "            cnt = cnt + 1\n",
    "            # get items of the session, look up the cache first \n",
    "            session_items_test = self.items_for_session(session)\n",
    "\n",
    "            similarity = getattr(self, self.similarity)(session_items_test, session_items)\n",
    "            if similarity > 0:\n",
    "                neighbors.append((session, similarity))\n",
    "\n",
    "        return neighbors\n",
    "\n",
    "    # -----------------\n",
    "    # Find a set of neighbors, returns a list of tuples (sessionid: similarity) \n",
    "    # -----------------\n",
    "    def find_neighbors(self, session_items, input_item_id, session_id):\n",
    "        '''\n",
    "        Finds the k nearest neighbors for the given session_id and the current item input_item_id. \n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        session_items: set of item ids\n",
    "        input_item_id: int \n",
    "        session_id: int\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : list of tuple (session_id, similarity)           \n",
    "        '''\n",
    "        possible_neighbors = self.possible_neighbor_sessions(session_items, input_item_id, session_id)\n",
    "        possible_neighbors = self.calc_similarity(session_items, possible_neighbors)\n",
    "\n",
    "        possible_neighbors = sorted(possible_neighbors, reverse=True, key=lambda x: x[1])\n",
    "        possible_neighbors = possible_neighbors[:self.k]\n",
    "\n",
    "        return possible_neighbors\n",
    "\n",
    "    def score_items(self, neighbors, input_item_id):\n",
    "        '''\n",
    "        Compute a set of scores for all items given a set of neighbors.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        neighbors: set of session ids\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : list of tuple (item, score)           \n",
    "        '''\n",
    "        # now we have the set of relevant items to make predictions\n",
    "        scores = dict()\n",
    "        # iterate over the sessions\n",
    "        for session in neighbors:\n",
    "            # get the items in this session\n",
    "            items = self.items_for_session(session[0])\n",
    "\n",
    "            for item in items:\n",
    "\n",
    "                if input_item_id in self.followed_by and item in self.followed_by[\n",
    "                    input_item_id]:  # hard filter the candidates\n",
    "\n",
    "                    old_score = scores.get(item)\n",
    "                    new_score = session[1]\n",
    "\n",
    "                    if old_score is None:\n",
    "                        scores.update({item: new_score})\n",
    "                    else:\n",
    "                        new_score = old_score + new_score\n",
    "                        scores.update({item: new_score})\n",
    "\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_ZZUnJQKvnN"
   },
   "outputs": [],
   "source": [
    "class VMSessionKNN:\n",
    "    '''\n",
    "    VMSessionKNN( k, sample_size=1000, sampling='recent', similarity='cosine', weighting='div', dwelling_time=False, last_n_days=None, last_n_clicks=None, extend=False, weighting_score='div_score', weighting_time=False, normalize=True, session_key = 'SessionId', item_key= 'ItemId', time_key= 'Time')\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    k : int\n",
    "        Number of neighboring session to calculate the item scores from. (Default value: 100)\n",
    "    sample_size : int\n",
    "        Defines the length of a subset of all training sessions to calculate the nearest neighbors from. (Default value: 500)\n",
    "    sampling : string\n",
    "        String to define the sampling method for sessions (recent, random). (default: recent)\n",
    "    similarity : string\n",
    "        String to define the method for the similarity calculation (jaccard, cosine, binary, tanimoto). (default: jaccard)\n",
    "    weighting : string\n",
    "        Decay function to determine the importance/weight of individual actions in the current session (linear, same, div, log, quadratic). (default: div)\n",
    "    weighting_score : string\n",
    "        Decay function to lower the score of candidate items from a neighboring sessions that were selected by less recently clicked items in the current session. (linear, same, div, log, quadratic). (default: div_score)\n",
    "    weighting_time : boolean\n",
    "        Experimental function to give less weight to items from older sessions (default: False)\n",
    "    dwelling_time : boolean\n",
    "        Experimental function to use the dwelling time for item view actions as a weight in the similarity calculation. (default: False)\n",
    "    last_n_days : int\n",
    "        Use only data from the last N days. (default: None)\n",
    "    last_n_clicks : int\n",
    "        Use only the last N clicks of the current session when recommending. (default: None)\n",
    "    extend : bool\n",
    "        Add evaluated sessions to the maps.\n",
    "    normalize : bool\n",
    "        Normalize the scores in the end.\n",
    "    session_key : string\n",
    "        Header of the session ID column in the input file. (default: 'SessionId')\n",
    "    item_key : string\n",
    "        Header of the item ID column in the input file. (default: 'ItemId')\n",
    "    time_key : string\n",
    "        Header of the timestamp column in the input file. (default: 'Time')\n",
    "    '''\n",
    "\n",
    "    def __init__(self, k, sample_size=1000, sampling='recent', similarity='cosine', weighting='div',\n",
    "                 dwelling_time=False, last_n_days=None, last_n_clicks=None, extend=False, weighting_score='div_score',\n",
    "                 weighting_time=False, normalize=True, session_key='SessionId', item_key='ItemId', time_key='Time'):\n",
    "\n",
    "        self.k = k\n",
    "        self.sample_size = sample_size\n",
    "        self.sampling = sampling\n",
    "        self.weighting = weighting\n",
    "        self.dwelling_time = dwelling_time\n",
    "        self.weighting_score = weighting_score\n",
    "        self.weighting_time = weighting_time\n",
    "        self.similarity = similarity\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.extend = extend\n",
    "        self.normalize = normalize\n",
    "        self.last_n_days = last_n_days\n",
    "        self.last_n_clicks = last_n_clicks\n",
    "\n",
    "        # updated while recommending\n",
    "        self.session = -1\n",
    "        self.session_items = []\n",
    "        self.relevant_sessions = set()\n",
    "\n",
    "        # cache relations once at startup\n",
    "        self.session_item_map = dict()\n",
    "        self.item_session_map = dict()\n",
    "        self.session_time = dict()\n",
    "        self.min_time = -1\n",
    "\n",
    "        self.sim_time = 0\n",
    "\n",
    "    def fit(self, data, items=None):\n",
    "        '''\n",
    "        Trains the predictor.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        data: pandas.DataFrame\n",
    "            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n",
    "            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n",
    "            \n",
    "        '''\n",
    "\n",
    "        if self.last_n_days != None:\n",
    "\n",
    "            max_time = dt.fromtimestamp(data[self.time_key].max())\n",
    "            date_threshold = max_time.date() - td(self.last_n_days)\n",
    "            stamp = dt.combine(date_threshold, dt.min.time()).timestamp()\n",
    "            train = data[data[self.time_key] >= stamp]\n",
    "\n",
    "        else:\n",
    "            train = data\n",
    "\n",
    "        self.num_items = train[self.item_key].max()\n",
    "\n",
    "        index_session = train.columns.get_loc(self.session_key)\n",
    "        index_item = train.columns.get_loc(self.item_key)\n",
    "        index_time = train.columns.get_loc(self.time_key)\n",
    "        self.itemids = train[self.item_key].unique()\n",
    "\n",
    "        session = -1\n",
    "        session_items = set()\n",
    "        time = -1\n",
    "        # cnt = 0\n",
    "        for row in train.itertuples(index=False):\n",
    "            # cache items of sessions\n",
    "            if row[index_session] != session:\n",
    "                if len(session_items) > 0:\n",
    "                    self.session_item_map.update({session: session_items})\n",
    "                    # cache the last time stamp of the session\n",
    "                    self.session_time.update({session: time})\n",
    "                    if time < self.min_time:\n",
    "                        self.min_time = time\n",
    "                session = row[index_session]\n",
    "                session_items = set()\n",
    "            time = row[index_time]\n",
    "            session_items.add(row[index_item])\n",
    "\n",
    "            # cache sessions involving an item\n",
    "            map_is = self.item_session_map.get(row[index_item])\n",
    "            if map_is is None:\n",
    "                map_is = set()\n",
    "                self.item_session_map.update({row[index_item]: map_is})\n",
    "            map_is.add(row[index_session])\n",
    "\n",
    "        # Add the last tuple    \n",
    "        self.session_item_map.update({session: session_items})\n",
    "        self.session_time.update({session: time})\n",
    "\n",
    "    def predict_next(self, session_id, input_item_id, predict_for_item_ids=None, skip=False, type='view', timestamp=0):\n",
    "        '''\n",
    "        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n",
    "                \n",
    "        Parameters\n",
    "        --------\n",
    "        session_id : int or string\n",
    "            The session IDs of the event.\n",
    "        input_item_id : int or string\n",
    "            The item ID of the event. Must be in the set of item IDs of the training set.\n",
    "        predict_for_item_ids : 1D array\n",
    "            IDs of items for which the network should give prediction scores. Every ID must be in the set of item IDs of the training set.\n",
    "            \n",
    "        Returns\n",
    "        --------\n",
    "        out : pandas.Series\n",
    "            Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs.\n",
    "        \n",
    "        '''\n",
    "\n",
    "        #         gc.collect()\n",
    "        #         process = psutil.Process(os.getpid())\n",
    "        #         print( 'cknn.predict_next: ', process.memory_info().rss, ' memory used')\n",
    "\n",
    "        if (self.session != session_id):  # new session\n",
    "\n",
    "            if (self.extend):\n",
    "                item_set = set(self.session_items)\n",
    "                self.session_item_map[self.session] = item_set;\n",
    "                for item in item_set:\n",
    "                    map_is = self.item_session_map.get(item)\n",
    "                    if map_is is None:\n",
    "                        map_is = set()\n",
    "                        self.item_session_map.update({item: map_is})\n",
    "                    map_is.add(self.session)\n",
    "\n",
    "                ts = time.time()\n",
    "                self.session_time.update({self.session: ts})\n",
    "\n",
    "            self.last_ts = -1\n",
    "            self.session = session_id\n",
    "            self.session_items = list()\n",
    "            self.dwelling_times = list()\n",
    "            self.relevant_sessions = set()\n",
    "\n",
    "        if type == 'view':\n",
    "            self.session_items.append(input_item_id)\n",
    "            if self.dwelling_time:\n",
    "                if self.last_ts > 0:\n",
    "                    self.dwelling_times.append(timestamp - self.last_ts)\n",
    "                self.last_ts = timestamp\n",
    "\n",
    "        if skip:\n",
    "            return\n",
    "\n",
    "        items = self.session_items if self.last_n_clicks is None else self.session_items[-self.last_n_clicks:]\n",
    "        neighbors = self.find_neighbors(items, input_item_id, session_id, self.dwelling_times, timestamp)\n",
    "        scores = self.score_items(neighbors, items, timestamp)\n",
    "\n",
    "        # Create things in the format ..\n",
    "        if predict_for_item_ids is None:\n",
    "            predict_for_item_ids = self.itemids\n",
    "        predictions = np.zeros(len(predict_for_item_ids))\n",
    "        mask = np.in1d(predict_for_item_ids, list(scores.keys()))\n",
    "\n",
    "        items = predict_for_item_ids[mask]\n",
    "        values = [scores[x] for x in items]\n",
    "        predictions[mask] = values\n",
    "        series = pd.Series(data=predictions, index=predict_for_item_ids)\n",
    "\n",
    "        if self.normalize:\n",
    "            series = series / series.max()\n",
    "\n",
    "        return series\n",
    "\n",
    "    def item_pop(self, sessions):\n",
    "        '''\n",
    "        Returns a dict(item,score) of the item popularity for the given list of sessions (only a set of ids)\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        sessions: set\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        out : dict            \n",
    "        '''\n",
    "        result = dict()\n",
    "        max_pop = 0\n",
    "        for session, weight in sessions:\n",
    "            items = self.items_for_session(session)\n",
    "            for item in items:\n",
    "\n",
    "                count = result.get(item)\n",
    "                if count is None:\n",
    "                    result.update({item: 1})\n",
    "                else:\n",
    "                    result.update({item: count + 1})\n",
    "\n",
    "                if (result.get(item) > max_pop):\n",
    "                    max_pop = result.get(item)\n",
    "\n",
    "        for key in result:\n",
    "            result.update({key: (result[key] / max_pop)})\n",
    "\n",
    "        return result\n",
    "\n",
    "    def jaccard(self, first, second):\n",
    "        '''\n",
    "        Calculates the jaccard index for two sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        sc = time.clock()\n",
    "        intersection = len(first & second)\n",
    "        union = len(first | second)\n",
    "        res = intersection / union\n",
    "\n",
    "        self.sim_time += (time.clock() - sc)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def cosine(self, first, second):\n",
    "        '''\n",
    "        Calculates the cosine similarity for two sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        li = len(first & second)\n",
    "        la = len(first)\n",
    "        lb = len(second)\n",
    "        result = li / sqrt(la) * sqrt(lb)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def tanimoto(self, first, second):\n",
    "        '''\n",
    "        Calculates the cosine tanimoto similarity for two sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        li = len(first & second)\n",
    "        la = len(first)\n",
    "        lb = len(second)\n",
    "        result = li / (la + lb - li)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def binary(self, first, second):\n",
    "        '''\n",
    "        Calculates the ? for 2 sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        a = len(first & second)\n",
    "        b = len(first)\n",
    "        c = len(second)\n",
    "\n",
    "        result = (2 * a) / ((2 * a) + b + c)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def vec(self, first, second, map):\n",
    "        '''\n",
    "        Calculates the ? for 2 sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        a = first & second\n",
    "        sum = 0\n",
    "        for i in a:\n",
    "            sum += map[i]\n",
    "\n",
    "        result = sum / len(map)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def items_for_session(self, session):\n",
    "        '''\n",
    "        Returns all items in the session\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        session: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : set           \n",
    "        '''\n",
    "        return self.session_item_map.get(session);\n",
    "\n",
    "    def vec_for_session(self, session):\n",
    "        '''\n",
    "        Returns all items in the session\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        session: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : set           \n",
    "        '''\n",
    "        return self.session_vec_map.get(session);\n",
    "\n",
    "    def sessions_for_item(self, item_id):\n",
    "        '''\n",
    "        Returns all session for an item\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        item: Id of the item session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : set           \n",
    "        '''\n",
    "        return self.item_session_map.get(item_id) if item_id in self.item_session_map else set()\n",
    "\n",
    "    def most_recent_sessions(self, sessions, number):\n",
    "        '''\n",
    "        Find the most recent sessions in the given set\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        sessions: set of session ids\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : set           \n",
    "        '''\n",
    "        sample = set()\n",
    "\n",
    "        tuples = list()\n",
    "        for session in sessions:\n",
    "            time = self.session_time.get(session)\n",
    "            if time is None:\n",
    "                print(' EMPTY TIMESTAMP!! ', session)\n",
    "            tuples.append((session, time))\n",
    "\n",
    "        tuples = sorted(tuples, key=itemgetter(1), reverse=True)\n",
    "        # print 'sorted list ', sortedList\n",
    "        cnt = 0\n",
    "        for element in tuples:\n",
    "            cnt = cnt + 1\n",
    "            if cnt > number:\n",
    "                break\n",
    "            sample.add(element[0])\n",
    "        # print 'returning sample of size ', len(sample)\n",
    "        return sample\n",
    "\n",
    "    def possible_neighbor_sessions(self, session_items, input_item_id, session_id):\n",
    "        '''\n",
    "        Find a set of session to later on find neighbors in.\n",
    "        A self.sample_size of 0 uses all sessions in which any item of the current session appears.\n",
    "        self.sampling can be performed with the options \"recent\" or \"random\".\n",
    "        \"recent\" selects the self.sample_size most recent sessions while \"random\" just choses randomly. \n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        sessions: set of session ids\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : set           \n",
    "        '''\n",
    "\n",
    "        self.relevant_sessions = self.relevant_sessions | self.sessions_for_item(input_item_id)\n",
    "\n",
    "        if self.sample_size == 0:  # use all session as possible neighbors\n",
    "\n",
    "            print('!!!!! runnig KNN without a sample size (check config)')\n",
    "            return self.relevant_sessions\n",
    "\n",
    "        else:  # sample some sessions\n",
    "\n",
    "            if len(self.relevant_sessions) > self.sample_size:\n",
    "\n",
    "                if self.sampling == 'recent':\n",
    "                    sample = self.most_recent_sessions(self.relevant_sessions, self.sample_size)\n",
    "                elif self.sampling == 'random':\n",
    "                    sample = random.sample(self.relevant_sessions, self.sample_size)\n",
    "                else:\n",
    "                    sample = self.relevant_sessions[:self.sample_size]\n",
    "\n",
    "                return sample\n",
    "            else:\n",
    "                return self.relevant_sessions\n",
    "\n",
    "    def calc_similarity(self, session_items, sessions, dwelling_times, timestamp):\n",
    "        '''\n",
    "        Calculates the configured similarity for the items in session_items and each session in sessions.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        session_items: set of item ids\n",
    "        sessions: list of session ids\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : list of tuple (session_id,similarity)           \n",
    "        '''\n",
    "\n",
    "        pos_map = {}\n",
    "        length = len(session_items)\n",
    "\n",
    "        count = 1\n",
    "        for item in session_items:\n",
    "            if self.weighting is not None:\n",
    "                pos_map[item] = getattr(self, self.weighting)(count, length)\n",
    "                count += 1\n",
    "            else:\n",
    "                pos_map[item] = 1\n",
    "\n",
    "        dt = dwelling_times.copy()\n",
    "        dt.append(0)\n",
    "        dt = pd.Series(dt, index=session_items)\n",
    "        dt = dt / dt.max()\n",
    "        # dt[session_items[-1]] = dt.mean() if len(session_items) > 1 else 1\n",
    "        dt[session_items[-1]] = 1\n",
    "\n",
    "        if self.dwelling_time:\n",
    "            # print(dt)\n",
    "            for i in range(len(dt)):\n",
    "                pos_map[session_items[i]] *= dt.iloc[i]\n",
    "            # print(pos_map)\n",
    "        # print 'nb of sessions to test ', len(sessionsToTest), ' metric: ', self.metric\n",
    "        items = set(session_items)\n",
    "        neighbors = []\n",
    "        cnt = 0\n",
    "        for session in sessions:\n",
    "            cnt = cnt + 1\n",
    "            # get items of the session, look up the cache first \n",
    "            n_items = self.items_for_session(session)\n",
    "            sts = self.session_time[session]\n",
    "\n",
    "            similarity = self.vec(items, n_items, pos_map)\n",
    "            if similarity > 0:\n",
    "\n",
    "                if self.weighting_time:\n",
    "                    diff = timestamp - sts\n",
    "                    days = round(diff / 60 / 60 / 24)\n",
    "                    decay = pow(7 / 8, days)\n",
    "                    similarity *= decay\n",
    "\n",
    "                # print(\"days:\",days,\" => \",decay)\n",
    "\n",
    "                neighbors.append((session, similarity))\n",
    "\n",
    "        return neighbors\n",
    "\n",
    "    # -----------------\n",
    "    # Find a set of neighbors, returns a list of tuples (sessionid: similarity) \n",
    "    # -----------------\n",
    "    def find_neighbors(self, session_items, input_item_id, session_id, dwelling_times, timestamp):\n",
    "        '''\n",
    "        Finds the k nearest neighbors for the given session_id and the current item input_item_id. \n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        session_items: set of item ids\n",
    "        input_item_id: int \n",
    "        session_id: int\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : list of tuple (session_id, similarity)           \n",
    "        '''\n",
    "        possible_neighbors = self.possible_neighbor_sessions(session_items, input_item_id, session_id)\n",
    "        possible_neighbors = self.calc_similarity(session_items, possible_neighbors, dwelling_times, timestamp)\n",
    "\n",
    "        possible_neighbors = sorted(possible_neighbors, reverse=True, key=lambda x: x[1])\n",
    "        possible_neighbors = possible_neighbors[:self.k]\n",
    "\n",
    "        return possible_neighbors\n",
    "\n",
    "    def score_items(self, neighbors, current_session, timestamp):\n",
    "        '''\n",
    "        Compute a set of scores for all items given a set of neighbors.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        neighbors: set of session ids\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : list of tuple (item, score)           \n",
    "        '''\n",
    "        # now we have the set of relevant items to make predictions\n",
    "        scores = dict()\n",
    "        # iterate over the sessions\n",
    "        for session in neighbors:\n",
    "            # get the items in this session\n",
    "            items = self.items_for_session(session[0])\n",
    "            step = 1\n",
    "\n",
    "            for item in reversed(current_session):\n",
    "                if item in items:\n",
    "                    decay = getattr(self, self.weighting_score)(step)\n",
    "                    break\n",
    "                step += 1\n",
    "\n",
    "            for item in items:\n",
    "                old_score = scores.get(item)\n",
    "                similarity = session[1]\n",
    "\n",
    "                if old_score is None:\n",
    "                    scores.update({item: (similarity * decay)})\n",
    "                else:\n",
    "                    new_score = old_score + (similarity * decay)\n",
    "                    scores.update({item: new_score})\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def linear_score(self, i):\n",
    "        return 1 - (0.1 * i) if i <= 100 else 0\n",
    "\n",
    "    def same_score(self, i):\n",
    "        return 1\n",
    "\n",
    "    def div_score(self, i):\n",
    "        return 1 / i\n",
    "\n",
    "    def log_score(self, i):\n",
    "        return 1 / (log10(i + 1.7))\n",
    "\n",
    "    def quadratic_score(self, i):\n",
    "        return 1 / (i * i)\n",
    "\n",
    "    def linear(self, i, length):\n",
    "        return 1 - (0.1 * (length - i)) if i <= 10 else 0\n",
    "\n",
    "    def same(self, i, length):\n",
    "        return 1\n",
    "\n",
    "    def div(self, i, length):\n",
    "        return i / length\n",
    "\n",
    "    def log(self, i, length):\n",
    "        return 1 / (log10((length - i) + 1.7))\n",
    "\n",
    "    def quadratic(self, i, length):\n",
    "        return (i / length) ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ca4d5ldIKvkT"
   },
   "outputs": [],
   "source": [
    "class SessionKNN:\n",
    "    '''\n",
    "    SessionKNN( k, sample_size=500, sampling='recent',  similarity = 'jaccard', remind=False, pop_boost=0, session_key = 'SessionId', item_key= 'ItemId')\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    k : int\n",
    "        Number of neighboring session to calculate the item scores from. (Default value: 100)\n",
    "    sample_size : int\n",
    "        Defines the length of a subset of all training sessions to calculate the nearest neighbors from. (Default value: 500)\n",
    "    sampling : string\n",
    "        String to define the sampling method for sessions (recent, random). (default: recent)\n",
    "    similarity : string\n",
    "        String to define the method for the similarity calculation (jaccard, cosine, binary, tanimoto). (default: jaccard)\n",
    "    remind : bool\n",
    "        Should the last items of the current session be boosted to the top as reminders\n",
    "    pop_boost : int\n",
    "        Push popular items in the neighbor sessions by this factor. (default: 0 to leave out)\n",
    "    extend : bool\n",
    "        Add evaluated sessions to the maps\n",
    "    normalize : bool\n",
    "        Normalize the scores in the end\n",
    "    session_key : string\n",
    "        Header of the session ID column in the input file. (default: 'SessionId')\n",
    "    item_key : string\n",
    "        Header of the item ID column in the input file. (default: 'ItemId')\n",
    "    time_key : string\n",
    "        Header of the timestamp column in the input file. (default: 'Time')\n",
    "    '''\n",
    "\n",
    "    def __init__(self, k, sample_size=1000, sampling='recent', similarity='jaccard', remind=False, pop_boost=0,\n",
    "                 extend=False, normalize=True, session_key='SessionId', item_key='ItemId', time_key='Time'):\n",
    "\n",
    "        self.remind = remind\n",
    "        self.k = k\n",
    "        self.sample_size = sample_size\n",
    "        self.sampling = sampling\n",
    "        self.similarity = similarity\n",
    "        self.pop_boost = pop_boost\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.extend = extend\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # updated while recommending\n",
    "        self.session = -1\n",
    "        self.session_items = []\n",
    "        self.relevant_sessions = set()\n",
    "\n",
    "        # cache relations once at startup\n",
    "        self.session_item_map = dict()\n",
    "        self.item_session_map = dict()\n",
    "        self.session_time = dict()\n",
    "\n",
    "        self.sim_time = 0\n",
    "\n",
    "    def fit(self, train):\n",
    "        '''\n",
    "        Trains the predictor.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        data: pandas.DataFrame\n",
    "            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n",
    "            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n",
    "            \n",
    "        '''\n",
    "\n",
    "        index_session = train.columns.get_loc(self.session_key)\n",
    "        index_item = train.columns.get_loc(self.item_key)\n",
    "        index_time = train.columns.get_loc(self.time_key)\n",
    "        self.itemids = train[self.item_key].unique()\n",
    "\n",
    "        session = -1\n",
    "        session_items = set()\n",
    "        time = -1\n",
    "        # cnt = 0\n",
    "        for row in train.itertuples(index=False):\n",
    "            # cache items of sessions\n",
    "            if row[index_session] != session:\n",
    "                if len(session_items) > 0:\n",
    "                    self.session_item_map.update({session: session_items})\n",
    "                    # cache the last time stamp of the session\n",
    "                    self.session_time.update({session: time})\n",
    "                session = row[index_session]\n",
    "                session_items = set()\n",
    "            time = row[index_time]\n",
    "            session_items.add(row[index_item])\n",
    "\n",
    "            # cache sessions involving an item\n",
    "            map_is = self.item_session_map.get(row[index_item])\n",
    "            if map_is is None:\n",
    "                map_is = set()\n",
    "                self.item_session_map.update({row[index_item]: map_is})\n",
    "            map_is.add(row[index_session])\n",
    "\n",
    "        # Add the last tuple    \n",
    "        self.session_item_map.update({session: session_items})\n",
    "        self.session_time.update({session: time})\n",
    "\n",
    "    def predict_next(self, session_id, input_item_id, predict_for_item_ids=None, skip=False, type='view', timestamp=0):\n",
    "        '''\n",
    "        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n",
    "                \n",
    "        Parameters\n",
    "        --------\n",
    "        session_id : int or string\n",
    "            The session IDs of the event.\n",
    "        input_item_id : int or string\n",
    "            The item ID of the event. Must be in the set of item IDs of the training set.\n",
    "        predict_for_item_ids : 1D array\n",
    "            IDs of items for which the network should give prediction scores. Every ID must be in the set of item IDs of the training set.\n",
    "            \n",
    "        Returns\n",
    "        --------\n",
    "        out : pandas.Series\n",
    "            Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs.\n",
    "        \n",
    "        '''\n",
    "\n",
    "        #         gc.collect()\n",
    "        #         process = psutil.Process(os.getpid())\n",
    "        #         print( 'cknn.predict_next: ', process.memory_info().rss, ' memory used')\n",
    "\n",
    "        if (self.session != session_id):  # new session\n",
    "\n",
    "            if (self.extend):\n",
    "                item_set = set(self.session_items)\n",
    "                self.session_item_map[self.session] = item_set;\n",
    "                for item in item_set:\n",
    "                    map_is = self.item_session_map.get(item)\n",
    "                    if map_is is None:\n",
    "                        map_is = set()\n",
    "                        self.item_session_map.update({item: map_is})\n",
    "                    map_is.add(self.session)\n",
    "\n",
    "                ts = time.time()\n",
    "                self.session_time.update({self.session: ts})\n",
    "\n",
    "            self.session = session_id\n",
    "            self.session_items = list()\n",
    "            self.relevant_sessions = set()\n",
    "\n",
    "        if type == 'view':\n",
    "            self.session_items.append(input_item_id)\n",
    "\n",
    "        if skip:\n",
    "            return\n",
    "\n",
    "        neighbors = self.find_neighbors(set(self.session_items), input_item_id, session_id)\n",
    "        scores = self.score_items(neighbors)\n",
    "\n",
    "        # add some reminders\n",
    "        if self.remind:\n",
    "\n",
    "            reminderScore = 5\n",
    "            takeLastN = 3\n",
    "\n",
    "            cnt = 0\n",
    "            for elem in self.session_items[-takeLastN:]:\n",
    "                cnt = cnt + 1\n",
    "                # reminderScore = reminderScore + (cnt/100)\n",
    "\n",
    "                oldScore = scores.get(elem)\n",
    "                newScore = 0\n",
    "                if oldScore is None:\n",
    "                    newScore = reminderScore\n",
    "                else:\n",
    "                    newScore = oldScore + reminderScore\n",
    "                # print 'old score ', oldScore\n",
    "                # update the score and add a small number for the position \n",
    "                newScore = (newScore * reminderScore) + (cnt / 100)\n",
    "\n",
    "                scores.update({elem: newScore})\n",
    "\n",
    "        # push popular ones\n",
    "        if self.pop_boost > 0:\n",
    "\n",
    "            pop = self.item_pop(neighbors)\n",
    "            # Iterate over the item neighbors\n",
    "            # print itemScores\n",
    "            for key in scores:\n",
    "                item_pop = pop.get(key)\n",
    "                # Gives some minimal MRR boost?\n",
    "                scores.update({key: (scores[key] + (self.pop_boost * item_pop))})\n",
    "\n",
    "        # Create things in the format ..\n",
    "        if predict_for_item_ids is None:\n",
    "            predict_for_item_ids = self.itemids\n",
    "        predictions = np.zeros(len(predict_for_item_ids))\n",
    "        mask = np.in1d(predict_for_item_ids, list(scores.keys()))\n",
    "\n",
    "        items = predict_for_item_ids[mask]\n",
    "        values = [scores[x] for x in items]\n",
    "        predictions[mask] = values\n",
    "        series = pd.Series(data=predictions, index=predict_for_item_ids)\n",
    "\n",
    "        if self.normalize:\n",
    "            series = series / series.max()\n",
    "\n",
    "        return series\n",
    "\n",
    "    def item_pop(self, sessions):\n",
    "        '''\n",
    "        Returns a dict(item,score) of the item popularity for the given list of sessions (only a set of ids)\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        sessions: set\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        out : dict            \n",
    "        '''\n",
    "        result = dict()\n",
    "        max_pop = 0\n",
    "        for session, weight in sessions:\n",
    "            items = self.items_for_session(session)\n",
    "            for item in items:\n",
    "\n",
    "                count = result.get(item)\n",
    "                if count is None:\n",
    "                    result.update({item: 1})\n",
    "                else:\n",
    "                    result.update({item: count + 1})\n",
    "\n",
    "                if (result.get(item) > max_pop):\n",
    "                    max_pop = result.get(item)\n",
    "\n",
    "        for key in result:\n",
    "            result.update({key: (result[key] / max_pop)})\n",
    "\n",
    "        return result\n",
    "\n",
    "    def jaccard(self, first, second):\n",
    "        '''\n",
    "        Calculates the jaccard index for two sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        sc = time.clock()\n",
    "        intersection = len(first & second)\n",
    "        union = len(first | second)\n",
    "        res = intersection / union\n",
    "\n",
    "        self.sim_time += (time.clock() - sc)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def cosine(self, first, second):\n",
    "        '''\n",
    "        Calculates the cosine similarity for two sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        li = len(first & second)\n",
    "        la = len(first)\n",
    "        lb = len(second)\n",
    "        result = li / sqrt(la) * sqrt(lb)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def tanimoto(self, first, second):\n",
    "        '''\n",
    "        Calculates the cosine tanimoto similarity for two sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        li = len(first & second)\n",
    "        la = len(first)\n",
    "        lb = len(second)\n",
    "        result = li / (la + lb - li)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def binary(self, first, second):\n",
    "        '''\n",
    "        Calculates the ? for 2 sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        a = len(first & second)\n",
    "        b = len(first)\n",
    "        c = len(second)\n",
    "\n",
    "        result = (2 * a) / ((2 * a) + b + c)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def random(self, first, second):\n",
    "        '''\n",
    "        Calculates the ? for 2 sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        return random.random()\n",
    "\n",
    "    def items_for_session(self, session):\n",
    "        '''\n",
    "        Returns all items in the session\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        session: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : set           \n",
    "        '''\n",
    "        return self.session_item_map.get(session);\n",
    "\n",
    "    def sessions_for_item(self, item_id):\n",
    "        '''\n",
    "        Returns all session for an item\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        item: Id of the item session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : set           \n",
    "        '''\n",
    "        return self.item_session_map.get(item_id)\n",
    "\n",
    "    def most_recent_sessions(self, sessions, number):\n",
    "        '''\n",
    "        Find the most recent sessions in the given set\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        sessions: set of session ids\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : set           \n",
    "        '''\n",
    "        sample = set()\n",
    "\n",
    "        tuples = list()\n",
    "        for session in sessions:\n",
    "            time = self.session_time.get(session)\n",
    "            if time is None:\n",
    "                print(' EMPTY TIMESTAMP!! ', session)\n",
    "            tuples.append((session, time))\n",
    "\n",
    "        tuples = sorted(tuples, key=itemgetter(1), reverse=True)\n",
    "        # print 'sorted list ', sortedList\n",
    "        cnt = 0\n",
    "        for element in tuples:\n",
    "            cnt = cnt + 1\n",
    "            if cnt > number:\n",
    "                break\n",
    "            sample.add(element[0])\n",
    "        # print 'returning sample of size ', len(sample)\n",
    "        return sample\n",
    "\n",
    "    def possible_neighbor_sessions(self, session_items, input_item_id, session_id):\n",
    "        '''\n",
    "        Find a set of session to later on find neighbors in.\n",
    "        A self.sample_size of 0 uses all sessions in which any item of the current session appears.\n",
    "        self.sampling can be performed with the options \"recent\" or \"random\".\n",
    "        \"recent\" selects the self.sample_size most recent sessions while \"random\" just choses randomly. \n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        sessions: set of session ids\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : set           \n",
    "        '''\n",
    "\n",
    "        self.relevant_sessions = self.relevant_sessions | self.sessions_for_item(input_item_id);\n",
    "\n",
    "        if self.sample_size == 0:  # use all session as possible neighbors\n",
    "\n",
    "            print('!!!!! runnig KNN without a sample size (check config)')\n",
    "            return self.relevant_sessions\n",
    "\n",
    "        else:  # sample some sessions\n",
    "\n",
    "            self.relevant_sessions = self.relevant_sessions | self.sessions_for_item(input_item_id);\n",
    "\n",
    "            if len(self.relevant_sessions) > self.sample_size:\n",
    "\n",
    "                if self.sampling == 'recent':\n",
    "                    sample = self.most_recent_sessions(self.relevant_sessions, self.sample_size)\n",
    "                elif self.sampling == 'random':\n",
    "                    sample = random.sample(self.relevant_sessions, self.sample_size)\n",
    "                else:\n",
    "                    sample = self.relevant_sessions[:self.sample_size]\n",
    "\n",
    "                return sample\n",
    "            else:\n",
    "                return self.relevant_sessions\n",
    "\n",
    "    def calc_similarity(self, session_items, sessions):\n",
    "        '''\n",
    "        Calculates the configured similarity for the items in session_items and each session in sessions.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        session_items: set of item ids\n",
    "        sessions: list of session ids\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : list of tuple (session_id,similarity)           \n",
    "        '''\n",
    "\n",
    "        # print 'nb of sessions to test ', len(sessionsToTest), ' metric: ', self.metric\n",
    "        neighbors = []\n",
    "        cnt = 0\n",
    "        for session in sessions:\n",
    "            cnt = cnt + 1\n",
    "            # get items of the session, look up the cache first \n",
    "            session_items_test = self.items_for_session(session)\n",
    "\n",
    "            similarity = getattr(self, self.similarity)(session_items_test, session_items)\n",
    "            if similarity > 0:\n",
    "                neighbors.append((session, similarity))\n",
    "\n",
    "        return neighbors\n",
    "\n",
    "    # -----------------\n",
    "    # Find a set of neighbors, returns a list of tuples (sessionid: similarity) \n",
    "    # -----------------\n",
    "    def find_neighbors(self, session_items, input_item_id, session_id):\n",
    "        '''\n",
    "        Finds the k nearest neighbors for the given session_id and the current item input_item_id. \n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        session_items: set of item ids\n",
    "        input_item_id: int \n",
    "        session_id: int\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : list of tuple (session_id, similarity)           \n",
    "        '''\n",
    "        possible_neighbors = self.possible_neighbor_sessions(session_items, input_item_id, session_id)\n",
    "        possible_neighbors = self.calc_similarity(session_items, possible_neighbors)\n",
    "\n",
    "        possible_neighbors = sorted(possible_neighbors, reverse=True, key=lambda x: x[1])\n",
    "        possible_neighbors = possible_neighbors[:self.k]\n",
    "\n",
    "        return possible_neighbors\n",
    "\n",
    "    def score_items(self, neighbors):\n",
    "        '''\n",
    "        Compute a set of scores for all items given a set of neighbors.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        neighbors: set of session ids\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : list of tuple (item, score)           \n",
    "        '''\n",
    "        # now we have the set of relevant items to make predictions\n",
    "        scores = dict()\n",
    "        # iterate over the sessions\n",
    "        for session in neighbors:\n",
    "            # get the items in this session\n",
    "            items = self.items_for_session(session[0])\n",
    "\n",
    "            for item in items:\n",
    "                old_score = scores.get(item)\n",
    "                new_score = session[1]\n",
    "\n",
    "                if old_score is None:\n",
    "                    scores.update({item: new_score})\n",
    "                else:\n",
    "                    new_score = old_score + new_score\n",
    "                    scores.update({item: new_score})\n",
    "\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDo_oIxeKvhN"
   },
   "outputs": [],
   "source": [
    "class SeqSessionKNN:\n",
    "    '''\n",
    "    SeqSessionKNN( k, sample_size=500, sampling='recent',  similarity = 'jaccard', remind=False, pop_boost=0, session_key = 'SessionId', item_key= 'ItemId')\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    k : int\n",
    "        Number of neighboring session to calculate the item scores from. (Default value: 100)\n",
    "    sample_size : int\n",
    "        Defines the length of a subset of all training sessions to calculate the nearest neighbors from. (Default value: 500)\n",
    "    sampling : string\n",
    "        String to define the sampling method for sessions (recent, random). (default: recent)\n",
    "    similarity : string\n",
    "        String to define the method for the similarity calculation (jaccard, cosine, binary, tanimoto). (default: jaccard)\n",
    "    remind : bool\n",
    "        Should the last items of the current session be boosted to the top as reminders\n",
    "    pop_boost : int\n",
    "        Push popular items in the neighbor sessions by this factor. (default: 0 to leave out)\n",
    "    extend : bool\n",
    "        Add evaluated sessions to the maps\n",
    "    normalize : bool\n",
    "        Normalize the scores in the end\n",
    "    session_key : string\n",
    "        Header of the session ID column in the input file. (default: 'SessionId')\n",
    "    item_key : string\n",
    "        Header of the item ID column in the input file. (default: 'ItemId')\n",
    "    time_key : string\n",
    "        Header of the timestamp column in the input file. (default: 'Time')\n",
    "    '''\n",
    "\n",
    "    def __init__(self, k, sample_size=1000, sampling='recent', similarity='jaccard', weighting='div', remind=False,\n",
    "                 pop_boost=0, extend=False, normalize=True, session_key='SessionId', item_key='ItemId',\n",
    "                 time_key='Time'):\n",
    "\n",
    "        self.remind = remind\n",
    "        self.k = k\n",
    "        self.sample_size = sample_size\n",
    "        self.sampling = sampling\n",
    "        self.weighting = weighting\n",
    "        self.similarity = similarity\n",
    "        self.pop_boost = pop_boost\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.extend = extend\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # updated while recommending\n",
    "        self.session = -1\n",
    "        self.session_items = []\n",
    "        self.relevant_sessions = set()\n",
    "\n",
    "        # cache relations once at startup\n",
    "        self.session_item_map = dict()\n",
    "        self.item_session_map = dict()\n",
    "        self.session_time = dict()\n",
    "\n",
    "        self.sim_time = 0\n",
    "\n",
    "    def fit(self, train, items=None):\n",
    "        '''\n",
    "        Trains the predictor.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        data: pandas.DataFrame\n",
    "            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n",
    "            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n",
    "            \n",
    "        '''\n",
    "\n",
    "        index_session = train.columns.get_loc(self.session_key)\n",
    "        index_item = train.columns.get_loc(self.item_key)\n",
    "        index_time = train.columns.get_loc(self.time_key)\n",
    "        self.itemids = train[self.item_key].unique()\n",
    "\n",
    "        session = -1\n",
    "        session_items = set()\n",
    "        time = -1\n",
    "        # cnt = 0\n",
    "        for row in train.itertuples(index=False):\n",
    "            # cache items of sessions\n",
    "            if row[index_session] != session:\n",
    "                if len(session_items) > 0:\n",
    "                    self.session_item_map.update({session: session_items})\n",
    "                    # cache the last time stamp of the session\n",
    "                    self.session_time.update({session: time})\n",
    "                session = row[index_session]\n",
    "                session_items = set()\n",
    "            time = row[index_time]\n",
    "            session_items.add(row[index_item])\n",
    "\n",
    "            # cache sessions involving an item\n",
    "            map_is = self.item_session_map.get(row[index_item])\n",
    "            if map_is is None:\n",
    "                map_is = set()\n",
    "                self.item_session_map.update({row[index_item]: map_is})\n",
    "            map_is.add(row[index_session])\n",
    "\n",
    "        # Add the last tuple    \n",
    "        self.session_item_map.update({session: session_items})\n",
    "        self.session_time.update({session: time})\n",
    "\n",
    "    def predict_next(self, session_id, input_item_id, predict_for_item_ids=None, skip=False, type='view', timestamp=0):\n",
    "        '''\n",
    "        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n",
    "                \n",
    "        Parameters\n",
    "        --------\n",
    "        session_id : int or string\n",
    "            The session IDs of the event.\n",
    "        input_item_id : int or string\n",
    "            The item ID of the event. Must be in the set of item IDs of the training set.\n",
    "        predict_for_item_ids : 1D array\n",
    "            IDs of items for which the network should give prediction scores. Every ID must be in the set of item IDs of the training set.\n",
    "            \n",
    "        Returns\n",
    "        --------\n",
    "        out : pandas.Series\n",
    "            Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs.\n",
    "        \n",
    "        '''\n",
    "\n",
    "        #         gc.collect()\n",
    "        #         process = psutil.Process(os.getpid())\n",
    "        #         print( 'cknn.predict_next: ', process.memory_info().rss, ' memory used')\n",
    "\n",
    "        if (self.session != session_id):  # new session\n",
    "\n",
    "            if (self.extend):\n",
    "                item_set = set(self.session_items)\n",
    "                self.session_item_map[self.session] = item_set\n",
    "                for item in item_set:\n",
    "                    map_is = self.item_session_map.get(item)\n",
    "                    if map_is is None:\n",
    "                        map_is = set()\n",
    "                        self.item_session_map.update({item: map_is})\n",
    "                    map_is.add(self.session)\n",
    "\n",
    "                ts = time.time()\n",
    "                self.session_time.update({self.session: ts})\n",
    "\n",
    "            self.session = session_id\n",
    "            self.session_items = list()\n",
    "            self.relevant_sessions = set()\n",
    "\n",
    "        if type == 'view':\n",
    "            self.session_items.append(input_item_id)\n",
    "\n",
    "        if skip:\n",
    "            return\n",
    "\n",
    "        neighbors = self.find_neighbors(set(self.session_items), input_item_id, session_id)\n",
    "        scores = self.score_items(neighbors, self.session_items)\n",
    "\n",
    "        # add some reminders\n",
    "        if self.remind:\n",
    "\n",
    "            reminderScore = 5\n",
    "            takeLastN = 3\n",
    "\n",
    "            cnt = 0\n",
    "            for elem in self.session_items[-takeLastN:]:\n",
    "                cnt = cnt + 1\n",
    "                # reminderScore = reminderScore + (cnt/100)\n",
    "\n",
    "                oldScore = scores.get(elem)\n",
    "                newScore = 0\n",
    "                if oldScore is None:\n",
    "                    newScore = reminderScore\n",
    "                else:\n",
    "                    newScore = oldScore + reminderScore\n",
    "                # print 'old score ', oldScore\n",
    "                # update the score and add a small number for the position \n",
    "                newScore = (newScore * reminderScore) + (cnt / 100)\n",
    "\n",
    "                scores.update({elem: newScore})\n",
    "\n",
    "        # push popular ones\n",
    "        if self.pop_boost > 0:\n",
    "\n",
    "            pop = self.item_pop(neighbors)\n",
    "            # Iterate over the item neighbors\n",
    "            # print itemScores\n",
    "            for key in scores:\n",
    "                item_pop = pop.get(key)\n",
    "                # Gives some minimal MRR boost?\n",
    "                scores.update({key: (scores[key] + (self.pop_boost * item_pop))})\n",
    "\n",
    "        # Create things in the format ..\n",
    "        if predict_for_item_ids is None:\n",
    "            predict_for_item_ids = self.itemids\n",
    "        predictions = np.zeros(len(predict_for_item_ids))\n",
    "        mask = np.in1d(predict_for_item_ids, list(scores.keys()))\n",
    "\n",
    "        items = predict_for_item_ids[mask]\n",
    "        values = [scores[x] for x in items]\n",
    "        predictions[mask] = values\n",
    "        series = pd.Series(data=predictions, index=predict_for_item_ids)\n",
    "\n",
    "        if self.normalize:\n",
    "            series = series / series.max()\n",
    "\n",
    "        return series\n",
    "\n",
    "    def item_pop(self, sessions):\n",
    "        '''\n",
    "        Returns a dict(item,score) of the item popularity for the given list of sessions (only a set of ids)\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        sessions: set\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        out : dict            \n",
    "        '''\n",
    "        result = dict()\n",
    "        max_pop = 0\n",
    "        for session, weight in sessions:\n",
    "            items = self.items_for_session(session)\n",
    "            for item in items:\n",
    "\n",
    "                count = result.get(item)\n",
    "                if count is None:\n",
    "                    result.update({item: 1})\n",
    "                else:\n",
    "                    result.update({item: count + 1})\n",
    "\n",
    "                if (result.get(item) > max_pop):\n",
    "                    max_pop = result.get(item)\n",
    "\n",
    "        for key in result:\n",
    "            result.update({key: (result[key] / max_pop)})\n",
    "\n",
    "        return result\n",
    "\n",
    "    def jaccard(self, first, second):\n",
    "        '''\n",
    "        Calculates the jaccard index for two sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        sc = time.clock()\n",
    "        intersection = len(first & second)\n",
    "        union = len(first | second)\n",
    "        res = intersection / union\n",
    "\n",
    "        self.sim_time += (time.clock() - sc)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def cosine(self, first, second):\n",
    "        '''\n",
    "        Calculates the cosine similarity for two sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        li = len(first & second)\n",
    "        la = len(first)\n",
    "        lb = len(second)\n",
    "        result = li / sqrt(la) * sqrt(lb)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def tanimoto(self, first, second):\n",
    "        '''\n",
    "        Calculates the cosine tanimoto similarity for two sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        li = len(first & second)\n",
    "        la = len(first)\n",
    "        lb = len(second)\n",
    "        result = li / (la + lb - li)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def binary(self, first, second):\n",
    "        '''\n",
    "        Calculates the ? for 2 sessions\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        first: Id of a session\n",
    "        second: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : float value           \n",
    "        '''\n",
    "        a = len(first & second)\n",
    "        b = len(first)\n",
    "        c = len(second)\n",
    "\n",
    "        result = (2 * a) / ((2 * a) + b + c)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def items_for_session(self, session):\n",
    "        '''\n",
    "        Returns all items in the session\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        session: Id of a session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : set           \n",
    "        '''\n",
    "        return self.session_item_map.get(session);\n",
    "\n",
    "    def sessions_for_item(self, item_id):\n",
    "        '''\n",
    "        Returns all session for an item\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        item: Id of the item session\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : set           \n",
    "        '''\n",
    "        return self.item_session_map.get(item_id)\n",
    "\n",
    "    def most_recent_sessions(self, sessions, number):\n",
    "        '''\n",
    "        Find the most recent sessions in the given set\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        sessions: set of session ids\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : set           \n",
    "        '''\n",
    "        sample = set()\n",
    "\n",
    "        tuples = list()\n",
    "        for session in sessions:\n",
    "            time = self.session_time.get(session)\n",
    "            if time is None:\n",
    "                print(' EMPTY TIMESTAMP!! ', session)\n",
    "            tuples.append((session, time))\n",
    "\n",
    "        tuples = sorted(tuples, key=itemgetter(1), reverse=True)\n",
    "        # print 'sorted list ', sortedList\n",
    "        cnt = 0\n",
    "        for element in tuples:\n",
    "            cnt = cnt + 1\n",
    "            if cnt > number:\n",
    "                break\n",
    "            sample.add(element[0])\n",
    "        # print 'returning sample of size ', len(sample)\n",
    "        return sample\n",
    "\n",
    "    def possible_neighbor_sessions(self, session_items, input_item_id, session_id):\n",
    "        '''\n",
    "        Find a set of session to later on find neighbors in.\n",
    "        A self.sample_size of 0 uses all sessions in which any item of the current session appears.\n",
    "        self.sampling can be performed with the options \"recent\" or \"random\".\n",
    "        \"recent\" selects the self.sample_size most recent sessions while \"random\" just choses randomly. \n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        sessions: set of session ids\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : set           \n",
    "        '''\n",
    "\n",
    "        self.relevant_sessions = self.relevant_sessions | self.sessions_for_item(input_item_id);\n",
    "\n",
    "        if self.sample_size == 0:  # use all session as possible neighbors\n",
    "\n",
    "            print('!!!!! runnig KNN without a sample size (check config)')\n",
    "            return self.relevant_sessions\n",
    "\n",
    "        else:  # sample some sessions\n",
    "\n",
    "            if len(self.relevant_sessions) > self.sample_size:\n",
    "\n",
    "                if self.sampling == 'recent':\n",
    "                    sample = self.most_recent_sessions(self.relevant_sessions, self.sample_size)\n",
    "                elif self.sampling == 'random':\n",
    "                    sample = random.sample(self.relevant_sessions, self.sample_size)\n",
    "                else:\n",
    "                    sample = self.relevant_sessions[:self.sample_size]\n",
    "\n",
    "                return sample\n",
    "            else:\n",
    "                return self.relevant_sessions\n",
    "\n",
    "    def calc_similarity(self, session_items, sessions):\n",
    "        '''\n",
    "        Calculates the configured similarity for the items in session_items and each session in sessions.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        session_items: set of item ids\n",
    "        sessions: list of session ids\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : list of tuple (session_id,similarity)           \n",
    "        '''\n",
    "\n",
    "        # print 'nb of sessions to test ', len(sessionsToTest), ' metric: ', self.metric\n",
    "        neighbors = []\n",
    "        cnt = 0\n",
    "        for session in sessions:\n",
    "            cnt = cnt + 1\n",
    "            # get items of the session, look up the cache first \n",
    "            session_items_test = self.items_for_session(session)\n",
    "\n",
    "            similarity = getattr(self, self.similarity)(session_items_test, session_items)\n",
    "            if similarity > 0:\n",
    "                neighbors.append((session, similarity))\n",
    "\n",
    "        return neighbors\n",
    "\n",
    "    # -----------------\n",
    "    # Find a set of neighbors, returns a list of tuples (sessionid: similarity) \n",
    "    # -----------------\n",
    "    def find_neighbors(self, session_items, input_item_id, session_id):\n",
    "        '''\n",
    "        Finds the k nearest neighbors for the given session_id and the current item input_item_id. \n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        session_items: set of item ids\n",
    "        input_item_id: int \n",
    "        session_id: int\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : list of tuple (session_id, similarity)           \n",
    "        '''\n",
    "        possible_neighbors = self.possible_neighbor_sessions(session_items, input_item_id, session_id)\n",
    "        possible_neighbors = self.calc_similarity(session_items, possible_neighbors)\n",
    "\n",
    "        possible_neighbors = sorted(possible_neighbors, reverse=True, key=lambda x: x[1])\n",
    "        possible_neighbors = possible_neighbors[:self.k]\n",
    "\n",
    "        return possible_neighbors\n",
    "\n",
    "    def score_items(self, neighbors, current_session):\n",
    "        '''\n",
    "        Compute a set of scores for all items given a set of neighbors.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        neighbors: set of session ids\n",
    "        \n",
    "        Returns \n",
    "        --------\n",
    "        out : list of tuple (item, score)           \n",
    "        '''\n",
    "        # now we have the set of relevant items to make predictions\n",
    "        scores = dict()\n",
    "        # iterate over the sessions\n",
    "        for session in neighbors:\n",
    "            # get the items in this session\n",
    "            items = self.items_for_session(session[0])\n",
    "            step = 1\n",
    "\n",
    "            for item in reversed(current_session):\n",
    "                if item in items:\n",
    "                    decay = getattr(self, self.weighting)(step)\n",
    "                    break\n",
    "                step += 1\n",
    "\n",
    "            for item in items:\n",
    "                old_score = scores.get(item)\n",
    "                similarity = session[1]\n",
    "\n",
    "                if old_score is None:\n",
    "                    scores.update({item: (similarity * decay)})\n",
    "                else:\n",
    "                    new_score = old_score + (similarity * decay)\n",
    "                    scores.update({item: new_score})\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def linear(self, i):\n",
    "        return 1 - (0.1 * i) if i <= 100 else 0\n",
    "\n",
    "    def same(self, i):\n",
    "        return 1\n",
    "\n",
    "    def div(self, i):\n",
    "        return 1 / i\n",
    "\n",
    "    def log(self, i):\n",
    "        return 1 / (log10(i + 1.7))\n",
    "\n",
    "    def quadratic(self, i):\n",
    "        return 1 / (i * i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlgAz-sSKQSQ"
   },
   "outputs": [],
   "source": [
    "class KNNRecommender(ISeqRecommender):\n",
    "    \"\"\"\n",
    "    Interface to ItemKNN and Session-based KNN methods. Based on:\n",
    "\n",
    "    Evaluation of Session-based Recommendation Algorithms, Malte Ludewig and Dietmar Jannach\n",
    "    \"\"\"\n",
    "    knn_models = {\n",
    "        'iknn': ItemKNN,\n",
    "        'sknn': SessionKNN,\n",
    "        'v-sknn': VMSessionKNN,\n",
    "        's-sknn': SeqSessionKNN,\n",
    "        'sf-sknn': SeqFilterSessionKNN\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 model='cknn',\n",
    "                 **init_args):\n",
    "        \"\"\"\n",
    "        :param model: One among the following KNN models:\n",
    "            - iknn: ItemKNN, item-to-item KNN based on the *last* item in the session to determine the items to be recommended.\n",
    "            - sknn: SessionKNN, compares the *entire* current session with the past sessions in the training data to\n",
    "                    determine the items to be recommended.\n",
    "            - v-sknn: VMSessionKNN, use linearly decayed real-valued vectors to encode the current session,\n",
    "                    then compares the current session with the past sessions in the training data using the dot-product\n",
    "                    to determine the items to be recommended.\n",
    "            - s-sknn: SeqSessionKNN, this variant also puts more weight on elements that appear later in the session by\n",
    "                using a custom scoring function (see the paper by Ludewng and Jannach).\n",
    "            - sf-sknn: SeqFilterSessionKNN, this variant also puts more weight on elements that appear later in the session\n",
    "                in a more restrictive way by using a custom scoring function (see the paper by Ludewng and Jannach).\n",
    "\n",
    "        :param init_args: The model initialization arguments. See the following initializations or\n",
    "            check `util.knn` for more details on each model:\n",
    "            - iknn: ItemKNN(n_sims=100, lmbd=20, alpha=0.5)\n",
    "            - sknn: SessionKNN(k, sample_size=500, sampling='recent', similarity='jaccard', remind=False, pop_boost=0)\n",
    "            - v-sknn: VMSessionKNN(k, sample_size=1000, sampling='recent', similarity='cosine', weighting='div',\n",
    "                 dwelling_time=False, last_n_days=None, last_n_clicks=None, extend=False, weighting_score='div_score',\n",
    "                 weighting_time=False, normalize=True)\n",
    "            - s-knn: SeqSessionKNN(k, sample_size=1000, sampling='recent', similarity='jaccard', weighting='div',\n",
    "                remind=False, pop_boost=0, extend=False, normalize=True)\n",
    "            - sf-sknn: SeqFilterSessionKNN(k, sample_size=1000, sampling='recent', similarity='jaccard', remind=False, pop_boost=0,\n",
    "                 extend=False, normalize=True)\n",
    "        \"\"\"\n",
    "        super(KNNRecommender).__init__()\n",
    "        if model not in self.knn_models:\n",
    "            raise ValueError(\"Unknown KNN model '{}'. The available ones are: {}\".format(\n",
    "                model, list(self.knn_models.keys())\n",
    "            ))\n",
    "        self.init_args = init_args\n",
    "        self.init_args.update(dict(session_key='session_id',\n",
    "                                   item_key='item_id',\n",
    "                                   time_key='ts'))\n",
    "        self.model = self.knn_models[model](**self.init_args)\n",
    "        self.pseudo_session_id = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.model)\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        self.logger.info('Converting training data to GRU4Rec format')\n",
    "        # parse training data to GRU4Rec format\n",
    "        train_data = dataset_to_gru4rec_format(dataset=train_data)\n",
    "\n",
    "        self.logger.info('Training started')\n",
    "        self.model.fit(train_data)\n",
    "        self.logger.info('Training completed')\n",
    "        self.pseudo_session_id = 0\n",
    "\n",
    "    def recommend(self, user_profile, user_id=None):\n",
    "        for item in user_profile:\n",
    "            pred = self.model.predict_next(session_id=self.pseudo_session_id,\n",
    "                                           input_item_id=item)\n",
    "        # sort items by predicted score\n",
    "        pred.sort_values(0, ascending=False, inplace=True)\n",
    "        # increase the psuedo-session id so that future call to recommend() won't be connected\n",
    "        self.pseudo_session_id += 1\n",
    "        # convert to the required output format\n",
    "        return [([x.index], x._2) for x in pred.reset_index().itertuples()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HsMa77CGKHQi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 14:10:51,373 - INFO - Converting training data to GRU4Rec format\n",
      "2021-04-25 14:10:51,398 - INFO - Training started\n",
      "2021-04-25 14:10:51,429 - INFO - Training completed\n"
     ]
    }
   ],
   "source": [
    "knnrecommender = KNNRecommender(model='sknn', k=10)\n",
    "knnrecommender.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvnL5bbiStl5"
   },
   "source": [
    "## Sequential Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8DUB_hPtvKg"
   },
   "source": [
    "In the evaluation of sequence-aware recommenders, each sequence in the test set is split into:\n",
    "- the _user profile_, used to compute recommendations, is composed by the first *k* events in the sequence;\n",
    "- the _ground truth_, used for performance evaluation, is composed by the remainder of the sequence.\n",
    "\n",
    "In the cells below, you can control the dimension of the _user profile_ by assigning a **positive** value to `GIVEN_K`, which correspond to the number of events from the beginning of the sequence that will be assigned to the initial user profile. This ensures that each user profile in the test set will have exactly the same initial size, but the size of the ground truth will change for every sequence.\n",
    "\n",
    "Alternatively, by assigning a **negative** value to `GIVEN_K`, you will set the initial size of the _ground truth_. In this way the _ground truth_ will have the same size for all sequences, but the dimension of the user profile will differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNbyaQ0Muk5U"
   },
   "outputs": [],
   "source": [
    "def precision(ground_truth, prediction):\n",
    "    \"\"\"\n",
    "    Compute Precision metric\n",
    "    :param ground_truth: the ground truth set or sequence\n",
    "    :param prediction: the predicted set or sequence\n",
    "    :return: the value of the metric\n",
    "    \"\"\"\n",
    "    ground_truth = remove_duplicates(ground_truth)\n",
    "    prediction = remove_duplicates(prediction)\n",
    "    precision_score = count_a_in_b_unique(prediction, ground_truth) / float(len(prediction))\n",
    "    assert 0 <= precision_score <= 1\n",
    "    return precision_score\n",
    "\n",
    "\n",
    "def recall(ground_truth, prediction):\n",
    "    \"\"\"\n",
    "    Compute Recall metric\n",
    "    :param ground_truth: the ground truth set or sequence\n",
    "    :param prediction: the predicted set or sequence\n",
    "    :return: the value of the metric\n",
    "    \"\"\"\n",
    "    ground_truth = remove_duplicates(ground_truth)\n",
    "    prediction = remove_duplicates(prediction)\n",
    "    recall_score = 0 if len(prediction) == 0 else count_a_in_b_unique(prediction, ground_truth) / float(\n",
    "        len(ground_truth))\n",
    "    assert 0 <= recall_score <= 1\n",
    "    return recall_score\n",
    "\n",
    "\n",
    "def mrr(ground_truth, prediction):\n",
    "    \"\"\"\n",
    "    Compute Mean Reciprocal Rank metric. Reciprocal Rank is set 0 if no predicted item is in contained the ground truth.\n",
    "    :param ground_truth: the ground truth set or sequence\n",
    "    :param prediction: the predicted set or sequence\n",
    "    :return: the value of the metric\n",
    "    \"\"\"\n",
    "    rr = 0.\n",
    "    for rank, p in enumerate(prediction):\n",
    "        if p in ground_truth:\n",
    "            rr = 1. / (rank + 1)\n",
    "            break\n",
    "    return rr\n",
    "\n",
    "\n",
    "def count_a_in_b_unique(a, b):\n",
    "    \"\"\"\n",
    "    :param a: list of lists\n",
    "    :param b: list of lists\n",
    "    :return: number of elements of a in b\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for el in a:\n",
    "        if el in b:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def remove_duplicates(l):\n",
    "    return [list(x) for x in set(tuple(x) for x in l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buCSWNqXhLE7"
   },
   "outputs": [],
   "source": [
    "METRICS = {'precision':precision, \n",
    "           'recall':recall,\n",
    "           'mrr': mrr}\n",
    "TOPN=100 # length of the recommendation list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TJUt7x-ubU8"
   },
   "outputs": [],
   "source": [
    "def sequential_evaluation(recommender,\n",
    "                          test_sequences,\n",
    "                          evaluation_functions,\n",
    "                          users=None,\n",
    "                          given_k=1,\n",
    "                          look_ahead=1,\n",
    "                          top_n=10,\n",
    "                          scroll=True,\n",
    "                          step=1):\n",
    "    \"\"\"\n",
    "    Runs sequential evaluation of a recommender over a set of test sequences\n",
    "    :param recommender: the instance of the recommender to test\n",
    "    :param test_sequences: the set of test sequences\n",
    "    :param evaluation_functions: list of evaluation metric functions\n",
    "    :param users: (optional) the list of user ids associated to each test sequence. Required by personalized models like FPMC.\n",
    "    :param given_k: (optional) the initial size of each user profile, starting from the first interaction in the sequence.\n",
    "                    If <0, start counting from the end of the sequence. It must be != 0.\n",
    "    :param look_ahead: (optional) number of subsequent interactions in the sequence to be considered as ground truth.\n",
    "                    It can be any positive number or 'all' to extend the ground truth until the end of the sequence.\n",
    "    :param top_n: (optional) size of the recommendation list\n",
    "    :param scroll: (optional) whether to scroll the ground truth until the end of the sequence.\n",
    "                If True, expand the user profile and move the ground truth forward of `step` interactions. Recompute and evaluate recommendations every time.\n",
    "                If False, evaluate recommendations once per sequence without expanding the user profile.\n",
    "    :param step: (optional) number of interactions that will be added to the user profile at each step of the sequential evaluation.\n",
    "    :return: the list of the average values for each evaluation metric\n",
    "    \"\"\"\n",
    "    if given_k == 0:\n",
    "        raise ValueError('given_k must be != 0')\n",
    "\n",
    "    metrics = np.zeros(len(evaluation_functions))\n",
    "    with tqdm(total=len(test_sequences)) as pbar:\n",
    "        for i, test_seq in enumerate(test_sequences):\n",
    "            if users is not None:\n",
    "                user = users[i]\n",
    "            else:\n",
    "                user = None\n",
    "            if scroll:\n",
    "                metrics += sequence_sequential_evaluation(recommender,\n",
    "                                                          test_seq,\n",
    "                                                          evaluation_functions,\n",
    "                                                          user,\n",
    "                                                          given_k,\n",
    "                                                          look_ahead,\n",
    "                                                          top_n,\n",
    "                                                          step)\n",
    "            else:\n",
    "                metrics += evaluate_sequence(recommender,\n",
    "                                             test_seq,\n",
    "                                             evaluation_functions,\n",
    "                                             user,\n",
    "                                             given_k,\n",
    "                                             look_ahead,\n",
    "                                             top_n)\n",
    "            pbar.update(1)\n",
    "    return metrics / len(test_sequences)\n",
    "\n",
    "\n",
    "def evaluate_sequence(recommender, seq, evaluation_functions, user, given_k, look_ahead, top_n):\n",
    "    \"\"\"\n",
    "    :param recommender: which recommender to use\n",
    "    :param seq: the user_profile/ context\n",
    "    :param given_k: last element used as ground truth. NB if <0 it is interpreted as first elements to keep\n",
    "    :param evaluation_functions: which function to use to evaluate the rec performance\n",
    "    :param look_ahead: number of elements in ground truth to consider. if look_ahead = 'all' then all the ground_truth sequence is considered\n",
    "    :return: performance of recommender\n",
    "    \"\"\"\n",
    "    # safety checks\n",
    "    if given_k < 0:\n",
    "        given_k = len(seq) + given_k\n",
    "\n",
    "    user_profile = seq[:given_k]\n",
    "    ground_truth = seq[given_k:]\n",
    "\n",
    "    # restrict ground truth to look_ahead\n",
    "    ground_truth = ground_truth[:look_ahead] if look_ahead != 'all' else ground_truth\n",
    "    ground_truth = list(map(lambda x: [x], ground_truth))  # list of list format\n",
    "\n",
    "    if not user_profile or not ground_truth:\n",
    "        # if any of the two missing all evaluation functions are 0\n",
    "        return np.zeros(len(evaluation_functions))\n",
    "\n",
    "    r = recommender.recommend(user_profile, user)[:top_n]\n",
    "\n",
    "    if not r:\n",
    "        # no recommendation found\n",
    "        return np.zeros(len(evaluation_functions))\n",
    "    reco_list = recommender.get_recommendation_list(r)\n",
    "\n",
    "    tmp_results = []\n",
    "    for f in evaluation_functions:\n",
    "        tmp_results.append(f(ground_truth, reco_list))\n",
    "    return np.array(tmp_results)\n",
    "\n",
    "\n",
    "def sequence_sequential_evaluation(recommender, seq, evaluation_functions, user, given_k, look_ahead, top_n, step):\n",
    "    if given_k < 0:\n",
    "        given_k = len(seq) + given_k\n",
    "\n",
    "    eval_res = 0.0\n",
    "    eval_cnt = 0\n",
    "    for gk in range(given_k, len(seq), step):\n",
    "        eval_res += evaluate_sequence(recommender, seq, evaluation_functions, user, gk, look_ahead, top_n)\n",
    "        eval_cnt += 1\n",
    "    return eval_res / eval_cnt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rE0qQg1MuEx5"
   },
   "source": [
    "### Evaluation with sequentially revealed user-profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnW4qI4RS6DB"
   },
   "source": [
    "Here we evaluate the quality of the recommendations in a setting in which user profiles are revealed _sequentially_.\n",
    "\n",
    "The _user profile_ starts from the first `GIVEN_K` events (or, alternatively, from the last `-GIVEN_K` events if `GIVEN_K<0`).  \n",
    "The recommendations are evaluated against the next `LOOK_AHEAD` events (the _ground truth_).  \n",
    "The _user profile_ is next expanded to the next `STEP` events, the ground truth is scrolled forward accordingly, and the evaluation continues until the sequence ends.\n",
    "\n",
    "In typical **next-item recommendation**, we start with `GIVEN_K=1`, generate a set of **alternatives** that will evaluated against the next event in the sequence (`LOOK_AHEAD=1`), move forward of one step (`STEP=1`) and repeat until the sequence ends.\n",
    "\n",
    "You can set the `LOOK_AHEAD='all'` to see what happens if you had to recommend a **whole sequence** instead of a set of a set of alternatives to a user.\n",
    "\n",
    "NOTE: Metrics are averaged over each sequence first, then averaged over all test sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYQz-aKXj_50"
   },
   "source": [
    "<img src='https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4b67fedc-22b5-4445-ad22-49e32f9445e6%2FUntitled.png?table=block&id=e35f084b-10b0-47d4-bd59-c336f459217d&spaceId=63b72b1f-0e90-4ab8-a6df-a060a6545a56&width=2000&userId=21ec183f-f0be-4b6b-9b3e-6f0d4e5c5469&cache=v2'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMxhmX5ptnMl"
   },
   "source": [
    "GIVEN_K=1, LOOK_AHEAD=1, STEP=1 corresponds to the classical next-item evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHwXEiOm0pWK"
   },
   "outputs": [],
   "source": [
    "def eval_seqreveal(recommender, user_flg=0):\n",
    "  GIVEN_K = 1\n",
    "  LOOK_AHEAD = 1\n",
    "  STEP=1\n",
    "\n",
    "  if user_flg:\n",
    "    test_sequences, test_users = get_test_sequences_and_users(test_data, GIVEN_K, train_data['user_id'].values) # we need user ids now!\n",
    "    print('{} sequences available for evaluation ({} users)'.format(len(test_sequences), len(np.unique(test_users))))\n",
    "    results = sequential_evaluation(recommender,\n",
    "                                           test_sequences=test_sequences,\n",
    "                                           users=test_users,\n",
    "                                           given_k=GIVEN_K,\n",
    "                                           look_ahead=LOOK_AHEAD,\n",
    "                                           evaluation_functions=METRICS.values(),\n",
    "                                           top_n=TOPN,\n",
    "                                           scroll=True,  # scrolling averages metrics over all profile lengths\n",
    "                                           step=STEP)\n",
    "  else:\n",
    "    test_sequences = get_test_sequences(test_data, GIVEN_K)\n",
    "    print('{} sequences available for evaluation'.format(len(test_sequences)))\n",
    "    results = sequential_evaluation(recommender,\n",
    "                                           test_sequences=test_sequences,\n",
    "                                           given_k=GIVEN_K,\n",
    "                                           look_ahead=LOOK_AHEAD,\n",
    "                                           evaluation_functions=METRICS.values(),\n",
    "                                           top_n=TOPN,\n",
    "                                           scroll=True,  # scrolling averages metrics over all profile lengths\n",
    "                                           step=STEP)\n",
    "\n",
    "\n",
    "  \n",
    "  # print('Sequential evaluation (GIVEN_K={}, LOOK_AHEAD={}, STEP={})'.format(GIVEN_K, LOOK_AHEAD, STEP))\n",
    "  # for mname, mvalue in zip(METRICS.keys(), results):\n",
    "  #     print('\\t{}@{}: {:.4f}'.format(mname, TOPN, mvalue))\n",
    "  return [results, GIVEN_K, LOOK_AHEAD, STEP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Elx1bvtAkEzR"
   },
   "source": [
    "<img src='https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fd7873215-bcd2-4e0e-a48c-ac28feb4f208%2FUntitled.png?table=block&id=527b67cd-0771-4848-94d6-c6be9b208ebf&spaceId=63b72b1f-0e90-4ab8-a6df-a060a6545a56&width=2000&userId=21ec183f-f0be-4b6b-9b3e-6f0d4e5c5469&cache=v2'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBOJ6TqOOn5X"
   },
   "source": [
    "#### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtpuWQtSV9r_"
   },
   "outputs": [],
   "source": [
    "for model in [poprecommender, fsmrecommender,  mmcrecommender, p2vrecommender,\n",
    "              rnnrecommender, fpmcrecommender, prnnrecommender,\n",
    "              ]:\n",
    "  if model in [fpmcrecommender, prnnrecommender]:\n",
    "    results = eval_seqreveal(model, user_flg=1)\n",
    "    # results = eval_staticprofile(model, user_flg=1)\n",
    "  else:\n",
    "    results = eval_seqreveal(model)\n",
    "\n",
    "  wandb.init(name='seqreveal-'+type(model).__name__, \n",
    "           project='SARS Music30 x1',\n",
    "           notes='sequentially revelaed user profile evaluation', \n",
    "           tags=['sequence', 'music', 'seqreveal'])\n",
    "  wandb.log({\n",
    "      \"Model\": type(model).__name__,\n",
    "      \"GIVEN_K\": results[1],\n",
    "      \"LOOK_AHEAD\": results[2],\n",
    "      \"STEP\": results[3],\n",
    "      \"Precision@100\": results[0][0],\n",
    "      \"Recall@100\": results[0][1],\n",
    "      \"MRR@100\": results[0][2],\n",
    "      })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HINfNEgth72l"
   },
   "source": [
    "```text\n",
    "Run summary:\n",
    "\n",
    "Model\tProd2VecRecommender\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "Precision@100\t0.01922\n",
    "Recall@100\t0.28801\n",
    "MRR@100\t0.10133\n",
    "_runtime\t144\n",
    "_timestamp\t1619363510\n",
    "_step\t3\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁▁▁▁\n",
    "LOOK_AHEAD\t▁▁▁▁\n",
    "STEP\t▁▁▁▁\n",
    "Precision@100\t▁█▆▄\n",
    "Recall@100\t▂▁█▅\n",
    "MRR@100\t▁▅█▆\n",
    "_runtime\t▁▅▅█\n",
    "_timestamp\t▁▅▅█\n",
    "_step\t▁▃▆█\n",
    "\n",
    "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced seqreveal: https://wandb.ai/sparsh121/SARS%20Music30/runs/3gg65exl\n",
    "...Successfully finished last run (ID:3gg65exl). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run seqreveal-PopularityRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/3maoqo8c\n",
    "Run data is saved locally in /content/wandb/run-20210425_153508-3maoqo8c\n",
    "\n",
    "  1%|          | 19/2891 [00:00<00:15, 189.65it/s]2891 sequences available for evaluation\n",
    "100%|██████████| 2891/2891 [00:17<00:00, 163.97it/s]\n",
    "Finishing last run (ID:3maoqo8c) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 1516\n",
    "Program ended successfully.\n",
    "0.01MB of 0.01MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_153508-3maoqo8c/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_153508-3maoqo8c/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tPopularityRecommende...\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "Precision@100\t0.00176\n",
    "Recall@100\t0.17585\n",
    "MRR@100\t0.01509\n",
    "_runtime\t3\n",
    "_timestamp\t1619364913\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "Precision@100\t▁\n",
    "Recall@100\t▁\n",
    "MRR@100\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced seqreveal-PopularityRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/3maoqo8c\n",
    "...Successfully finished last run (ID:3maoqo8c). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run seqreveal-FSMRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/25j3u2ea\n",
    "Run data is saved locally in /content/wandb/run-20210425_153531-25j3u2ea\n",
    "\n",
    "  3%|▎         | 99/2891 [00:00<00:02, 986.59it/s]2891 sequences available for evaluation\n",
    "100%|██████████| 2891/2891 [00:02<00:00, 1312.88it/s]\n",
    "Finishing last run (ID:25j3u2ea) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 1549\n",
    "Program ended successfully.\n",
    "0.01MB of 0.01MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_153531-25j3u2ea/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_153531-25j3u2ea/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tFSMRecommender\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "Precision@100\t0.04325\n",
    "Recall@100\t0.11793\n",
    "MRR@100\t0.0898\n",
    "_runtime\t2\n",
    "_timestamp\t1619364936\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "Precision@100\t▁\n",
    "Recall@100\t▁\n",
    "MRR@100\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced seqreveal-FSMRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/25j3u2ea\n",
    "...Successfully finished last run (ID:25j3u2ea). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run seqreveal-MixedMarkovChainRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2ae9o59i\n",
    "Run data is saved locally in /content/wandb/run-20210425_153539-2ae9o59i\n",
    "\n",
    "  1%|          | 15/2891 [00:00<00:21, 136.79it/s]2891 sequences available for evaluation\n",
    "100%|██████████| 2891/2891 [00:17<00:00, 164.35it/s]\n",
    "Finishing last run (ID:2ae9o59i) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 1580\n",
    "Program ended successfully.\n",
    "0.01MB of 0.01MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_153539-2ae9o59i/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_153539-2ae9o59i/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tMixedMarkovChainReco...\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "Precision@100\t0.03049\n",
    "Recall@100\t0.40207\n",
    "MRR@100\t0.13409\n",
    "_runtime\t3\n",
    "_timestamp\t1619364945\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "Precision@100\t▁\n",
    "Recall@100\t▁\n",
    "MRR@100\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced seqreveal-MixedMarkovChainRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2ae9o59i\n",
    "...Successfully finished last run (ID:2ae9o59i). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run seqreveal-Prod2VecRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/1efzboqz\n",
    "Run data is saved locally in /content/wandb/run-20210425_153602-1efzboqz\n",
    "\n",
    "  0%|          | 0/2891 [00:00<?, ?it/s]2891 sequences available for evaluation\n",
    "100%|██████████| 2891/2891 [02:02<00:00, 23.66it/s]\n",
    "Finishing last run (ID:1efzboqz) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 1614\n",
    "Program ended successfully.\n",
    "0.01MB of 0.01MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_153602-1efzboqz/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_153602-1efzboqz/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tProd2VecRecommender\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "Precision@100\t0.01922\n",
    "Recall@100\t0.28801\n",
    "MRR@100\t0.10133\n",
    "_runtime\t3\n",
    "_timestamp\t1619364968\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "Precision@100\t▁\n",
    "Recall@100\t▁\n",
    "MRR@100\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced seqreveal-Prod2VecRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/1efzboqz\n",
    "...Successfully finished last run (ID:1efzboqz). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run seqreveal-RNNRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/coi5um5y\n",
    "Run data is saved locally in /content/wandb/run-20210425_153810-coi5um5y\n",
    "\n",
    "  0%|          | 0/1079 [00:00<?, ?it/s]1079 sequences available for evaluation (1079 users)\n",
    "100%|██████████| 1079/1079 [00:59<00:00, 18.14it/s]\n",
    "Finishing last run (ID:coi5um5y) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 1652\n",
    "Program ended successfully.\n",
    "0.01MB of 0.01MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_153810-coi5um5y/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_153810-coi5um5y/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tRNNRecommender\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "Precision@100\t0.00488\n",
    "Recall@100\t0.48776\n",
    "MRR@100\t0.24084\n",
    "_runtime\t3\n",
    "_timestamp\t1619365096\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "Precision@100\t▁\n",
    "Recall@100\t▁\n",
    "MRR@100\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced seqreveal-RNNRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/coi5um5y\n",
    "...Successfully finished last run (ID:coi5um5y). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run seqreveal-FPMCRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/pv3hklma\n",
    "Run data is saved locally in /content/wandb/run-20210425_153915-pv3hklma\n",
    "\n",
    "  0%|          | 0/1079 [00:00<?, ?it/s]1079 sequences available for evaluation (1079 users)\n",
    "100%|██████████| 1079/1079 [00:47<00:00, 22.64it/s]\n",
    "Finishing last run (ID:pv3hklma) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 1682\n",
    "Program ended successfully.\n",
    "0.01MB of 0.01MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_153915-pv3hklma/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_153915-pv3hklma/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tFPMCRecommender\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "Precision@100\t0.00184\n",
    "Recall@100\t0.18351\n",
    "MRR@100\t0.04037\n",
    "_runtime\t3\n",
    "_timestamp\t1619365161\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "Precision@100\t▁\n",
    "Recall@100\t▁\n",
    "MRR@100\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced seqreveal-FPMCRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/pv3hklma\n",
    "...Successfully finished last run (ID:pv3hklma). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run seqreveal-RNNRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2xkkfena\n",
    "Run data is saved locally in /content/wandb/run-20210425_154009-2xkkfena\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1-BeYw-vClK"
   },
   "source": [
    "### Evaluation with \"static\" user-profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiDXrYn-TGOr"
   },
   "source": [
    "Here we evaluate the quality of the recommendations in a setting in which user profiles are instead _static_.\n",
    "\n",
    "The _user profile_ starts from the first `GIVEN_K` events (or, alternatively, from the last `-GIVEN_K` events if `GIVEN_K<0`).  \n",
    "The recommendations are evaluated against the next `LOOK_AHEAD` events (the _ground truth_).  \n",
    "\n",
    "The user profile is *not extended* and the ground truth *doesn't move forward*.\n",
    "This allows to obtain \"snapshots\" of the recommendation performance for different user profile and ground truth lenghts.\n",
    "\n",
    "Also here you can set the `LOOK_AHEAD='all'` to see what happens if you had to recommend a **whole sequence** instead of a set of a set of alternatives to a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNoA78EhpFiH"
   },
   "outputs": [],
   "source": [
    "def eval_staticprofile(recommender, user_flg=0):\n",
    "  GIVEN_K = 1\n",
    "  LOOK_AHEAD = 'all'\n",
    "  STEP=1\n",
    "\n",
    "  if user_flg:\n",
    "    test_sequences, test_users = get_test_sequences_and_users(test_data, GIVEN_K, train_data['user_id'].values) # we need user ids now!\n",
    "    print('{} sequences available for evaluation ({} users)'.format(len(test_sequences), len(np.unique(test_users))))\n",
    "    results = sequential_evaluation(recommender,\n",
    "                                           test_sequences=test_sequences,\n",
    "                                           users=test_users,\n",
    "                                           given_k=GIVEN_K,\n",
    "                                           look_ahead=LOOK_AHEAD,\n",
    "                                           evaluation_functions=METRICS.values(),\n",
    "                                           top_n=TOPN,\n",
    "                                           scroll=False  # notice that scrolling is disabled!\n",
    "                                    )                                \n",
    "  else:\n",
    "    test_sequences = get_test_sequences(test_data, GIVEN_K)\n",
    "    print('{} sequences available for evaluation'.format(len(test_sequences)))\n",
    "    results = sequential_evaluation(recommender,\n",
    "                                            test_sequences=test_sequences,\n",
    "                                            given_k=GIVEN_K,\n",
    "                                            look_ahead=LOOK_AHEAD,\n",
    "                                            evaluation_functions=METRICS.values(),\n",
    "                                            top_n=TOPN,\n",
    "                                            scroll=False  # notice that scrolling is disabled!\n",
    "                                    )\n",
    "    \n",
    "  return [results, GIVEN_K, LOOK_AHEAD, STEP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMiNw9WdOk-C"
   },
   "source": [
    "#### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZQ92S6usOY6"
   },
   "outputs": [],
   "source": [
    "for model in [poprecommender, fsmrecommender,  mmcrecommender, p2vrecommender,\n",
    "              rnnrecommender, fpmcrecommender, prnnrecommender,\n",
    "              ]:\n",
    "  if model in [fpmcrecommender, prnnrecommender]:\n",
    "    results = eval_staticprofile(model, user_flg=1)\n",
    "  else:\n",
    "    results = eval_staticprofile(model)\n",
    "\n",
    "\n",
    "  wandb.init(name='staticprofile-'+type(model).__name__, \n",
    "           project='SARS Music30 x1',\n",
    "           notes='sequentially static user profile evaluation', \n",
    "           tags=['sequence', 'music', 'staticprofile'])\n",
    "  wandb.log({\n",
    "      \"Model\": type(model).__name__,\n",
    "      \"GIVEN_K\": results[1],\n",
    "      \"LOOK_AHEAD\": results[2],\n",
    "      \"STEP\": results[3],\n",
    "      \"Precision@100\": results[0][0],\n",
    "      \"Recall@100\": results[0][1],\n",
    "      \"MRR@100\": results[0][2],\n",
    "      })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0A8HbYRJiIda"
   },
   "source": [
    "```text\n",
    "Run summary:\n",
    "\n",
    "Model\tRNNRecommender\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "Precision@100\t0.00445\n",
    "Recall@100\t0.44485\n",
    "MRR@100\t0.05859\n",
    "_runtime\t3\n",
    "_timestamp\t1619365214\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "Precision@100\t▁\n",
    "Recall@100\t▁\n",
    "MRR@100\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced seqreveal-RNNRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2xkkfena\n",
    "...Successfully finished last run (ID:2xkkfena). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run staticprofile-PopularityRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/30hai298\n",
    "Run data is saved locally in /content/wandb/run-20210425_154439-30hai298\n",
    "\n",
    "  8%|▊         | 242/2891 [00:00<00:01, 2415.10it/s]2891 sequences available for evaluation\n",
    "100%|██████████| 2891/2891 [00:01<00:00, 2526.75it/s]\n",
    "Finishing last run (ID:30hai298) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 1806\n",
    "Program ended successfully.\n",
    "0.01MB of 0.01MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_154439-30hai298/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_154439-30hai298/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tPopularityRecommende...\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\tall\n",
    "STEP\t1\n",
    "Precision@100\t0.00837\n",
    "Recall@100\t0.175\n",
    "MRR@100\t0.05131\n",
    "_runtime\t2\n",
    "_timestamp\t1619365485\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "STEP\t▁\n",
    "Precision@100\t▁\n",
    "Recall@100\t▁\n",
    "MRR@100\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced staticprofile-PopularityRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/30hai298\n",
    "...Successfully finished last run (ID:30hai298). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run staticprofile-FSMRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/bx66eotg\n",
    "Run data is saved locally in /content/wandb/run-20210425_154446-bx66eotg\n",
    "\n",
    " 16%|█▋        | 471/2891 [00:00<00:00, 4702.07it/s]2891 sequences available for evaluation\n",
    "100%|██████████| 2891/2891 [00:00<00:00, 5916.58it/s]\n",
    "Finishing last run (ID:bx66eotg) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 1835\n",
    "Program ended successfully.\n",
    "0.01MB of 0.01MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_154446-bx66eotg/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_154446-bx66eotg/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tFSMRecommender\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\tall\n",
    "STEP\t1\n",
    "Precision@100\t0.10803\n",
    "Recall@100\t0.09833\n",
    "MRR@100\t0.13287\n",
    "_runtime\t2\n",
    "_timestamp\t1619365491\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "STEP\t▁\n",
    "Precision@100\t▁\n",
    "Recall@100\t▁\n",
    "MRR@100\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced staticprofile-FSMRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/bx66eotg\n",
    "...Successfully finished last run (ID:bx66eotg). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run staticprofile-MixedMarkovChainRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2rmv1ke5\n",
    "Run data is saved locally in /content/wandb/run-20210425_154452-2rmv1ke5\n",
    "\n",
    "  9%|▉         | 265/2891 [00:00<00:00, 2643.59it/s]2891 sequences available for evaluation\n",
    "100%|██████████| 2891/2891 [00:00<00:00, 3322.28it/s]\n",
    "Finishing last run (ID:2rmv1ke5) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 1866\n",
    "Program ended successfully.\n",
    "0.01MB of 0.01MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_154452-2rmv1ke5/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_154452-2rmv1ke5/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tMixedMarkovChainReco...\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\tall\n",
    "STEP\t1\n",
    "Precision@100\t0.09713\n",
    "Recall@100\t0.39545\n",
    "MRR@100\t0.23382\n",
    "_runtime\t2\n",
    "_timestamp\t1619365497\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "STEP\t▁\n",
    "Precision@100\t▁\n",
    "Recall@100\t▁\n",
    "MRR@100\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced staticprofile-MixedMarkovChainRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2rmv1ke5\n",
    "...Successfully finished last run (ID:2rmv1ke5). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run staticprofile-Prod2VecRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2ur3bf4o\n",
    "Run data is saved locally in /content/wandb/run-20210425_154458-2ur3bf4o\n",
    "\n",
    "  1%|          | 19/2891 [00:00<00:15, 181.37it/s]2891 sequences available for evaluation\n",
    "100%|██████████| 2891/2891 [00:12<00:00, 232.78it/s]\n",
    "Finishing last run (ID:2ur3bf4o) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 1896\n",
    "Program ended successfully.\n",
    "0.01MB of 0.01MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_154458-2ur3bf4o/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_154458-2ur3bf4o/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tProd2VecRecommender\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\tall\n",
    "STEP\t1\n",
    "Precision@100\t0.10218\n",
    "Recall@100\t0.19719\n",
    "MRR@100\t0.20517\n",
    "_runtime\t2\n",
    "_timestamp\t1619365503\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "STEP\t▁\n",
    "Precision@100\t▁\n",
    "Recall@100\t▁\n",
    "MRR@100\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced staticprofile-Prod2VecRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2ur3bf4o\n",
    "...Successfully finished last run (ID:2ur3bf4o). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run staticprofile-RNNRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/44zzy1je\n",
    "Run data is saved locally in /content/wandb/run-20210425_154516-44zzy1je\n",
    "\n",
    "  1%|          | 7/1079 [00:00<00:17, 62.97it/s]1079 sequences available for evaluation (1079 users)\n",
    "100%|██████████| 1079/1079 [00:13<00:00, 81.83it/s]\n",
    "Finishing last run (ID:44zzy1je) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 1925\n",
    "Program ended successfully.\n",
    "0.01MB of 0.01MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_154516-44zzy1je/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_154516-44zzy1je/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tRNNRecommender\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\tall\n",
    "STEP\t1\n",
    "Precision@100\t0.01542\n",
    "Recall@100\t0.38361\n",
    "MRR@100\t0.29362\n",
    "_runtime\t3\n",
    "_timestamp\t1619365522\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "STEP\t▁\n",
    "Precision@100\t▁\n",
    "Recall@100\t▁\n",
    "MRR@100\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced staticprofile-RNNRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/44zzy1je\n",
    "...Successfully finished last run (ID:44zzy1je). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run staticprofile-FPMCRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/263glj4g\n",
    "Run data is saved locally in /content/wandb/run-20210425_154535-263glj4g\n",
    "\n",
    "  2%|▏         | 20/1079 [00:00<00:05, 199.38it/s]1079 sequences available for evaluation (1079 users)\n",
    "100%|██████████| 1079/1079 [00:04<00:00, 221.29it/s]\n",
    "Finishing last run (ID:263glj4g) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 1957\n",
    "Program ended successfully.\n",
    "0.01MB of 0.01MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_154535-263glj4g/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_154535-263glj4g/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tFPMCRecommender\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\tall\n",
    "STEP\t1\n",
    "Precision@100\t0.00651\n",
    "Recall@100\t0.17196\n",
    "MRR@100\t0.08583\n",
    "_runtime\t3\n",
    "_timestamp\t1619365541\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "STEP\t▁\n",
    "Precision@100\t▁\n",
    "Recall@100\t▁\n",
    "MRR@100\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced staticprofile-FPMCRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/263glj4g\n",
    "...Successfully finished last run (ID:263glj4g). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run staticprofile-RNNRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/4c27los9\n",
    "Run data is saved locally in /content/wandb/run-20210425_154546-4c27los9\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jK3neOtvRq8"
   },
   "source": [
    "## Analysis of next-item recommendation\n",
    "\n",
    "Here we propose to analyse the performance of the recommender system in the scenario of *next-item recommendation* over the following dimensions:\n",
    "\n",
    "* the *length* of the **recommendation list**, and\n",
    "* the *length* of the **user profile**.\n",
    "\n",
    "NOTE: This evaluation is by no means exhaustive, as different the hyper-parameters of the recommendation algorithm should be *carefully tuned* before drawing any conclusions. Unfortunately, given the time constraints for this tutorial, we had to leave hyper-parameter tuning out. A very useful reference about careful evaluation of (session-based) recommenders can be found at:\n",
    "\n",
    "*  Evaluation of Session-based Recommendation Algorithms, Ludewig and Jannach, 2018 ([paper](https://arxiv.org/abs/1803.09587))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMDWiDE-vgYO"
   },
   "source": [
    "### Evaluation for different recommendation list lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkdLasrO1Hj0"
   },
   "outputs": [],
   "source": [
    "def eval_reclength(recommender, user_flg=0):\n",
    "  GIVEN_K = 1\n",
    "  LOOK_AHEAD = 1\n",
    "  STEP = 1\n",
    "  topn_list = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "  res_list = []\n",
    "\n",
    "  if user_flg:\n",
    "    test_sequences, test_users = get_test_sequences_and_users(test_data, GIVEN_K, train_data['user_id'].values) # we need user ids now!\n",
    "    print('{} sequences available for evaluation ({} users)'.format(len(test_sequences), len(np.unique(test_users))))\n",
    "    for topn in topn_list:\n",
    "      print('Evaluating recommendation lists with length: {}'.format(topn)) \n",
    "      res_tmp = sequential_evaluation(recommender,\n",
    "                                            test_sequences=test_sequences,\n",
    "                                            users=test_users,\n",
    "                                            given_k=GIVEN_K,\n",
    "                                            look_ahead=LOOK_AHEAD,\n",
    "                                            evaluation_functions=METRICS.values(),\n",
    "                                            top_n=topn,\n",
    "                                            scroll=True,  # here we average over all profile lengths\n",
    "                                            step=STEP\n",
    "                                      )\n",
    "      mvalues = list(zip(METRICS.keys(), res_tmp))\n",
    "      res_list.append((topn, mvalues))                            \n",
    "  else:\n",
    "    test_sequences = get_test_sequences(test_data, GIVEN_K)\n",
    "    print('{} sequences available for evaluation'.format(len(test_sequences)))\n",
    "    for topn in topn_list:\n",
    "      print('Evaluating recommendation lists with length: {}'.format(topn))      \n",
    "      res_tmp = sequential_evaluation(recommender,\n",
    "                                                test_sequences=test_sequences,\n",
    "                                                given_k=GIVEN_K,\n",
    "                                                look_ahead=LOOK_AHEAD,\n",
    "                                                evaluation_functions=METRICS.values(),\n",
    "                                                top_n=topn,\n",
    "                                                scroll=True,  # here we average over all profile lengths\n",
    "                                                step=STEP)\n",
    "      mvalues = list(zip(METRICS.keys(), res_tmp))\n",
    "      res_list.append((topn, mvalues))\n",
    "\n",
    "  # show separate plots per metric\n",
    "  # fig, axes = plt.subplots(nrows=1, ncols=len(METRICS), figsize=(15,5))\n",
    "  res_list_t = list(zip(*res_list))\n",
    "  results = []\n",
    "  for midx, metric in enumerate(METRICS):\n",
    "      mvalues = [res_list_t[1][j][midx][1] for j in range(len(res_list_t[1]))]\n",
    "      fig, ax = plt.subplots(figsize=(5,5))\n",
    "      ax.plot(topn_list, mvalues)\n",
    "      ax.set_title(metric)\n",
    "      ax.set_xticks(topn_list)\n",
    "      ax.set_xlabel('List length')\n",
    "      fig.tight_layout()\n",
    "      results.append(fig)\n",
    "  return [results, GIVEN_K, LOOK_AHEAD, STEP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1mz9PlhOiQv"
   },
   "source": [
    "#### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukJ_r3DOsOUB"
   },
   "outputs": [],
   "source": [
    "for model in [poprecommender, fsmrecommender,  mmcrecommender, p2vrecommender,\n",
    "              rnnrecommender, fpmcrecommender, prnnrecommender,\n",
    "              ]:\n",
    "  if model in [fpmcrecommender, prnnrecommender]:\n",
    "    results = eval_reclength(model, user_flg=1)\n",
    "  else:\n",
    "    results = eval_reclength(model)\n",
    "    \n",
    "  wandb.init(name='plotreclen-'+type(model).__name__, \n",
    "           project='SARS Music30 x1',\n",
    "           notes='rec list length variation evaluation', \n",
    "           tags=['sequence', 'music', 'plotreclen'])\n",
    "  wandb.log({\"Precision\": results[0][0],\n",
    "             \"Recall\": results[0][1],\n",
    "             \"MRR\": results[0][2],\n",
    "             \"Model\": type(model).__name__,\n",
    "             \"GIVEN_K\": results[1],\n",
    "             \"LOOK_AHEAD\": results[2],\n",
    "             \"STEP\": results[3],\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ojmr46E8iSKr"
   },
   "source": [
    "```text\n",
    "Run summary:\n",
    "\n",
    "Model\tPopularityRecommende...\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "_runtime\t3\n",
    "_timestamp\t1619368731\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced test-PopularityRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/3jzh0nf4\n",
    "...Successfully finished last run (ID:3jzh0nf4). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run plotreclen-PopularityRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/258r6fiv\n",
    "Run data is saved locally in /content/wandb/run-20210425_164026-258r6fiv\n",
    "\n",
    "  1%|          | 16/2891 [00:00<00:18, 155.20it/s]2891 sequences available for evaluation\n",
    "Evaluating recommendation lists with length: 1\n",
    "100%|██████████| 2891/2891 [00:16<00:00, 174.48it/s]\n",
    "  1%|          | 25/2891 [00:00<00:14, 191.72it/s]Evaluating recommendation lists with length: 5\n",
    "100%|██████████| 2891/2891 [00:16<00:00, 176.09it/s]\n",
    "  1%|          | 25/2891 [00:00<00:14, 201.35it/s]Evaluating recommendation lists with length: 10\n",
    "100%|██████████| 2891/2891 [00:16<00:00, 174.50it/s]\n",
    "  1%|          | 25/2891 [00:00<00:14, 195.01it/s]Evaluating recommendation lists with length: 20\n",
    "100%|██████████| 2891/2891 [00:16<00:00, 173.53it/s]\n",
    "  1%|          | 25/2891 [00:00<00:14, 194.88it/s]Evaluating recommendation lists with length: 50\n",
    "100%|██████████| 2891/2891 [00:16<00:00, 175.39it/s]\n",
    "  1%|          | 25/2891 [00:00<00:13, 206.00it/s]Evaluating recommendation lists with length: 100\n",
    "100%|██████████| 2891/2891 [00:16<00:00, 175.23it/s]\n",
    "Finishing last run (ID:258r6fiv) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 2717\n",
    "Program ended successfully.\n",
    "0.03MB of 0.03MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_164026-258r6fiv/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_164026-258r6fiv/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tPopularityRecommende...\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "_runtime\t3\n",
    "_timestamp\t1619368831\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced plotreclen-PopularityRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/258r6fiv\n",
    "...Successfully finished last run (ID:258r6fiv). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run plotreclen-FSMRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/1o3gi66n\n",
    "Run data is saved locally in /content/wandb/run-20210425_164211-1o3gi66n\n",
    "\n",
    "  3%|▎         | 99/2891 [00:00<00:02, 986.29it/s]2891 sequences available for evaluation\n",
    "Evaluating recommendation lists with length: 1\n",
    "100%|██████████| 2891/2891 [00:01<00:00, 1940.59it/s]\n",
    "  6%|▌         | 160/2891 [00:00<00:01, 1598.42it/s]Evaluating recommendation lists with length: 5\n",
    "100%|██████████| 2891/2891 [00:01<00:00, 1896.56it/s]\n",
    "  6%|▋         | 182/2891 [00:00<00:01, 1792.34it/s]Evaluating recommendation lists with length: 10\n",
    "100%|██████████| 2891/2891 [00:01<00:00, 1836.88it/s]\n",
    "  5%|▌         | 148/2891 [00:00<00:01, 1476.30it/s]Evaluating recommendation lists with length: 20\n",
    "100%|██████████| 2891/2891 [00:01<00:00, 1682.26it/s]\n",
    "  5%|▌         | 148/2891 [00:00<00:01, 1463.26it/s]Evaluating recommendation lists with length: 50\n",
    "100%|██████████| 2891/2891 [00:01<00:00, 1552.85it/s]\n",
    "  4%|▍         | 130/2891 [00:00<00:02, 1296.16it/s]Evaluating recommendation lists with length: 100\n",
    "100%|██████████| 2891/2891 [00:01<00:00, 1471.60it/s]\n",
    "Finishing last run (ID:1o3gi66n) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 2771\n",
    "Program ended successfully.\n",
    "0.03MB of 0.03MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_164211-1o3gi66n/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_164211-1o3gi66n/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tFSMRecommender\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "_runtime\t3\n",
    "_timestamp\t1619368937\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced plotreclen-FSMRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/1o3gi66n\n",
    "...Successfully finished last run (ID:1o3gi66n). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run plotreclen-MixedMarkovChainRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2bf08nlv\n",
    "Run data is saved locally in /content/wandb/run-20210425_164229-2bf08nlv\n",
    "\n",
    "  1%|          | 24/2891 [00:00<00:12, 237.99it/s]2891 sequences available for evaluation\n",
    "Evaluating recommendation lists with length: 1\n",
    "100%|██████████| 2891/2891 [00:14<00:00, 193.21it/s]\n",
    "  1%|          | 25/2891 [00:00<00:11, 245.30it/s]Evaluating recommendation lists with length: 5\n",
    "100%|██████████| 2891/2891 [00:14<00:00, 193.92it/s]\n",
    "  1%|          | 27/2891 [00:00<00:11, 259.46it/s]Evaluating recommendation lists with length: 10\n",
    "100%|██████████| 2891/2891 [00:14<00:00, 193.13it/s]\n",
    "  1%|          | 25/2891 [00:00<00:11, 239.56it/s]Evaluating recommendation lists with length: 20\n",
    "100%|██████████| 2891/2891 [00:15<00:00, 189.56it/s]\n",
    "  1%|          | 25/2891 [00:00<00:12, 233.18it/s]Evaluating recommendation lists with length: 50\n",
    "100%|██████████| 2891/2891 [00:15<00:00, 184.47it/s]\n",
    "  1%|          | 25/2891 [00:00<00:12, 228.19it/s]Evaluating recommendation lists with length: 100\n",
    "100%|██████████| 2891/2891 [00:15<00:00, 181.99it/s]\n",
    "Finishing last run (ID:2bf08nlv) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 2813\n",
    "Program ended successfully.\n",
    "0.03MB of 0.03MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_164229-2bf08nlv/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_164229-2bf08nlv/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tMixedMarkovChainReco...\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "_runtime\t3\n",
    "_timestamp\t1619368954\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced plotreclen-MixedMarkovChainRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2bf08nlv\n",
    "...Successfully finished last run (ID:2bf08nlv). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run plotreclen-Prod2VecRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2avtq2c8\n",
    "Run data is saved locally in /content/wandb/run-20210425_164407-2avtq2c8\n",
    "\n",
    "  0%|          | 3/2891 [00:00<01:49, 26.47it/s]2891 sequences available for evaluation\n",
    "Evaluating recommendation lists with length: 1\n",
    "100%|██████████| 2891/2891 [01:55<00:00, 25.08it/s]\n",
    "  0%|          | 7/2891 [00:00<00:44, 65.33it/s]Evaluating recommendation lists with length: 5\n",
    "100%|██████████| 2891/2891 [01:56<00:00, 24.74it/s]\n",
    "  0%|          | 7/2891 [00:00<00:44, 65.47it/s]Evaluating recommendation lists with length: 10\n",
    "100%|██████████| 2891/2891 [01:57<00:00, 24.71it/s]\n",
    "  0%|          | 9/2891 [00:00<00:32, 89.50it/s]Evaluating recommendation lists with length: 20\n",
    "100%|██████████| 2891/2891 [01:56<00:00, 24.71it/s]\n",
    "  0%|          | 7/2891 [00:00<00:42, 68.27it/s]Evaluating recommendation lists with length: 50\n",
    "100%|██████████| 2891/2891 [01:57<00:00, 24.62it/s]\n",
    "  0%|          | 8/2891 [00:00<00:36, 79.41it/s]Evaluating recommendation lists with length: 100\n",
    "100%|██████████| 2891/2891 [02:00<00:00, 24.05it/s]\n",
    "Finishing last run (ID:2avtq2c8) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 2869\n",
    "Program ended successfully.\n",
    "0.03MB of 0.03MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_164407-2avtq2c8/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_164407-2avtq2c8/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tProd2VecRecommender\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "_runtime\t3\n",
    "_timestamp\t1619369052\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced plotreclen-Prod2VecRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2avtq2c8\n",
    "...Successfully finished last run (ID:2avtq2c8). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run plotreclen-RNNRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/yq96ley7\n",
    "Run data is saved locally in /content/wandb/run-20210425_165558-yq96ley7\n",
    "\n",
    "  0%|          | 5/1079 [00:00<00:40, 26.63it/s]1079 sequences available for evaluation (1079 users)\n",
    "Evaluating recommendation lists with length: 1\n",
    "100%|██████████| 1079/1079 [00:58<00:00, 18.47it/s]\n",
    "  0%|          | 5/1079 [00:00<00:35, 30.02it/s]Evaluating recommendation lists with length: 5\n",
    "100%|██████████| 1079/1079 [00:58<00:00, 18.52it/s]\n",
    "  0%|          | 5/1079 [00:00<00:36, 29.20it/s]Evaluating recommendation lists with length: 10\n",
    "100%|██████████| 1079/1079 [00:57<00:00, 18.68it/s]\n",
    "  0%|          | 5/1079 [00:00<00:34, 31.20it/s]Evaluating recommendation lists with length: 20\n",
    "100%|██████████| 1079/1079 [00:57<00:00, 18.85it/s]\n",
    "  0%|          | 5/1079 [00:00<00:37, 28.42it/s]Evaluating recommendation lists with length: 50\n",
    "100%|██████████| 1079/1079 [00:58<00:00, 18.51it/s]\n",
    "  0%|          | 5/1079 [00:00<00:37, 28.75it/s]Evaluating recommendation lists with length: 100\n",
    "100%|██████████| 1079/1079 [00:59<00:00, 18.12it/s]\n",
    "Finishing last run (ID:yq96ley7) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 3003\n",
    "Program ended successfully.\n",
    "0.03MB of 0.03MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_165558-yq96ley7/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_165558-yq96ley7/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tRNNRecommender\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "_runtime\t4\n",
    "_timestamp\t1619369765\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced plotreclen-RNNRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/yq96ley7\n",
    "...Successfully finished last run (ID:yq96ley7). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run plotreclen-FPMCRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/29wevfbw\n",
    "Run data is saved locally in /content/wandb/run-20210425_170156-29wevfbw\n",
    "\n",
    "  0%|          | 5/1079 [00:00<00:28, 37.53it/s]1079 sequences available for evaluation (1079 users)\n",
    "Evaluating recommendation lists with length: 1\n",
    "100%|██████████| 1079/1079 [00:44<00:00, 24.32it/s]\n",
    "  0%|          | 0/1079 [00:00<?, ?it/s]Evaluating recommendation lists with length: 5\n",
    "100%|██████████| 1079/1079 [00:44<00:00, 24.37it/s]\n",
    "  0%|          | 5/1079 [00:00<00:22, 48.64it/s]Evaluating recommendation lists with length: 10\n",
    "100%|██████████| 1079/1079 [00:44<00:00, 24.37it/s]\n",
    "  0%|          | 5/1079 [00:00<00:21, 49.92it/s]Evaluating recommendation lists with length: 20\n",
    "100%|██████████| 1079/1079 [00:44<00:00, 24.04it/s]\n",
    "  0%|          | 0/1079 [00:00<?, ?it/s]Evaluating recommendation lists with length: 50\n",
    "100%|██████████| 1079/1079 [00:45<00:00, 23.96it/s]\n",
    "  0%|          | 5/1079 [00:00<00:21, 49.89it/s]Evaluating recommendation lists with length: 100\n",
    "100%|██████████| 1079/1079 [00:44<00:00, 24.04it/s]\n",
    "Finishing last run (ID:29wevfbw) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 3041\n",
    "Program ended successfully.\n",
    "0.03MB of 0.03MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_170156-29wevfbw/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_170156-29wevfbw/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tFPMCRecommender\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "_runtime\t3\n",
    "_timestamp\t1619370121\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced plotreclen-FPMCRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/29wevfbw\n",
    "...Successfully finished last run (ID:29wevfbw). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run plotreclen-RNNRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/3r0qzmw7\n",
    "Run data is saved locally in /content/wandb/run-20210425_170630-3r0qzmw7\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhhP0tqDv3s1"
   },
   "source": [
    "### Evaluation for different user profile lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ue5HcQzZbyNJ"
   },
   "outputs": [],
   "source": [
    "def eval_profilelength(recommender, user_flg=0):\n",
    "  given_k_list = [1, 2, 3, 4]\n",
    "  LOOK_AHEAD = 1\n",
    "  STEP = 1\n",
    "  TOPN = 20\n",
    "\n",
    "  res_list = []\n",
    "\n",
    "  if user_flg:\n",
    "    test_sequences, test_users = get_test_sequences_and_users(test_data, max(given_k_list), train_data['user_id'].values) # we need user ids now!\n",
    "    print('{} sequences available for evaluation ({} users)'.format(len(test_sequences), len(np.unique(test_users))))\n",
    "    for gk in given_k_list:\n",
    "      print('Evaluating profiles having length: {}'.format(gk))\n",
    "      res_tmp = sequential_evaluation(recommender,\n",
    "                                                test_sequences=test_sequences,\n",
    "                                                users=test_users,\n",
    "                                                given_k=gk,\n",
    "                                                look_ahead=LOOK_AHEAD,\n",
    "                                                evaluation_functions=METRICS.values(),\n",
    "                                                top_n=TOPN,\n",
    "                                                scroll=False,  # here we stop at each profile length\n",
    "                                                step=STEP)\n",
    "      mvalues = list(zip(METRICS.keys(), res_tmp))\n",
    "      res_list.append((gk, mvalues))                          \n",
    "  else:\n",
    "    test_sequences = get_test_sequences(test_data, max(given_k_list))\n",
    "    print('{} sequences available for evaluation'.format(len(test_sequences)))\n",
    "    for gk in given_k_list:\n",
    "      print('Evaluating profiles having length: {}'.format(gk))\n",
    "      res_tmp = sequential_evaluation(recommender,\n",
    "                                                test_sequences=test_sequences,\n",
    "                                                given_k=gk,\n",
    "                                                look_ahead=LOOK_AHEAD,\n",
    "                                                evaluation_functions=METRICS.values(),\n",
    "                                                top_n=TOPN,\n",
    "                                                scroll=False,  # here we stop at each profile length\n",
    "                                                step=STEP)\n",
    "      mvalues = list(zip(METRICS.keys(), res_tmp))\n",
    "      res_list.append((gk, mvalues))\n",
    "\n",
    "  # show separate plots per metric\n",
    "  # fig, axes = plt.subplots(nrows=1, ncols=len(METRICS), figsize=(15,5))\n",
    "  res_list_t = list(zip(*res_list))\n",
    "  results = []\n",
    "  for midx, metric in enumerate(METRICS):\n",
    "      mvalues = [res_list_t[1][j][midx][1] for j in range(len(res_list_t[1]))]\n",
    "      fig, ax = plt.subplots(figsize=(5,5))\n",
    "      ax.plot(given_k_list, mvalues)\n",
    "      ax.set_title(metric)\n",
    "      ax.set_xticks(given_k_list)\n",
    "      ax.set_xlabel('Profile length')\n",
    "      fig.tight_layout()\n",
    "      results.append(fig)\n",
    "  return [results, TOPN, LOOK_AHEAD, STEP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_DC5_W6OXS3"
   },
   "source": [
    "#### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oX1A8oppsONk"
   },
   "outputs": [],
   "source": [
    "for model in [poprecommender, fsmrecommender,  mmcrecommender, p2vrecommender,\n",
    "              rnnrecommender, fpmcrecommender, prnnrecommender,\n",
    "              ]:\n",
    "  if model in [fpmcrecommender, prnnrecommender]:\n",
    "    results = eval_profilelength(model, user_flg=1)\n",
    "  else:\n",
    "    results = eval_profilelength(model)\n",
    "    \n",
    "  wandb.init(name='plotproflen-'+type(model).__name__, \n",
    "           project='SARS Music30 x1',\n",
    "           notes='profile length variation evaluation', \n",
    "           tags=['sequence', 'music', 'plotproflen'])\n",
    "  wandb.log({\"Precision\": results[0][0],\n",
    "             \"Recall\": results[0][1],\n",
    "             \"MRR\": results[0][2],\n",
    "             \"Model\": type(model).__name__,\n",
    "             \"TOP_N\": results[1],\n",
    "             \"LOOK_AHEAD\": results[2],\n",
    "             \"STEP\": results[3],\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoCXgqXbidnW"
   },
   "source": [
    "```text\n",
    "Run summary:\n",
    "\n",
    "Model\tRNNRecommender\n",
    "GIVEN_K\t1\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "_runtime\t3\n",
    "_timestamp\t1619370395\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "GIVEN_K\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced plotreclen-RNNRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/3r0qzmw7\n",
    "...Successfully finished last run (ID:3r0qzmw7). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run plotproflen-PopularityRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/35gzppld\n",
    "Run data is saved locally in /content/wandb/run-20210425_172520-35gzppld\n",
    "\n",
    " 14%|█▍        | 162/1164 [00:00<00:00, 1619.48it/s]1164 sequences available for evaluation\n",
    "Evaluating profiles having length: 1\n",
    "100%|██████████| 1164/1164 [00:00<00:00, 2203.24it/s]\n",
    " 14%|█▎        | 158/1164 [00:00<00:00, 1565.03it/s]Evaluating profiles having length: 2\n",
    "100%|██████████| 1164/1164 [00:00<00:00, 1372.39it/s]\n",
    "  9%|▉         | 104/1164 [00:00<00:01, 1039.84it/s]Evaluating profiles having length: 3\n",
    "100%|██████████| 1164/1164 [00:01<00:00, 952.05it/s]\n",
    "  6%|▌         | 70/1164 [00:00<00:01, 686.40it/s]Evaluating profiles having length: 4\n",
    "100%|██████████| 1164/1164 [00:01<00:00, 738.39it/s]\n",
    "Finishing last run (ID:35gzppld) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 3161\n",
    "Program ended successfully.\n",
    "0.03MB of 0.03MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_172520-35gzppld/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_172520-35gzppld/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tPopularityRecommende...\n",
    "TOP_N\t20\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "_runtime\t3\n",
    "_timestamp\t1619371526\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "TOP_N\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced plotproflen-PopularityRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/35gzppld\n",
    "...Successfully finished last run (ID:35gzppld). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run plotproflen-FSMRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/yl7evwah\n",
    "Run data is saved locally in /content/wandb/run-20210425_172531-yl7evwah\n",
    "\n",
    " 32%|███▏      | 375/1164 [00:00<00:00, 3722.11it/s]1164 sequences available for evaluation\n",
    "Evaluating profiles having length: 1\n",
    "100%|██████████| 1164/1164 [00:00<00:00, 4971.87it/s]\n",
    "100%|██████████| 1164/1164 [00:00<00:00, 6303.63it/s]\n",
    "  0%|          | 0/1164 [00:00<?, ?it/s]Evaluating profiles having length: 2\n",
    "Evaluating profiles having length: 3\n",
    "100%|██████████| 1164/1164 [00:00<00:00, 6272.96it/s]\n",
    "100%|██████████| 1164/1164 [00:00<00:00, 6726.65it/s]\n",
    "Evaluating profiles having length: 4\n",
    "Finishing last run (ID:yl7evwah) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 3199\n",
    "Program ended successfully.\n",
    "0.03MB of 0.03MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_172531-yl7evwah/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_172531-yl7evwah/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tFSMRecommender\n",
    "TOP_N\t20\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "_runtime\t3\n",
    "_timestamp\t1619371537\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "TOP_N\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced plotproflen-FSMRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/yl7evwah\n",
    "...Successfully finished last run (ID:yl7evwah). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run plotproflen-MixedMarkovChainRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2j9putlt\n",
    "Run data is saved locally in /content/wandb/run-20210425_172540-2j9putlt\n",
    "\n",
    " 23%|██▎       | 268/1164 [00:00<00:00, 2676.49it/s]1164 sequences available for evaluation\n",
    "Evaluating profiles having length: 1\n",
    "100%|██████████| 1164/1164 [00:00<00:00, 3108.76it/s]\n",
    " 18%|█▊        | 213/1164 [00:00<00:00, 2123.98it/s]Evaluating profiles having length: 2\n",
    "100%|██████████| 1164/1164 [00:00<00:00, 1741.40it/s]\n",
    " 12%|█▏        | 138/1164 [00:00<00:00, 1373.66it/s]Evaluating profiles having length: 3\n",
    "100%|██████████| 1164/1164 [00:00<00:00, 1288.90it/s]\n",
    " 10%|█         | 118/1164 [00:00<00:00, 1179.31it/s]Evaluating profiles having length: 4\n",
    "100%|██████████| 1164/1164 [00:01<00:00, 1048.89it/s]\n",
    "Finishing last run (ID:2j9putlt) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 3235\n",
    "Program ended successfully.\n",
    "0.03MB of 0.03MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_172540-2j9putlt/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_172540-2j9putlt/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tMixedMarkovChainReco...\n",
    "TOP_N\t20\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "_runtime\t3\n",
    "_timestamp\t1619371545\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "TOP_N\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced plotproflen-MixedMarkovChainRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2j9putlt\n",
    "...Successfully finished last run (ID:2j9putlt). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run plotproflen-Prod2VecRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/1hmj6j0q\n",
    "Run data is saved locally in /content/wandb/run-20210425_172550-1hmj6j0q\n",
    "\n",
    "  1%|          | 11/1164 [00:00<00:10, 107.21it/s]1164 sequences available for evaluation\n",
    "Evaluating profiles having length: 1\n",
    "100%|██████████| 1164/1164 [00:06<00:00, 182.55it/s]\n",
    "  1%|▏         | 17/1164 [00:00<00:06, 166.10it/s]Evaluating profiles having length: 2\n",
    "100%|██████████| 1164/1164 [00:07<00:00, 157.29it/s]\n",
    "  1%|▏         | 15/1164 [00:00<00:08, 143.04it/s]Evaluating profiles having length: 3\n",
    "100%|██████████| 1164/1164 [00:08<00:00, 138.63it/s]\n",
    "  1%|          | 11/1164 [00:00<00:10, 107.33it/s]Evaluating profiles having length: 4\n",
    "100%|██████████| 1164/1164 [00:09<00:00, 122.52it/s]\n",
    "Finishing last run (ID:1hmj6j0q) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 3271\n",
    "Program ended successfully.\n",
    "0.03MB of 0.03MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_172550-1hmj6j0q/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_172550-1hmj6j0q/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tProd2VecRecommender\n",
    "TOP_N\t20\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "_runtime\t3\n",
    "_timestamp\t1619371555\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "TOP_N\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced plotproflen-Prod2VecRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/1hmj6j0q\n",
    "...Successfully finished last run (ID:1hmj6j0q). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run plotproflen-RNNRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/qe2pqcf2\n",
    "Run data is saved locally in /content/wandb/run-20210425_172628-qe2pqcf2\n",
    "\n",
    "  2%|▏         | 7/463 [00:00<00:07, 63.98it/s]463 sequences available for evaluation (463 users)\n",
    "Evaluating profiles having length: 1\n",
    "100%|██████████| 463/463 [00:06<00:00, 76.39it/s]\n",
    "  2%|▏         | 9/463 [00:00<00:05, 86.68it/s]Evaluating profiles having length: 2\n",
    "100%|██████████| 463/463 [00:05<00:00, 85.28it/s]\n",
    "  2%|▏         | 10/463 [00:00<00:04, 90.95it/s]Evaluating profiles having length: 3\n",
    "100%|██████████| 463/463 [00:05<00:00, 83.33it/s]\n",
    "  2%|▏         | 9/463 [00:00<00:05, 86.91it/s]Evaluating profiles having length: 4\n",
    "100%|██████████| 463/463 [00:05<00:00, 83.94it/s]\n",
    "Finishing last run (ID:qe2pqcf2) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 3309\n",
    "Program ended successfully.\n",
    "0.03MB of 0.03MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_172628-qe2pqcf2/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_172628-qe2pqcf2/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tRNNRecommender\n",
    "TOP_N\t20\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "_runtime\t3\n",
    "_timestamp\t1619371594\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "TOP_N\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced plotproflen-RNNRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/qe2pqcf2\n",
    "...Successfully finished last run (ID:qe2pqcf2). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run plotproflen-FPMCRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2g9qwnhv\n",
    "Run data is saved locally in /content/wandb/run-20210425_172658-2g9qwnhv\n",
    "\n",
    "  4%|▎         | 17/463 [00:00<00:02, 162.60it/s]463 sequences available for evaluation (463 users)\n",
    "Evaluating profiles having length: 1\n",
    "100%|██████████| 463/463 [00:01<00:00, 248.82it/s]\n",
    "  4%|▍         | 19/463 [00:00<00:02, 186.26it/s]Evaluating profiles having length: 2\n",
    "100%|██████████| 463/463 [00:02<00:00, 200.31it/s]\n",
    "  3%|▎         | 16/463 [00:00<00:02, 157.61it/s]Evaluating profiles having length: 3\n",
    "100%|██████████| 463/463 [00:03<00:00, 142.72it/s]\n",
    "  3%|▎         | 15/463 [00:00<00:03, 148.40it/s]Evaluating profiles having length: 4\n",
    "100%|██████████| 463/463 [00:03<00:00, 140.47it/s]\n",
    "Finishing last run (ID:2g9qwnhv) before initializing another...\n",
    "\n",
    "Waiting for W&B process to finish, PID 3345\n",
    "Program ended successfully.\n",
    "0.03MB of 0.03MB uploaded (0.00MB deduped)\n",
    "Find user logs for this run at: /content/wandb/run-20210425_172658-2g9qwnhv/logs/debug.log\n",
    "Find internal logs for this run at: /content/wandb/run-20210425_172658-2g9qwnhv/logs/debug-internal.log\n",
    "Run summary:\n",
    "\n",
    "Model\tFPMCRecommender\n",
    "TOP_N\t20\n",
    "LOOK_AHEAD\t1\n",
    "STEP\t1\n",
    "_runtime\t3\n",
    "_timestamp\t1619371623\n",
    "_step\t0\n",
    "Run history:\n",
    "\n",
    "TOP_N\t▁\n",
    "LOOK_AHEAD\t▁\n",
    "STEP\t▁\n",
    "_runtime\t▁\n",
    "_timestamp\t▁\n",
    "_step\t▁\n",
    "\n",
    "Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)\n",
    "\n",
    "Synced plotproflen-FPMCRecommender: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2g9qwnhv\n",
    "...Successfully finished last run (ID:2g9qwnhv). Initializing new run:\n",
    "\n",
    "Tracking run with wandb version 0.10.27\n",
    "Syncing run plotproflen-RNNRecommender to Weights & Biases (Documentation).\n",
    "Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1\n",
    "Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/31wyg3ur\n",
    "Run data is saved locally in /content/wandb/run-20210425_172716-31wyg3ur\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ks_yfCCYO4eP"
   },
   "source": [
    "## Artifact versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Mkp7_H3ORAI"
   },
   "source": [
    "### Model logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ds9QHv4VHu7M"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2uswdiud) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3744<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197f244ca32f484b8fc9651559388945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 34.21MB of 34.21MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/content/wandb/run-20210425_175159-2uswdiud/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/content/wandb/run-20210425_175159-2uswdiud/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 0 media file(s), 7 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">misty-dew-37</strong>: <a href=\"https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2uswdiud\" target=\"_blank\">https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2uswdiud</a><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2uswdiud). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "Tracking run with wandb version 0.10.27<br/>\n",
       "Syncing run <strong style=\"color:#cdcd00\">artifact-model</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "Project page: <a href=\"https://wandb.ai/sparsh121/SARS%20Music30%20x1\" target=\"_blank\">https://wandb.ai/sparsh121/SARS%20Music30%20x1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/1hx1av67\" target=\"_blank\">https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/1hx1av67</a><br/>\n",
       "Run data is saved locally in <code>/content/wandb/run-20210425_175524-1hx1av67</code><br/><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "run = wandb.init(job_type=\"model-logging\",\n",
    "                 name=\"artifact-model\",\n",
    "                 project='SARS Music30 x1')\n",
    "\n",
    "for model in [poprecommender, fsmrecommender,  mmcrecommender, p2vrecommender,\n",
    "              rnnrecommender, fpmcrecommender, prnnrecommender, knnrecommender]:\n",
    "  # with open(type(model).__name__+'.p', 'wb') as handle:\n",
    "    # pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "  artifact = wandb.Artifact(type(model).__name__, type='model')\n",
    "  artifact.add_file(type(model).__name__+'.p')\n",
    "  run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V72rqeWMOMIg"
   },
   "source": [
    "### Data logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQYmaXloGTYQ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1lfb8icq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3950<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116aef4cef364694b3b232219f53fc7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.38MB of 0.38MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/content/wandb/run-20210425_180606-1lfb8icq/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/content/wandb/run-20210425_180606-1lfb8icq/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">artifact-data</strong>: <a href=\"https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/1lfb8icq\" target=\"_blank\">https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/1lfb8icq</a><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:1lfb8icq). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "Tracking run with wandb version 0.10.27<br/>\n",
       "Syncing run <strong style=\"color:#cdcd00\">artifact-data</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "Project page: <a href=\"https://wandb.ai/sparsh121/SARS%20Music30%20x1\" target=\"_blank\">https://wandb.ai/sparsh121/SARS%20Music30%20x1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/3tjioy7t\" target=\"_blank\">https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/3tjioy7t</a><br/>\n",
       "Run data is saved locally in <code>/content/wandb/run-20210425_180817-3tjioy7t</code><br/><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x7f2a22eeeb50>"
      ]
     },
     "execution_count": 135,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = wandb.init(job_type=\"data-logging\",\n",
    "                 name=\"artifact-data\",\n",
    "                 project='SARS Music30 x1')\n",
    "artifact = wandb.Artifact('datasets', type='dataset')\n",
    "\n",
    "train_data.name = 'train_dataset'\n",
    "test_data.name = 'test_dataset'\n",
    "\n",
    "for dataset in [train_data, test_data]:\n",
    "  dataset.to_csv(dataset.name+'.p', index=False)\n",
    "  artifact.add_file(dataset.name+'.p')\n",
    "\n",
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lFQ3-h7cGCO"
   },
   "source": [
    "## W&B Experiment Link\n",
    "https://wandb.ai/sparsh121/SARS%20Music30%20x1/overview?workspace=user-sparsh121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1d5VeUokKzX"
   },
   "source": [
    "<img src='https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F657b8726-7c78-42e7-8cff-dda688ebc411%2FUntitled.png?table=block&id=d7de3cfe-b62c-45f2-8575-9797aeccc439&spaceId=63b72b1f-0e90-4ab8-a6df-a060a6545a56&width=2000&userId=21ec183f-f0be-4b6b-9b3e-6f0d4e5c5469&cache=v2'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_hn-eIidvWY"
   },
   "source": [
    "[Credits](https://github.com/mquad/sars_tutorial/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "T315965_Sequence_Aware_Recommenders_Music.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}