!pip install -q -U cornac

import cornac
cornac.__version__

import os
import sys

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats, sparse

from cornac.data import Reader
from cornac.datasets import movielens
from cornac.eval_methods import RatioSplit
from cornac.models import UserKNN, ItemKNN
from cornac.models import MF, NMF, BaselineOnly
from cornac.models import BPR, WMF
from cornac.models import GMF, MLP, NeuMF, VAECF, WMF

%tensorflow_version 1.x

df_100k3 = pd.read_csv('./data/ml-100k/ratings.csv',
                 usecols=["UserId","MovieId","Rating"])
df_100k3.columns = ["user_id", "item_id", "rating"]
df_100k3.head()

df_user_100k = pd.read_csv('./data/ml-100k/users.csv').set_index("UserID")
df_user_100k.head()

df_item_100k = pd.read_csv('./data/ml-100k/items.csv').set_index("ItemID").drop(columns=["Video Release Date", "IMDb URL", "unknown"])
df_item_100k.head()

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))
sns.countplot(x="rating", data=df, palette="ch:.25", ax=axes[0])
sns.boxplot(x="rating", data=df, palette="ch:.25", ax=axes[1])

def print_sparsity(df):
  n_users = df.user_id.nunique()
  n_items = df.item_id.nunique()
  n_ratings = len(df)
  rating_matrix_size = n_users * n_items
  sparsity = 1 - n_ratings / rating_matrix_size

  print(f"Number of users: {n_users}")
  print(f"Number of items: {n_items}")
  print(f"Number of available ratings: {n_ratings}")
  print(f"Number of all possible ratings: {rating_matrix_size}")
  print("-" * 40)
  print(f"SPARSITY: {sparsity * 100.0:.2f}%")

print_sparsity(df_100k3)

item_rate_count = df_100k3.groupby("item_id")["user_id"].nunique().sort_values(ascending=False)

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))
axes[0].bar(x=item_rate_count.index, height=item_rate_count.values, width=1.0, align="edge")
axes[0].set_xticks([])
axes[0].set(title="long tail of rating frequency", 
            xlabel="item ordered by decreasing frequency", 
            ylabel="#ratings")

count = item_rate_count.value_counts()
sns.scatterplot(x=np.log(count.index), y=np.log(count.values), ax=axes[1])
axes[1].set(title="log-log plot", xlabel="#ratings (log scale)", ylabel="#items (log scale)");

dataset = cornac.data.Dataset.from_uir(df.itertuples(index=False))
uknn_pearson = UserKNN(k=50, similarity="pearson", name="UserKNN-Pearson", verbose=False).fit(dataset)

uknn_pearson.train_set.uid_map[1]

def user_profiling(UID, df, user_df, item_df, TOPK=5):
  dataset = cornac.data.Dataset.from_uir(df.itertuples(index=False))
  uknn_pearson = UserKNN(k=50, similarity="pearson", name="UserKNN-Pearson", verbose=False).fit(dataset)

  rating_mat = uknn_pearson.train_set.matrix
  user_id2idx = uknn_pearson.train_set.uid_map
  user_idx2id = list(uknn_pearson.train_set.user_ids)
  item_id2idx = uknn_pearson.train_set.iid_map
  item_idx2id = list(uknn_pearson.train_set.item_ids)

  UIDX = uknn_pearson.train_set.uid_map[UID]

  print(f"UserID = {UID}")
  print("-" * 25)
  print(user_df.loc[UID])

  rating_arr = rating_mat[UIDX].A.ravel()
  top_rated_items = np.argsort(rating_arr)[-TOPK:]
  print(f"\nTOP {TOPK} RATED ITEMS BY USER {UID}:")
  print(item_df.loc[[int(item_idx2id[i]) for i in top_rated_items]])

user_profiling(2, df_100k3, df_user_100k, df_item_100k)

'''Let's do a simple experiment with the popularity approach. In this 
experiment, we will split the rating data into 5 folds for cross-validation. 
For each run, 4 folds will be used for training and the remaining fold will 
be used for evaluation. We measure the recommendation performance using 
Recall@20 metric.'''

def itempop_cornac(df):
  df = df.astype({'user_id':object, 'item_id':object})
  records = df.to_records(index=False)
  result = list(records)
  eval_method = cornac.eval_methods.CrossValidation(result, n_folds=5, seed=42) # 5-fold cross validation
  most_pop = cornac.models.MostPop() # recommender system based on item popularity
  rec_20 = cornac.metrics.Recall(k=20) # recall@20 metric
  cornac.Experiment(eval_method=eval_method, models=[most_pop], metrics=[rec_20]).run() # put everything together into an experiment

itempop_cornac(df_100k3)

def userknn_cornac(df):
  df = df.astype({'user_id':object, 'item_id':object})
  records = df.to_records(index=False)
  result = list(records)
  K = 50  # number of nearest neighbors
  VERBOSE = False
  SEED = 42
  uknn_cosine = UserKNN(k=K, similarity="cosine", name="UserKNN-Cosine", verbose=VERBOSE)
  uknn_cosine_mc = UserKNN(k=K, similarity="cosine", mean_centered=True, name="UserKNN-Cosine-MC", verbose=VERBOSE)
  uknn_pearson = UserKNN(k=K, similarity="pearson", name="UserKNN-Pearson", verbose=VERBOSE)
  uknn_pearson_mc = UserKNN(k=K, similarity="pearson", mean_centered=True, name="UserKNN-Pearson-MC", verbose=VERBOSE)
  ratio_split = RatioSplit(result, test_size=0.1, seed=SEED, verbose=VERBOSE)
  cornac.Experiment(eval_method=ratio_split,
                    models=[uknn_cosine, uknn_cosine_mc, uknn_pearson, uknn_pearson_mc],
                    metrics=[cornac.metrics.RMSE()],
                    ).run()

userknn_cornac(df_100k3)

def itemknn_cornac(df):
  df = df.astype({'user_id':object, 'item_id':object})
  records = df.to_records(index=False)
  result = list(records)
  K = 50  # number of nearest neighbors
  VERBOSE = False
  SEED = 42
  iknn_cosine = ItemKNN(k=K, similarity="cosine", name="ItemKNN-Cosine", verbose=VERBOSE)
  iknn_cosine_mc = ItemKNN(k=K, similarity="cosine", mean_centered=True, name="ItemKNN-AdjustedCosine", verbose=VERBOSE)
  iknn_pearson = ItemKNN(k=K, similarity="pearson", name="ItemKNN-Pearson", verbose=VERBOSE)
  iknn_pearson_mc = ItemKNN(k=K, similarity="pearson", mean_centered=True, name="ItemKNN-Pearson-MC", verbose=VERBOSE)
  ratio_split = RatioSplit(result, test_size=0.1, seed=SEED, verbose=VERBOSE)
  cornac.Experiment(eval_method=ratio_split,
                    models=[iknn_cosine, iknn_cosine_mc, iknn_pearson, iknn_pearson_mc],
                    metrics=[cornac.metrics.RMSE()],
                    ).run()

itemknn_cornac(df_100k3)

def mf_cornac(df):
  df = df.astype({'user_id':object, 'item_id':object})
  records = df.to_records(index=False)
  result = list(records)
  K = 100
  VERBOSE = False
  SEED = 42
  lbd = 0.01
  baseline = BaselineOnly(max_iter=20, learning_rate=0.01, lambda_reg=lbd, verbose=VERBOSE)
  mf1 = MF(k=K, max_iter=20, learning_rate=0.01, lambda_reg=0.0, use_bias=False, verbose=VERBOSE, seed=SEED, name=f"MF(K={K})")
  mf2 = MF(k=K, max_iter=20, learning_rate=0.01, lambda_reg=lbd, use_bias=False, verbose=VERBOSE, seed=SEED, name=f"MF(K={K},lambda={lbd:.4f})")
  mf3 = MF(k=K, max_iter=20, learning_rate=0.01, lambda_reg=lbd, use_bias=True, verbose=VERBOSE, seed=SEED, name=f"MF(K={K},bias)")
  nmf = NMF(k=K, max_iter=200, learning_rate=0.01, use_bias=False, verbose=VERBOSE, seed=SEED, name=f"NMF(K={K})")
  ratio_split = RatioSplit(result, test_size=0.1, seed=SEED, verbose=VERBOSE)
  cornac.Experiment(eval_method=ratio_split,
                    models=[baseline, mf1, mf2, mf3, nmf],
                    metrics=[cornac.metrics.RMSE()],
                    ).run()

mf_cornac(df_100k3)

def implicitmf_cornac(df):
  df = df.astype({'user_id':object, 'item_id':object})
  records = df.to_records(index=False)
  result = list(records)

  K = 50
  VERBOSE = False
  SEED = 42
  mf = MF(k=K, max_iter=20, learning_rate=0.01, lambda_reg=0.01, verbose=VERBOSE, seed=SEED, name=f"MF(K={K})")
  wmf = WMF(k=K, max_iter=100, a=1.0, b=0.01, learning_rate=0.001, lambda_u=0.01, lambda_v=0.01, verbose=VERBOSE, seed=SEED, name=f"WMF(K={K})")
  bpr = BPR(k=K, max_iter=200, learning_rate=0.01, lambda_reg=0.001, verbose=VERBOSE, seed=SEED, name=f"BPR(K={K})")

  eval_metrics = [
  cornac.metrics.RMSE(), 
  cornac.metrics.AUC(),
  cornac.metrics.Precision(k=10),
  cornac.metrics.Recall(k=10),
  cornac.metrics.FMeasure(k=10),
  cornac.metrics.NDCG(k=[10, 20, 30]),
  cornac.metrics.MRR(),
  cornac.metrics.MAP()
  ]

  rs = RatioSplit(result, test_size=0.2, seed=SEED, verbose=VERBOSE)

  cornac.Experiment(eval_method=rs,
                    models=[mf, wmf, bpr],
                    metrics=eval_metrics,
                    ).run()

implicitmf_cornac(df_100k3)

gmf = GMF(num_factors=GMF_FACTORS, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, 
          num_neg=NEG_SAMPLES, lr=LEARNING_RATE, seed=SEED, verbose=VERBOSE)
mlp = MLP(layers=MLP_LAYERS, act_fn=ACTIVATION, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, 
          num_neg=NEG_SAMPLES, lr=LEARNING_RATE, seed=SEED, verbose=VERBOSE)
neumf = NeuMF(num_factors=GMF_FACTORS, layers=MLP_LAYERS, act_fn=ACTIVATION, num_epochs=NUM_EPOCHS,
              num_neg=NEG_SAMPLES, batch_size=BATCH_SIZE, lr=LEARNING_RATE, seed=SEED, verbose=VERBOSE)

wmf = WMF(k=GMF_FACTORS, max_iter=200, learning_rate=0.001, seed=SEED, verbose=VERBOSE)

ml_100k = movielens.load_feedback(variant="100K", reader=Reader(bin_threshold=4.0))
ratio_split = RatioSplit(
  data=ml_100k, test_size=0.2, exclude_unknowns=True, seed=SEED, verbose=VERBOSE
)
ndcg_50 = cornac.metrics.NDCG(k=50)
rec_50 = cornac.metrics.Recall(k=50)

cornac.Experiment(
  eval_method=ratio_split,
  models=[gmf, mlp, neumf, wmf],
  metrics=[ndcg_50, rec_50],
).run()

vaecf = VAECF(k=NUM_FACTORS, autoencoder_structure=AE_LAYERS, act_fn=ACTIVATION,
              likelihood=LIKELIHOOD, n_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,
              learning_rate=LEARNING_RATE, seed=SEED, verbose=VERBOSE, use_gpu=True)

wmf = WMF(k=NUM_FACTORS, max_iter=200, learning_rate=0.001, seed=SEED, verbose=VERBOSE)

ml_100k = movielens.load_feedback(variant="100K", reader=Reader(bin_threshold=4.0))
ratio_split = RatioSplit(
  data=ml_100k, test_size=0.2, exclude_unknowns=True, seed=SEED, verbose=VERBOSE
)

cornac.Experiment(
  eval_method=ratio_split, models=[vaecf, wmf], metrics=[rec_50, ndcg_50],
).run()

import cornac
from cornac.eval_methods import RatioSplit
from cornac.models import MF, PMF, BPR
from cornac.metrics import MAE, RMSE, Precision, Recall, NDCG, AUC

# load the built-in MovieLens 100K and split the data based on ratio
ml_100k = cornac.datasets.movielens.load_feedback()
rs = RatioSplit(data=ml_100k, test_size=0.2, rating_threshold=4.0, seed=123)

# initialize models, here we are comparing: Biased MF, PMF, and BPR
models = [
    MF(k=10, max_iter=25, learning_rate=0.01, lambda_reg=0.02, use_bias=True, seed=123),
    PMF(k=10, max_iter=100, learning_rate=0.001, lambda_reg=0.001, seed=123),
    BPR(k=10, max_iter=200, learning_rate=0.001, lambda_reg=0.01, seed=123),
]

# define metrics to evaluate the models
metrics = [MAE(), RMSE(), Precision(k=10), Recall(k=10), NDCG(k=10), AUC()]

# put it together in an experiment, voilà!
cornac.Experiment(eval_method=rs, models=models, metrics=metrics, user_based=True).run()

import cornac
from cornac.datasets import movielens
from cornac.eval_methods import RatioSplit
from cornac.models import PMF


# Load the MovieLens 100K dataset
ml_100k = movielens.load_feedback()

# Instantiate an evaluation method.
ratio_split = RatioSplit(
    data=ml_100k, test_size=0.2, rating_threshold=4.0, exclude_unknowns=False
)

# Instantiate a PMF recommender model.
pmf = PMF(k=10, max_iter=100, learning_rate=0.001, lambda_reg=0.001)

# Instantiate evaluation metrics.
mae = cornac.metrics.MAE()
rmse = cornac.metrics.RMSE()
rec_20 = cornac.metrics.Recall(k=20)
pre_20 = cornac.metrics.Precision(k=20)

# Instantiate and then run an experiment.
cornac.Experiment(
    eval_method=ratio_split,
    models=[pmf],
    metrics=[mae, rmse, rec_20, pre_20],
    user_based=True,
).run()

from cornac.data import Reader
from cornac.eval_methods import BaseMethod
from cornac.models import MF
from cornac.metrics import MAE, RMSE
from cornac.utils import cache


# Download MovieLens 100K provided train and test sets
reader = Reader()
train_data = reader.read(
    cache(url="http://files.grouplens.org/datasets/movielens/ml-100k/u1.base")
)
test_data = reader.read(
    cache(url="http://files.grouplens.org/datasets/movielens/ml-100k/u1.test")
)

# Instantiate a Base evaluation method using the provided train and test sets
eval_method = BaseMethod.from_splits(
    train_data=train_data, test_data=test_data, exclude_unknowns=False, verbose=True
)

# Instantiate the MF model
mf = MF(
    k=10,
    max_iter=25,
    learning_rate=0.01,
    lambda_reg=0.02,
    use_bias=True,
    early_stop=True,
    verbose=True,
)

# Evaluation
test_result, val_result = eval_method.evaluate(
    model=mf, metrics=[MAE(), RMSE()], user_based=True
)
print(test_result)

"""Fit to and evaluate C2PF [1] on the Office Amazon dataset.
[1] Salah, Aghiles, and Hady W. Lauw. A Bayesian Latent Variable Model of User Preferences with Item Context. \
    In IJCAI, pp. 2667-2674. 2018.
"""

from cornac.data import GraphModality
from cornac.eval_methods import RatioSplit
from cornac.experiment import Experiment
from cornac import metrics
from cornac.models import C2PF
from cornac.datasets import amazon_office as office


# In addition to user-item feedback, C2PF integrates item-to-item contextual relationships
# The necessary data can be loaded as follows
ratings = office.load_feedback()
contexts = office.load_graph()

# Instantiate a GraphModality, it makes it convenient to work with graph (network) auxiliary information
# For more details, please refer to the tutorial on how to work with auxiliary data
item_graph_modality = GraphModality(data=contexts)

# Define an evaluation method to split feedback into train and test sets
ratio_split = RatioSplit(
    data=ratings,
    test_size=0.2,
    rating_threshold=3.5,
    exclude_unknowns=True,
    verbose=True,
    item_graph=item_graph_modality,
)

# Instantiate C2PF
c2pf = C2PF(k=100, max_iter=80, variant="c2pf")

# Evaluation metrics
ndcg = metrics.NDCG(k=-1)
mrr = metrics.MRR()
rec = metrics.Recall(k=20)
pre = metrics.Precision(k=20)

# Put everything together into an experiment and run it
Experiment(eval_method=ratio_split, models=[c2pf], metrics=[ndcg, mrr, rec, pre]).run()

"""Example for hyper-parameter searching with Matrix Factorization"""

import numpy as np
import cornac
from cornac.datasets import movielens
from cornac.eval_methods import RatioSplit
from cornac.hyperopt import Discrete, Continuous
from cornac.hyperopt import GridSearch, RandomSearch


# Load MovieLens 100K ratings
ml_100k = movielens.load_feedback(variant="100K")

# Define an evaluation method to split feedback into train, validation and test sets
ratio_split = RatioSplit(data=ml_100k, test_size=0.1, val_size=0.1, verbose=True)

# Instantiate MAE and RMSE for evaluation
mae = cornac.metrics.MAE()
rmse = cornac.metrics.RMSE()

# Define a base MF model with fixed hyper-parameters
mf = cornac.models.MF(max_iter=20, learning_rate=0.01, early_stop=True, verbose=True)

# Wrap MF model inside GridSearch along with the searching space
gs_mf = GridSearch(
    model=mf,
    space=[
        Discrete("k", [10, 30, 50]),
        Discrete("use_bias", [True, False]),
        Discrete("lambda_reg", [1e-1, 1e-2, 1e-3, 1e-4]),
    ],
    metric=rmse,
    eval_method=ratio_split,
)

# Wrap MF model inside RandomSearch along with the searching space, try 30 times
rs_mf = RandomSearch(
    model=mf,
    space=[
        Discrete("k", [10, 30, 50]),
        Discrete("use_bias", [True, False]),
        Continuous("lambda_reg", low=1e-4, high=1e-1),
    ],
    metric=rmse,
    eval_method=ratio_split,
    n_trails=30,
)

# Put everything together into an experiment and run it
cornac.Experiment(
    eval_method=ratio_split,
    models=[gs_mf, rs_mf],
    metrics=[mae, rmse],
    user_based=False,
).run()

import cornac
from cornac.data import Reader, GraphModality
from cornac.datasets import epinions
from cornac.eval_methods import RatioSplit


# SBPR integrates user social network into Bayesian Personalized Ranking.
# The necessary data can be loaded as follows
feedback = epinions.load_feedback(
    Reader(bin_threshold=4.0)
)  # feedback is binarised (turned into implicit) using Reader.
trust = epinions.load_trust()

# Instantiate a GraphModality, it makes it convenient to work with graph (network) auxiliary information
# For more details, please refer to the tutorial on how to work with auxiliary data
user_graph_modality = GraphModality(data=trust)

# Define an evaluation method to split feedback into train and test sets
ratio_split = RatioSplit(
    data=feedback,
    test_size=0.1,
    rating_threshold=0.5,
    exclude_unknowns=True,
    verbose=True,
    user_graph=user_graph_modality,
)

# Instantiate SBPR model
sbpr = cornac.models.SBPR(
    k=10,
    max_iter=50,
    learning_rate=0.001,
    lambda_u=0.015,
    lambda_v=0.025,
    lambda_b=0.01,
    verbose=True,
)

# Use Recall@10 for evaluation
rec_10 = cornac.metrics.Recall(k=10)

# Put everything together into an experiment and run it
cornac.Experiment(eval_method=ratio_split, models=[sbpr], metrics=[rec_10]).run()
