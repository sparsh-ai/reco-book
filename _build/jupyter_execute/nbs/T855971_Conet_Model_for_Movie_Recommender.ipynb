{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wvyb-8J6gj9E"
   },
   "source": [
    "# CoNet model\n",
    "> Applying Co-occurrence Neural Networks for Recommendation on MovieLens dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79GiSwT8T2jf"
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ci0tBUaOhAY"
   },
   "source": [
    "CoNet stands for *Co-occurrence Neural Networks for Recommendation Chinese explanation*. At present, most recommendation algorithms assume that users-users and commodities-commodities are independent and identically distributed, so they only model the interaction between users-commodities, and ignore the relationship between commodities and commodities. CoNet assumes that commodities always appear in pairs, that is, commodities co-occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMxk6cDv4Exp"
   },
   "source": [
    "<img src='https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F596ff4d4-d1f7-4c1b-84f0-86a82bde686a%2FUntitled.png?table=block&id=ff8ebedf-3c25-48b2-b232-d672f5fcfc44&spaceId=63b72b1f-0e90-4ab8-a6df-a060a6545a56&width=2000&userId=21ec183f-f0be-4b6b-9b3e-6f0d4e5c5469&cache=v2'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dABgGo43PEZz"
   },
   "source": [
    "1. Give a small example, as shown in the figure above. For example, \"Harry Potter 1\" and \"Harry Potter 2\" are always watched by users who like magic at the same time. This is the co-occurrence model of commodities. In order to learn this model, we need to model the user-commodity and the product-commodity at the same time.\n",
    "\n",
    "2. At the same time, CoNet assume that the more two commodities appear together, the more similar they are. For example, in the movie viewing record, \"Harry Potter 1\" and \"Harry Potter 2\" co-occur more than \"Harry Potter 1\" and \"Robot Walle\", we think that \"Harry Potter 1\" and \"Harry Potter 2\" are more similar.\n",
    "\n",
    "3. Further, CoNet use the attention mechanism to learn the user's comparative preferences. When rated the two products separately, authors gave the same score. For example, when they watched \"Harry Potter 1\" and \"Harry Potter 2\", they found them to look good, and gave them 5 points. However, when compare the two of them, it always felt that one of them might be better. Therefore, authors used the attention mechanism to model and learn this psychological preference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ay2bwrc1T4vU"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pj9k-BsT6bM"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import heapq # for retrieval topK\n",
    "import multiprocessing\n",
    "from time import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUavP4FHT6Xo"
   },
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIOItkgqTxp9"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZJ6UOC5T6TZ"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/sparsh-ai/reco-data/raw/master/ml-conet.zip\n",
    "!unzip ml-conet.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hy-nFOu1VeWi"
   },
   "outputs": [],
   "source": [
    "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSVrsL5tOgX_"
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "        self.trainMatrix = self.load_rating_file_as_matrix(path + \".train.rating\")\n",
    "        self.testRatings = self.load_rating_file_as_list(path + \".test.rating\")\n",
    "        self.testNegatives = self.load_negative_file(path + \".test.negative\")\n",
    "        assert len(self.testRatings) == len(self.testNegatives)\n",
    "        \n",
    "        self.num_users, self.num_items = self.trainMatrix.shape\n",
    "        \n",
    "    def load_rating_file_as_list(self, filename):\n",
    "        ratingList = []\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                user, item = int(arr[0]), int(arr[1])\n",
    "                ratingList.append([user, item])\n",
    "                line = f.readline()\n",
    "        return ratingList\n",
    "    \n",
    "    def load_negative_file(self, filename):\n",
    "        negativeList = []\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                negatives = []\n",
    "                for x in arr[1: ]:\n",
    "                    negatives.append(int(x))\n",
    "                negativeList.append(negatives)\n",
    "                line = f.readline()\n",
    "        return negativeList\n",
    "    \n",
    "    def load_rating_file_as_matrix(self, filename):\n",
    "        '''\n",
    "        Read .rating file and Return dok matrix.\n",
    "        The first line of .rating file is: num_users\\t num_items\n",
    "        '''\n",
    "        # Get number of users and items\n",
    "        num_users, num_items = 0, 0\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                u, i = int(arr[0]), int(arr[1])\n",
    "                num_users = max(num_users, u)\n",
    "                num_items = max(num_items, i)\n",
    "                line = f.readline()\n",
    "        # Construct matrix\n",
    "        mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "                if (rating > 0):\n",
    "                    mat[user, item] = 1.0\n",
    "                line = f.readline()    \n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wslT-mijUkMB"
   },
   "outputs": [],
   "source": [
    "def generate_instances(train_mat, positive_size=2, negative_time=8, is_sparse=False):\n",
    "    data = []\n",
    "    users_num,items_num = train_mat.shape\n",
    "    \n",
    "    if is_sparse:\n",
    "        indptr = train_mat.indptr\n",
    "        indices = train_mat.indices\n",
    "    for u in range(users_num):\n",
    "        if is_sparse:\n",
    "            rated_items = indices[indptr[u]:indptr[u+1]] #用户u中有评分项的id\n",
    "        else:\n",
    "            rated_items = np.where(train_mat[u,:]>0)[0]\n",
    "        \n",
    "        for item0 in rated_items:\n",
    "            for item1 in np.random.choice(rated_items, size=positive_size):\n",
    "                data.append([u,item0,item1,1.])\n",
    "            for _ in range(positive_size*negative_time):\n",
    "                item1 = np.random.randint(items_num) # no matter item1 is positive or negtive\n",
    "                item2 = np.random.randint(items_num)\n",
    "                while item2 in rated_items:\n",
    "                    item2 = np.random.randint(items_num)\n",
    "                data.append([u,item2,item1,0.])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHWD1KM3S8dS"
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikktWL7yTBCu"
   },
   "source": [
    "CoNet consists of 7 parts: input layer, embedding layer, attention module, co-occurrence layer, interaction layer, hidden layer and prediction layer (output layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5OZvciF4JAf"
   },
   "source": [
    "<img src='https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F19487e17-d979-48d5-8170-07bc7319bc74%2FUntitled.png?table=block&id=e49c8276-1ab0-4d87-8209-a9d25fdec6f7&spaceId=63b72b1f-0e90-4ab8-a6df-a060a6545a56&width=2000&userId=21ec183f-f0be-4b6b-9b3e-6f0d4e5c5469&cache=v2'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1k1exS1U6W5"
   },
   "outputs": [],
   "source": [
    "class CoNet(nn.Module):\n",
    "    def __init__(self, users_num, items_num, embedding_size_users=64, embedding_size_items = 64, \n",
    "                 hidden_size = [64,32,16,8], is_attention = False):\n",
    "        super(CoNet, self).__init__()\n",
    "        self.embedding_size_users, self.embedding_size_items= embedding_size_users, embedding_size_items \n",
    "        self.items_num, self.users_num = items_num, users_num\n",
    "        self.hidden_size, self.is_attention = hidden_size, is_attention\n",
    "        self.embedding_user  = nn.Embedding(self.users_num, self.embedding_size_users)\n",
    "        self.embedding_item = nn.Embedding(self.items_num, self.embedding_size_items)\n",
    "        self.layer1 = nn.Linear(self.embedding_size_users + self.embedding_size_items, self.hidden_size[0])\n",
    "        self.layers = [nn.Sequential(nn.Linear(self.hidden_size[i], self.hidden_size[i+1]), nn.ReLU()) for i in range(len(self.hidden_size) - 1)]\n",
    "        self.linear = nn.Linear(self.hidden_size[-1], 1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        embed_users = self.embedding_user(x[:,0])\n",
    "        embed_items0 = self.embedding_item(x[:,1])\n",
    "        embed_items1 = self.embedding_item(x[:,2])\n",
    "        \n",
    "        embed_items = (embed_items0 + embed_items1)/2.\n",
    "        if self.is_attention:\n",
    "            score0 = torch.reshape(torch.sum(embed_users * embed_items0, 1), shape=[-1,1])\n",
    "            score1 = torch.reshape(torch.sum(embed_users * embed_items1, 1), shape=[-1,1])\n",
    "            alpha = torch.sigmoid(score0 - score1)\n",
    "            embed_items = alpha * embed_items0 + (1. - alpha) * embed_items1\n",
    "            \n",
    "        out = torch.cat([embed_users, embed_items],1)\n",
    "        out = self.layer1(out)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        out = self.linear(out) \n",
    "        return out\n",
    "    \n",
    "    def predict(self, pairs, batch_size, verbose):\n",
    "        \"\"\"Computes predictions for a given set of user-item pairs.\n",
    "        Args:\n",
    "          pairs: A pair of lists (users, items) of the same length.\n",
    "          batch_size: unused.\n",
    "          verbose: unused.\n",
    "        Returns:\n",
    "          predictions: A list of the same length as users and items, such that\n",
    "          predictions[i] is the models prediction for (users[i], items[i]).\n",
    "        \"\"\"\n",
    "        del batch_size, verbose\n",
    "        num_examples = len(pairs[0])\n",
    "        assert num_examples == len(pairs[1])\n",
    "        predictions = np.empty(num_examples)\n",
    "        pairs = np.array(pairs, dtype=np.int16)\n",
    "        for i in range(num_examples):\n",
    "            x = np.c_[pairs[0][i],pairs[1][i],pairs[1][i]]\n",
    "            x = torch.from_numpy(x).long()\n",
    "            out = self.forward(x)\n",
    "            predictions[i] = out.reshape(-1).data.numpy()\n",
    "        return predictions\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        idx = torch.LongTensor([i for i in range(self.items_num)])\n",
    "        embeddings = self.embedding_item(idx)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50d_gmEjUF1x"
   },
   "source": [
    "### Evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "559LI-shUAM_"
   },
   "outputs": [],
   "source": [
    "# Global variables that are shared across processes\n",
    "_model = None\n",
    "_testRatings = None\n",
    "_testNegatives = None\n",
    "_K = None\n",
    "\n",
    "def evaluate_model(model, testRatings, testNegatives, K, num_thread):\n",
    "    \"\"\"\n",
    "    Evaluate the performance (Hit_Ratio, NDCG) of top-K recommendation\n",
    "    Return: score of each test rating.\n",
    "    \"\"\"\n",
    "    global _model\n",
    "    global _testRatings\n",
    "    global _testNegatives\n",
    "    global _K\n",
    "    _model = model\n",
    "    _testRatings = testRatings\n",
    "    _testNegatives = testNegatives\n",
    "    _K = K\n",
    "        \n",
    "    hits, ndcgs = [],[]\n",
    "    if(num_thread > 1): # Multi-thread\n",
    "        pool = multiprocessing.Pool(processes=num_thread)\n",
    "        res = pool.map(eval_one_rating, range(len(_testRatings)))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        hits = [r[0] for r in res]\n",
    "        ndcgs = [r[1] for r in res]\n",
    "        return (hits, ndcgs)\n",
    "    # Single thread\n",
    "    for idx in range(len(_testRatings)):\n",
    "        (hr,ndcg) = eval_one_rating(idx)\n",
    "        hits.append(hr)\n",
    "        ndcgs.append(ndcg)      \n",
    "    return (hits, ndcgs)\n",
    "\n",
    "def eval_one_rating(idx):\n",
    "    rating = _testRatings[idx]\n",
    "    items = _testNegatives[idx]\n",
    "    u = rating[0]\n",
    "    gtItem = rating[1]\n",
    "    items.append(gtItem)\n",
    "    # Get prediction scores\n",
    "    map_item_score = {}\n",
    "    users = np.full(len(items), u, dtype = 'int32')\n",
    "    predictions = _model.predict([users, np.array(items)], \n",
    "                                 batch_size=100, verbose=0)\n",
    "    for i in range(len(items)):\n",
    "        item = items[i]\n",
    "        map_item_score[item] = predictions[i]\n",
    "    items.pop()\n",
    "    \n",
    "    # Evaluate top rank list\n",
    "    ranklist = heapq.nlargest(_K, map_item_score, key=map_item_score.get)\n",
    "    hr = getHitRatio(ranklist, gtItem)\n",
    "    ndcg = getNDCG(ranklist, gtItem)\n",
    "    return (hr, ndcg)\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dTeXurSYIy0"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_ratings, test_negatives, K=10):\n",
    "    \"\"\"Helper that calls evaluate from the NCF libraries.\"\"\"\n",
    "    (hits, ndcgs) = evaluate_model(model, test_ratings, test_negatives, K=K, num_thread=1)\n",
    "    return np.array(hits).mean(), np.array(ndcgs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7sgokhqUrFK"
   },
   "outputs": [],
   "source": [
    "def get_similar_items(item_mat, idx, topk=5):\n",
    "    m,k = item_mat.shape\n",
    "    target_item = item_mat[idx,:]\n",
    "    target_mat = np.reshape(np.tile(target_item,m),(-1,k))\n",
    "    sim = [np.dot(target_mat[i], item_mat[i])/(np.linalg.norm(target_mat[i])*np.linalg.norm(item_mat[i])) \n",
    "           for i in range(m)] \n",
    "    sorted_items = np.argsort(-np.array(sim))\n",
    "    return sorted_items[:topk+1] # the most similar is itself\n",
    "\n",
    "def get_key(item_dict, value):\n",
    "    key = -1\n",
    "    for (k, v) in item_dict.items():\n",
    "        if v == value:\n",
    "            key = k\n",
    "    return key\n",
    "\n",
    "\n",
    "# read original records\n",
    "def get_item_dict(file_dir):\n",
    "    # output: \n",
    "    # N: the number of user;\n",
    "    # M: the number of item\n",
    "    # data: the list of rating information\n",
    "    user_ids_dict, rated_item_ids_dict = {},{}\n",
    "    N, M, u_idx, i_idx = 0,0,0,0 \n",
    "    data_rating = []\n",
    "    data_time = []\n",
    "    f = open(file_dir)\n",
    "    for line in f.readlines():\n",
    "        if '::' in line:\n",
    "            u, i, r = line.split('::')[:3]\n",
    "        elif ',' in line:\n",
    "            u, i, r = line.split(',')[:3]\n",
    "        else:\n",
    "            u, i, r = line.split()[:3]\n",
    "    \n",
    "        if u not in user_ids_dict:\n",
    "            user_ids_dict[u]=u_idx\n",
    "            u_idx+=1\n",
    "        if i not in rated_item_ids_dict:\n",
    "            rated_item_ids_dict[i]=i_idx\n",
    "            i_idx+=1\n",
    "        data_rating.append([user_ids_dict[u],rated_item_ids_dict[i],float(r)])\n",
    "    \n",
    "    f.close()\n",
    "    N = u_idx\n",
    "    M = i_idx\n",
    "\n",
    "    return rated_item_ids_dict\n",
    "\n",
    "# read id and its name\n",
    "def id_name(file_dir):\n",
    "    id_name_dict = {}\n",
    "    f = open(file_dir, 'r', encoding='latin-1')\n",
    "    for line in f.readlines():\n",
    "        movie_id, movie_name = line.split('|')[:2]\n",
    "        id_name_dict[int(movie_id)] = movie_name\n",
    "        \n",
    "    return id_name_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipZIayxlU-Kp"
   },
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCs4D7oxU_Wt"
   },
   "outputs": [],
   "source": [
    "def train(model, train_mat, test_ratings, test_negatives, users_num, items_num, train_list=None, test_list=None,\n",
    "          learning_rate = 1e-2, weight_decay=0.01, positive_size=1, negative_time=4, epochs=64, \n",
    "          batch_size=1024, topK=10, mode='hr'):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if train_list!=None:\n",
    "        train_mat= sequence2mat(sequence=train_list, N=users_num, M=items_num) # train data : user-item matrix\n",
    "        is_sparse = False\n",
    "    \n",
    "    hr_list=[]\n",
    "    ndcg_list=[]\n",
    "    hr, ndcg = evaluate(model, test_ratings, test_negatives, K=topK)\n",
    "    embeddings = model.get_embeddings()\n",
    "    hr_list.append(hr)\n",
    "    ndcg_list.append(ndcg)\n",
    "    print('Init: HR = %.4f, NDCG = %.4f' %(hr, ndcg))\n",
    "    best_hr, best_ndcg = hr, ndcg\n",
    "    for epoch in range(epochs):\n",
    "        data_sequence = generate_instances(train_mat, positive_size=positive_size, negative_time=negative_time, is_sparse=True)\n",
    "        #data_sequence = read_list(\"output/\" + str(epoch) + \".txt\")\n",
    "        \n",
    "        train_size = len(data_sequence)\n",
    "        np.random.shuffle(data_sequence)\n",
    "        batch_size = batch_size\n",
    "        total_batch = math.ceil(train_size/batch_size)\n",
    "\n",
    "        for batch in range(total_batch):\n",
    "            start = (batch*batch_size)% train_size\n",
    "            end = min(start + batch_size, train_size)\n",
    "            data_array = np.array(data_sequence[start:end])\n",
    "            x = torch.from_numpy(data_array[:,:3]).long()\n",
    "            y = torch.from_numpy(data_array[:,-1]).reshape(-1,1)\n",
    "            y_ = model(x)\n",
    "            loss = criterion(y_.float(), y.float())\n",
    "            optimizer.zero_grad()              # clear gradients for this training step\n",
    "            loss.backward()                    # backpropagation, compute gradients\n",
    "            optimizer.step()                   # apply gradients\n",
    "            \n",
    "        # Evaluation\n",
    "        hr, ndcg = evaluate(model, test_ratings, test_negatives, K=topK)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)\n",
    "        print('epoch=%d, loss=%.4f, HR=%.4f, NDCG=%.4f' %(epoch, loss, hr, ndcg))\n",
    "        \n",
    "        mlist = hr_list\n",
    "        if mode == 'ndcg':\n",
    "            mlist = ndcg_list\n",
    "        if (len(mlist) > 20) and (mlist[-2] < mlist[-3] > mlist[-1]):\n",
    "            best_hr, best_ndcg = hr_list[-3], ndcg_list[-3]\n",
    "            embeddings = model.get_embeddings()\n",
    "            break\n",
    "        best_hr, best_ndcg = hr, ndcg\n",
    "        embeddings = model.get_embeddings()\n",
    "            \n",
    "    print(\"End. Best HR = %.4f, NDCG = %.4f. \" %(best_hr, best_ndcg))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjM_mEC1VFLr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: #user=943, #item=1682, #train_pairs=99057, #test_pairs=943\n",
      "Init: HR = 0.1039, NDCG = 0.0442\n",
      "epoch=0, loss=0.0817, HR=0.4655, NDCG=0.2639\n",
      "epoch=1, loss=0.0533, HR=0.4836, NDCG=0.2628\n",
      "epoch=2, loss=0.0545, HR=0.5673, NDCG=0.3271\n",
      "epoch=3, loss=0.0580, HR=0.6235, NDCG=0.3512\n",
      "epoch=4, loss=0.0556, HR=0.6193, NDCG=0.3493\n",
      "epoch=5, loss=0.0545, HR=0.6373, NDCG=0.3633\n",
      "epoch=6, loss=0.0300, HR=0.6384, NDCG=0.3699\n",
      "epoch=7, loss=0.0482, HR=0.6596, NDCG=0.3723\n",
      "epoch=8, loss=0.0507, HR=0.6490, NDCG=0.3774\n",
      "epoch=9, loss=0.0433, HR=0.6564, NDCG=0.3802\n",
      "End. Best HR = 0.6564, NDCG = 0.3802. \n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '/content/100k'\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Dataset(dataset_path)\n",
    "train_mat, test_ratings, test_negatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "print('Dataset: #user=%d, #item=%d, #train_pairs=%d, #test_pairs=%d' \n",
    "      % (dataset.num_users, dataset.num_items, train_mat.nnz, len(test_ratings)))\n",
    "\n",
    "embedding_size_users = 64\n",
    "embedding_size_items = 64\n",
    "hidden_size = [64,32,16]\n",
    "is_attention = True\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "positive_size = 2\n",
    "negative_time = 8\n",
    "epochs = 10\n",
    "batch_size = 1024\n",
    "topK = 10\n",
    "mode = 'hr'\n",
    "seed = 18\n",
    "\n",
    "setup_seed(seed)\n",
    "# Initialize the model\n",
    "model = CoNet(users_num=dataset.num_users, items_num=dataset.num_items, embedding_size_users=embedding_size_users, \n",
    "              embedding_size_items=embedding_size_items, hidden_size=hidden_size, is_attention=is_attention)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Train and evaluate model\n",
    "embeddings = train(model=model, \n",
    "                  train_mat=train_mat.tocsr(), \n",
    "                  test_ratings=test_ratings, \n",
    "                  test_negatives=test_negatives, \n",
    "                  users_num=dataset.num_users, \n",
    "                  items_num=dataset.num_items,  \n",
    "                  learning_rate=learning_rate,\n",
    "                  weight_decay=weight_decay,\n",
    "                  positive_size=positive_size,\n",
    "                  negative_time=negative_time,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  topK=topK,\n",
    "                  mode=mode)\n",
    "print('----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVkrbwidTNr5"
   },
   "source": [
    "### Experiment results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH-TSQRP4NoW"
   },
   "source": [
    "<img src='https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F2cee8ae9-2577-4ae9-97bf-88806d00cb41%2FUntitled.png?table=block&id=a9555527-4c8e-4267-96d0-775e67ee5c2a&spaceId=63b72b1f-0e90-4ab8-a6df-a060a6545a56&width=2000&userId=21ec183f-f0be-4b6b-9b3e-6f0d4e5c5469&cache=v2'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGsE0hnlTgAW"
   },
   "source": [
    "### Effect of attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GrqNgxlTlGG"
   },
   "source": [
    "The red curve is the attention mechanism, and the blue is the result of not paying attention.\n",
    "\n",
    "In fact, it can be understood that the attention mechanism provides more parameters to fit the user's comparative preferences to achieve better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O62s4vrW4Ruu"
   },
   "source": [
    "<img src='https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F430f2295-d9bb-4966-8525-00863a52d192%2FUntitled.png?table=block&id=c91dd900-4d55-456f-97e9-bf8083479932&spaceId=63b72b1f-0e90-4ab8-a6df-a060a6545a56&width=2000&userId=21ec183f-f0be-4b6b-9b3e-6f0d4e5c5469&cache=v2'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swqo10z2YiqE"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0guTc2D3VQOg"
   },
   "outputs": [],
   "source": [
    "file_dir = '/content/ml-100k/u.item'\n",
    "id_name_dict = id_name(file_dir) # original id : movie name\n",
    "\n",
    "file_dir = '/content/ml-100k/u.data'\n",
    "item_dict = get_item_dict(file_dir) # original id : new id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZiL3xIuVR-_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MovieID: 113 ; MovieName: Horseman on the Roof, The (Hussard sur le toit, Le) (1995)\n",
      "\n",
      "1: Cérémonie, La (1995)\n",
      "\n",
      "2: Ed's Next Move (1996)\n",
      "\n",
      "3: Flower of My Secret, The (Flor de mi secreto, La) (1995)\n",
      "\n",
      "4: Margaret's Museum (1995)\n",
      "\n",
      "5: Thieves (Voleurs, Les) (1996)\n",
      "------------------------------------------------------------------\n",
      "MovieID: 347 ; MovieName: Wag the Dog (1997)\n",
      "\n",
      "1: Apt Pupil (1998)\n",
      "\n",
      "2: Deconstructing Harry (1997)\n",
      "\n",
      "3: Kolya (1996)\n",
      "\n",
      "4: Jackal, The (1997)\n",
      "\n",
      "5: Wings of the Dove, The (1997)\n",
      "------------------------------------------------------------------\n",
      "MovieID: 537 ; MovieName: My Own Private Idaho (1991)\n",
      "\n",
      "1: Bronx Tale, A (1993)\n",
      "\n",
      "2: Red Rock West (1992)\n",
      "\n",
      "3: Paris, Texas (1984)\n",
      "\n",
      "4: Carlito's Way (1993)\n",
      "\n",
      "5: Some Folks Call It a Sling Blade (1993)\n",
      "------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "movieid_list = [113, 347, 537]\n",
    "    \n",
    "for movieid in movieid_list:\n",
    "    print('MovieID:', movieid, '; MovieName:', id_name_dict[movieid])\n",
    "    original_id = str(movieid)\n",
    "    target_item = item_dict[original_id]\n",
    "\n",
    "    top5 = get_similar_items(embeddings.data.numpy(), idx=target_item)\n",
    "    movie_list = [get_key(item_dict=item_dict, value=i) for i in top5]\n",
    "    rec_list = [id_name_dict[int(movie_id)] for movie_id in movie_list[1:]]\n",
    "    for i in range(len(rec_list)):\n",
    "        print('\\n{0}: {1}'.format(i+1, rec_list[i]))\n",
    "    print('------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SVkrbwidTNr5"
   ],
   "name": "T855971_Conet_Model_for_Movie_Recommender.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}