{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sparsh-ai/reco-book/blob/stage/nbs/T331379_bayesian_personalized_ranking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFQKpBK2o-34"
   },
   "source": [
    "# BPR from scratch\n",
    "\n",
    "**Description:** Implementing BPR-MF model from scratch in multiple ways including pure-python and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzOufCLdjbTq"
   },
   "source": [
    "## BPR from scratch PyTorch referencing RecBole library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlQXlVMjhXkP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import xavier_normal_, constant_\n",
    "\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cM8Zu3fihUu"
   },
   "outputs": [],
   "source": [
    "def set_color(log, color, highlight=True):\n",
    "    color_set = ['black', 'red', 'green', 'yellow', 'blue', 'pink', 'cyan', 'white']\n",
    "    try:\n",
    "        index = color_set.index(color)\n",
    "    except:\n",
    "        index = len(color_set) - 1\n",
    "    prev_log = '\\033['\n",
    "    if highlight:\n",
    "        prev_log += '1;3'\n",
    "    else:\n",
    "        prev_log += '0;3'\n",
    "    prev_log += str(index) + 'm'\n",
    "    return prev_log + log + '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXT1qGUZiNmK"
   },
   "outputs": [],
   "source": [
    "class ModelType(Enum):\n",
    "    \"\"\"Type of models.\n",
    "    - ``GENERAL``: General Recommendation\n",
    "    - ``SEQUENTIAL``: Sequential Recommendation\n",
    "    - ``CONTEXT``: Context-aware Recommendation\n",
    "    - ``KNOWLEDGE``: Knowledge-based Recommendation\n",
    "    \"\"\"\n",
    "\n",
    "    GENERAL = 1\n",
    "    SEQUENTIAL = 2\n",
    "    CONTEXT = 3\n",
    "    KNOWLEDGE = 4\n",
    "    TRADITIONAL = 5\n",
    "    DECISIONTREE = 6\n",
    "\n",
    "\n",
    "class KGDataLoaderState(Enum):\n",
    "    \"\"\"States for Knowledge-based DataLoader.\n",
    "    - ``RSKG``: Return both knowledge graph information and user-item interaction information.\n",
    "    - ``RS``: Only return the user-item interaction.\n",
    "    - ``KG``: Only return the triplets with negative examples in a knowledge graph.\n",
    "    \"\"\"\n",
    "\n",
    "    RSKG = 1\n",
    "    RS = 2\n",
    "    KG = 3\n",
    "\n",
    "\n",
    "class EvaluatorType(Enum):\n",
    "    \"\"\"Type for evaluation metrics.\n",
    "    - ``RANKING``: Ranking-based metrics like NDCG, Recall, etc.\n",
    "    - ``VALUE``: Value-based metrics like AUC, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    RANKING = 1\n",
    "    VALUE = 2\n",
    "\n",
    "\n",
    "class InputType(Enum):\n",
    "    \"\"\"Type of Models' input.\n",
    "    - ``POINTWISE``: Point-wise input, like ``uid, iid, label``.\n",
    "    - ``PAIRWISE``: Pair-wise input, like ``uid, pos_iid, neg_iid``.\n",
    "    \"\"\"\n",
    "\n",
    "    POINTWISE = 1\n",
    "    PAIRWISE = 2\n",
    "    LISTWISE = 3\n",
    "\n",
    "\n",
    "class FeatureType(Enum):\n",
    "    \"\"\"Type of features.\n",
    "    - ``TOKEN``: Token features like user_id and item_id.\n",
    "    - ``FLOAT``: Float features like rating and timestamp.\n",
    "    - ``TOKEN_SEQ``: Token sequence features like review.\n",
    "    - ``FLOAT_SEQ``: Float sequence features like pretrained vector.\n",
    "    \"\"\"\n",
    "\n",
    "    TOKEN = 'token'\n",
    "    FLOAT = 'float'\n",
    "    TOKEN_SEQ = 'token_seq'\n",
    "    FLOAT_SEQ = 'float_seq'\n",
    "\n",
    "\n",
    "class FeatureSource(Enum):\n",
    "    \"\"\"Source of features.\n",
    "    - ``INTERACTION``: Features from ``.inter`` (other than ``user_id`` and ``item_id``).\n",
    "    - ``USER``: Features from ``.user`` (other than ``user_id``).\n",
    "    - ``ITEM``: Features from ``.item`` (other than ``item_id``).\n",
    "    - ``USER_ID``: ``user_id`` feature in ``inter_feat`` and ``user_feat``.\n",
    "    - ``ITEM_ID``: ``item_id`` feature in ``inter_feat`` and ``item_feat``.\n",
    "    - ``KG``: Features from ``.kg``.\n",
    "    - ``NET``: Features from ``.net``.\n",
    "    \"\"\"\n",
    "\n",
    "    INTERACTION = 'inter'\n",
    "    USER = 'user'\n",
    "    ITEM = 'item'\n",
    "    USER_ID = 'user_id'\n",
    "    ITEM_ID = 'item_id'\n",
    "    KG = 'kg'\n",
    "    NET = 'net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmI3-anMhyEC"
   },
   "outputs": [],
   "source": [
    "class AbstractRecommender(nn.Module):\n",
    "    r\"\"\"Base class for all models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.logger = getLogger()\n",
    "        super(AbstractRecommender, self).__init__()\n",
    "\n",
    "    def calculate_loss(self, interaction):\n",
    "        r\"\"\"Calculate the training loss for a batch data.\n",
    "        Args:\n",
    "            interaction (Interaction): Interaction class of the batch.\n",
    "        Returns:\n",
    "            torch.Tensor: Training loss, shape: []\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, interaction):\n",
    "        r\"\"\"Predict the scores between users and items.\n",
    "        Args:\n",
    "            interaction (Interaction): Interaction class of the batch.\n",
    "        Returns:\n",
    "            torch.Tensor: Predicted scores for given users and items, shape: [batch_size]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def full_sort_predict(self, interaction):\n",
    "        r\"\"\"full sort prediction function.\n",
    "        Given users, calculate the scores between users and all candidate items.\n",
    "        Args:\n",
    "            interaction (Interaction): Interaction class of the batch.\n",
    "        Returns:\n",
    "            torch.Tensor: Predicted scores for given users and all candidate items,\n",
    "            shape: [n_batch_users * n_candidate_items]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def other_parameter(self):\n",
    "        if hasattr(self, 'other_parameter_name'):\n",
    "            return {key: getattr(self, key) for key in self.other_parameter_name}\n",
    "        return dict()\n",
    "\n",
    "    def load_other_parameter(self, para):\n",
    "        if para is None:\n",
    "            return\n",
    "        for key, value in para.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Model prints with number of trainable parameters\n",
    "        \"\"\"\n",
    "        model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "        return super().__str__() + set_color('\\nTrainable parameters', 'blue') + f': {params}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EX8z1eC9h_o8"
   },
   "outputs": [],
   "source": [
    "class GeneralRecommender(AbstractRecommender):\n",
    "    \"\"\"This is a abstract general recommender. All the general model should implement this class.\n",
    "    The base general recommender class provide the basic dataset and parameters information.\n",
    "    \"\"\"\n",
    "    type = ModelType.GENERAL\n",
    "\n",
    "    def __init__(self, config, dataset):\n",
    "        super(GeneralRecommender, self).__init__()\n",
    "\n",
    "        # load dataset info\n",
    "        self.USER_ID = config['USER_ID_FIELD']\n",
    "        self.ITEM_ID = config['ITEM_ID_FIELD']\n",
    "        self.NEG_ITEM_ID = config['NEG_PREFIX'] + self.ITEM_ID\n",
    "        self.n_users = dataset.num(self.USER_ID)\n",
    "        self.n_items = dataset.num(self.ITEM_ID)\n",
    "\n",
    "        # load parameters info\n",
    "        self.device = config['device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ew6eY5uciqU-"
   },
   "outputs": [],
   "source": [
    "def xavier_normal_initialization(module):\n",
    "    r\"\"\" using `xavier_normal_`_ in PyTorch to initialize the parameters in\n",
    "    nn.Embedding and nn.Linear layers. For bias in nn.Linear layers,\n",
    "    using constant 0 to initialize.\n",
    "    .. _`xavier_normal_`:\n",
    "        https://pytorch.org/docs/stable/nn.init.html?highlight=xavier_normal_#torch.nn.init.xavier_normal_\n",
    "    Examples:\n",
    "        >>> self.apply(xavier_normal_initialization)\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Embedding):\n",
    "        xavier_normal_(module.weight.data)\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        xavier_normal_(module.weight.data)\n",
    "        if module.bias is not None:\n",
    "            constant_(module.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3pg3ICSi4Dr"
   },
   "outputs": [],
   "source": [
    "class BPRLoss(nn.Module):\n",
    "    \"\"\" BPRLoss, based on Bayesian Personalized Ranking\n",
    "    Args:\n",
    "        - gamma(float): Small value to avoid division by zero\n",
    "    Shape:\n",
    "        - Pos_score: (N)\n",
    "        - Neg_score: (N), same shape as the Pos_score\n",
    "        - Output: scalar.\n",
    "    Examples::\n",
    "        >>> loss = BPRLoss()\n",
    "        >>> pos_score = torch.randn(3, requires_grad=True)\n",
    "        >>> neg_score = torch.randn(3, requires_grad=True)\n",
    "        >>> output = loss(pos_score, neg_score)\n",
    "        >>> output.backward()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=1e-10):\n",
    "        super(BPRLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, pos_score, neg_score):\n",
    "        loss = -torch.log(self.gamma + torch.sigmoid(pos_score - neg_score)).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qp7Fd64bi8HL"
   },
   "outputs": [],
   "source": [
    "class BPR(GeneralRecommender):\n",
    "    r\"\"\"BPR is a basic matrix factorization model that be trained in the pairwise way.\n",
    "\n",
    "    \"\"\"\n",
    "    input_type = InputType.PAIRWISE\n",
    "\n",
    "    def __init__(self, config, dataset):\n",
    "        super(BPR, self).__init__(config, dataset)\n",
    "\n",
    "        # load parameters info\n",
    "        self.embedding_size = config['embedding_size']\n",
    "\n",
    "        # define layers and loss\n",
    "        self.user_embedding = nn.Embedding(self.n_users, self.embedding_size)\n",
    "        self.item_embedding = nn.Embedding(self.n_items, self.embedding_size)\n",
    "        self.loss = BPRLoss()\n",
    "\n",
    "        # parameters initialization\n",
    "        self.apply(xavier_normal_initialization)\n",
    "\n",
    "    def get_user_embedding(self, user):\n",
    "        r\"\"\" Get a batch of user embedding tensor according to input user's id.\n",
    "\n",
    "        Args:\n",
    "            user (torch.LongTensor): The input tensor that contains user's id, shape: [batch_size, ]\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: The embedding tensor of a batch of user, shape: [batch_size, embedding_size]\n",
    "        \"\"\"\n",
    "        return self.user_embedding(user)\n",
    "\n",
    "    def get_item_embedding(self, item):\n",
    "        r\"\"\" Get a batch of item embedding tensor according to input item's id.\n",
    "\n",
    "        Args:\n",
    "            item (torch.LongTensor): The input tensor that contains item's id, shape: [batch_size, ]\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: The embedding tensor of a batch of item, shape: [batch_size, embedding_size]\n",
    "        \"\"\"\n",
    "        return self.item_embedding(item)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        user_e = self.get_user_embedding(user)\n",
    "        item_e = self.get_item_embedding(item)\n",
    "        return user_e, item_e\n",
    "\n",
    "    def calculate_loss(self, interaction):\n",
    "        user = interaction[self.USER_ID]\n",
    "        pos_item = interaction[self.ITEM_ID]\n",
    "        neg_item = interaction[self.NEG_ITEM_ID]\n",
    "\n",
    "        user_e, pos_e = self.forward(user, pos_item)\n",
    "        neg_e = self.get_item_embedding(neg_item)\n",
    "        pos_item_score, neg_item_score = torch.mul(user_e, pos_e).sum(dim=1), torch.mul(user_e, neg_e).sum(dim=1)\n",
    "        loss = self.loss(pos_item_score, neg_item_score)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, interaction):\n",
    "        user = interaction[self.USER_ID]\n",
    "        item = interaction[self.ITEM_ID]\n",
    "        user_e, item_e = self.forward(user, item)\n",
    "        return torch.mul(user_e, item_e).sum(dim=1)\n",
    "\n",
    "    def full_sort_predict(self, interaction):\n",
    "        user = interaction[self.USER_ID]\n",
    "        user_e = self.get_user_embedding(user)\n",
    "        all_item_e = self.item_embedding.weight\n",
    "        score = torch.matmul(user_e, all_item_e.transpose(0, 1))\n",
    "        return score.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNNm3IN8jiEr"
   },
   "source": [
    "## BPR from scratch referencing cornac library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NJqvSp6glX16"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from tqdm import trange\n",
    "from subprocess import call\n",
    "from itertools import islice\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix, dok_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "K5T23WD_jjx5"
   },
   "outputs": [],
   "source": [
    "file_dir = 'ml-100k'\n",
    "file_path = os.path.join(file_dir, 'u.data')\n",
    "if not os.path.isdir(file_dir):\n",
    "    call(['curl', '-O', 'http://files.grouplens.org/datasets/movielens/' + file_dir + '.zip'])\n",
    "    call(['unzip', file_dir + '.zip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KY-En9ualQFN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimension: \n",
      " (100000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will not be using the timestamp column\n",
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv(file_path, sep = '\\t', names = names)\n",
    "print('data dimension: \\n', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRd_PdpKlMq3"
   },
   "source": [
    "Because BPR assumes binary implicit feedback (meaing there's only positive and negative items), here we'll assume that an item is positive only if he/she gave the item a ratings above 3 (feel free to experiment and change the threshold). The next few code chunks, creates the sparse interaction matrix and split into train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MROU2tKnlKqp"
   },
   "outputs": [],
   "source": [
    "def create_matrix(data, users_col, items_col, ratings_col, threshold = None):\n",
    "    \"\"\"\n",
    "    creates the sparse user-item interaction matrix,\n",
    "    if the data is not in the format where the interaction only\n",
    "    contains the positive items (indicated by 1), then use the \n",
    "    threshold parameter to determine which items are considered positive\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "        implicit rating data\n",
    "\n",
    "    users_col : str\n",
    "        user column name\n",
    "\n",
    "    items_col : str\n",
    "        item column name\n",
    "    \n",
    "    ratings_col : str\n",
    "        implicit rating column name\n",
    "\n",
    "    threshold : int, default None\n",
    "        threshold to determine whether the user-item pair is \n",
    "        a positive feedback\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ratings : scipy sparse csr_matrix, shape [n_users, n_items]\n",
    "        user/item ratings matrix\n",
    "\n",
    "    data : DataFrame\n",
    "        implict rating data that retains only the positive feedback\n",
    "        (if specified to do so)\n",
    "    \"\"\"\n",
    "    if threshold is not None:\n",
    "        data = data[data[ratings_col] >= threshold]\n",
    "        data[ratings_col] = 1\n",
    "    \n",
    "    for col in (items_col, users_col, ratings_col):\n",
    "        data[col] = data[col].astype('category')\n",
    "\n",
    "    ratings = csr_matrix((data[ratings_col],\n",
    "                          (data[users_col].cat.codes, data[items_col].cat.codes)))\n",
    "    ratings.eliminate_zeros()\n",
    "    return ratings, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Sgj5GOoslJPk"
   },
   "outputs": [],
   "source": [
    "items_col = 'item_id'\n",
    "users_col = 'user_id'\n",
    "ratings_col = 'rating'\n",
    "threshold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qQm5nF6BlH3F"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<943x1574 sparse matrix of type '<class 'numpy.longlong'>'\n",
       "\twith 82520 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, df = create_matrix(df, users_col, items_col, ratings_col, threshold)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dsSRZ3H2lF4y"
   },
   "outputs": [],
   "source": [
    "def create_train_test(ratings, test_size = 0.2, seed = 1234):\n",
    "    \"\"\"\n",
    "    split the user-item interactions matrix into train and test set\n",
    "    by removing some of the interactions from every user and pretend\n",
    "    that we never seen them\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ratings : scipy sparse csr_matrix, shape [n_users, n_items]\n",
    "        The user-item interactions matrix\n",
    "    \n",
    "    test_size : float between 0.0 and 1.0, default 0.2\n",
    "        Proportion of the user-item interactions for each user\n",
    "        in the dataset to move to the test set; e.g. if set to 0.2\n",
    "        and a user has 10 interactions, then 2 will be moved to the\n",
    "        test set\n",
    "    \n",
    "    seed : int, default 1234\n",
    "        Seed for reproducible random splitting the \n",
    "        data into train/test set\n",
    "    \n",
    "    Returns\n",
    "    ------- \n",
    "    train : scipy sparse csr_matrix, shape [n_users, n_items]\n",
    "        Training set\n",
    "    \n",
    "    test : scipy sparse csr_matrix, shape [n_users, n_items]\n",
    "        Test set\n",
    "    \"\"\"\n",
    "    assert test_size < 1.0 and test_size > 0.0\n",
    "\n",
    "    # Dictionary Of Keys based sparse matrix is more efficient\n",
    "    # for constructing sparse matrices incrementally compared with csr_matrix\n",
    "    train = ratings.copy().todok()\n",
    "    test = dok_matrix(train.shape)\n",
    "    \n",
    "    # for all the users assign randomly chosen interactions\n",
    "    # to the test and assign those interactions to zero in the training;\n",
    "    # when computing the interactions to go into the test set, \n",
    "    # remember to round up the numbers (e.g. a user has 4 ratings, if the\n",
    "    # test_size is 0.2, then 0.8 ratings will go to test, thus we need to\n",
    "    # round up to ensure the test set gets at least 1 rating)\n",
    "    rstate = np.random.RandomState(seed)\n",
    "    for u in range(ratings.shape[0]):\n",
    "        split_index = ratings[u].indices\n",
    "        n_splits = ceil(test_size * split_index.shape[0])\n",
    "        test_index = rstate.choice(split_index, size = n_splits, replace = False)\n",
    "        test[u, test_index] = ratings[u, test_index]\n",
    "        train[u, test_index] = 0\n",
    "    \n",
    "    train, test = train.tocsr(), test.tocsr()\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "pIzQWiFElEGq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<943x1574 sparse matrix of type '<class 'numpy.longlong'>'\n",
       "\twith 65641 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test = create_train_test(X, test_size = 0.2, seed = 1234)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Wra9k39k7xF"
   },
   "source": [
    "The following section provides a implementation of the algorithm from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "88aLcnS4k9Py"
   },
   "outputs": [],
   "source": [
    "class BPR:\n",
    "    \"\"\"\n",
    "    Bayesian Personalized Ranking (BPR) for implicit feedback data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate : float, default 0.01\n",
    "        learning rate for gradient descent\n",
    "\n",
    "    n_factors : int, default 20\n",
    "        Number/dimension of user and item latent factors\n",
    "\n",
    "    n_iters : int, default 15\n",
    "        Number of iterations to train the algorithm\n",
    "        \n",
    "    batch_size : int, default 1000\n",
    "        batch size for batch gradient descent, the original paper\n",
    "        uses stochastic gradient descent (i.e., batch size of 1),\n",
    "        but this can make the training unstable (very sensitive to\n",
    "        learning rate)\n",
    "\n",
    "    reg : int, default 0.01\n",
    "        Regularization term for the user and item latent factors\n",
    "\n",
    "    seed : int, default 1234\n",
    "        Seed for the randomly initialized user, item latent factors\n",
    "\n",
    "    verbose : bool, default True\n",
    "        Whether to print progress bar while training\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    user_factors : 2d ndarray, shape [n_users, n_factors]\n",
    "        User latent factors learnt\n",
    "\n",
    "    item_factors : 2d ndarray, shape [n_items, n_factors]\n",
    "        Item latent factors learnt\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    S. Rendle, C. Freudenthaler, Z. Gantner, L. Schmidt-Thieme \n",
    "    Bayesian Personalized Ranking from Implicit Feedback\n",
    "    - https://arxiv.org/abs/1205.2618\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate = 0.01, n_factors = 15, n_iters = 10, \n",
    "                 batch_size = 1000, reg = 0.01, seed = 1234, verbose = True):\n",
    "        self.reg = reg\n",
    "        self.seed = seed\n",
    "        self.verbose = verbose\n",
    "        self.n_iters = n_iters\n",
    "        self.n_factors = n_factors\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # to avoid re-computation at predict\n",
    "        self._prediction = None\n",
    "        \n",
    "    def fit(self, ratings):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        ratings : scipy sparse csr_matrix, shape [n_users, n_items]\n",
    "            sparse matrix of user-item interactions\n",
    "        \"\"\"\n",
    "        indptr = ratings.indptr\n",
    "        indices = ratings.indices\n",
    "        n_users, n_items = ratings.shape\n",
    "        \n",
    "        # ensure batch size makes sense, since the algorithm involves\n",
    "        # for each step randomly sample a user, thus the batch size\n",
    "        # should be smaller than the total number of users or else\n",
    "        # we would be sampling the user with replacement\n",
    "        batch_size = self.batch_size\n",
    "        if n_users < batch_size:\n",
    "            batch_size = n_users\n",
    "            sys.stderr.write('WARNING: Batch size is greater than number of users,'\n",
    "                             'switching to a batch size of {}\\n'.format(n_users))\n",
    "\n",
    "        batch_iters = n_users // batch_size\n",
    "        \n",
    "        # initialize random weights\n",
    "        rstate = np.random.RandomState(self.seed)\n",
    "        self.user_factors = rstate.normal(size = (n_users, self.n_factors))\n",
    "        self.item_factors = rstate.normal(size = (n_items, self.n_factors))\n",
    "        \n",
    "        # progress bar for training iteration if verbose is turned on\n",
    "        loop = range(self.n_iters)\n",
    "        if self.verbose:\n",
    "            loop = trange(self.n_iters, desc = self.__class__.__name__)\n",
    "        \n",
    "        for _ in loop:\n",
    "            for _ in range(batch_iters):\n",
    "                sampled = self._sample(n_users, n_items, indices, indptr)\n",
    "                sampled_users, sampled_pos_items, sampled_neg_items = sampled\n",
    "                self._update(sampled_users, sampled_pos_items, sampled_neg_items)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def _sample(self, n_users, n_items, indices, indptr):\n",
    "        \"\"\"sample batches of random triplets u, i, j\"\"\"\n",
    "        sampled_pos_items = np.zeros(self.batch_size, dtype = np.int)\n",
    "        sampled_neg_items = np.zeros(self.batch_size, dtype = np.int)\n",
    "        sampled_users = np.random.choice(\n",
    "            n_users, size = self.batch_size, replace = False)\n",
    "\n",
    "        for idx, user in enumerate(sampled_users):\n",
    "            pos_items = indices[indptr[user]:indptr[user + 1]]\n",
    "            pos_item = np.random.choice(pos_items)\n",
    "            neg_item = np.random.choice(n_items)\n",
    "            while neg_item in pos_items:\n",
    "                neg_item = np.random.choice(n_items)\n",
    "\n",
    "            sampled_pos_items[idx] = pos_item\n",
    "            sampled_neg_items[idx] = neg_item\n",
    "\n",
    "        return sampled_users, sampled_pos_items, sampled_neg_items\n",
    "                \n",
    "    def _update(self, u, i, j):\n",
    "        \"\"\"\n",
    "        update according to the bootstrapped user u, \n",
    "        positive item i and negative item j\n",
    "        \"\"\"\n",
    "        user_u = self.user_factors[u]\n",
    "        item_i = self.item_factors[i]\n",
    "        item_j = self.item_factors[j]\n",
    "        \n",
    "        # decompose the estimator, compute the difference between\n",
    "        # the score of the positive items and negative items; a\n",
    "        # naive implementation might look like the following:\n",
    "        # r_ui = np.diag(user_u.dot(item_i.T))\n",
    "        # r_uj = np.diag(user_u.dot(item_j.T))\n",
    "        # r_uij = r_ui - r_uj\n",
    "        \n",
    "        # however, we can do better, so\n",
    "        # for batch dot product, instead of doing the dot product\n",
    "        # then only extract the diagonal element (which is the value\n",
    "        # of that current batch), we perform a hadamard product, \n",
    "        # i.e. matrix element-wise product then do a sum along the column will\n",
    "        # be more efficient since it's less operations\n",
    "        # http://people.revoledu.com/kardi/tutorial/LinearAlgebra/HadamardProduct.html\n",
    "        # r_ui = np.sum(user_u * item_i, axis = 1)\n",
    "        #\n",
    "        # then we can achieve another speedup by doing the difference\n",
    "        # on the positive and negative item up front instead of computing\n",
    "        # r_ui and r_uj separately, these two idea will speed up the operations\n",
    "        # from 1:14 down to 0.36\n",
    "        r_uij = np.sum(user_u * (item_i - item_j), axis = 1)\n",
    "        sigmoid = np.exp(-r_uij) / (1.0 + np.exp(-r_uij))\n",
    "        \n",
    "        # repeat the 1 dimension sigmoid n_factors times so\n",
    "        # the dimension will match when doing the update\n",
    "        sigmoid_tiled = np.tile(sigmoid, (self.n_factors, 1)).T\n",
    "\n",
    "        # update using gradient descent\n",
    "        grad_u = sigmoid_tiled * (item_j - item_i) + self.reg * user_u\n",
    "        grad_i = sigmoid_tiled * -user_u + self.reg * item_i\n",
    "        grad_j = sigmoid_tiled * user_u + self.reg * item_j\n",
    "        self.user_factors[u] -= self.learning_rate * grad_u\n",
    "        self.item_factors[i] -= self.learning_rate * grad_i\n",
    "        self.item_factors[j] -= self.learning_rate * grad_j\n",
    "        return self\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Obtain the predicted ratings for every users and items\n",
    "        by doing a dot product of the learnt user and item vectors.\n",
    "        The result will be cached to avoid re-computing it every time\n",
    "        we call predict, thus there will only be an overhead the first\n",
    "        time we call it. Note, ideally you probably don't need to compute\n",
    "        this as it returns a dense matrix and may take up huge amounts of\n",
    "        memory for large datasets\n",
    "        \"\"\"\n",
    "        if self._prediction is None:\n",
    "            self._prediction = self.user_factors.dot(self.item_factors.T)\n",
    "\n",
    "        return self._prediction\n",
    "\n",
    "    def _predict_user(self, user):\n",
    "        \"\"\"\n",
    "        returns the predicted ratings for the specified user,\n",
    "        this is mainly used in computing evaluation metric\n",
    "        \"\"\"\n",
    "        user_pred = self.user_factors[user].dot(self.item_factors.T)\n",
    "        return user_pred\n",
    "\n",
    "    def recommend(self, ratings, N = 5):\n",
    "        \"\"\"\n",
    "        Returns the top N ranked items for given user id,\n",
    "        excluding the ones that the user already liked\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        ratings : scipy sparse csr_matrix, shape [n_users, n_items]\n",
    "            sparse matrix of user-item interactions \n",
    "        \n",
    "        N : int, default 5\n",
    "            top-N similar items' N\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        recommendation : 2d ndarray, shape [number of users, N]\n",
    "            each row is the top-N ranked item for each query user\n",
    "        \"\"\"\n",
    "        n_users = ratings.shape[0]\n",
    "        recommendation = np.zeros((n_users, N), dtype = np.uint32)\n",
    "        for user in range(n_users):\n",
    "            top_n = self._recommend_user(ratings, user, N)\n",
    "            recommendation[user] = top_n\n",
    "\n",
    "        return recommendation\n",
    "\n",
    "    def _recommend_user(self, ratings, user, N):\n",
    "        \"\"\"the top-N ranked items for a given user\"\"\"\n",
    "        scores = self._predict_user(user)\n",
    "\n",
    "        # compute the top N items, removing the items that the user already liked\n",
    "        # from the result and ensure that we don't get out of bounds error when \n",
    "        # we ask for more recommendations than that are available\n",
    "        liked = set(ratings[user].indices)\n",
    "        count = N + len(liked)\n",
    "        if count < scores.shape[0]:\n",
    "\n",
    "            # when trying to obtain the top-N indices from the score,\n",
    "            # using argpartition to retrieve the top-N indices in \n",
    "            # unsorted order and then sort them will be faster than doing\n",
    "            # straight up argort on the entire score\n",
    "            # http://stackoverflow.com/questions/42184499/cannot-understand-numpy-argpartition-output\n",
    "            ids = np.argpartition(scores, -count)[-count:]\n",
    "            best_ids = np.argsort(scores[ids])[::-1]\n",
    "            best = ids[best_ids]\n",
    "        else:\n",
    "            best = np.argsort(scores)[::-1]\n",
    "\n",
    "        top_n = list(islice((rec for rec in best if rec not in liked), N))\n",
    "        return top_n\n",
    "    \n",
    "    def get_similar_items(self, N = 5, item_ids = None):\n",
    "        \"\"\"\n",
    "        return the top N similar items for itemid, where\n",
    "        cosine distance is used as the distance metric\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        N : int, default 5\n",
    "            top-N similar items' N\n",
    "            \n",
    "        item_ids : 1d iterator, e.g. list or numpy array, default None\n",
    "            the item ids that we wish to find the similar items\n",
    "            of, the default None will compute the similar items\n",
    "            for all the items\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        similar_items : 2d ndarray, shape [number of query item_ids, N]\n",
    "            each row is the top-N most similar item id for each\n",
    "            query item id\n",
    "        \"\"\"\n",
    "        # cosine distance is proportional to normalized euclidean distance,\n",
    "        # thus we normalize the item vectors and use euclidean metric so\n",
    "        # we can use the more efficient kd-tree for nearest neighbor search;\n",
    "        # also the item will always to nearest to itself, so we add 1 to \n",
    "        # get an additional nearest item and remove itself at the end\n",
    "        normed_factors = normalize(self.item_factors)\n",
    "        knn = NearestNeighbors(n_neighbors = N + 1, metric = 'euclidean')\n",
    "        knn.fit(normed_factors)\n",
    "\n",
    "        # returns a distance, index tuple,\n",
    "        # we don't actually need the distance\n",
    "        if item_ids is not None:\n",
    "            normed_factors = normed_factors[item_ids]\n",
    "\n",
    "        _, items = knn.kneighbors(normed_factors)\n",
    "        similar_items = items[:, 1:].astype(np.uint32)\n",
    "        return similar_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "zdd2n-33lBEH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BPR: 100%|██████████| 160/160 [00:02<00:00, 59.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.BPR at 0x7f545b0ca390>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters were randomly chosen\n",
    "bpr_params = {'reg': 0.01,\n",
    "              'learning_rate': 0.1,\n",
    "              'n_iters': 160,\n",
    "              'n_factors': 15,\n",
    "              'batch_size': 100}\n",
    "\n",
    "bpr = BPR(**bpr_params)\n",
    "bpr.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vZzKWgykx8v"
   },
   "source": [
    "### Evaluation\n",
    "In recommender systems, we are often interested in how well the method can rank a given set of items. And to do that we'll use AUC (Area Under ROC Curve as our evaluation metric. The best possible value that the AUC evaluation metric can take is 1, and any non-random ranking that makes sense would have an AUC > 0.5. An intuitive explanation of AUC is it specifies the probability that when we draw two examples at random, their predicted pairwise ranking is correct. The following documentation has a more detailed discussion on AUC in case you're not familiar with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9Orl71G1kzig"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8971582016228873\n",
      "0.8278732737462875\n"
     ]
    }
   ],
   "source": [
    "def auc_score(model, ratings):\n",
    "    \"\"\"\n",
    "    computes area under the ROC curve (AUC).\n",
    "    The full name should probably be mean\n",
    "    auc score as it is computing the auc\n",
    "    for every user's prediction and actual\n",
    "    interaction and taking the average for\n",
    "    all users\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : BPR instance\n",
    "        Trained BPR model\n",
    "        \n",
    "    ratings : scipy sparse csr_matrix, shape [n_users, n_items]\n",
    "        sparse matrix of user-item interactions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    auc : float 0.0 ~ 1.0\n",
    "    \"\"\"\n",
    "    auc = 0.0\n",
    "    n_users, n_items = ratings.shape\n",
    "    for user, row in enumerate(ratings):\n",
    "        y_pred = model._predict_user(user)\n",
    "        y_true = np.zeros(n_items)\n",
    "        y_true[row.indices] = 1\n",
    "        auc += roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    auc /= n_users\n",
    "    return auc\n",
    "\n",
    "print(auc_score(bpr, X_train))\n",
    "print(auc_score(bpr, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYMYeQEIkmaN"
   },
   "source": [
    "### Item Recommendations\n",
    "Given the trained model, we can get the most similar items by using the get_similar_items method, we can specify the number of most similar items by specifying the N argument. And this can be seen as the people who like/buy this also like/buy this functionality, since it's recommending similar items for a given item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "vIR5p0qMkm_Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 236,   24,  449,  150,  281],\n",
       "       [  53,  687,  163,  430,  722],\n",
       "       [1028, 1163,  678, 1380, 1159],\n",
       "       ...,\n",
       "       [1016,  724, 1336,  762,  530],\n",
       "       [ 863, 1443,  664,  114, 1258],\n",
       "       [1465,  815, 1139,  457,  728]], dtype=uint32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpr.get_similar_items(N = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "sWbtfgAmkp3A"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[492, 421,  10, 316, 508],\n",
       "       [293, 671,  99, 319, 123],\n",
       "       [285, 241, 743, 291, 271],\n",
       "       ...,\n",
       "       [469, 621, 403, 178, 274],\n",
       "       [287, 744, 313, 293, 741],\n",
       "       [233, 153, 477, 203, 490]], dtype=uint32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpr.recommend(X_train, N = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUGKT-vClyIg"
   },
   "source": [
    "For these two methods, we can go one step further and look-up the actual item for these indices to see if they make intuitive sense. If we wish to do this, the movielens dataset has a u.item file that contains metadata about the movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAR8iFNhmhlu"
   },
   "source": [
    "## BPR from scratch in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfKNHLT9nCbX"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JKJh70rrmjXm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users.npy           100%[===================>]   5.37K  --.-KB/s    in 0s      \n",
      "movies.npy          100%[===================>]  70.95K  --.-KB/s    in 0.005s  \n",
      "small_ratings.npy   100%[===================>]  14.75K  --.-KB/s    in 0s      \n"
     ]
    }
   ],
   "source": [
    "!wget -q --show-progress https://github.com/leafinity/gradient_dscent_svd/raw/master/the-movies-dataset/numpy/users.npy\n",
    "!wget -q --show-progress https://github.com/leafinity/gradient_dscent_svd/raw/master/the-movies-dataset/numpy/movies.npy\n",
    "!wget -q --show-progress https://github.com/leafinity/gradient_dscent_svd/raw/master/the-movies-dataset/numpy/small_ratings.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "6xiIBYK3oRub"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta.zip            100%[===================>]  12.20M  33.1MB/s    in 0.4s    \n",
      "Archive:  meta.zip\n",
      "  inflating: movies_metadata.csv     \n"
     ]
    }
   ],
   "source": [
    "# this link is temporary, you can generate a new one by visiting kaggle data site https://www.kaggle.com/rounakbanik/the-movies-dataset\n",
    "!wget -O meta.zip -q --show-progress \"https://storage.googleapis.com/kaggle-data-sets/3405/6663/compressed/movies_metadata.csv.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20211001%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211001T113251Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=3144a3fa20ed4c96a11e2c8a91ea021b3ecbdd0d8b7452ede9395d43bd93b0a808cf1ec71d08416ff2c12e7d8861c8154fb4936b7544f78de4668b1dc21926cc7848b44447eeae139fd5e31fe43c8ae7abe8c39e1d8c5fc61e88a6d7d10805a67740fefd53d38908d73e34f902a5afdc0008dbecfa3873a7bb55740ef127e90e6f95056bfe7d7dc91e8f1306153918dc8bcc3f22224d13ea4a4e639fb09abf5a0470d7ad0f1c8b320192be2f2b04b317d95c11de6c5e618ef95d7fe99745d1200ed83f65c4ae534342e97bd638fb50ea0e27b05a2a1c358fa4529a3f442ec8c9d95e75780a3ede5849fefa66a9b1767de4cce3a49815c117806f4dbae0553b47\"\n",
    "!unzip meta.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "sfpfsxzDmutX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "rli5q1XAmxfV"
   },
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mYBOX7vnEKj"
   },
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "q14G9s3amx03"
   },
   "outputs": [],
   "source": [
    "rating = np.load('small_ratings.npy')\n",
    "users = np.load('users.npy')[:100]\n",
    "movies = np.load('movies.npy')[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K11EONpSnGny"
   },
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "SRyg0iyfm0Vh"
   },
   "outputs": [],
   "source": [
    "users_num, movies_num, k = len(users), len(movies), 5\n",
    "rating_len = len(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "-luFbm-Sm0x9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "99.0\n",
      "99\n",
      "624\n"
     ]
    }
   ],
   "source": [
    "print(rating[:, 2].max())\n",
    "print(rating[:, 0].max())\n",
    "print(len(users)-1)\n",
    "print(rating_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-JNAUN2Hm4eB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# normalization\n",
    "rating[:, 2] -= 2.5\n",
    "rating[:, 2] /= 2.5\n",
    "\n",
    "# thita\n",
    "W = torch.randn(users_num, k, device=device, dtype=dtype) / 10\n",
    "H = torch.randn(movies_num, k, device=device, dtype=dtype) / 10\n",
    "\n",
    "print(rating[:, 2].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "m9Nt7TsAm60n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 10, 8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = []\n",
    "for ui, i, ri in rating:\n",
    "    for uj, j, rj in rating:\n",
    "        if ui != uj or i == j:\n",
    "            continue\n",
    "        if ri > rj:\n",
    "            ds.append((int(ui), int(i), int(j)))\n",
    "\n",
    "ds = list(set(ds))\n",
    "\n",
    "ds[np.random.randint(len(ds))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgD6s1q_m-bN"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "txDA7UBcnJJs"
   },
   "outputs": [],
   "source": [
    "def predict(u, i):\n",
    "    Wu = W[u].view(1, W[u].size()[0])\n",
    "    return torch.mv(Wu, H[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "SXXIM2AenNs7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0175)\n"
     ]
    }
   ],
   "source": [
    "def predict_diff(u, i, j):\n",
    "    return  (predict(u, i) - predict(u, j))[0]\n",
    "\n",
    "print(predict_diff(0, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "MESdIow3nOxB"
   },
   "outputs": [],
   "source": [
    "def partial_BPR(x_uij, partial_x):\n",
    "    exp_x = np.exp(-x_uij)\n",
    "    return exp_x / (1 + exp_x) * partial_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "llgnrfYMnQo_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0390, -0.0523,  0.0951,  0.0222,  0.2095])\n",
      "tensor([-0.0755,  0.0155, -0.0040, -0.0755,  0.0861])\n",
      "tensor([ 0.0385, -0.0506,  0.0953,  0.0244,  0.2105])\n",
      "tensor([-0.0757,  0.0148, -0.0038, -0.0756,  0.0867])\n",
      "tensor([ 0.0380, -0.0487,  0.0959,  0.0272,  0.2116])\n",
      "tensor([-0.0758,  0.0139, -0.0039, -0.0754,  0.0870])\n",
      "tensor([ 0.0374, -0.0471,  0.0965,  0.0294,  0.2126])\n",
      "tensor([-0.0756,  0.0131, -0.0040, -0.0754,  0.0873])\n",
      "tensor([ 0.0373, -0.0451,  0.0966,  0.0319,  0.2139])\n",
      "tensor([-0.0758,  0.0123, -0.0040, -0.0756,  0.0878])\n",
      "tensor([ 0.0367, -0.0433,  0.0969,  0.0343,  0.2153])\n",
      "tensor([-0.0758,  0.0111, -0.0043, -0.0755,  0.0880])\n",
      "tensor([ 0.0362, -0.0422,  0.0977,  0.0368,  0.2163])\n",
      "tensor([-0.0758,  0.0101, -0.0041, -0.0756,  0.0885])\n",
      "tensor([ 0.0359, -0.0408,  0.0986,  0.0397,  0.2174])\n",
      "tensor([-0.0760,  0.0092, -0.0041, -0.0757,  0.0889])\n",
      "tensor([ 0.0357, -0.0396,  0.0990,  0.0417,  0.2190])\n",
      "tensor([-0.0762,  0.0083, -0.0044, -0.0758,  0.0893])\n",
      "tensor([ 0.0352, -0.0383,  0.0997,  0.0439,  0.2205])\n",
      "tensor([-0.0762,  0.0075, -0.0042, -0.0759,  0.0898])\n"
     ]
    }
   ],
   "source": [
    "iteration = 100000\n",
    "lr = 1e-4\n",
    "\n",
    "def train(W, H, lr=1e-3, rr=0.02):\n",
    "    for itr in range(iteration):\n",
    "        u, i, j = ds[np.random.randint(len(ds))]\n",
    "        x_uij = predict_diff(u, i, j)\n",
    "\n",
    "        for f in range(k):\n",
    "            W[u][f] -= lr * (partial_BPR(x_uij, H[i][f] - H[j][f]) + rr * W[u][f])\n",
    "            H[i][f] -= lr * (partial_BPR(x_uij, W[u][f]) + rr * H[i][f])\n",
    "            H[j][f] -= lr * (partial_BPR(x_uij, -W[u][f]) + rr * H[f][f])\n",
    "        \n",
    "        if itr % 10000 == 0:\n",
    "            print(W[18])\n",
    "            print(H[0])\n",
    "            \n",
    "    return W, H\n",
    "\n",
    "W, H = train(W, H, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "pWmYk73KnSHP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0351, -0.0369,  0.1005,  0.0462,  0.2218])\n",
      "tensor([-0.0766,  0.0066, -0.0043, -0.0759,  0.0903])\n"
     ]
    }
   ],
   "source": [
    "print(W[18])\n",
    "print(H[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Bs5Xs4YhnTvx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0351, -0.0369,  0.1005,  0.0462,  0.2218])\n",
      "tensor([-0.0766,  0.0066, -0.0043, -0.0759,  0.0903])\n"
     ]
    }
   ],
   "source": [
    "print(W[18])\n",
    "print(H[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBxUPPmGnVBr"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "MQD_Ykd9nWov"
   },
   "outputs": [],
   "source": [
    "user_id = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "66iXyRrKnYeb"
   },
   "outputs": [],
   "source": [
    "Wu = W[user_id].view(1, W[user_id].size()[0])\n",
    "prediction = list(zip(list(range(movies_num)), torch.mm(Wu, H.t()).tolist()[0]))\n",
    "prediction.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "aorziDEBnYbm"
   },
   "outputs": [],
   "source": [
    "movie_rates = []\n",
    "movie_predict_rates = []\n",
    "\n",
    "for u, i, r in rating:\n",
    "    if u == user_id:\n",
    "        movie_rates.append((int(i), r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "5p1XW6pOnYZT"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "movie_data = []\n",
    "df = pd.read_csv('movies_metadata.csv')\n",
    "\n",
    "for index, row in df.iloc[:, [3, 8]].iterrows():\n",
    "    movie_data += [{'title': row['original_title'], 'genres': [x['name'] for x in json.loads(row['genres'].replace('\\'', '\"'))]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "9GhIiDp7nYU4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User  19\n",
      "from rating, he/she likes:\n",
      "movie_id               movie_title                                movie_genres\n",
      "      13                     Nixon                        ['History', 'Drama']\n",
      "      15                    Casino                          ['Drama', 'Crime']\n",
      "      33                      Babe    ['Fantasy', 'Drama', 'Comedy', 'Family']\n",
      "      46                     Se7en            ['Crime', 'Mystery', 'Thriller']\n",
      "      49        The Usual Suspects              ['Drama', 'Crime', 'Thriller']\n",
      "      69       From Dusk Till Dawn   ['Horror', 'Action', 'Thriller', 'Crime']\n",
      "      96                  Shopping ['Action', 'Adventure', 'Drama', 'Science Fiction']\n",
      "\n",
      "from rating, he/she might like:\n",
      "movie_id               movie_title                                movie_genres\n",
      "      36    Across the Sea of Time ['Adventure', 'History', 'Drama', 'Family']\n",
      "      45  How To Make An American                         ['Drama', 'Romance']\n",
      "      14          Cutthroat Island                     ['Action', 'Adventure']\n",
      "      11  Dracula: Dead and Loving                        ['Comedy', 'Horror']\n",
      "     107  Headless Body in Topless                                   ['Crime']\n"
     ]
    }
   ],
   "source": [
    "print('User ', users[user_id])\n",
    "print('from rating, he/she likes:')\n",
    "print('%s %25s %43s' % ('movie_id', 'movie_title', 'movie_genres'))\n",
    "for m, r in movie_rates:\n",
    "    if r > 0.5:\n",
    "        mid = movies[m]-1\n",
    "        print('%8s %25s %43s' % (mid, movie_data[mid]['title'][:24], movie_data[mid]['genres'][:4]))\n",
    "\n",
    "print('')\n",
    "print('from rating, he/she might like:')\n",
    "print('%s %25s %43s' % ('movie_id', 'movie_title', 'movie_genres'))\n",
    "for m, r in prediction[:5]:\n",
    "    mid = movies[m]-1\n",
    "    r = r * 2.5 + 2.5\n",
    "    print('%8s %25s %43s' % (mid, movie_data[mid]['title'][:24], movie_data[mid]['genres'][:4]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "T331379_bayesian_personalized_ranking.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}