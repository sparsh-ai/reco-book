{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sparsh-ai/reco-book/blob/stage/nbs/T472955_GCSAN_Session_based_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEZ7VF0_qJDP"
   },
   "source": [
    "# GCSAN Session-based Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITlrRkkHp1Y_"
   },
   "source": [
    "Session-based recommendation, which aims to predict the userâ€™s immediate next action based on anonymous sessions, is a key task in many online services (e.g. e-commerce, media streaming). Recently, Self-Attention Network (SAN) has achieved significant success in various sequence modeling tasks without using either recurrent or convolutional network. However, SAN lacks local dependencies that exist over adjacent items and limits its capacity for learning contextualized representations of items in sequences. In this paper, we propose a graph contextualized self-attention model (GC-SAN), which utilizes both graph neural network and self-attention mechanism, for session-based recommendation. In GC-SAN, we dynamically construct a graph structure for session sequences and capture rich local dependencies via graph neural network (GNN). Then each session learns long-range dependencies by applying the self-attention mechanism. Finally, each session is represented as a linear combination of the global preference and the current interest of that session. Extensive experiments on two real-world datasets show that GC-SAN outperforms state-of-the-art methods consistently.\n",
    "\n",
    "<img src='https://s3.us-west-2.amazonaws.com/secure.notion-static.com/3e6982bf-c712-4079-a8ec-9a8d7c4307d5/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20211011%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20211011T092042Z&X-Amz-Expires=86400&X-Amz-Signature=d92a0879ffc1455bebd69a1b0b71d3e2b4e167baa146fd2f7132fe4ed556dbf0&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22'>\n",
    "\n",
    "The general architecture of the proposed model. We first construct a directed graph of all session sequences. Based on the graph, we apply graph neural network to obtain all node vectors involved in the session graph. After that, we use a multi-layer self-attention network to capture long-range dependencies between items in the session. In prediction layer, we represent each session as a linear of the global preference and the current interest of that session. Finally, we compute the ranking scores of each candidate item for recommendation.\n",
    "\n",
    "<img src='https://s3.us-west-2.amazonaws.com/secure.notion-static.com/285b4be3-11a7-40c4-9b24-b54a9337f3d4/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20211011%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20211011T092105Z&X-Amz-Expires=86400&X-Amz-Signature=98648aab12b68ae0909069effd3a27d00f41512520b861bbd5cacdf60d521dc0&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pyLgtYzWgk0H"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYZROGBRg3o2"
   },
   "outputs": [],
   "source": [
    "class SequentialRecommender(AbstractRecommender):\n",
    "    \"\"\"\n",
    "    This is a abstract sequential recommender. All the sequential model should implement This class.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, dataset):\n",
    "        super(SequentialRecommender, self).__init__()\n",
    "\n",
    "        # load dataset info\n",
    "        self.USER_ID = config['USER_ID_FIELD']\n",
    "        self.ITEM_ID = config['ITEM_ID_FIELD']\n",
    "        self.ITEM_SEQ = self.ITEM_ID + config['LIST_SUFFIX']\n",
    "        self.ITEM_SEQ_LEN = config['ITEM_LIST_LENGTH_FIELD']\n",
    "        self.POS_ITEM_ID = self.ITEM_ID\n",
    "        self.NEG_ITEM_ID = config['NEG_PREFIX'] + self.ITEM_ID\n",
    "        self.max_seq_length = config['MAX_ITEM_LIST_LENGTH']\n",
    "        self.n_items = dataset.num(self.ITEM_ID)\n",
    "\n",
    "    def gather_indexes(self, output, gather_index):\n",
    "        \"\"\"Gathers the vectors at the specific positions over a minibatch\"\"\"\n",
    "        gather_index = gather_index.view(-1, 1, 1).expand(-1, -1, output.shape[-1])\n",
    "        output_tensor = output.gather(dim=1, index=gather_index)\n",
    "        return output_tensor.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dksftU-h_Wc"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head Self-attention layers, a attention score dropout layer is introduced.\n",
    "    Args:\n",
    "        input_tensor (torch.Tensor): the input of the multi-head self-attention layer\n",
    "        attention_mask (torch.Tensor): the attention mask for input tensor\n",
    "    Returns:\n",
    "        hidden_states (torch.Tensor): the output of the multi-head self-attention layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, hidden_size, hidden_dropout_prob, attn_dropout_prob, layer_norm_eps):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        if hidden_size % n_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (hidden_size, n_heads)\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = n_heads\n",
    "        self.attention_head_size = int(hidden_size / n_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout_prob)\n",
    "\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
    "        self.out_dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask):\n",
    "        mixed_query_layer = self.query(input_tensor)\n",
    "        mixed_key_layer = self.key(input_tensor)\n",
    "        mixed_value_layer = self.value(input_tensor)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "        # [batch_size heads seq_len seq_len] scores\n",
    "        # [batch_size 1 1 seq_len]\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        hidden_states = self.dense(context_layer)\n",
    "        hidden_states = self.out_dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4N4FVR-hh4zh"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Point-wise feed-forward layer is implemented by two dense layers.\n",
    "    Args:\n",
    "        input_tensor (torch.Tensor): the input of the point-wise feed-forward layer\n",
    "    Returns:\n",
    "        hidden_states (torch.Tensor): the output of the point-wise feed-forward layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, inner_size, hidden_dropout_prob, hidden_act, layer_norm_eps):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.dense_1 = nn.Linear(hidden_size, inner_size)\n",
    "        self.intermediate_act_fn = self.get_hidden_act(hidden_act)\n",
    "\n",
    "        self.dense_2 = nn.Linear(inner_size, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def get_hidden_act(self, act):\n",
    "        ACT2FN = {\n",
    "            \"gelu\": self.gelu,\n",
    "            \"relu\": fn.relu,\n",
    "            \"swish\": self.swish,\n",
    "            \"tanh\": torch.tanh,\n",
    "            \"sigmoid\": torch.sigmoid,\n",
    "        }\n",
    "        return ACT2FN[act]\n",
    "\n",
    "    def gelu(self, x):\n",
    "        \"\"\"Implementation of the gelu activation function.\n",
    "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results)::\n",
    "            0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "        Also see https://arxiv.org/abs/1606.08415\n",
    "        \"\"\"\n",
    "        return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "    def swish(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        hidden_states = self.dense_1(input_tensor)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "\n",
    "        hidden_states = self.dense_2(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RQP1Pp6hxCS"
   },
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    One transformer layer consists of a multi-head self-attention layer and a point-wise feed-forward layer.\n",
    "    Args:\n",
    "        hidden_states (torch.Tensor): the input of the multi-head self-attention sublayer\n",
    "        attention_mask (torch.Tensor): the attention mask for the multi-head self-attention sublayer\n",
    "    Returns:\n",
    "        feedforward_output (torch.Tensor): The output of the point-wise feed-forward sublayer,\n",
    "                                           is the output of the transformer layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_heads, hidden_size, intermediate_size, hidden_dropout_prob, attn_dropout_prob, hidden_act,\n",
    "        layer_norm_eps\n",
    "    ):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(\n",
    "            n_heads, hidden_size, hidden_dropout_prob, attn_dropout_prob, layer_norm_eps\n",
    "        )\n",
    "        self.feed_forward = FeedForward(hidden_size, intermediate_size, hidden_dropout_prob, hidden_act, layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        attention_output = self.multi_head_attention(hidden_states, attention_mask)\n",
    "        feedforward_output = self.feed_forward(attention_output)\n",
    "        return feedforward_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTQZ2HkShrZw"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    r\"\"\" One TransformerEncoder consists of several TransformerLayers.\n",
    "        - n_layers(num): num of transformer layers in transformer encoder. Default: 2\n",
    "        - n_heads(num): num of attention heads for multi-head attention layer. Default: 2\n",
    "        - hidden_size(num): the input and output hidden size. Default: 64\n",
    "        - inner_size(num): the dimensionality in feed-forward layer. Default: 256\n",
    "        - hidden_dropout_prob(float): probability of an element to be zeroed. Default: 0.5\n",
    "        - attn_dropout_prob(float): probability of an attention score to be zeroed. Default: 0.5\n",
    "        - hidden_act(str): activation function in feed-forward layer. Default: 'gelu'\n",
    "                      candidates: 'gelu', 'relu', 'swish', 'tanh', 'sigmoid'\n",
    "        - layer_norm_eps(float): a value added to the denominator for numerical stability. Default: 1e-12\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers=2,\n",
    "        n_heads=2,\n",
    "        hidden_size=64,\n",
    "        inner_size=256,\n",
    "        hidden_dropout_prob=0.5,\n",
    "        attn_dropout_prob=0.5,\n",
    "        hidden_act='gelu',\n",
    "        layer_norm_eps=1e-12\n",
    "    ):\n",
    "\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        layer = TransformerLayer(\n",
    "            n_heads, hidden_size, inner_size, hidden_dropout_prob, attn_dropout_prob, hidden_act, layer_norm_eps\n",
    "        )\n",
    "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): the input of the TransformerEncoder\n",
    "            attention_mask (torch.Tensor): the attention mask for the input hidden_states\n",
    "            output_all_encoded_layers (Bool): whether output all transformer layers' output\n",
    "        Returns:\n",
    "            all_encoder_layers (list): if output_all_encoded_layers is True, return a list consists of all transformer\n",
    "            layers' output, otherwise return a list only consists of the output of last transformer layer.\n",
    "        \"\"\"\n",
    "        all_encoder_layers = []\n",
    "        for layer_module in self.layer:\n",
    "            hidden_states = layer_module(hidden_states, attention_mask)\n",
    "            if output_all_encoded_layers:\n",
    "                all_encoder_layers.append(hidden_states)\n",
    "        if not output_all_encoded_layers:\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "        return all_encoder_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86jlA1NliKMP"
   },
   "outputs": [],
   "source": [
    "class BPRLoss(nn.Module):\n",
    "    \"\"\" BPRLoss, based on Bayesian Personalized Ranking\n",
    "    Args:\n",
    "        - gamma(float): Small value to avoid division by zero\n",
    "    Shape:\n",
    "        - Pos_score: (N)\n",
    "        - Neg_score: (N), same shape as the Pos_score\n",
    "        - Output: scalar.\n",
    "    Examples::\n",
    "        >>> loss = BPRLoss()\n",
    "        >>> pos_score = torch.randn(3, requires_grad=True)\n",
    "        >>> neg_score = torch.randn(3, requires_grad=True)\n",
    "        >>> output = loss(pos_score, neg_score)\n",
    "        >>> output.backward()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=1e-10):\n",
    "        super(BPRLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, pos_score, neg_score):\n",
    "        loss = -torch.log(self.gamma + torch.sigmoid(pos_score - neg_score)).mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class EmbLoss(nn.Module):\n",
    "    \"\"\" EmbLoss, regularization on embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, norm=2):\n",
    "        super(EmbLoss, self).__init__()\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, *embeddings):\n",
    "        emb_loss = torch.zeros(1).to(embeddings[-1].device)\n",
    "        for embedding in embeddings:\n",
    "            emb_loss += torch.norm(embedding, p=self.norm)\n",
    "        emb_loss /= embeddings[-1].shape[0]\n",
    "        return emb_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7_uHNd7iQGd"
   },
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    r\"\"\"Graph neural networks are well-suited for session-based recommendation,\n",
    "    because it can automatically extract features of session graphs with considerations of rich node connections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_size, step=1):\n",
    "        super(GNN, self).__init__()\n",
    "        self.step = step\n",
    "        self.embedding_size = embedding_size\n",
    "        self.input_size = embedding_size * 2\n",
    "        self.gate_size = embedding_size * 3\n",
    "        self.w_ih = Parameter(torch.Tensor(self.gate_size, self.input_size))\n",
    "        self.w_hh = Parameter(torch.Tensor(self.gate_size, self.embedding_size))\n",
    "        self.b_ih = Parameter(torch.Tensor(self.gate_size))\n",
    "        self.b_hh = Parameter(torch.Tensor(self.gate_size))\n",
    "\n",
    "        self.linear_edge_in = nn.Linear(self.embedding_size, self.embedding_size, bias=True)\n",
    "        self.linear_edge_out = nn.Linear(self.embedding_size, self.embedding_size, bias=True)\n",
    "\n",
    "        # parameters initialization\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.embedding_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def GNNCell(self, A, hidden):\n",
    "        r\"\"\"Obtain latent vectors of nodes via gated graph neural network.\n",
    "\n",
    "        Args:\n",
    "            A (torch.FloatTensor): The connection matrix,shape of [batch_size, max_session_len, 2 * max_session_len]\n",
    "\n",
    "            hidden (torch.FloatTensor): The item node embedding matrix, shape of\n",
    "                [batch_size, max_session_len, embedding_size]\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: Latent vectors of nodes,shape of [batch_size, max_session_len, embedding_size]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        input_in = torch.matmul(A[:, :, :A.size(1)], self.linear_edge_in(hidden))\n",
    "        input_out = torch.matmul(A[:, :, A.size(1):2 * A.size(1)], self.linear_edge_out(hidden))\n",
    "        # [batch_size, max_session_len, embedding_size * 2]\n",
    "        inputs = torch.cat([input_in, input_out], 2)\n",
    "\n",
    "        # gi.size equals to gh.size, shape of [batch_size, max_session_len, embedding_size * 3]\n",
    "        gi = F.linear(inputs, self.w_ih, self.b_ih)\n",
    "        gh = F.linear(hidden, self.w_hh, self.b_hh)\n",
    "        # (batch_size, max_session_len, embedding_size)\n",
    "        i_r, i_i, i_n = gi.chunk(3, 2)\n",
    "        h_r, h_i, h_n = gh.chunk(3, 2)\n",
    "        reset_gate = torch.sigmoid(i_r + h_r)\n",
    "        input_gate = torch.sigmoid(i_i + h_i)\n",
    "        new_gate = torch.tanh(i_n + reset_gate * h_n)\n",
    "        hy = (1 - input_gate) * hidden + input_gate * new_gate\n",
    "        return hy\n",
    "\n",
    "    def forward(self, A, hidden):\n",
    "        for i in range(self.step):\n",
    "            hidden = self.GNNCell(A, hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHDoJAl7iVZc"
   },
   "outputs": [],
   "source": [
    "class GCSAN(SequentialRecommender):\n",
    "    r\"\"\"GCSAN captures rich local dependencies via graph neural network,\n",
    "     and learns long-range dependencies by applying the self-attention mechanism.\n",
    "     \n",
    "    Note:\n",
    "\n",
    "        In the original paper, the attention mechanism in the self-attention layer is a single head,\n",
    "        for the reusability of the project code, we use a unified transformer component.\n",
    "        According to the experimental results, we only applied regularization to embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, dataset):\n",
    "        super(GCSAN, self).__init__(config, dataset)\n",
    "\n",
    "        # load parameters info\n",
    "        self.n_layers = config['n_layers']\n",
    "        self.n_heads = config['n_heads']\n",
    "        self.hidden_size = config['hidden_size']  # same as embedding_size\n",
    "        self.inner_size = config['inner_size']  # the dimensionality in feed-forward layer\n",
    "        self.hidden_dropout_prob = config['hidden_dropout_prob']\n",
    "        self.attn_dropout_prob = config['attn_dropout_prob']\n",
    "        self.hidden_act = config['hidden_act']\n",
    "        self.layer_norm_eps = config['layer_norm_eps']\n",
    "\n",
    "        self.step = config['step']\n",
    "        self.device = config['device']\n",
    "        self.weight = config['weight']\n",
    "        self.reg_weight = config['reg_weight']\n",
    "        self.loss_type = config['loss_type']\n",
    "        self.initializer_range = config['initializer_range']\n",
    "\n",
    "        # define layers and loss\n",
    "        self.item_embedding = nn.Embedding(self.n_items, self.hidden_size, padding_idx=0)\n",
    "        self.gnn = GNN(self.hidden_size, self.step)\n",
    "        self.self_attention = TransformerEncoder(\n",
    "            n_layers=self.n_layers,\n",
    "            n_heads=self.n_heads,\n",
    "            hidden_size=self.hidden_size,\n",
    "            inner_size=self.inner_size,\n",
    "            hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "            attn_dropout_prob=self.attn_dropout_prob,\n",
    "            hidden_act=self.hidden_act,\n",
    "            layer_norm_eps=self.layer_norm_eps\n",
    "        )\n",
    "        self.reg_loss = EmbLoss()\n",
    "        if self.loss_type == 'BPR':\n",
    "            self.loss_fct = BPRLoss()\n",
    "        elif self.loss_type == 'CE':\n",
    "            self.loss_fct = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            raise NotImplementedError(\"Make sure 'loss_type' in ['BPR', 'CE']!\")\n",
    "\n",
    "        # parameters initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def get_attention_mask(self, item_seq):\n",
    "        \"\"\"Generate left-to-right uni-directional attention mask for multi-head attention.\"\"\"\n",
    "        attention_mask = (item_seq > 0).long()\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # torch.int64\n",
    "        # mask for left-to-right unidirectional\n",
    "        max_len = attention_mask.size(-1)\n",
    "        attn_shape = (1, max_len, max_len)\n",
    "        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1)  # torch.uint8\n",
    "        subsequent_mask = (subsequent_mask == 0).unsqueeze(1)\n",
    "        subsequent_mask = subsequent_mask.long().to(item_seq.device)\n",
    "\n",
    "        extended_attention_mask = extended_attention_mask * subsequent_mask\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        return extended_attention_mask\n",
    "\n",
    "\n",
    "    def _get_slice(self, item_seq):\n",
    "        items, n_node, A, alias_inputs = [], [], [], []\n",
    "        max_n_node = item_seq.size(1)\n",
    "        item_seq = item_seq.cpu().numpy()\n",
    "\n",
    "        for u_input in item_seq:\n",
    "            node = np.unique(u_input)\n",
    "            items.append(node.tolist() + (max_n_node - len(node)) * [0])\n",
    "            u_A = np.zeros((max_n_node, max_n_node))\n",
    "            for i in np.arange(len(u_input) - 1):\n",
    "                if u_input[i + 1] == 0:\n",
    "                    break\n",
    "                u = np.where(node == u_input[i])[0][0]\n",
    "                v = np.where(node == u_input[i + 1])[0][0]\n",
    "                u_A[u][v] = 1\n",
    "            u_sum_in = np.sum(u_A, 0)\n",
    "            u_sum_in[np.where(u_sum_in == 0)] = 1\n",
    "            u_A_in = np.divide(u_A, u_sum_in)\n",
    "            u_sum_out = np.sum(u_A, 1)\n",
    "            u_sum_out[np.where(u_sum_out == 0)] = 1\n",
    "            u_A_out = np.divide(u_A.transpose(), u_sum_out)\n",
    "            u_A = np.concatenate([u_A_in, u_A_out]).transpose()\n",
    "            A.append(u_A)\n",
    "\n",
    "            alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n",
    "        # The relative coordinates of the item node, shape of [batch_size, max_session_len]\n",
    "        alias_inputs = torch.LongTensor(alias_inputs).to(self.device)\n",
    "        # The connecting matrix, shape of [batch_size, max_session_len, 2 * max_session_len]\n",
    "        A = torch.FloatTensor(A).to(self.device)\n",
    "        # The unique item nodes, shape of [batch_size, max_session_len]\n",
    "        items = torch.LongTensor(items).to(self.device)\n",
    "\n",
    "        return alias_inputs, A, items\n",
    "\n",
    "    def forward(self, item_seq, item_seq_len):\n",
    "        assert 0 <= self.weight <= 1\n",
    "        alias_inputs, A, items = self._get_slice(item_seq)\n",
    "        hidden = self.item_embedding(items)\n",
    "        hidden = self.gnn(A, hidden)\n",
    "        alias_inputs = alias_inputs.view(-1, alias_inputs.size(1), 1).expand(-1, -1, self.hidden_size)\n",
    "        seq_hidden = torch.gather(hidden, dim=1, index=alias_inputs)\n",
    "        # fetch the last hidden state of last timestamp\n",
    "        ht = self.gather_indexes(seq_hidden, item_seq_len - 1)\n",
    "        a = seq_hidden\n",
    "        attention_mask = self.get_attention_mask(item_seq)\n",
    "\n",
    "        outputs = self.self_attention(a, attention_mask, output_all_encoded_layers=True)\n",
    "        output = outputs[-1]\n",
    "        at = self.gather_indexes(output, item_seq_len - 1)\n",
    "        seq_output = self.weight * at + (1 - self.weight) * ht\n",
    "        return seq_output\n",
    "\n",
    "\n",
    "    def calculate_loss(self, interaction):\n",
    "        item_seq = interaction[self.ITEM_SEQ]\n",
    "        item_seq_len = interaction[self.ITEM_SEQ_LEN]\n",
    "        seq_output = self.forward(item_seq, item_seq_len)\n",
    "        pos_items = interaction[self.POS_ITEM_ID]\n",
    "        if self.loss_type == 'BPR':\n",
    "            neg_items = interaction[self.NEG_ITEM_ID]\n",
    "            pos_items_emb = self.item_embedding(pos_items)\n",
    "            neg_items_emb = self.item_embedding(neg_items)\n",
    "            pos_score = torch.sum(seq_output * pos_items_emb, dim=-1)  # [B]\n",
    "            neg_score = torch.sum(seq_output * neg_items_emb, dim=-1)  # [B]\n",
    "            loss = self.loss_fct(pos_score, neg_score)\n",
    "        else:  # self.loss_type = 'CE'\n",
    "            test_item_emb = self.item_embedding.weight\n",
    "            logits = torch.matmul(seq_output, test_item_emb.transpose(0, 1))\n",
    "            loss = self.loss_fct(logits, pos_items)\n",
    "\n",
    "        reg_loss = self.reg_loss(self.item_embedding.weight)\n",
    "        total_loss = loss + self.reg_weight * reg_loss\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    def predict(self, interaction):\n",
    "        item_seq = interaction[self.ITEM_SEQ]\n",
    "        item_seq_len = interaction[self.ITEM_SEQ_LEN]\n",
    "        test_item = interaction[self.ITEM_ID]\n",
    "        seq_output = self.forward(item_seq, item_seq_len)\n",
    "        test_item_emb = self.item_embedding(test_item)\n",
    "        scores = torch.mul(seq_output, test_item_emb).sum(dim=1)  # [B]\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def full_sort_predict(self, interaction):\n",
    "        item_seq = interaction[self.ITEM_SEQ]\n",
    "        item_seq_len = interaction[self.ITEM_SEQ_LEN]\n",
    "        seq_output = self.forward(item_seq, item_seq_len)\n",
    "        test_items_emb = self.item_embedding.weight\n",
    "        scores = torch.matmul(seq_output, test_items_emb.transpose(0, 1))  # [B, n_items]\n",
    "        return scores"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "T472955_GCSAN_Session_based_Model.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}