!pip install google_trans_new

from collections import OrderedDict
from tabulate import tabulate
import matplotlib.pyplot as plt

import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

from google_trans_new import google_translator  

!wget https://github.com/sparsh-ai/reco-data/raw/master/anime/anime_cleaned.csv
!wget https://github.com/sparsh-ai/reco-data/raw/master/anime/anime_genres.csv
!wget https://github.com/sparsh-ai/reco-data/raw/master/anime/clusters.csv
!wget https://github.com/sparsh-ai/reco-data/raw/master/anime/inputFormater.csv
!gdown --id 1LV7VHOTqU5WgBYxfRcUeY31dbhcBqyzb
!gdown --id 14x3TgzhFl-XCHjJHtX-mZrtTSkJgIIey

def top_animes(genre, ani_genre, all_anime):
    top = []
    print("\nTop", genre)
    temp = list(ani_genre[ani_genre[genre]==1]['anime_id'])
    temp = list(filter(lambda x: x in all_anime.index, temp))
    temp.sort(key=lambda x: all_anime['score'][x], reverse=True)

    for i in range(5):
        r = [i+1, temp[i], all_anime['title'][temp[i]], all_anime['title_english'][temp[i]],
             all_anime['score'][temp[i]], all_anime['genre'][temp[i]]]
        top.append(r)

    table = tabulate(top, headers=['S.No.', 'Anime ID', 'Title', 'English Title',
                                   'Anime Score', 'Anime Genre'], tablefmt='orgtbl')
    print(table)

results = pd.read_csv('clusters.csv')
results.head()

clusters = []

for i in range(222):
    clusters.append([])

for i in range(len(results)):
    clusters[results['alpha'][i]].append(results['anime_id'][i])

def getCluster(anime_id, opposite=False):
    if opposite == False:
        temp = results[results['anime_id'] == anime_id]['alpha'].reset_index(drop=True)
        clusterID = temp[0]
        return clusters[clusterID]
    else:
        temp = results[results['anime_id'] == anime_id]['zeta'].reset_index(drop=True)
        clusterID = temp[0]
        return clusters[clusterID]

input_formater = pd.read_csv("inputFormater.csv")
input_formater.iloc[:5, :5]

class UserVector(Dataset):
    def __init__(self, age, gender, uratings):
        if age<11:
            self.age=2
        elif age<16:
            self.age=3
        elif age<20:
            self.age=4
        else:
            self.age=5

        if gender.lower() == 'male':
            self.gender = 0
        else:
            self.gender = 1

        self.data = input_formater
        self.data.loc[0, 'Gender'] = self.gender
        self.data.loc[0, 'Category'+str(self.age)] = 1

        self.columns = list(self.data.columns)

        self.aniId_to_ind = pd.Series(data=range(len(self.columns)), index=self.columns)

        for aniId in uratings.keys():
            self.data.loc[0, str(aniId)] = uratings[aniId]
        self.data.fillna(0, inplace=True)

        self.data = self.data.iloc[:,:]

        self.transform =  transforms.Compose([transforms.ToTensor()])

        self.data = self.transform(np.array(self.data))

    def __len__(self):
        return len(self.data[0])

    def __getitem__(self, ind):
        user_vector = self.data.data[0][ind]
        return user_vector

    def get_anime_id(self, ind):
        return int(self.columns[ind])

    def anime_to_index(self):
        return self.aniId_to_ind

ani_genre = pd.read_csv("anime_genres.csv", index_col=[0])
ani_genre.head()

# Get similar anime
def similarAnime(uratings, all_anime):

    if len(uratings) == 0:
        anime_data = all_anime.set_index('anime_id')

        top_animes('Shounen', ani_genre, anime_data)
        top_animes('Supernatural', ani_genre, anime_data)
        top_animes('Romance', ani_genre, anime_data)
        top_animes('Slice of Life', ani_genre, anime_data)

        return []

    else:
        temp = list(reversed(uratings.items()))
        SimilarAnime = []

        i = 0
        while i < len(temp) and i < 3:
            if temp[i][1] >= 6:
                SimilarAnime += getCluster(temp[i][0], opposite=False)
            else:
                SimilarAnime += getCluster(temp[i][0], opposite=True)
            i+=1
        return SimilarAnime

# Get Anime You May Like
def animeYouMayLike(age, gender, uratings, model, all_anime, aniId_to_index):
    # Get Similar Anime
    SimilarAnime = similarAnime(uratings, all_anime)
    if len(SimilarAnime) == 0:
        return [], []
    
    # User Data Column
    user_data = UserVector(age, gender, uratings)

    # User Data Loader
    user_dl = DataLoader(dataset=user_data, num_workers=1)

    # Get model Predictions
    preds = model.getPredictedRatings(user_data, user_dl)
    preds = preds.reshape(-1)

    # Get top predicted anime
    animes = list(preds.argsort()[-1000:][::-1])
    animes = list(map(user_data.get_anime_id, animes))

    # Generate 'Similar Anime' and 'Anime You May Like'
    FinalList1 = []
    FinalList2 = []

    for aniID in animes:
        index = int(aniId_to_index.at[aniID])
        r = [aniID, all_anime['title'][index], all_anime['title_english'][index], all_anime['genre'][index]]

        if aniID in SimilarAnime and len(FinalList1) <10 and aniID not in uratings:
            FinalList1.append(r)
        elif aniID not in SimilarAnime and len(FinalList2) <10 and aniID not in uratings:
            FinalList2.append(r)
        elif len(FinalList1) == 10 and len(FinalList2) == 10:
            break

    return FinalList1, FinalList2

def showRecommendations(age, gender, uratings, model, all_anime, aniId_to_index):
    # Get both the lists
    List1, List2 = animeYouMayLike(age, gender, uratings, model, all_anime, aniId_to_index)
    if len(List1) == 0 and len(List2) == 0:
        return
    
    # Tabulate the Results
    print("similar Anime")
    table = tabulate(List1, headers=['Anime ID', 'JP Title', 'EN Title', 'Genre'], tablefmt='orgtbl')
    print(table)

    print("Anime You May Like")
    table = tabulate(List2, headers=['Anime ID', 'JP Title', 'EN Title', 'Genre'], tablefmt='orgtbl')
    print(table)

def activation(input, type):
  
    if type.lower()=='selu':
        return F.selu(input)
    elif type.lower()=='elu':
        return F.elu(input)
    elif type.lower()=='relu':
        return F.relu(input)
    elif type.lower()=='relu6':
        return F.relu6(input)
    elif type.lower()=='lrelu':
        return F.leaky_relu(input)
    elif type.lower()=='tanh':
        return F.tanh(input)
    elif type.lower()=='sigmoid':
        return F.sigmoid(input)
    elif type.lower()=='swish':
        return F.sigmoid(input)*input
    elif type.lower()=='identity':
        return input
    else:
        raise ValueError("Unknown non-Linearity Type")

class AutoEncoder(nn.Module):

  def __init__(self, layer_sizes, nl_type='selu', is_constrained=True, dp_drop_prob=0.0, last_layer_activations=True):

    super(AutoEncoder, self).__init__()

    self.layer_sizes = layer_sizes
    self.nl_type = nl_type
    self.is_constrained = is_constrained
    self.dp_drop_prob = dp_drop_prob
    self.last_layer_activations = last_layer_activations

    if dp_drop_prob>0:
      self.drop = nn.Dropout(dp_drop_prob)

    self._last = len(layer_sizes) - 2

    # Initaialize Weights
    self.encoder_weights = nn.ParameterList( [nn.Parameter(torch.rand(layer_sizes[i+1], layer_sizes[i])) for i in range(len(layer_sizes) - 1)  ] )

    # "Xavier Initialization" ( Understanding the Difficulty in training deep feed forward neural networks - by Glorot, X. & Bengio, Y. )
    # ( Values are sampled from uniform distribution )
    for weights in self.encoder_weights:
      init.xavier_uniform_(weights)

    # Encoder Bias
    self.encoder_bias = nn.ParameterList( [nn.Parameter(torch.zeros(layer_sizes[i+1])) for i in range(len(layer_sizes) - 1) ] )

    reverse_layer_sizes = list(reversed(layer_sizes)) 
    # reversed returns iterator

    # Decoder Weights
    if is_constrained == False:
      self.decoder_weights = nn.ParameterList( [nn.Parameter(torch.rand(reverse_layer_sizes[i+1], reverse_layer_sizes[i])) for i in range(len(reverse_layer_sizes) - 1) ] )

      for weights in self.decoder_weights:
        init.xavier_uniform_(weights)

    self.decoder_bias = nn.ParameterList( [nn.Parameter(torch.zeros(reverse_layer_sizes[i+1])) for i in range(len(reverse_layer_sizes) - 1) ] )



  def encode(self,x):
    for i,w in enumerate(self.encoder_weights):
      x = F.linear(input=x, weight = w, bias = self.encoder_bias[i] )
      x = activation(input=x, type=self.nl_type)

    # Apply Dropout on the last layer
    if self.dp_drop_prob > 0:
      x = self.drop(x)

    return x


  def decode(self,x):
    if self.is_constrained == True:
      # Weights are tied
      for i,w in zip(range(len(self.encoder_weights)),list(reversed(self.encoder_weights))):
        x = F.linear(input=x, weight=w.t(), bias = self.decoder_bias[i] )
        x = activation(input=x, type=self.nl_type if i != self._last or self.last_layer_activations else 'relu')

    else:

      for i,w in enumerate(self.decoder_weights):
        x = F.linear(input=x, weight = w, bias = self.decoder_weights[i])
        x = activation(input=x, type=self.nl_type if i != self._last or self.last_layer_activations else 'relu')

    return x

  def forward(self,x):
    # Forward Pass
    return self.decode(self.encode(x))

class PredictionEngine:
  def __init__(self):
    self.layer_sizes = [6673, 8192, 2048, 512, 256]
    self.model = AutoEncoder(layer_sizes=self.layer_sizes, nl_type='selu', is_constrained=True, dp_drop_prob=0.0, last_layer_activations=False)
    self.model.load_state_dict(torch.load('autoEncoder.pth'))
    try:
        self.model = self.model.cuda()
    except:
        pass

  def getPredictedRatings(self, user_dat, user_dl):
    for data in user_dl:
      inputs = data
      try:
          inputs = inputs.cuda()
      except:
          pass
      inputs = inputs.float()

      outputs = self.model(inputs)
      break

    return outputs.cpu().detach().numpy()

# Search anime in database
def find_anime(input_anime, name_to_id):
    print('Anime Id', '\t', 'Title')
    flag = 0
    for n in name_to_id.index:
        if input_anime in n.lower():
            flag = 1
            print(name_to_id[n], '\t', n)
    return flag

# Load all the datasets
all_anime = pd.read_csv("anime_cleaned.csv")
display(all_anime.head())

all_anime.image_url[0]

from PIL import Image
import requests
from io import BytesIO

def id_to_image(anime_id):
  url = all_anime.loc[all_anime['anime_id']==anime_id, 'image_url'].values[0]
  url = url.split('/')
  url = ['https://cdn.myanimelist.net'] + url[3:]
  url = '/'.join(url)
  response = requests.get(url)
  img = Image.open(BytesIO(response.content))
  return img

id_to_image(12365)

all_anime_small = all_anime.sample(200, random_state=42)
translator = google_translator()
all_anime_small['title_english_new'] = all_anime_small.apply(lambda row: translator.translate(row.title_japanese), axis=1)
all_anime_small.title_english_new.unique()

name_to_id = pd.Series(list(all_anime['anime_id']), index=all_anime['title'])
aniId_to_index = pd.Series(all_anime.index, index=all_anime['anime_id'])

print("Starting...\n")

# Load the AutoEncoder
model = PredictionEngine()

# Get basic information from the user
age = int(input("Enter Age: "))
gender = input("Enter Gender (Male/Female): ")
input_ratings = OrderedDict()

# Let the user rate some animes
print("\nIt is recommended to rate atleast 5 animes in the beginning.")
print("Note:- Currently search mechanism searches for anime using the Japanese Title only.")

# List for storing the user ratings of recommended table
user_score = []
c = 1

# Start the recommendation process
k1 = input("\nStart the process? [y/n]: ")

while k1 == 'y' or k1 == 'Y':

    # If user want to search and rate
    k2 = input("\nSearch and rate? [y/n]: ")
    while k2 == 'y' or k2 == 'Y':
        p = 'n'
        while p == 'n' or p == 'N':
            input_anime = input("Enter Anime title: ")
            flag = find_anime(input_anime.lower(), name_to_id)
            if flag==0:
                print("\nAnime not found in dataset. Please try searching only a part of the title or another anime!!")
                continue
            p = input("Anime found? [y/n]: ")

        aniId = int(input("Enter anime id: "))
        rate = int(input("Your rating (1 - 10): "))
        if not type(rate) is int:
              raise TypeError("Only integers are allowed")
              
        input_ratings[aniId] = rate

        k2 = input("Search and rate more? [y/n]: ")

    # Main Game
    showRecommendations(age, gender, input_ratings, model, all_anime, aniId_to_index)

    # If user want to rate anime from above list
    k2 = input("\nRate anime from above list? [y/n]:")
    while k2 == 'y' or k2 == 'Y':
        aniId = int(input("Enter anime id: "))
        rate = int(input("Your rating (1 - 10): "))
        input_ratings[aniId] = rate

        k2 = input("Rate again from above list? [y/n]: ")

    k2 = int(input("Your score for the table of recommended anime (1 - 10):"))
    user_score.append([c, k2])
    c += 1
    
    k1 = input("\nKeep going? [y/n]: ")

# Displaying the user score over iterations
print('\n\nTable of user scores')
table = tabulate(user_score, headers=['Iterations', 'User Score'], tablefmt='grid')
print(table)

rated_anime = [10690, 3768, 31699, 2056, 37858]
ratings = [7, 6, 8, 7, 9]
rated_anime_images = [id_to_image(id) for id in rated_anime]
plt.figure(figsize=(20,10))
columns = 5
for i, image in enumerate(rated_anime_images):
    plt.subplot(len(rated_anime_images) / columns + 1, columns, i + 1)
    plt.gca().set_title(ratings[i])
    plt.imshow(image)

similar_anime = [1724, 1771, 1155, 2723, 8353]
similar_anime_images = [id_to_image(id) for id in similar_anime]
plt.figure(figsize=(20,10))
columns = 5
for i, image in enumerate(similar_anime_images):
    plt.subplot(len(similar_anime_images) / columns + 1, columns, i + 1)
    plt.imshow(image)

personalized_anime = [30851, 33473, 5267, 7308, 35805]
personalized_anime_images = [id_to_image(id) for id in personalized_anime]
plt.figure(figsize=(20,10))
columns = 5
for i, image in enumerate(personalized_anime_images):
    plt.subplot(len(personalized_anime_images) / columns + 1, columns, i + 1)
    plt.imshow(image)
