
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Theory &#8212; My Jupyter Book</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e8f53015daec13862f6db5e763c41738.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Theory" href="recobook-us739178-v2.html" />
    <link rel="prev" title="Main Header (Placeholder)" href="../index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <h1 class="site-logo" id="site-title">My Jupyter Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Main Header (Placeholder)
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Section 1
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="recobook-us739178-v2.html">
   Theory
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Section 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="recobook-us739178-v4.html">
   Theory
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/nbs/recobook-us739178-v1.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/nbs/recobook-us739178-v1.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Theory
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pytorch-geometric">
     PyTorch Geometric
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#version-control">
     Version Control
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#installation">
     Installation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-ingestion">
   Data Ingestion
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cora">
     CORA
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computation-graph">
   Computation Graph
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#information-aggregation-function">
   Information Aggregation Function
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#shared-parameters">
     Shared Parameters
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aggregation-functions">
     Aggregation Functions
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#code-practice">
     Code Practice
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#prototype">
       Prototype
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#scripting">
       Scripting
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graph-attention-networks-gat">
   Graph attention networks (GAT)
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overview">
     Overview
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Code Practice
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#structure">
       Structure
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#linear-transformation">
       Linear Transformation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#attention-mechanism">
       Attention Mechanism
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#masked-attention">
       Masked Attention
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loading-the-dataset">
     Loading the dataset
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#assembling-the-components">
     Assembling the components
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#use-it">
     Use it
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graph-representation">
   Graph representation
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graph-convolutions">
   Graph Convolutions
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graph-attention">
   Graph Attention
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convolution-fundamentals">
   Convolution Fundamentals
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#imports">
     Imports
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#definition">
     Definition
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fourier-transform">
     Fourier transform
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#definition-of-the-fourier-transform">
       Definition of the Fourier transform
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#connection-with-the-laplacian">
       Connection with the Laplacian
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#summary">
       Summary
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolution-on-graphs">
     Convolution on graphs
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#graph-laplacian">
       Graph Laplacian
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#graph-spectrum-fourier-transform-and-convolution">
       Graph spectrum, Fourier transform, and convolution
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#spectral-convolutional-layers-in-pytorch-geometric">
   Spectral-convolutional layers in PyTorch Geometric
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chebconv">
     ChebConv
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#goal">
       Goal:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#chebyshev-approximation">
       Chebyshev approximation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#chebyshev-approximation-of-the-filter">
       Chebyshev approximation of the filter
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#property">
       Property
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fast-approximated-convolution">
       Fast approximated convolution
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#properties">
       Properties:
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gcnconv">
     GCNConv
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#aggregation-functions-in-gnns">
   Aggregation Functions in GNNs
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#context">
     Context
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wl-isomorphism-test">
     WL Isomorphism Test
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Imports
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#message-passing-class">
     Message Passing Class
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#laf-aggregation-module">
     LAF Aggregation Module
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pna-aggregation">
     PNA Aggregation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#test-the-new-classes">
     Test the new classes
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#laf-model">
     LAF Model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pna-model">
     PNA Model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gin-model">
     GIN Model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training">
     Training
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graph-autoencoders-gae-vgae">
   Graph AutoEncoders - GAE &amp; VGAE
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Context
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#loss-function">
       Loss function
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     Imports
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#load-the-citeseer-data">
     Load the CiteSeer data
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#define-the-encoder">
     Define the Encoder
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#define-the-autoencoder">
     Define the Autoencoder
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#result-analysis-with-tensorboard">
     Result analysis with Tensorboard
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graph-variational-autoencoder-gvae">
     Graph Variational AutoEncoder (GVAE)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#end">
   End
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="theory">
<h1>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h1>
<div class="section" id="pytorch-geometric">
<h2>PyTorch Geometric<a class="headerlink" href="#pytorch-geometric" title="Permalink to this headline">¶</a></h2>
<p>We had mentioned before that implementing graph networks with adjacency matrix is simple and straight-forward but can be computationally expensive for large graphs. Many real-world graphs can reach over 200k nodes, for which adjacency matrix-based implementations fail. There are a lot of optimizations possible when implementing GNNs, and luckily, there exist packages that provide such layers. The most popular packages for PyTorch are <a class="reference external" href="https://pytorch-geometric.readthedocs.io/en/latest/">PyTorch Geometric</a> and the <a class="reference external" href="https://www.dgl.ai/">Deep Graph Library</a> (the latter being actually framework agnostic). Which one to use depends on the project you are planning to do and personal taste. In this tutorial, we will look at PyTorch Geometric as part of the PyTorch family.</p>
<p>A graph is used to model pairwise relations (edges) between objects (nodes). A single graph in PyG is described by an instance of <code class="docutils literal notranslate"><span class="pre">torch_geometric.data.Data</span></code>, which holds the following attributes by default:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">data.x</span></code>: Node feature matrix with shape [num_nodes, num_node_features]</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data.edge_index</span></code>: Graph connectivity in COO format with shape [2, num_edges] and type torch.long</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data.edge_attr</span></code>: Edge feature matrix with shape [num_edges, num_edge_features]</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data.y</span></code>: Target to train against (may have arbitrary shape), e.g., node-level targets of shape [num_nodes, *] or graph-level targets of shape [1, *]</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data.pos</span></code>: Node position matrix with shape [num_nodes, num_dimensions]</p></li>
</ul>
<p>None of these attributes are required. In fact, the Data object is not even restricted to these attributes. We can, e.g., extend it by data.face to save the connectivity of triangles from a 3D mesh in a tensor with shape [3, num_faces] and type torch.long.</p>
</div>
</div>
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<div class="section" id="version-control">
<h2>Version Control<a class="headerlink" href="#version-control" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">project_name</span> <span class="o">=</span> <span class="s2">&quot;recobase&quot;</span><span class="p">;</span> <span class="n">branch</span> <span class="o">=</span> <span class="s2">&quot;US739178&quot;</span><span class="p">;</span> <span class="n">account</span> <span class="o">=</span> <span class="s2">&quot;recohut&quot;</span>
<span class="n">project_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;/content&#39;</span><span class="p">,</span> <span class="n">branch</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">project_path</span><span class="p">):</span>
    <span class="o">!</span>pip install -U -q dvc dvc<span class="o">[</span>gdrive<span class="o">]</span>
    <span class="o">!</span>cp -r /content/drive/MyDrive/git_credentials/. ~
    <span class="o">!</span>mkdir <span class="s2">&quot;{project_path}&quot;</span>
    <span class="o">%</span><span class="k">cd</span> &quot;{project_path}&quot;
    <span class="o">!</span>git init
    <span class="o">!</span>git remote add origin https://github.com/<span class="s2">&quot;{account}&quot;</span>/<span class="s2">&quot;{project_name}&quot;</span>.git
    <span class="o">!</span>git pull origin <span class="s2">&quot;{branch}&quot;</span>
    <span class="o">!</span>git checkout -b <span class="s2">&quot;{branch}&quot;</span>
    <span class="o">%</span><span class="k">reload_ext</span> autoreload
    <span class="o">%</span><span class="k">autoreload</span> 2
<span class="k">else</span><span class="p">:</span>
    <span class="o">%</span><span class="k">cd</span> &quot;{project_path}&quot;
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/content/US739178
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>git status -u
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>On branch US739178
nothing to commit, working tree clean
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>git add .
<span class="o">!</span>git commit -m <span class="s1">&#39;commit&#39;</span>
<span class="o">!</span>git push origin <span class="s2">&quot;{branch}&quot;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>On branch US739178
nothing to commit, working tree clean
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Everything up-to-date
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html
<span class="o">!</span>pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html
<span class="o">!</span>pip install -q torch-cluster -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html
<span class="o">!</span>pip install -q torch-geometric
</pre></div>
</div>
</div>
</div>
<p>While the theory and math behind GNNs might first seem complicated, the implementation of those models is quite simple and helps in understanding the methodology. Therefore, we will discuss the implementation of basic network layers of a GNN, namely graph convolutions, and attention layers. Finally, we will apply a GNN on a node-level, edge-level, and graph-level tasks.</p>
</div>
</div>
<div class="section" id="data-ingestion">
<h1>Data Ingestion<a class="headerlink" href="#data-ingestion" title="Permalink to this headline">¶</a></h1>
<div class="section" id="cora">
<h2>CORA<a class="headerlink" href="#cora" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>mkdir /content/x <span class="o">&amp;&amp;</span> git clone https://github.com/AntonioLonga/PytorchGeometricTutorial.git /content/x
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mkdir: cannot create directory ‘/content/x’: File exists
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>mkdir -p data/bronze/cora
<span class="o">!</span>mkdir -p data/silver/cora

<span class="o">!</span>cp -r /content/x/Tutorial1/tutorial1/Cora/raw/* data/bronze/cora
<span class="o">!</span>cp -r /content/x/Tutorial1/tutorial1/Cora/processed/* data/silver/cora
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>dvc add data/bronze/cora/*
<span class="o">!</span>dvc add data/silver/cora/*
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
Collecting targets          |0.00 [00:00,     ?file/s]
                                                      
<span class=" -Color -Color-Red">ERROR</span>: DVC file &#39;ind.cora.allx.dvc&#39; cannot be an output.

</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
Collecting targets          |0.00 [00:00,     ?file/s]
                                                      
<span class=" -Color -Color-Red">ERROR</span>: DVC file &#39;data.pt.dvc&#39; cannot be an output.

</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>dvc commit data/bronze/cora/*
<span class="o">!</span>dvc push data/bronze/cora/*

<span class="o">!</span>dvc commit data/silver/cora/*
<span class="o">!</span>dvc push data/silver/cora/*
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>!
          |0.00 [00:00,       ?it/s]
                                    

.HrSiNormqLBViNoKvbxTFR.tmp:   0% 0/1 [00:00&lt;?, ?it/s]
.HrSiNormqLBViNoKvbxTFR.tmp:   0% 0/1 [00:00&lt;?, ?it/s{&#39;info&#39;: &#39;&#39;}]
                                                                  

!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
          |0.00 [00:00,       ?it/s]
                                    

!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
          |0.00 [00:00,       ?it/s]
                                    

!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
          |0.00 [00:00,       ?it/s]
                                    

!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
          |0.00 [00:00,       ?it/s]
                                    
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
          |0.00 [00:00,       ?it/s]
                                    

!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
          |0.00 [00:00,       ?it/s]
                                    

!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>!
          |0.00 [00:00,       ?it/s]
                                    

!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>!
          |0.00 [00:00,     ?file/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Go to the following link in your browser:

    https://accounts.google.com/o/oauth2/auth?client_id=710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.appdata&amp;access_type=offline&amp;response_type=code&amp;approval_prompt=force

Enter verification code: 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                    
<span class=" -Color -Color-Red">ERROR</span>: interrupted by the user

</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>!
          |0.00 [00:00,       ?it/s]
                                    

.cYtyQXe8HxccCh7RFikvfw.tmp:   0% 0/1 [00:00&lt;?, ?it/s]
.cYtyQXe8HxccCh7RFikvfw.tmp:   0% 0/1 [00:00&lt;?, ?it/s{&#39;info&#39;: &#39;&#39;}]
                                                                  

!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
          |0.00 [00:00,       ?it/s]
                                    

!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

!
          |0.00 [00:00,       ?it/s]
                                    
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>!
If DVC froze, see `hardlink_lock` in &lt;<span class=" -Color -Color-Cyan">https://man.dvc.org/config#core</span>&gt;
                                                                      

</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="computation-graph">
<h1>Computation Graph<a class="headerlink" href="#computation-graph" title="Permalink to this headline">¶</a></h1>
<p>The neighbors of a node define its computation graph</p>
<p>Every node has its own computation graph</p>
</div>
<div class="section" id="information-aggregation-function">
<h1>Information Aggregation Function<a class="headerlink" href="#information-aggregation-function" title="Permalink to this headline">¶</a></h1>
<p>A slightly general representation of this function is to replace the summation with aggregation. This will give us flexibility to aggregate the neighborhood weights.</p>
<div class="section" id="shared-parameters">
<h2>Shared Parameters<a class="headerlink" href="#shared-parameters" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Note: When a new node come, it will get pre-trained weights in this way.</p>
</div></blockquote>
</div>
<div class="section" id="aggregation-functions">
<h2>Aggregation Functions<a class="headerlink" href="#aggregation-functions" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="code-practice">
<h2>Code Practice<a class="headerlink" href="#code-practice" title="Permalink to this headline">¶</a></h2>
<div class="section" id="prototype">
<h3>Prototype<a class="headerlink" href="#prototype" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_geometric</span>
<span class="kn">from</span> <span class="nn">torch_geometric.datasets</span> <span class="kn">import</span> <span class="n">Planetoid</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">use_cuda_if_available</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">Planetoid</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&quot;/content/cora&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span> <span class="s2">&quot;Cora&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;number of graphs:</span><span class="se">\t\t</span><span class="s2">&quot;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;number of classes:</span><span class="se">\t\t</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">dataset</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;number of node features:</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">dataset</span><span class="o">.</span><span class="n">num_node_features</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;number of edge features:</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">dataset</span><span class="o">.</span><span class="n">num_edge_features</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cora()
number of graphs:		 1
number of classes:		 7
number of node features:	 1433
number of edge features:	 0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;edge_index:</span><span class="se">\t\t</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">edge_index</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">edge_index</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;train_mask:</span><span class="se">\t\t</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">train_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">train_mask</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x:</span><span class="se">\t\t</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y:</span><span class="se">\t\t</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>edge_index:		 torch.Size([2, 10556])
tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],
        [ 633, 1862, 2582,  ...,  598, 1473, 2706]])


train_mask:		 torch.Size([2708])
tensor([ True,  True,  True,  ..., False, False, False])


x:		 torch.Size([2708, 1433])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])


y:		 torch.Size([2708])
tensor([3, 4, 4,  ..., 3, 3, 3])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os.path</span> <span class="k">as</span> <span class="nn">osp</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">SAGEConv</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">SAGEConv</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span>
                             <span class="n">dataset</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
                             <span class="n">aggr</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">)</span> <span class="c1"># max, mean, add ...)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">edge_index</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">use_cuda_if_available</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>device(type=&#39;cpu&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">model</span><span class="p">()[</span><span class="n">data</span><span class="o">.</span><span class="n">train_mask</span><span class="p">],</span> <span class="n">data</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">train_mask</span><span class="p">])</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">test</span><span class="p">():</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">accs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(),</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">mask</span> <span class="ow">in</span> <span class="n">data</span><span class="p">(</span><span class="s1">&#39;train_mask&#39;</span><span class="p">,</span> <span class="s1">&#39;val_mask&#39;</span><span class="p">,</span> <span class="s1">&#39;test_mask&#39;</span><span class="p">):</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">accs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">accs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_val_acc</span> <span class="o">=</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">train</span><span class="p">()</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="n">tmp_test_acc</span> <span class="o">=</span> <span class="n">test</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">val_acc</span> <span class="o">&gt;</span> <span class="n">best_val_acc</span><span class="p">:</span>
        <span class="n">best_val_acc</span> <span class="o">=</span> <span class="n">val_acc</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">tmp_test_acc</span>
    <span class="n">log</span> <span class="o">=</span> <span class="s1">&#39;Epoch: </span><span class="si">{:03d}</span><span class="s1">, Val: </span><span class="si">{:.4f}</span><span class="s1">, Test: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span>
    
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">log</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">best_val_acc</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 010, Val: 0.7200, Test: 0.7190
Epoch: 020, Val: 0.7200, Test: 0.7190
Epoch: 030, Val: 0.7200, Test: 0.7190
Epoch: 040, Val: 0.7200, Test: 0.7190
Epoch: 050, Val: 0.7200, Test: 0.7190
Epoch: 060, Val: 0.7200, Test: 0.7190
Epoch: 070, Val: 0.7220, Test: 0.7120
Epoch: 080, Val: 0.7220, Test: 0.7120
Epoch: 090, Val: 0.7220, Test: 0.7120
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="scripting">
<h3>Scripting<a class="headerlink" href="#scripting" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">writefile</span> src/datasets/vectorial.py
import torch.nn as nn
import torch


#%% Dataset to manage vector to vector data
class VectorialDataset(torch.utils.data.Dataset):
    def __init__(self, input_data, output_data):
        super(VectorialDataset, self).__init__()
        self.input_data = torch.tensor(input_data.astype(&#39;f&#39;))
        self.output_data = torch.tensor(output_data.astype(&#39;f&#39;))
        
    def __len__(self):
        return self.input_data.shape[0]
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        sample = (self.input_data[idx, :], 
                  self.output_data[idx, :])  
        return sample 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing src/datasets/vectorial.py
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">writefile</span> src/datasets/__init__.py
from .vectorial import VectorialDataset
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing src/datasets/__init__.py
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">writefile</span> src/models/linear.py
import torch.nn as nn
import torch

#%% Linear layer
class LinearModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearModel, self).__init__()

        self.input_dim = input_dim
        self.output_dim = output_dim

        self.linear = nn.Linear(self.input_dim, self.output_dim, bias=True)

    def forward(self, x):
        out = self.linear(x)
        return out
    
    def reset(self):
        self.linear.reset_parameters()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing src/models/linear.py
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">writefile</span> src/models/__init__.py
from .linear import LinearModel
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing src/models/__init__.py
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="graph-attention-networks-gat">
<h1>Graph attention networks (GAT)<a class="headerlink" href="#graph-attention-networks-gat" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="id1">
<h2>Code Practice<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="structure">
<h3>Structure<a class="headerlink" href="#structure" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GATLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simple PyTorch Implementation of the Graph Attention layer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GATLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
      
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">adj</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s start from the forward method</p>
</div>
<div class="section" id="linear-transformation">
<h3>Linear Transformation<a class="headerlink" href="#linear-transformation" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[
\bar{h'}_i = \textbf{W}\cdot \bar{h}_i
\]</div>
<p>with <span class="math notranslate nohighlight">\(\textbf{W}\in\mathbb R^{F'\times F}\)</span> and <span class="math notranslate nohighlight">\(\bar{h}_i\in\mathbb R^{F}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\bar{h'}_i \in \mathbb{R}^{F'}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">in_features</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">out_features</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">nb_nodes</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)))</span> <span class="c1">#xavier paramiter inizializator</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.414</span><span class="p">)</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">nb_nodes</span><span class="p">,</span><span class="n">in_features</span><span class="p">)</span> 


<span class="c1"># linear transformation</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([3, 2])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="attention-mechanism">
<h3>Attention Mechanism<a class="headerlink" href="#attention-mechanism" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">out_features</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span> <span class="c1">#xavier parameter inizializator</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.414</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">leakyrelu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>  <span class="c1"># LeakyReLU</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([4, 1])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">h</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">N</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">h</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">out_features</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>![image.png](data:image/png;base64,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)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">e</span> <span class="o">=</span> <span class="n">leakyrelu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a_input</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">a_input</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a_input</span><span class="p">,</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a_input</span><span class="p">,</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([3, 3, 4]) torch.Size([4, 1])

torch.Size([3, 3, 1])

torch.Size([3, 3])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="masked-attention">
<h3>Masked Attention<a class="headerlink" href="#masked-attention" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Masked Attention</span>
<span class="n">adj</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">zero_vec</span>  <span class="o">=</span> <span class="o">-</span><span class="mf">9e15</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">zero_vec</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([3, 3])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">attention</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">adj</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">zero_vec</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">adj</span><span class="p">,</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">e</span><span class="p">,</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">zero_vec</span><span class="p">)</span>
<span class="n">attention</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0, 0, 1],
        [1, 1, 1],
        [1, 0, 1]]) 
 tensor([[-0.1585, -0.0303, -0.1897],
        [-0.1986, -0.0704, -0.2298],
        [-0.2208, -0.0925, -0.2519]], grad_fn=&lt;LeakyReluBackward0&gt;) 
 tensor([[-9.0000e+15, -9.0000e+15, -9.0000e+15],
        [-9.0000e+15, -9.0000e+15, -9.0000e+15],
        [-9.0000e+15, -9.0000e+15, -9.0000e+15]])
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-9.0000e+15, -9.0000e+15, -1.8969e-01],
        [-1.9861e-01, -7.0399e-02, -2.2977e-01],
        [-2.2075e-01, -9.0000e+15, -2.5191e-01]], grad_fn=&lt;SWhereBackward&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">h_prime</span>   <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">attention</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.0000, 0.0000, 1.0000],
        [0.3219, 0.3660, 0.3121],
        [0.5078, 0.0000, 0.4922]], grad_fn=&lt;SoftmaxBackward&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h_prime</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1.0922, 0.8739],
        [0.5854, 0.9056],
        [0.9083, 0.6195]], grad_fn=&lt;MmBackward&gt;)
</pre></div>
</div>
</div>
</div>
<p>h_prime vs h</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">33</span><span class="nb">print</span><span class="p">(</span><span class="n">h_prime</span><span class="p">,</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">h</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.3717, -0.7218],
        [-0.1823, -0.5417],
        [-0.2831, -0.6622]], grad_fn=&lt;MmBackward&gt;) 
 tensor([[-0.1524, -0.5741],
        [ 0.4143,  0.0255],
        [-0.3717, -0.7218]], grad_fn=&lt;MmBackward&gt;)
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="loading-the-dataset">
<h2>Loading the dataset<a class="headerlink" href="#loading-the-dataset" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch_geometric.data</span> <span class="kn">import</span> <span class="n">Data</span>
<span class="kn">from</span> <span class="nn">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">GATConv</span>
<span class="kn">from</span> <span class="nn">torch_geometric.datasets</span> <span class="kn">import</span> <span class="n">Planetoid</span>
<span class="kn">import</span> <span class="nn">torch_geometric.transforms</span> <span class="k">as</span> <span class="nn">T</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">name_data</span> <span class="o">=</span> <span class="s1">&#39;Cora&#39;</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">Planetoid</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s1">&#39;/content/&#39;</span> <span class="o">+</span> <span class="n">name_data</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="n">name_data</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">NormalizeFeatures</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of Classes in </span><span class="si">{</span><span class="n">name_data</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of Node Features in </span><span class="si">{</span><span class="n">name_data</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_node_features</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="assembling-the-components">
<h2>Assembling the components<a class="headerlink" href="#assembling-the-components" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GATLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">concat</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GATLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span>       <span class="o">=</span> <span class="n">dropout</span>        <span class="c1"># drop prob = 0.6</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span>   <span class="o">=</span> <span class="n">in_features</span>    <span class="c1"># </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span>  <span class="o">=</span> <span class="n">out_features</span>   <span class="c1"># </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span>         <span class="o">=</span> <span class="n">alpha</span>          <span class="c1"># LeakyReLU with negative input slope, alpha = 0.2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span>        <span class="o">=</span> <span class="n">concat</span>         <span class="c1"># conacat = True for all layers except the output layer.</span>

        
        <span class="c1"># Xavier Initialization of Weights</span>
        <span class="c1"># Alternatively use weights_init to apply weights of choice </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)))</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.414</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">out_features</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.414</span><span class="p">)</span>
        
        <span class="c1"># LeakyReLU</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">leakyrelu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">adj</span><span class="p">):</span>
        <span class="c1"># Linear Transformation</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="c1"># matrix multiplication</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

        <span class="c1"># Attention Mechanism</span>
        <span class="n">a_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">h</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">N</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">h</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">)</span>
        <span class="n">e</span>       <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">leakyrelu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a_input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

        <span class="c1"># Masked Attention</span>
        <span class="n">zero_vec</span>  <span class="o">=</span> <span class="o">-</span><span class="mf">9e15</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">adj</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">zero_vec</span><span class="p">)</span>
        
        <span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">h_prime</span>   <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="n">h_prime</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">h_prime</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GAT</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GAT</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hid</span> <span class="o">=</span> <span class="mi">8</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_head</span> <span class="o">=</span> <span class="mi">8</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_head</span> <span class="o">=</span> <span class="mi">1</span>
        
        
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">GATConv</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hid</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">in_head</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">GATConv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hid</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">in_head</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">concat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                             <span class="n">heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">out_head</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">edge_index</span>
                
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="use-it">
<h2>Use it<a class="headerlink" href="#use-it" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GAT</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>


<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">train_mask</span><span class="p">],</span> <span class="n">data</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">train_mask</span><span class="p">])</span>
    
    <span class="k">if</span> <span class="n">epoch</span><span class="o">%</span><span class="k">200</span> == 0:
        <span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1.9455, grad_fn=&lt;NllLossBackward&gt;)
tensor(0.6532, grad_fn=&lt;NllLossBackward&gt;)
tensor(0.6773, grad_fn=&lt;NllLossBackward&gt;)
tensor(0.6679, grad_fn=&lt;NllLossBackward&gt;)
tensor(0.4795, grad_fn=&lt;NllLossBackward&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">_</span><span class="p">,</span> <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">correct</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">pred</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">test_mask</span><span class="p">]</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">test_mask</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">data</span><span class="o">.</span><span class="n">test_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acc</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 0.8220
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="graph-representation">
<h1>Graph representation<a class="headerlink" href="#graph-representation" title="Permalink to this headline">¶</a></h1>
<p>Before starting the discussion of specific neural network operations on graphs, we should consider how to represent a graph. Mathematically, a graph <span class="math notranslate nohighlight">\(\mathcal{G}\)</span> is defined as a tuple of a set of nodes/vertices <span class="math notranslate nohighlight">\(V\)</span>, and a set of edges/links <span class="math notranslate nohighlight">\(E\)</span>: <span class="math notranslate nohighlight">\(\mathcal{G}=(V,E)\)</span>. Each edge is a pair of two vertices, and represents a connection between them. For instance, let’s look at the following graph:</p>
<center width="100%" style="padding:10px"><img src="data:image/png;base64,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" width="250px"></center><p>The vertices are <span class="math notranslate nohighlight">\(V=\{1,2,3,4\}\)</span>, and edges <span class="math notranslate nohighlight">\(E=\{(1,2), (2,3), (2,4), (3,4)\}\)</span>. Note that for simplicity, we assume the graph to be undirected and hence don’t add mirrored pairs like <span class="math notranslate nohighlight">\((2,1)\)</span>. In application, vertices and edge can often have specific attributes, and edges can even be directed. The question is how we could represent this diversity in an efficient way for matrix operations. Usually, for the edges, we decide between two variants: an adjacency matrix, or a list of paired vertex indices.</p>
<p>The <strong>adjacency matrix</strong> <span class="math notranslate nohighlight">\(A\)</span> is a square matrix whose elements indicate whether pairs of vertices are adjacent, i.e. connected, or not. In the simplest case, <span class="math notranslate nohighlight">\(A_{ij}\)</span> is 1 if there is a connection from node <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span>, and otherwise 0. If we have edge attributes or different categories of edges in a graph, this information can be added to the matrix as well. For an undirected graph, keep in mind that <span class="math notranslate nohighlight">\(A\)</span> is a symmetric matrix (<span class="math notranslate nohighlight">\(A_{ij}=A_{ji}\)</span>). For the example graph above, we have the following adjacency matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{bmatrix}
    0 &amp; 1 &amp; 0 &amp; 0\\
    1 &amp; 0 &amp; 1 &amp; 1\\
    0 &amp; 1 &amp; 0 &amp; 1\\
    0 &amp; 1 &amp; 1 &amp; 0
\end{bmatrix}
\end{split}\]</div>
<p>While expressing a graph as a list of edges is more efficient in terms of memory and (possibly) computation, using an adjacency matrix is more intuitive and simpler to implement. In our implementations below, we will rely on the adjacency matrix to keep the code simple. However, common libraries use edge lists, which we will discuss later more.
Alternatively, we could also use the list of edges to define a sparse adjacency matrix with which we can work as if it was a dense matrix, but allows more memory-efficient operations. PyTorch supports this with the sub-package <code class="docutils literal notranslate"><span class="pre">torch.sparse</span></code> (<a class="reference external" href="https://pytorch.org/docs/stable/sparse.html">documentation</a>) which is however still in a beta-stage (API might change in future).</p>
</div>
<div class="section" id="graph-convolutions">
<h1>Graph Convolutions<a class="headerlink" href="#graph-convolutions" title="Permalink to this headline">¶</a></h1>
<p>Graph Convolutional Networks have been introduced by <a class="reference external" href="https://openreview.net/pdf?id=SJU4ayYgl">Kipf et al.</a> in 2016 at the University of Amsterdam. He also wrote a great <a class="reference external" href="https://tkipf.github.io/graph-convolutional-networks/">blog post</a> about this topic, which is recommended if you want to read about GCNs from a different perspective. GCNs are similar to convolutions in images in the sense that the “filter” parameters are typically shared over all locations in the graph. At the same time, GCNs rely on message passing methods, which means that vertices exchange information with the neighbors, and send “messages” to each other. Before looking at the math, we can try to visually understand how GCNs work. The first step is that each node creates a feature vector that represents the message it wants to send to all its neighbors. In the second step, the messages are sent to the neighbors, so that a node receives one message per adjacent node. Below we have visualized the two steps for our example graph.</p>
<center width="80%"><img src="data:image/png;base64,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" width="700px"></center><p>If we want to formulate that in more mathematical terms, we need to first decide how to combine all the messages a node receives. As the number of messages vary across nodes, we need an operation that works for any number. Hence, the usual way to go is to sum or take the mean. Given the previous features of nodes <span class="math notranslate nohighlight">\(H^{(l)}\)</span>, the GCN layer is defined as follows:</p>
<div class="math notranslate nohighlight">
\[H^{(l+1)} = \sigma\left(\hat{D}^{-1/2}\hat{A}\hat{D}^{-1/2}H^{(l)}W^{(l)}\right)\]</div>
<p><span class="math notranslate nohighlight">\(W^{(l)}\)</span> is the weight parameters with which we transform the input features into messages (<span class="math notranslate nohighlight">\(H^{(l)}W^{(l)}\)</span>). To the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> we add the identity matrix so that each node sends its own message also to itself: <span class="math notranslate nohighlight">\(\hat{A}=A+I\)</span>. Finally, to take the average instead of summing, we calculate the matrix <span class="math notranslate nohighlight">\(\hat{D}\)</span> which is a diagonal matrix with <span class="math notranslate nohighlight">\(D_{ii}\)</span> denoting the number of neighbors node <span class="math notranslate nohighlight">\(i\)</span> has. <span class="math notranslate nohighlight">\(\sigma\)</span> represents an arbitrary activation function, and not necessarily the sigmoid (usually a ReLU-based activation function is used in GNNs).</p>
<p>When implementing the GCN layer in PyTorch, we can take advantage of the flexible operations on tensors. Instead of defining a matrix <span class="math notranslate nohighlight">\(\hat{D}\)</span>, we can simply divide the summed messages by the number of neighbors afterward. Additionally, we replace the weight matrix with a linear layer, which additionally allows us to add a bias. Written as a PyTorch module, the GCN layer is defined as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span> <span class="k">as</span> <span class="nn">data</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GCNLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c_in</span><span class="p">,</span> <span class="n">c_out</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_in</span><span class="p">,</span> <span class="n">c_out</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_feats</span><span class="p">,</span> <span class="n">adj_matrix</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Inputs:</span>
<span class="sd">            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]</span>
<span class="sd">            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.</span>
<span class="sd">                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections. </span>
<span class="sd">                         Shape: [batch_size, num_nodes, num_nodes]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Num neighbours = number of incoming edges</span>
        <span class="n">num_neighbours</span> <span class="o">=</span> <span class="n">adj_matrix</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">node_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">node_feats</span><span class="p">)</span>
        <span class="n">node_feats</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">adj_matrix</span><span class="p">,</span> <span class="n">node_feats</span><span class="p">)</span>
        <span class="n">node_feats</span> <span class="o">=</span> <span class="n">node_feats</span> <span class="o">/</span> <span class="n">num_neighbours</span>
        <span class="k">return</span> <span class="n">node_feats</span>
</pre></div>
</div>
</div>
</div>
<p>To further understand the GCN layer, we can apply it to our example graph above. First, let’s specify some node features and the adjacency matrix with added self-connections:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">node_feats</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">adj_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                            <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Node features:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">node_feats</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Adjacency matrix:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">adj_matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Node features:
 tensor([[[0., 1.],
         [2., 3.],
         [4., 5.],
         [6., 7.]]])

Adjacency matrix:
 tensor([[[1., 1., 0., 0.],
         [1., 1., 1., 1.],
         [0., 1., 1., 1.],
         [0., 1., 1., 1.]]])
</pre></div>
</div>
</div>
</div>
<p>Next, let’s apply a GCN layer to it. For simplicity, we initialize the linear weight matrix as an identity matrix so that the input features are equal to the messages. This makes it easier for us to verify the message passing operation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layer</span> <span class="o">=</span> <span class="n">GCNLayer</span><span class="p">(</span><span class="n">c_in</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">c_out</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">layer</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>
<span class="n">layer</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">out_feats</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">node_feats</span><span class="p">,</span> <span class="n">adj_matrix</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Adjacency matrix&quot;</span><span class="p">,</span> <span class="n">adj_matrix</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input features&quot;</span><span class="p">,</span> <span class="n">node_feats</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output features&quot;</span><span class="p">,</span> <span class="n">out_feats</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Adjacency matrix tensor([[[1., 1., 0., 0.],
         [1., 1., 1., 1.],
         [0., 1., 1., 1.],
         [0., 1., 1., 1.]]])
Input features tensor([[[0., 1.],
         [2., 3.],
         [4., 5.],
         [6., 7.]]])
Output features tensor([[[1., 2.],
         [3., 4.],
         [4., 5.],
         [4., 5.]]])
</pre></div>
</div>
</div>
</div>
<p>Next, let’s apply a GCN layer to it. For simplicity, we initialize the linear weight matrix as an identity matrix so that the input features are equal to the messages. This makes it easier for us to verify the message passing operation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layer</span> <span class="o">=</span> <span class="n">GCNLayer</span><span class="p">(</span><span class="n">c_in</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">c_out</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">layer</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>
<span class="n">layer</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">out_feats</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">node_feats</span><span class="p">,</span> <span class="n">adj_matrix</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Adjacency matrix&quot;</span><span class="p">,</span> <span class="n">adj_matrix</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input features&quot;</span><span class="p">,</span> <span class="n">node_feats</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output features&quot;</span><span class="p">,</span> <span class="n">out_feats</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Adjacency matrix tensor([[[1., 1., 0., 0.],
         [1., 1., 1., 1.],
         [0., 1., 1., 1.],
         [0., 1., 1., 1.]]])
Input features tensor([[[0., 1.],
         [2., 3.],
         [4., 5.],
         [6., 7.]]])
Output features tensor([[[1., 2.],
         [3., 4.],
         [4., 5.],
         [4., 5.]]])
</pre></div>
</div>
</div>
</div>
<p>As we can see, the first node’s output values are the average of itself and the second node. Similarly, we can verify all other nodes. However, in a GNN, we would also want to allow feature exchange between nodes beyond its neighbors. This can be achieved by applying multiple GCN layers, which gives us the final layout of a GNN. The GNN can be build up by a sequence of GCN layers and non-linearities such as ReLU. For a visualization, see below (figure credit - <a class="reference external" href="https://tkipf.github.io/graph-convolutional-networks/">Thomas Kipf, 2016</a>).</p>
<p>However, one issue we can see from looking at the example above is that the output features for nodes 3 and 4 are the same because they have the same adjacent nodes (including itself). Therefore, GCN layers can make the network forget node-specific information if we just take a mean over all messages. Multiple possible improvements have been proposed. While the simplest option might be using residual connections, the more common approach is to either weigh the self-connections higher or define a separate weight matrix for the self-connections. Alternatively, we can re-visit a familiar concept: attention.</p>
</div>
<div class="section" id="graph-attention">
<h1>Graph Attention<a class="headerlink" href="#graph-attention" title="Permalink to this headline">¶</a></h1>
<p>If you remember from the last tutorial, attention describes a weighted average of multiple elements with the weights dynamically computed based on an input query and elements’ keys (if you haven’t read Tutorial 6 yet, it is recommended to at least go through the very first section called <a class="reference external" href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html#What-is-Attention?">What is Attention?</a>). This concept can be similarly applied to graphs, one of such is the Graph Attention Network (called GAT, proposed by <a class="reference external" href="https://arxiv.org/abs/1710.10903">Velickovic et al., 2017</a>). Similarly to the GCN, the graph attention layer creates a message for each node using a linear layer/weight matrix. For the attention part, it uses the message from the node itself as a query, and the messages to average as both keys and values (note that this also includes the message to itself). The score function <span class="math notranslate nohighlight">\(f_{attn}\)</span> is implemented as a one-layer MLP which maps the query and key to a single value. The MLP looks as follows (figure credit - <a class="reference external" href="https://arxiv.org/abs/1710.10903">Velickovic et al.</a>):</p>
<center width="100%" style="padding:10px"><img src="data:image/png;base64,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" width="250px"></center><p><span class="math notranslate nohighlight">\(h_i\)</span> and <span class="math notranslate nohighlight">\(h_j\)</span> are the original features from node <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> respectively, and represent the messages of the layer with <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> as weight matrix. <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> is the weight matrix of the MLP, which has the shape <span class="math notranslate nohighlight">\([1,2\times d_{\text{message}}]\)</span>, and <span class="math notranslate nohighlight">\(\alpha_{ij}\)</span> the final attention weight from node <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span>. The calculation can be described as follows:</p>
<div class="math notranslate nohighlight">
\[\alpha_{ij} = \frac{\exp\left(\text{LeakyReLU}\left(\mathbf{a}\left[\mathbf{W}h_i||\mathbf{W}h_j\right]\right)\right)}{\sum_{k\in\mathcal{N}_i} \exp\left(\text{LeakyReLU}\left(\mathbf{a}\left[\mathbf{W}h_i||\mathbf{W}h_k\right]\right)\right)}\]</div>
<p>The operator <span class="math notranslate nohighlight">\(||\)</span> represents the concatenation, and <span class="math notranslate nohighlight">\(\mathcal{N}_i\)</span> the indices of the neighbors of node <span class="math notranslate nohighlight">\(i\)</span>. Note that in contrast to usual practice, we apply a non-linearity (here LeakyReLU) before the softmax over elements. Although it seems like a minor change at first, it is crucial for the attention to depend on the original input. Specifically, let’s remove the non-linearity for a second, and try to simplify the expression:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
    \alpha_{ij} &amp; = \frac{\exp\left(\mathbf{a}\left[\mathbf{W}h_i||\mathbf{W}h_j\right]\right)}{\sum_{k\in\mathcal{N}_i} \exp\left(\mathbf{a}\left[\mathbf{W}h_i||\mathbf{W}h_k\right]\right)}\\[5pt]
    &amp; = \frac{\exp\left(\mathbf{a}_{:,:d/2}\mathbf{W}h_i+\mathbf{a}_{:,d/2:}\mathbf{W}h_j\right)}{\sum_{k\in\mathcal{N}_i} \exp\left(\mathbf{a}_{:,:d/2}\mathbf{W}h_i+\mathbf{a}_{:,d/2:}\mathbf{W}h_k\right)}\\[5pt]
    &amp; = \frac{\exp\left(\mathbf{a}_{:,:d/2}\mathbf{W}h_i\right)\cdot\exp\left(\mathbf{a}_{:,d/2:}\mathbf{W}h_j\right)}{\sum_{k\in\mathcal{N}_i} \exp\left(\mathbf{a}_{:,:d/2}\mathbf{W}h_i\right)\cdot\exp\left(\mathbf{a}_{:,d/2:}\mathbf{W}h_k\right)}\\[5pt]
    &amp; = \frac{\exp\left(\mathbf{a}_{:,d/2:}\mathbf{W}h_j\right)}{\sum_{k\in\mathcal{N}_i} \exp\left(\mathbf{a}_{:,d/2:}\mathbf{W}h_k\right)}\\
\end{split}
\end{split}\]</div>
<p>We can see that without the non-linearity, the attention term with <span class="math notranslate nohighlight">\(h_i\)</span> actually cancels itself out, resulting in the attention being independent of the node itself. Hence, we would have the same issue as the GCN of creating the same output features for nodes with the same neighbors. This is why the LeakyReLU is crucial and adds some dependency on <span class="math notranslate nohighlight">\(h_i\)</span> to the attention.</p>
<p>Once we obtain all attention factors, we can calculate the output features for each node by performing the weighted average:</p>
<div class="math notranslate nohighlight">
\[h_i'=\sigma\left(\sum_{j\in\mathcal{N}_i}\alpha_{ij}\mathbf{W}h_j\right)\]</div>
<p><span class="math notranslate nohighlight">\(\sigma\)</span> is yet another non-linearity, as in the GCN layer. Visually, we can represent the full message passing in an attention layer as follows (figure credit - <a class="reference external" href="https://arxiv.org/abs/1710.10903">Velickovic et al.</a>):</p>
<p>To increase the expressiveness of the graph attention network, <a class="reference external" href="https://arxiv.org/abs/1710.10903">Velickovic et al.</a> proposed to extend it to multiple heads similar to the Multi-Head Attention block in Transformers. This results in <span class="math notranslate nohighlight">\(N\)</span> attention layers being applied in parallel. In the image above, it is visualized as three different colors of arrows (green, blue, and purple) that are afterward concatenated. The average is only applied for the very final prediction layer in a network.</p>
<p>After having discussed the graph attention layer in detail, we can implement it below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span> <span class="k">as</span> <span class="nn">data</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GATLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c_in</span><span class="p">,</span> <span class="n">c_out</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">concat_heads</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Inputs:</span>
<span class="sd">            c_in - Dimensionality of input features</span>
<span class="sd">            c_out - Dimensionality of output features</span>
<span class="sd">            num_heads - Number of heads, i.e. attention mechanisms to apply in parallel. The </span>
<span class="sd">                        output features are equally split up over the heads if concat_heads=True.</span>
<span class="sd">            concat_heads - If True, the output of the different heads is concatenated instead of averaged.</span>
<span class="sd">            alpha - Negative slope of the LeakyReLU activation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat_heads</span> <span class="o">=</span> <span class="n">concat_heads</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat_heads</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">c_out</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Number of output features must be a multiple of the count of heads.&quot;</span>
            <span class="n">c_out</span> <span class="o">=</span> <span class="n">c_out</span> <span class="o">//</span> <span class="n">num_heads</span>
        
        <span class="c1"># Sub-modules and parameters needed in the layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_in</span><span class="p">,</span> <span class="n">c_out</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">c_out</span><span class="p">))</span> <span class="c1"># One per head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">leakyrelu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
        
        <span class="c1"># Initialization from the original implementation</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.414</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.414</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_feats</span><span class="p">,</span> <span class="n">adj_matrix</span><span class="p">,</span> <span class="n">print_attn_probs</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Inputs:</span>
<span class="sd">            node_feats - Input features of the node. Shape: [batch_size, c_in]</span>
<span class="sd">            adj_matrix - Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]</span>
<span class="sd">            print_attn_probs - If True, the attention weights are printed during the forward pass (for debugging purposes)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_nodes</span> <span class="o">=</span> <span class="n">node_feats</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">node_feats</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Apply linear layer and sort nodes by head</span>
        <span class="n">node_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">node_feats</span><span class="p">)</span>
        <span class="n">node_feats</span> <span class="o">=</span> <span class="n">node_feats</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># We need to calculate the attention logits for every edge in the adjacency matrix </span>
        <span class="c1"># Doing this on all possible combinations of nodes is very expensive</span>
        <span class="c1"># =&gt; Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges</span>
        <span class="n">edges</span> <span class="o">=</span> <span class="n">adj_matrix</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">as_tuple</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># Returns indices where the adjacency matrix is not 0 =&gt; edges</span>
        <span class="n">node_feats_flat</span> <span class="o">=</span> <span class="n">node_feats</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">edge_indices_row</span> <span class="o">=</span> <span class="n">edges</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_nodes</span> <span class="o">+</span> <span class="n">edges</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">edge_indices_col</span> <span class="o">=</span> <span class="n">edges</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_nodes</span> <span class="o">+</span> <span class="n">edges</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">a_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">node_feats_flat</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">edge_indices_row</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">node_feats_flat</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">edge_indices_col</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Index select returns a tensor with node_feats_flat being indexed at the desired positions along dim=0</span>
        
        <span class="c1"># Calculate attention MLP output (independent for each head)</span>
        <span class="n">attn_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bhc,hc-&gt;bh&#39;</span><span class="p">,</span> <span class="n">a_input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">)</span> 
        <span class="n">attn_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">leakyrelu</span><span class="p">(</span><span class="n">attn_logits</span><span class="p">)</span>
        
        <span class="c1"># Map list of attention values back into a matrix</span>
        <span class="n">attn_matrix</span> <span class="o">=</span> <span class="n">attn_logits</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="n">adj_matrix</span><span class="o">.</span><span class="n">shape</span><span class="o">+</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,))</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="o">-</span><span class="mf">9e15</span><span class="p">)</span>
        <span class="n">attn_matrix</span><span class="p">[</span><span class="n">adj_matrix</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">attn_logits</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Weighted average of attention</span>
        <span class="n">attn_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_matrix</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">print_attn_probs</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attention probs</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">attn_probs</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">node_feats</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bijh,bjhc-&gt;bihc&#39;</span><span class="p">,</span> <span class="n">attn_probs</span><span class="p">,</span> <span class="n">node_feats</span><span class="p">)</span>
        
        <span class="c1"># If heads should be concatenated, we can do this by reshaping. Otherwise, take mean</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat_heads</span><span class="p">:</span>
            <span class="n">node_feats</span> <span class="o">=</span> <span class="n">node_feats</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">node_feats</span> <span class="o">=</span> <span class="n">node_feats</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">node_feats</span> 
</pre></div>
</div>
</div>
</div>
<p>Again, we can apply the graph attention layer on our example graph above to understand the dynamics better. As before, the input layer is initialized as an identity matrix, but we set <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> to be a vector of arbitrary numbers to obtain different attention values. We use two heads to show the parallel, independent attention mechanisms working in the layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layer</span> <span class="o">=</span> <span class="n">GATLayer</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">layer</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>
<span class="n">layer</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>
<span class="n">layer</span><span class="o">.</span><span class="n">a</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">]])</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">out_feats</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">node_feats</span><span class="p">,</span> <span class="n">adj_matrix</span><span class="p">,</span> <span class="n">print_attn_probs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Adjacency matrix&quot;</span><span class="p">,</span> <span class="n">adj_matrix</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input features&quot;</span><span class="p">,</span> <span class="n">node_feats</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output features&quot;</span><span class="p">,</span> <span class="n">out_feats</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Attention probs
 tensor([[[[0.3543, 0.6457, 0.0000, 0.0000],
          [0.1096, 0.1450, 0.2642, 0.4813],
          [0.0000, 0.1858, 0.2885, 0.5257],
          [0.0000, 0.2391, 0.2696, 0.4913]],

         [[0.5100, 0.4900, 0.0000, 0.0000],
          [0.2975, 0.2436, 0.2340, 0.2249],
          [0.0000, 0.3838, 0.3142, 0.3019],
          [0.0000, 0.4018, 0.3289, 0.2693]]]])
Adjacency matrix tensor([[[1., 1., 0., 0.],
         [1., 1., 1., 1.],
         [0., 1., 1., 1.],
         [0., 1., 1., 1.]]])
Input features tensor([[[0., 1.],
         [2., 3.],
         [4., 5.],
         [6., 7.]]])
Output features tensor([[[1.2913, 1.9800],
         [4.2344, 3.7725],
         [4.6798, 4.8362],
         [4.5043, 4.7351]]])
</pre></div>
</div>
</div>
</div>
<p>We recommend that you try to calculate the attention matrix at least for one head and one node for yourself. The entries are 0 where there does not exist an edge between <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. For the others, we see a diverse set of attention probabilities. Moreover, the output features of node 3 and 4 are now different although they have the same neighbors.</p>
</div>
<div class="section" id="convolution-fundamentals">
<h1>Convolution Fundamentals<a class="headerlink" href="#convolution-fundamentals" title="Permalink to this headline">¶</a></h1>
<p><strong>Why convolution in ML?</strong></p>
<ul class="simple">
<li><p>Weight sharing</p></li>
<li><p>Detection of translational invariant and local features</p></li>
</ul>
<div class="section" id="imports">
<h2>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;font.size&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="definition">
<h2>Definition<a class="headerlink" href="#definition" title="Permalink to this headline">¶</a></h2>
<p>\begin{align*}
c[n] = (v * w)[n] = \sum_{m=0}^{N-1} v[m] \cdot w[n-m]
\end{align*}</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">conv</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)):</span>
        <span class="n">c</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)):</span>
            <span class="n">c</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">+=</span> <span class="n">v</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">n</span> <span class="o">-</span> <span class="n">m</span><span class="p">]</span>  
    <span class="k">return</span> <span class="n">c</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">v</span><span class="p">[</span><span class="mi">8</span><span class="p">:</span><span class="mi">12</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s1">&#39;.-&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="s1">&#39;.-&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="s1">&#39;.-&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/recobook-us739178-v1_109_0.png" src="../_images/recobook-us739178-v1_109_0.png" />
</div>
</div>
</div>
<div class="section" id="fourier-transform">
<h2>Fourier transform<a class="headerlink" href="#fourier-transform" title="Permalink to this headline">¶</a></h2>
<p>Transformation <span class="math notranslate nohighlight">\(\mathcal F: \mathbb{R}^N \to \mathbb{R}^N\)</span> with</p>
<p>\begin{align*}
\mathcal F^{-1}(\mathcal F (v)) &amp;= v\
\mathcal F(v * w) &amp;= \mathcal F(v) \cdot \mathcal F(w).
\end{align*}</p>
<p>This implies
\begin{align*}
v * w &amp;= \mathcal F^{-1}(\mathcal F (v * w))\
&amp;= \mathcal F^{-1}(\mathcal F(v) \cdot \mathcal F(w))
\end{align*}</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">v</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">conv</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([4.77844055, 4.16890293, 4.2094015 , 4.85675858, 4.06215214,
       3.90872955, 4.57520226, 4.46153879, 4.53669411, 4.82854389,
       4.3837407 , 4.04920654, 5.03368611, 3.95014887, 4.30986943,
       4.30560994, 4.08024085, 4.17150078, 4.49179264, 4.15049511])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.fft</span> <span class="kn">import</span> <span class="n">fft</span><span class="p">,</span> <span class="n">ifft</span> <span class="c1"># Fast Fourier Transform / Inverse FFT</span>
<span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ifft</span><span class="p">(</span><span class="n">fft</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">*</span> <span class="n">fft</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([4.77844055, 4.16890293, 4.2094015 , 4.85675858, 4.06215214,
       3.90872955, 4.57520226, 4.46153879, 4.53669411, 4.82854389,
       4.3837407 , 4.04920654, 5.03368611, 3.95014887, 4.30986943,
       4.30560994, 4.08024085, 4.17150078, 4.49179264, 4.15049511])
</pre></div>
</div>
</div>
</div>
<div class="section" id="definition-of-the-fourier-transform">
<h3>Definition of the Fourier transform<a class="headerlink" href="#definition-of-the-fourier-transform" title="Permalink to this headline">¶</a></h3>
<p>The Fourier transform can be computed as</p>
<p>\begin{align*}
\mathcal F(v) = U\cdot v, ;;\mathcal F^{-1}(v) = \frac{1}{N}\ U^H \cdot v
\end{align*}</p>
<p>where the <span class="math notranslate nohighlight">\(N\times N\)</span> matrix <span class="math notranslate nohighlight">\(U\)</span> is defined as
\begin{align*}
\
U =
\begin{bmatrix}
u_0(0) &amp; u_1(0) &amp; \dots &amp; u_{N-1}(0)\
u_0(1) &amp; u_1(1) &amp; \dots &amp; u_{N-1}(1)\
\vdots &amp; \vdots&amp; &amp; \vdots\
u_0(N-1) &amp; u_1(N-1) &amp; \dots &amp; u_{N-1}(N-1)\
\end{bmatrix}
\end{align*}</p>
<p>and <span class="math notranslate nohighlight">\(u_0, \dots, u_{N-1}\)</span> are functions defined as</p>
<p>\begin{align*}
u_n(x)&amp;:= \cos\left(2 \pi \frac{n}{N} x\right) - i \sin\left(2 \pi \frac{n}{N} x\right).
\end{align*}</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">matrix_U</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">u</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">n</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span><span class="n">j</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">))</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">U</span><span class="p">,</span> <span class="n">u</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">N</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">U</span>


<span class="k">def</span> <span class="nf">fourier_transform</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">matrix_U</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">U</span> <span class="o">@</span> <span class="n">v</span>


<span class="k">def</span> <span class="nf">inverse_fourier_transform</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">matrix_U</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">U</span><span class="o">.</span><span class="n">conj</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span> <span class="o">@</span> <span class="n">v</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fft</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">-</span> <span class="n">fourier_transform</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-1.77635684e-15-0.00000000e+00j,  6.66133815e-16-2.22044605e-16j,
        0.00000000e+00-1.66533454e-16j,  9.99200722e-16+6.66133815e-16j,
       -1.11022302e-16+5.55111512e-16j,  3.55271368e-15+6.10622664e-16j,
       -1.55431223e-15+9.99200722e-16j, -6.60582700e-15+0.00000000e+00j,
       -1.11022302e-16-2.49800181e-16j,  6.66133815e-16-2.22044605e-16j,
        4.44089210e-16+4.08296952e-15j,  3.16413562e-15-1.77635684e-15j,
       -1.11022302e-15-1.30451205e-15j,  2.22044605e-16-3.55271368e-15j,
       -4.44089210e-16+5.21804822e-15j,  4.66293670e-15+5.38458167e-15j,
        0.00000000e+00+1.55431223e-15j, -6.55031585e-15-3.77475828e-15j,
        5.32907052e-15+3.38618023e-15j, -4.88498131e-15-1.33226763e-15j])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ifft</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">-</span> <span class="n">inverse_fourier_transform</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 0.00000000e+00-0.00000000e+00j,  4.16333634e-17+0.00000000e+00j,
        0.00000000e+00+3.46944695e-18j,  4.85722573e-17-5.55111512e-17j,
       -1.73472348e-17-2.08166817e-17j,  1.73472348e-16-2.77555756e-17j,
       -9.02056208e-17-6.24500451e-17j, -3.22658567e-16+0.00000000e+00j,
       -6.93889390e-18+1.04083409e-17j,  2.42861287e-17+0.00000000e+00j,
       -4.16333634e-17-2.04148476e-16j,  1.45716772e-16+9.71445147e-17j,
       -5.55111512e-17+6.41847686e-17j,  6.93889390e-18+1.66533454e-16j,
       -3.46944695e-17-2.70616862e-16j,  2.32452946e-16-2.67147415e-16j,
        1.73472348e-17-7.63278329e-17j, -3.19189120e-16+2.04697370e-16j,
        2.53269627e-16-1.59594560e-16j, -2.22044605e-16+9.71445147e-17j])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="connection-with-the-laplacian">
<h3>Connection with the Laplacian<a class="headerlink" href="#connection-with-the-laplacian" title="Permalink to this headline">¶</a></h3>
<p>The functions <span class="math notranslate nohighlight">\(u_n\)</span> (the columns of the Fourier transform matrix) are eigenvectors of the Laplacian:</p>
<p>\begin{align*}
u_n(x)&amp;:= \cos\left(2 \pi \frac{n}{N} x\right) - i \sin\left(2 \pi \frac{n}{N} x\right)\
\Delta u_n(x)&amp;:= \left(-4 \pi\frac{n^2}{N^2}\right) u_n(x)
\end{align*}</p>
</div>
<div class="section" id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h3>
<p>\begin{align*}
v * w
= U^H ((U  w) \odot (U  v))
\end{align*}</p>
<p>or if <span class="math notranslate nohighlight">\(g_w=\mbox{diag}(U w)\)</span> is  filter
\begin{align*}
v * w
= U^H g_w U  w
\end{align*}</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">U</span> <span class="o">=</span> <span class="n">matrix_U</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">U</span><span class="o">.</span><span class="n">conj</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span> <span class="o">/</span> <span class="n">N</span><span class="p">)</span> <span class="o">@</span> <span class="p">((</span><span class="n">U</span> <span class="o">@</span> <span class="n">v</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">U</span> <span class="o">@</span> <span class="n">w</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([4.77844055, 4.16890293, 4.2094015 , 4.85675858, 4.06215214,
       3.90872955, 4.57520226, 4.46153879, 4.53669411, 4.82854389,
       4.3837407 , 4.04920654, 5.03368611, 3.95014887, 4.30986943,
       4.30560994, 4.08024085, 4.17150078, 4.49179264, 4.15049511])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">conv</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([4.77844055, 4.16890293, 4.2094015 , 4.85675858, 4.06215214,
       3.90872955, 4.57520226, 4.46153879, 4.53669411, 4.82854389,
       4.3837407 , 4.04920654, 5.03368611, 3.95014887, 4.30986943,
       4.30560994, 4.08024085, 4.17150078, 4.49179264, 4.15049511])
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="convolution-on-graphs">
<h2>Convolution on graphs<a class="headerlink" href="#convolution-on-graphs" title="Permalink to this headline">¶</a></h2>
<p><strong>Plan</strong>:</p>
<ul class="simple">
<li><p>Define the graph Laplacian</p></li>
<li><p>Compute the spectrum</p></li>
<li><p>Define a Fourier transform</p></li>
<li><p>Define convolution on a graph</p></li>
</ul>
<p><strong>Note:</strong> From now on <span class="math notranslate nohighlight">\(G = (V, E)\)</span> is an undirected, unweighted, simple graph.</p>
<div class="section" id="graph-laplacian">
<h3>Graph Laplacian<a class="headerlink" href="#graph-laplacian" title="Permalink to this headline">¶</a></h3>
<p>Adjacency matrix
\begin{align*}
A_{ij} = \left{
\begin{array}{ll}
1 &amp;\text{ if } e_{ij}\in E\
0 &amp;\text{ if } e_{ij}\notin E
\end{array}
\right.
\end{align*}</p>
<p>Degree matrix
\begin{align*}
D_{ij} = \left{
\begin{array}{ll}
\mbox{deg}(v_i) &amp;\text{ if } i=j\
0 &amp;\text{ if } i\neq j
\end{array}
\right.
\end{align*}</p>
<p>Laplacian
\begin{align*}
L &amp;= D - A.
\end{align*}</p>
<p>Normalized Laplacian
\begin{align*}
L &amp;= I - D^{-1/2} A D^{-1/2}.
\end{align*}</p>
</div>
<div class="section" id="graph-spectrum-fourier-transform-and-convolution">
<h3>Graph spectrum, Fourier transform, and convolution<a class="headerlink" href="#graph-spectrum-fourier-transform-and-convolution" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>Spectral decomposition of the Laplacian:
\begin{align*}
L = U \Lambda U^T\
\end{align*}</p></li>
<li><p>Fourier transform: if <span class="math notranslate nohighlight">\(v\)</span> is a vector of features on the graph, then
\begin{align*}
\mathcal F (v) = U \cdot v, ;;\mathcal F^{-1} (v) = U^T \cdot v\
\end{align*}</p></li>
<li><p>Convolution with a filter <span class="math notranslate nohighlight">\(U \cdot w\)</span>
\begin{align*}
v * w = U ((U^T  w) \odot (U^T  v) )
\end{align*}</p></li>
</ol>
<p>Or <span class="math notranslate nohighlight">\(g_w = \mbox{diag}(U^T w)\)</span> is a filter, then
\begin{align*}
v * w = U g_w U^T  v
\end{align*}</p>
</div>
</div>
</div>
<div class="section" id="spectral-convolutional-layers-in-pytorch-geometric">
<h1>Spectral-convolutional layers in PyTorch Geometric<a class="headerlink" href="#spectral-convolutional-layers-in-pytorch-geometric" title="Permalink to this headline">¶</a></h1>
<p><strong>Problem:</strong> Computing the spectrum is a global and very expensive property.</p>
<p><strong>Goal:</strong> Implementation as message passing.</p>
<div class="section" id="chebconv">
<h2>ChebConv<a class="headerlink" href="#chebconv" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Original <a class="reference external" href="https://arxiv.org/pdf/1606.09375.pdf">paper</a></p></li>
<li><p>PyTorch <a class="reference external" href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.ChebConv">doc</a></p></li>
</ul>
<div class="section" id="goal">
<h3>Goal:<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h3>
<p>Compute <span class="math notranslate nohighlight">\(U g_w U^T x\)</span> with <span class="math notranslate nohighlight">\(g_w = g_w(\Lambda)\)</span> a filter.</p>
</div>
<div class="section" id="chebyshev-approximation">
<h3>Chebyshev approximation<a class="headerlink" href="#chebyshev-approximation" title="Permalink to this headline">¶</a></h3>
<p>Chebyshev polynomials <span class="math notranslate nohighlight">\(T_k\)</span>:
\begin{align*}
T_{k}(x) = 2 x T_{k-1}(x) - T_{k-2}(x), ;; T_0(x) = 1, T_1(x) = x
\end{align*}</p>
</div>
<div class="section" id="chebyshev-approximation-of-the-filter">
<h3>Chebyshev approximation of the filter<a class="headerlink" href="#chebyshev-approximation-of-the-filter" title="Permalink to this headline">¶</a></h3>
<p>Aproximation of the filter:
\begin{align*}
g_w(\Lambda) = \sum_{k=0}^K \theta_k T_k(\tilde \Lambda),;;;;\tilde \Lambda = \frac{2}{\lambda_\max} \Lambda - I \cdot \lambda_\max
\end{align*}</p>
</div>
<div class="section" id="property">
<h3>Property<a class="headerlink" href="#property" title="Permalink to this headline">¶</a></h3>
<p>If <span class="math notranslate nohighlight">\(L = U \Lambda U^T\)</span> then <span class="math notranslate nohighlight">\(T_k(L) = U T_k(\Lambda) U^T\)</span>.</p>
</div>
<div class="section" id="fast-approximated-convolution">
<h3>Fast approximated convolution<a class="headerlink" href="#fast-approximated-convolution" title="Permalink to this headline">¶</a></h3>
<p>\begin{align*}
v * w &amp;= U g_w U^T x
= U \left(\sum_{k=0}^K \theta_k T_k(\tilde \Lambda) \right)U^T x
=\sum_{k=0}^K  \theta_k U  T_k(\tilde \Lambda) U^T x\
&amp;=\sum_{k=0}^K  \theta_k T_k(\tilde L) x
\end{align*}</p>
<p>\begin{align*}
\tilde L = \frac{2}{\lambda_\max} L - I
\end{align*}</p>
</div>
<div class="section" id="properties">
<h3>Properties:<a class="headerlink" href="#properties" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Depends on <span class="math notranslate nohighlight">\(L\)</span> and <span class="math notranslate nohighlight">\(\lambda_\max\)</span>, not on <span class="math notranslate nohighlight">\(U, \Sigma\)</span></p></li>
<li><p>Uses only <span class="math notranslate nohighlight">\(K\)</span>-powers <span class="math notranslate nohighlight">\(\Rightarrow\)</span> only the <span class="math notranslate nohighlight">\(K\)</span>-th neighborhood of each node, localized filter</p></li>
</ul>
</div>
</div>
<div class="section" id="gcnconv">
<h2>GCNConv<a class="headerlink" href="#gcnconv" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Original <a class="reference external" href="https://arxiv.org/pdf/1609.02907.pdf">paper</a></p></li>
<li><p>PyTorch <a class="reference external" href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv">doc</a></p></li>
</ul>
<p>Start from <code class="docutils literal notranslate"><span class="pre">ChebConv</span></code> and assume</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(K=1\)</span> (linear approximation) so
\begin{align*}
v * w
&amp;=\sum_{k=0}^1  \theta_k T_k(\tilde L) x
= \theta_0 x + \theta_1 \tilde L x\
\end{align*}</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda_\max =2\)</span> so
\begin{align*}
v * w
&amp;= \theta_0 x + \theta_1 (L - I) x\
&amp;= \theta_0 x - \theta_1 D^{-1/2} A D^{1/2} x\
\end{align*}</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_0=-\theta_1= \theta\)</span> so
\begin{align*}
v * w = \left(I + D^{-1/2} A D^{1/2}\right) x \theta
\end{align*}</p></li>
<li><p>Renormalization of <span class="math notranslate nohighlight">\(\theta\)</span> by using
\begin{align*}
\tilde A&amp;:= I + A\
\tilde D_{ii}&amp;:= \sum_j \tilde A_{ij}
\end{align*}
so
\begin{align*}
v * w = \left(D^{-1/2} A D^{1/2}\right) x \theta
\end{align*}</p></li>
</ol>
<p>If <span class="math notranslate nohighlight">\(x\)</span> is a <span class="math notranslate nohighlight">\(F\)</span>-dimensional feature vector, and we want an <span class="math notranslate nohighlight">\(F'\)</span>-dimensional feature vector as output:
use <span class="math notranslate nohighlight">\(W'\in \mathbb{R}^{F\times F'}\)</span>
\begin{align*}
v * w = \left(D^{-1/2} A D^{1/2}\right) x \Theta
\end{align*}</p>
</div>
</div>
<div class="section" id="aggregation-functions-in-gnns">
<h1>Aggregation Functions in GNNs<a class="headerlink" href="#aggregation-functions-in-gnns" title="Permalink to this headline">¶</a></h1>
<div class="section" id="context">
<h2>Context<a class="headerlink" href="#context" title="Permalink to this headline">¶</a></h2>
<p>We explore how to perform neighborhood aggregation in GNNs, describing the GIN model and other recent techniques for selecting the right aggregation (PNA) or learn it (LAF).</p>
<p>We will override the aggregation method of the GIN convolution module of Pytorch Geometric implementing the following methods:</p>
<ul class="simple">
<li><p>Principal Neighborhood Aggregation (PNA)</p></li>
<li><p>Learning Aggregation Functions (LAF)</p></li>
</ul>
</div>
<div class="section" id="wl-isomorphism-test">
<h2>WL Isomorphism Test<a class="headerlink" href="#wl-isomorphism-test" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="id2">
<h2>Imports<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">MessagePassing</span>
<span class="kn">from</span> <span class="nn">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">GINConv</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Linear</span>
<span class="kn">from</span> <span class="nn">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">MessagePassing</span><span class="p">,</span> <span class="n">SAGEConv</span><span class="p">,</span> <span class="n">GINConv</span><span class="p">,</span> <span class="n">global_add_pool</span>
<span class="kn">import</span> <span class="nn">torch_scatter</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Sequential</span><span class="p">,</span> <span class="n">Linear</span><span class="p">,</span> <span class="n">ReLU</span>
<span class="kn">from</span> <span class="nn">torch_geometric.datasets</span> <span class="kn">import</span> <span class="n">TUDataset</span>
<span class="kn">from</span> <span class="nn">torch_geometric.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">Module</span><span class="p">,</span> <span class="n">Sigmoid</span>
<span class="kn">import</span> <span class="nn">os.path</span> <span class="k">as</span> <span class="nn">osp</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;torch._C.Generator at 0x7fd92f31a9f0&gt;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="message-passing-class">
<h2>Message Passing Class<a class="headerlink" href="#message-passing-class" title="Permalink to this headline">¶</a></h2>
<p>We are interested in the <span style='color:Blue'>aggregate</span> method, or, if you are using a sparse adjacency matrix, in the <span style='color:Blue'>message_and_aggregate</span> method. Convolutional classes in PyG extend MessagePassing, we construct our custom convoutional class extending GINConv.</p>
<p>Scatter operation in <span style='color:Blue'>aggregate</span>:</p>
<img src="https://raw.githubusercontent.com/rusty1s/pytorch_scatter/master/docs/source/_figures/add.svg?sanitize=true" width="500"></div>
<div class="section" id="laf-aggregation-module">
<h2>LAF Aggregation Module<a class="headerlink" href="#laf-aggregation-module" title="Permalink to this headline">¶</a></h2>
<p><strong>LAF Layer</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AbstractLAFLayer</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AbstractLAFLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="s1">&#39;units&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="ow">or</span> <span class="s1">&#39;weights&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span>
        <span class="k">if</span> <span class="s1">&#39;device&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;device&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ngpus</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="s1">&#39;kernel_initializer&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">assert</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;kernel_initializer&#39;</span><span class="p">]</span> <span class="ow">in</span> <span class="p">[</span>
                <span class="s1">&#39;random_normal&#39;</span><span class="p">,</span>
                <span class="s1">&#39;glorot_normal&#39;</span><span class="p">,</span>
                <span class="s1">&#39;he_normal&#39;</span><span class="p">,</span>
                <span class="s1">&#39;random_uniform&#39;</span><span class="p">,</span>
                <span class="s1">&#39;glorot_uniform&#39;</span><span class="p">,</span>
                <span class="s1">&#39;he_uniform&#39;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;kernel_initializer&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span> <span class="o">=</span> <span class="s1">&#39;random_normal&#39;</span>

        <span class="k">if</span> <span class="s1">&#39;weights&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;weights&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> \
                                     <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;units&#39;</span><span class="p">]</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span> <span class="o">==</span> <span class="s1">&#39;random_normal&#39;</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span> <span class="o">==</span> <span class="s1">&#39;glorot_normal&#39;</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span> <span class="o">==</span> <span class="s1">&#39;he_normal&#39;</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span> <span class="o">==</span> <span class="s1">&#39;random_uniform&#39;</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span> <span class="o">==</span> <span class="s1">&#39;glorot_uniform&#39;</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span> <span class="o">==</span> <span class="s1">&#39;he_uniform&#39;</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> \
                                     <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">e</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">num_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span>\
                                <span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_idx</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">num_idx</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">den_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span>\
                                <span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">den_idx</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">den_idx</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        

<span class="k">class</span> <span class="nc">LAFLayer</span><span class="p">(</span><span class="n">AbstractLAFLayer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LAFLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span>
        <span class="n">sup</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">eps</span> 
        <span class="n">e</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">e</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">sup</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>        

        <span class="n">exps</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">e</span><span class="p">)</span><span class="o">/</span><span class="mf">2.</span> <span class="o">+</span> <span class="n">x</span><span class="o">*</span><span class="n">e</span> 
        <span class="n">exps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">exps</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">exps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">exps</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">]))</span>

        <span class="n">scatter</span> <span class="o">=</span> <span class="n">torch_scatter</span><span class="o">.</span><span class="n">scatter_add</span><span class="p">(</span><span class="n">exps</span><span class="p">,</span> <span class="n">index</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">scatter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">scatter</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>

        <span class="n">sqrt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">scatter</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">4</span><span class="p">:</span><span class="mi">8</span><span class="p">]))</span>
        <span class="n">alpha_beta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">8</span><span class="p">:</span><span class="mi">12</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">terms</span> <span class="o">=</span> <span class="n">sqrt</span> <span class="o">*</span> <span class="n">alpha_beta</span>

        <span class="n">num</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">terms</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_idx</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">den</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">terms</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">den_idx</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="n">multiplier</span> <span class="o">=</span> <span class="mf">2.0</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">den</span><span class="p">),</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.0</span>

        <span class="n">den</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">den</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">den</span> <span class="o">&gt;</span> <span class="o">-</span><span class="n">eps</span><span class="p">),</span> <span class="n">multiplier</span><span class="o">*</span><span class="n">eps</span><span class="p">,</span> <span class="n">den</span><span class="p">)</span>

        <span class="n">res</span> <span class="o">=</span> <span class="n">num</span> <span class="o">/</span> <span class="n">den</span>
        <span class="k">return</span> <span class="n">res</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GINLAFConv</span><span class="p">(</span><span class="n">GINConv</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nn</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">node_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GINLAFConv</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">laf</span> <span class="o">=</span> <span class="n">LAFLayer</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">units</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;random_uniform&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">node_dim</span><span class="o">*</span><span class="n">units</span><span class="p">,</span> <span class="n">node_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">node_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">=</span> <span class="n">units</span>
    
    <span class="k">def</span> <span class="nf">aggregate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">laf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="pna-aggregation">
<h2>PNA Aggregation<a class="headerlink" href="#pna-aggregation" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GINPNAConv</span><span class="p">(</span><span class="n">GINConv</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nn</span><span class="p">,</span> <span class="n">node_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GINPNAConv</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">node_dim</span><span class="o">*</span><span class="mi">12</span><span class="p">,</span> <span class="n">node_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">delta</span> <span class="o">=</span> <span class="mf">2.5749</span>
    
    <span class="k">def</span> <span class="nf">aggregate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="n">sums</span> <span class="o">=</span> <span class="n">torch_scatter</span><span class="o">.</span><span class="n">scatter_add</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">maxs</span> <span class="o">=</span> <span class="n">torch_scatter</span><span class="o">.</span><span class="n">scatter_max</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">means</span> <span class="o">=</span> <span class="n">torch_scatter</span><span class="o">.</span><span class="n">scatter_mean</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">torch_scatter</span><span class="o">.</span><span class="n">scatter_mean</span><span class="p">(</span><span class="n">inputs</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">means</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="n">aggrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">sums</span><span class="p">,</span> <span class="n">maxs</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">var</span><span class="p">]</span>
        <span class="n">c_idx</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">bincount</span><span class="p">()</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">l_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">c_idx</span> <span class="o">+</span> <span class="mf">1.</span><span class="p">)</span>
        
        <span class="n">amplification_scaler</span> <span class="o">=</span> <span class="p">[</span><span class="n">c_idx</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">delta</span> <span class="o">*</span> <span class="n">a</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">aggrs</span><span class="p">]</span>
        <span class="n">attenuation_scaler</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">delta</span> <span class="o">/</span> <span class="n">c_idx</span> <span class="o">*</span> <span class="n">a</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">aggrs</span><span class="p">]</span>
        <span class="n">combinations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">aggrs</span><span class="o">+</span> <span class="n">amplification_scaler</span><span class="o">+</span> <span class="n">attenuation_scaler</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">combinations</span><span class="p">)</span>
    
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="test-the-new-classes">
<h2>Test the new classes<a class="headerlink" href="#test-the-new-classes" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">osp</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;./&#39;</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;TU&#39;</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">TUDataset</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;MUTAG&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">shuffle</span><span class="p">()</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">//</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">//</span> <span class="mi">10</span><span class="p">:]</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip
Extracting data/TU/MUTAG/MUTAG.zip
Processing...
Done!
/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:13: UserWarning: &#39;data.DataLoader&#39; is deprecated, use &#39;loader.DataLoader&#39; instead
  warnings.warn(out)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="laf-model">
<h2>LAF Model<a class="headerlink" href="#laf-model" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LAFNet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LAFNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="n">num_features</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_features</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="mi">32</span>
        <span class="n">units</span> <span class="o">=</span> <span class="mi">3</span>
        
        <span class="n">nn1</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">GINLAFConv</span><span class="p">(</span><span class="n">nn1</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="n">units</span><span class="p">,</span> <span class="n">node_dim</span><span class="o">=</span><span class="n">num_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

        <span class="n">nn2</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">GINLAFConv</span><span class="p">(</span><span class="n">nn2</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="n">units</span><span class="p">,</span> <span class="n">node_dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

        <span class="n">nn3</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">GINLAFConv</span><span class="p">(</span><span class="n">nn3</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="n">units</span><span class="p">,</span> <span class="n">node_dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

        <span class="n">nn4</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv4</span> <span class="o">=</span> <span class="n">GINLAFConv</span><span class="p">(</span><span class="n">nn4</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="n">units</span><span class="p">,</span> <span class="n">node_dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn4</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

        <span class="n">nn5</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv5</span> <span class="o">=</span> <span class="n">GINLAFConv</span><span class="p">(</span><span class="n">nn5</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="n">units</span><span class="p">,</span> <span class="n">node_dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn5</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv4</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv5</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn5</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">global_add_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="pna-model">
<h2>PNA Model<a class="headerlink" href="#pna-model" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PNANet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PNANet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="n">num_features</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_features</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="mi">32</span>

        <span class="n">nn1</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">GINPNAConv</span><span class="p">(</span><span class="n">nn1</span><span class="p">,</span> <span class="n">node_dim</span><span class="o">=</span><span class="n">num_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

        <span class="n">nn2</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">GINPNAConv</span><span class="p">(</span><span class="n">nn2</span><span class="p">,</span> <span class="n">node_dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

        <span class="n">nn3</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">GINPNAConv</span><span class="p">(</span><span class="n">nn3</span><span class="p">,</span> <span class="n">node_dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

        <span class="n">nn4</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv4</span> <span class="o">=</span> <span class="n">GINPNAConv</span><span class="p">(</span><span class="n">nn4</span><span class="p">,</span> <span class="n">node_dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn4</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

        <span class="n">nn5</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv5</span> <span class="o">=</span> <span class="n">GINPNAConv</span><span class="p">(</span><span class="n">nn5</span><span class="p">,</span> <span class="n">node_dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn5</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv4</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv5</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn5</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">global_add_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="gin-model">
<h2>GIN Model<a class="headerlink" href="#gin-model" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GINNet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GINNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="n">num_features</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_features</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="mi">32</span>

        <span class="n">nn1</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">GINConv</span><span class="p">(</span><span class="n">nn1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

        <span class="n">nn2</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">GINConv</span><span class="p">(</span><span class="n">nn2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

        <span class="n">nn3</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">GINConv</span><span class="p">(</span><span class="n">nn3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

        <span class="n">nn4</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv4</span> <span class="o">=</span> <span class="n">GINConv</span><span class="p">(</span><span class="n">nn4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn4</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

        <span class="n">nn5</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv5</span> <span class="o">=</span> <span class="n">GINConv</span><span class="p">(</span><span class="n">nn5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn5</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv4</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv5</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn5</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">global_add_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="s2">&quot;PNA&quot;</span>
<span class="k">if</span> <span class="n">net</span> <span class="o">==</span> <span class="s2">&quot;LAF&quot;</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LAFNet</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">net</span> <span class="o">==</span> <span class="s2">&quot;PNA&quot;</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">PNANet</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">net</span> <span class="o">==</span> <span class="s2">&quot;GIN&quot;</span><span class="p">:</span>
    <span class="n">GINNet</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">51</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>

    <span class="n">loss_all</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">loss_all</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">data</span><span class="o">.</span><span class="n">num_graphs</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss_all</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="n">pred</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">):</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
    <span class="n">train_acc</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch: </span><span class="si">{:03d}</span><span class="s1">, Train Loss: </span><span class="si">{:.7f}</span><span class="s1">, &#39;</span>
          <span class="s1">&#39;Train Acc: </span><span class="si">{:.7f}</span><span class="s1">, Test Acc: </span><span class="si">{:.7f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span>
                                                       <span class="n">train_acc</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch_geometric.datasets</span> <span class="kn">import</span> <span class="n">Planetoid</span>
<span class="kn">import</span> <span class="nn">torch_geometric.transforms</span> <span class="k">as</span> <span class="nn">T</span>
<span class="kn">from</span> <span class="nn">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">GCNConv</span>
<span class="kn">from</span> <span class="nn">torch_geometric.utils</span> <span class="kn">import</span> <span class="n">train_test_split_edges</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="graph-autoencoders-gae-vgae">
<h1>Graph AutoEncoders - GAE &amp; VGAE<a class="headerlink" href="#graph-autoencoders-gae-vgae" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://arxiv.org/pdf/1611.07308.pdf">paper</a><br />
<a class="reference external" href="https://github.com/rusty1s/pytorch_geometric/blob/master/examples/autoencoder.py">code</a></p>
<div class="section" id="id3">
<h2>Context<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<div class="section" id="loss-function">
<h3>Loss function<a class="headerlink" href="#loss-function" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="id4">
<h2>Imports<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">GAE</span>
<span class="kn">from</span> <span class="nn">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">VGAE</span>
<span class="kn">from</span> <span class="nn">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch_geometric.datasets</span> <span class="kn">import</span> <span class="n">Planetoid</span>
<span class="kn">import</span> <span class="nn">torch_geometric.transforms</span> <span class="k">as</span> <span class="nn">T</span>
<span class="kn">from</span> <span class="nn">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">GCNConv</span>
<span class="kn">from</span> <span class="nn">torch_geometric.utils</span> <span class="kn">import</span> <span class="n">train_test_split_edges</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="load-the-citeseer-data">
<h2>Load the CiteSeer data<a class="headerlink" href="#load-the-citeseer-data" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">Planetoid</span><span class="p">(</span><span class="s2">&quot;\..&quot;</span><span class="p">,</span> <span class="s2">&quot;CiteSeer&quot;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">T</span><span class="o">.</span><span class="n">NormalizeFeatures</span><span class="p">())</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">data</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">data</span><span class="o">.</span><span class="n">train_mask</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">val_mask</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">test_mask</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">train_test_split_edges</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:13: UserWarning: &#39;train_test_split_edges&#39; is deprecated, use &#39;transforms.RandomLinkSplit&#39; instead
  warnings.warn(out)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data(x=[3327, 3703], y=[3327], val_pos_edge_index=[2, 227], test_pos_edge_index=[2, 455], train_pos_edge_index=[2, 7740], train_neg_adj_mask=[3327, 3327], val_neg_edge_index=[2, 227], test_neg_edge_index=[2, 455])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="define-the-encoder">
<h2>Define the Encoder<a class="headerlink" href="#define-the-encoder" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GCNEncoder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GCNEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">GCNConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">cached</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># cached only for transductive learning</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">GCNConv</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">cached</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># cached only for transductive learning</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span><span class="o">.</span><span class="n">relu</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="define-the-autoencoder">
<h2>Define the Autoencoder<a class="headerlink" href="#define-the-autoencoder" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># parameters</span>
<span class="n">out_channels</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">num_features</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_features</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GAE</span><span class="p">(</span><span class="n">GCNEncoder</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>

<span class="c1"># move to GPU (if available)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">train_pos_edge_index</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">train_pos_edge_index</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># inizialize the optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train_pos_edge_index</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">recon_loss</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">train_pos_edge_index</span><span class="p">)</span>
    <span class="c1">#if args.variational:</span>
    <span class="c1">#   loss = loss + (1 / data.num_nodes) * model.kl_loss()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">pos_edge_index</span><span class="p">,</span> <span class="n">neg_edge_index</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train_pos_edge_index</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">pos_edge_index</span><span class="p">,</span> <span class="n">neg_edge_index</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">()</span>

    <span class="n">auc</span><span class="p">,</span> <span class="n">ap</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">test_pos_edge_index</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">test_neg_edge_index</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch: </span><span class="si">{:03d}</span><span class="s1">, AUC: </span><span class="si">{:.4f}</span><span class="s1">, AP: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">auc</span><span class="p">,</span> <span class="n">ap</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train_pos_edge_index</span><span class="p">)</span>
<span class="n">Z</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.3132,  0.2351],
        [ 0.9216, -0.6546],
        [-0.8335,  0.5395],
        ...,
        [ 0.3987, -0.2366],
        [-0.8049,  0.5315],
        [-0.8706,  0.5499]], grad_fn=&lt;AddBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="result-analysis-with-tensorboard">
<h2>Result analysis with Tensorboard<a class="headerlink" href="#result-analysis-with-tensorboard" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># parameters</span>
<span class="n">out_channels</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">num_features</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_features</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GAE</span><span class="p">(</span><span class="n">GCNEncoder</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>

<span class="c1"># move to GPU (if available)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">train_pos_edge_index</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">train_pos_edge_index</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># inizialize the optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">writer</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="s1">&#39;runs/GAE1_experiment_&#39;</span><span class="o">+</span><span class="s1">&#39;2d_100_epochs&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">()</span>
    <span class="n">auc</span><span class="p">,</span> <span class="n">ap</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">test_pos_edge_index</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">test_neg_edge_index</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch: </span><span class="si">{:03d}</span><span class="s1">, AUC: </span><span class="si">{:.4f}</span><span class="s1">, AP: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">auc</span><span class="p">,</span> <span class="n">ap</span><span class="p">))</span>
    
    
    <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;auc train&#39;</span><span class="p">,</span><span class="n">auc</span><span class="p">,</span><span class="n">epoch</span><span class="p">)</span> <span class="c1"># new line</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;ap train&#39;</span><span class="p">,</span><span class="n">ap</span><span class="p">,</span><span class="n">epoch</span><span class="p">)</span>   <span class="c1"># new line</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="graph-variational-autoencoder-gvae">
<h2>Graph Variational AutoEncoder (GVAE)<a class="headerlink" href="#graph-variational-autoencoder-gvae" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">Planetoid</span><span class="p">(</span><span class="s2">&quot;\..&quot;</span><span class="p">,</span> <span class="s2">&quot;CiteSeer&quot;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">T</span><span class="o">.</span><span class="n">NormalizeFeatures</span><span class="p">())</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">data</span><span class="o">.</span><span class="n">train_mask</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">val_mask</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">test_mask</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">train_test_split_edges</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">VariationalGCNEncoder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VariationalGCNEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">GCNConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">cached</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># cached only for transductive learning</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_mu</span> <span class="o">=</span> <span class="n">GCNConv</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">cached</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_logstd</span> <span class="o">=</span> <span class="n">GCNConv</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">cached</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span><span class="o">.</span><span class="n">relu</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_mu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_logstd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:13: UserWarning: &#39;train_test_split_edges&#39; is deprecated, use &#39;transforms.RandomLinkSplit&#39; instead
  warnings.warn(out)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">out_channels</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">num_features</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_features</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">300</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">VGAE</span><span class="p">(</span><span class="n">VariationalGCNEncoder</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>  <span class="c1"># new line</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">train_pos_edge_index</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">train_pos_edge_index</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train_pos_edge_index</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">recon_loss</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">train_pos_edge_index</span><span class="p">)</span>
    
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">data</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">)</span> <span class="o">*</span> <span class="n">model</span><span class="o">.</span><span class="n">kl_loss</span><span class="p">()</span>  <span class="c1"># new line</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">pos_edge_index</span><span class="p">,</span> <span class="n">neg_edge_index</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train_pos_edge_index</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">pos_edge_index</span><span class="p">,</span> <span class="n">neg_edge_index</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">writer</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="s1">&#39;runs/VGAE_experiment_&#39;</span><span class="o">+</span><span class="s1">&#39;2d_100_epochs&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">()</span>
    <span class="n">auc</span><span class="p">,</span> <span class="n">ap</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">test_pos_edge_index</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">test_neg_edge_index</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch: </span><span class="si">{:03d}</span><span class="s1">, AUC: </span><span class="si">{:.4f}</span><span class="s1">, AP: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">auc</span><span class="p">,</span> <span class="n">ap</span><span class="p">))</span>
    
    
    <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;auc train&#39;</span><span class="p">,</span><span class="n">auc</span><span class="p">,</span><span class="n">epoch</span><span class="p">)</span> <span class="c1"># new line</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;ap train&#39;</span><span class="p">,</span><span class="n">ap</span><span class="p">,</span><span class="n">epoch</span><span class="p">)</span>   <span class="c1"># new line</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="end">
<h1>End<a class="headerlink" href="#end" title="Permalink to this headline">¶</a></h1>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./nbs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="../index.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Main Header <em>(Placeholder)</em></p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="recobook-us739178-v2.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Theory</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The Jupyter Book community<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>