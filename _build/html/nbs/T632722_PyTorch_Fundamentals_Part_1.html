
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>PyTorch Fundamentals Part 1 &#8212; Reco Book</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="PyTorch Fundamentals Part 2" href="T472467_PyTorch_Fundamentals_Part_2.html" />
    <link rel="prev" title="OBP Library Workshop Tutorials" href="T966055_OBP_Library_Workshop_Tutorials.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Reco Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Tutorials in Jupyter notebook format
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  User Stories
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="US780867_Transformer_based_Recommenders.html">
   Transformer-based Recommenders
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="T034923_BERT4Rec_on_ML1M_in_PyTorch.html">
     BERT4Rec on ML-1M in PyTorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T595874_BERT4Rec_on_ML25M_in_PyTorch_Lightning.html">
     BERT4Rec on ML-25M
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T088416_BST_Implementation_in_MXNet.html">
     BST Implementation in MXNet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T602245_BST_implementation_in_PyTorch.html">
     BST implementation in PyTorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T007665_BST_on_ML1M_in_Keras.html">
     A Transformer-based recommendation system
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T881207_BST_PTLightning_ML1M.html">
     Rating prediction using the Behavior Sequence Transformer (BST) model on ML-1M dataset in PyTorch Lightning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T025247_BST_using_Deepctr_library.html">
     BST using Deepctr library
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T757997_SASRec_PyTorch.html">
     SASRec implementation with PyTorch Library
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T225287_SASRec_PaddlePaddle.html">
     SASRec implementation with Paddle Library
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T701627_SR_SAN_Session_based_Model.html">
     SR-SAN Session-based Recommender
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T975104_SSEPT_ML1M_Tensorflow1x.html">
     SSE-PT Personalized Transformer Recommender on ML-1M in Tensorflow 1.x
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T472955_GCSAN_Session_based_Model.html">
     GCSAN Session-based Recommender
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T970274_Transformers4Rec_Session_based_Recommender_on_Yoochoose.html">
     End-to-end session-based recommendation with Transformers4Rec
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T382183_Transformers4Rec_XLNet_on_Synthetic_data.html">
     Transformers4Rec XLNet on Synthetic data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T793395_Session_based_recommendation_on_REES46_Dataset.html">
     End-to-end Session-based recommendation on REES46 Dataset
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Prototypes
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="T382881_DeepWalk_Karateclub.html">
   DeepWalk from scratch referencing Karateclub library
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T384270_DeepWalk_pure_python.html">
   DeepWalk in pure python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T677598_Jaccard_Cosine_SVD_DeepWalk_ML100K.html">
   Recommender System with DeepWalk Graph Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T815556_Node2vec_Karateclub.html">
   Node2vec from scratch referencing Karateclub library
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T894941_Node2vec_MovieLens_Keras.html">
   Graph representation learning with node2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T611050_Node2vec_PyG.html">
   Node2vec from scratch in PyTorch Geometric
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T186367_Node2vec_library.html">
   Node2vec from scratch referencing node2vec library
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T331379_bayesian_personalized_ranking.html">
   BPR from scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T081831_Data_Poisoning_Attacks_on_Factorization_Based_Collaborative_Filtering.html">
   Data Poisoning Attacks on Factorization-Based Collaborative Filtering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T102448_Adversarial_Learning_for_Recommendation.html">
   Adversarial Training (Regularization) on a Recommender System
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T865035_Simulating_Data_Poisoning_Attacks_against_Twitter_Recommender.html">
   Load and process dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T711285_Data_Poisoning_Attack_using_LFM_and_ItemAE_on_Synthetic_Dataset.html">
   Injection attack using LFM and ItemAE model trained on Toy dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T355514_Black_box_Attack_on_Sequential_Recs.html">
   Black-box Attack on Sequential Recs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T873451_Statistics_fundamentals.html">
   Statictics Fundamentals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T890478_Batch_Learning_from_Bandit_Feedback_%28BLBF%29.html">
   Imports
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T257798_Off_Policy_Learning_in_Two_stage_Recommender_Systems.html">
   Off-Policy Learning in Two-stage Recommender Systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T471827_Adaptive_Estimator_Selection_for_Off_Policy_Evaluation.html">
   Adaptive Estimator Selection for Off-Policy Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T902666_Evaluating_the_Robustness_of_Off_Policy_Evaluation.html">
   Evaluating the Robustness of Off-Policy Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T705904_Evaluation_of_Multiple_Off_Policy_Estimators_on_Synthetic_Dataset.html">
   Evaluation of Multiple Off-Policy Estimators on Synthetic Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T874693_Evaluating_Standard_Off_Policy_Estimators_with_Small_Sample_Open_Bandit_Dataset.html">
   Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T792262_Optimal_Off_Policy_Evaluation_from_Multiple_Logging_Policies.html">
   Optimal Off-Policy Evaluation from Multiple Logging Policies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T167249_Offline_Policy_Evaluation_with_VW_Command_Line.html">
   Offline Policy Evaluation with VW Command Line
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T966055_OBP_Library_Workshop_Tutorials.html">
   OBP Library Workshop Tutorials
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   PyTorch Fundamentals Part 1
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T472467_PyTorch_Fundamentals_Part_2.html">
   PyTorch Fundamentals Part 2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T206654_PyTorch_Fundamentals_Part_3.html">
   PyTorch Fundamentals Part 3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T536348_Attention_Mechanisms.html">
   Imports
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T500796_Agricultural_Satellite_Image_Segmentation.html">
   Agricultural Satellite Image Segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T611432_Image_Analysis_with_Tensorflow.html">
   Image Analysis with Tensorflow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T925716_MongoDB_to_CSV_Conversion.html">
   MongoDB to CSV conversion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T396469_PDF_to_Word_Cloud_via_Email.html">
   PDF to WordCloud via Email
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T030890_Job_Scraping_and_Clustering.html">
   Job scraping and clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T897054_Scene_Text_Recognition.html">
   Scene Text Recognition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T034809_Large_scale_Document_Retrieval_with_Elastic_Search.html">
   Large-scale Document Retrieval with ElasticSearch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T467251_vowpal_wabbit_contextual_recommender.html">
   Simulating a news personalization scenario using Contextual Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T686684_similar_product_recommender.html">
   Similar Product Recommender system using Deep Learning for an online e-commerce store
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T132203_Retail_Product_Recommendations_using_Word2vec.html">
   Retail Product Recommendations using word2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T501828_Recommender_Implicit_Negative_Feedback.html">
   Retail Product Recommendation with Negative Implicit Feedback
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T315965_Sequence_Aware_Recommenders_Music.html">
   Sequence Aware Recommender Systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T051777_image_similarity_recommendations.html">
   Similar Product Recommendations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990172_recobook_diversity_aware_book_recommender.html">
   Diversity Aware Book Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T488549_Goodreads_Diversity_Aware_Book_Recommender.html">
   Diversity Aware Book Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T023535_Kafka_MongoDB_Real_time_Streaming.html">
   Kafka MongoDB Real-time Streaming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T622304_Session_based_Recommender_Using_Word2vec.html">
   Session-based recommendation using word2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T416854_bandit_based_recommender_using_thompson_sampling_app.html">
   Bandit-based Online Learning using Thompson Sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T198578_Booking_dot_com_Trip_Recommendation.html">
   Booking.com Trip Recommendation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T519734_Vowpal_Wabbit_Contextual_Bandit.html">
   Vowpal Wabbit Contextual Bandit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T871537_Recommendation_Systems_using_Olist_Dataset.html">
   Recommendation systems using Olist dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T057885_Offline_Replayer_Evaluation.html">
   Offline Replayer Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T915054_method_for_effective_online_testing.html">
   Methods for effective online testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T513987_Recsys_2020_Feature_Engineering_Tutorial.html">
   Recsys’20 Feature Engineering Tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T227901_amazon_personalize_batch_job.html">
   Amazon Personalize Batch Job
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T022961_Amazon_Personalize_Workshop.html">
   Amazon Personalize Workshop
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T424437_Collaborative_Filtering_on_ML_latest_small.html">
   Collaborative Filtering on ML-latest-small
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T539160_Building_and_Deploying_ASOS_Fashion_Recommender.html">
   Building and deploying ASOS fashion recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T757697_Simple_Similarity_based_Recommender.html">
   Simple Similarity based Recommmendations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T051594_Analytics_Zoo.html">
   Analytics Zoo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T313645_A_B_Testing.html">
   A/B Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T475711_PinSage_Graph_based_Recommender.html">
   PinSage Graph-based Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T516490_Graph_Embeddings.html">
   Learn Embeddings using Graph Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T822164_movielens_milvus_redis_efficient_retrieval.html">
   Recommender with Redis and Milvus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T845186_Anime_Recommender.html">
   RekoNet Anime Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T855843_kafka_spark_streaming_colab.html">
   Kafka and Spark Streaming in Colab
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T460437_Building_Models_From_Scratch.html">
   Building Models from scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T855971_Conet_Model_for_Movie_Recommender.html">
   CoNet model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T996996_content_based_and_collaborative_movielens.html">
   Movie Recommendation with Content-Based and Collaborative Filtering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T239418_Simple_Movie_Recommenders.html">
   Simple Movie Recommenders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T138337_Simple_Movie_Recommender.html">
   Simple movie recommender in implicit, explicit, and cold-start settings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T935440_The_importance_of_Rating_Normalization.html">
   The importance of Rating Normalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T612622_cornac_examples.html">
   Cornac Examples
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T561435_Flower_Classification.html">
   Flower classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T680910_Trivago_Session_based_Recommender.html">
   Trivago Session-based Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_ads_selection_using_bandits.html">
   Best Ads detection using bandit methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_book_crossing_surprise_svd_nmf.html">
   Book-Crossing Recommendation System
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_book_recommender_kubeflow.html">
   Books recommendations with Kubeflow Pipelines on Scaleway Kapsule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_build_a_kubeflow_pipeline.html">
   Build a Kubeflow Pipeline
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_causal_inference.html">
   Causal Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_concept_embedding_nlp.html">
   Exploring Word Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_concept_neural_net.html">
   Neural Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_concept_nlp_basics.html">
   Natural Language Processing 101
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_concept_transformer_lm.html">
   TransformerLM Quick Start and Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_content_based_music_recommender_lyricsfreak.html">
   Content-based method for song recommendation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_evaluation_metrics_basics.html">
   Recommender System Evaluations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_implicit_synthetic.html">
   Comparing Implicit Models on Synthetic Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_movie_recommender_tensorflow.html">
   Recommendation Systems with TensorFlow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_movie_recommender_tensorflow_sagemaker.html">
   Movie recommender using Tensorflow in Sagemaker
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_movielens_eda_modeling.html">
   Movielens EDA and Modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_read_data_from_cassandra_into_pandas.html">
   Read Cassandra Data Snapshot as DataFrame
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_rl_in_action.html">
   Reinforcement Learning fundamentals in action
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_rnn_cnn_basics.html">
   Processing sequences using RNNs and CNNs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_tf_serving_in_action.html">
   TF Serving in action
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_training_indexing_movie_recommender.html">
   Training and indexing movie recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T207114_EduRec_MOOCCube_Course_Recommender.html">
   EduRec MOOCCube Course Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T273184_Multi_Task_Learning.html">
   Multi-task Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T661108_Book_Recommender_API.html">
   Book Recommender API
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T912764_Simple_Movie_Recommender_App.html">
   Simple Movie Recommender App
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T964554_Career_Village_Questions_Recommendation.html">
   CareerVillage Questions Recommendation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_amazon_women_apparel_tfidf_word2vec.html">
   Amazon Product Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_anime_recommender_graph_network.html">
   Anime Recommender with Bi-partite Graph Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_concept_self_attention.html">
   Self-Attention
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_course_recommender_svd_flask.html">
   Course Recommender with SVD based similarity
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_data_mining_similarity_measures.html">
   Concept - Data Mining Similarity Measures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_jaccard_recommender.html">
   Jaccard Similarity based Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_live_streamer_recommender.html">
   Live Streamer Recommender with Implicit feedback
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_songs_embedding_skipgram_recommender.html">
   Song Embeddings - Skipgram Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_toy_example_car_recommender_knn.html">
   Toy example - Car Recommender using KNN method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_wikirecs_recommender.html">
   WikiRecs
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/nbs/T632722_PyTorch_Fundamentals_Part_1.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sparsh-ai/reco-book/main?urlpath=tree/nbs/T632722_PyTorch_Fundamentals_Part_1.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-hot-problem">
   The Hot Problem
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset-classes">
   Dataset Classes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-pytorch-nn-module-torch-nn">
   The PyTorch NN Module (torch.nn)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-perceptron-the-simplest-neural-network">
   The Perceptron: The Simplest Neural Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-multilayer-perceptron">
   The Multilayer Perceptron
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#image-neural-model">
   Image Neural Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression">
   Linear Regression
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><a href="https://colab.research.google.com/github/sparsh-ai/reco-book/blob/stage/nbs/T632722_PyTorch_Fundamentals_Part_1.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<div class="section" id="pytorch-fundamentals-part-1">
<h1>PyTorch Fundamentals Part 1<a class="headerlink" href="#pytorch-fundamentals-part-1" title="Permalink to this headline">¶</a></h1>
<div class="section" id="the-hot-problem">
<h2>The Hot Problem<a class="headerlink" href="#the-hot-problem" title="Permalink to this headline">¶</a></h2>
<img src='https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781617295263/files/OEBPS/Images/CH05_F02_Stevens2_GS.png'><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">torch</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">edgeitems</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">75</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We just got back from a trip to some obscure location, and we brought back a fancy, wall-mounted analog thermometer. It looks great, and it’s a perfect fit for our living room. Its only flaw is that it doesn’t show units. Not to worry, we’ve got a plan: we’ll build a dataset of readings and corresponding temperature values in our favorite units, choose a model, adjust its weights iteratively until a measure of the error is low enough, and finally be able to interpret the new readings in units we understand.</p>
<p>We’ll start by making a note of temperature data in good old Celsius and measurements from our new thermometer, and figure things out. After a couple of weeks, here’s the data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t_c</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span>  <span class="mf">14.0</span><span class="p">,</span> <span class="mf">15.0</span><span class="p">,</span> <span class="mf">28.0</span><span class="p">,</span> <span class="mf">11.0</span><span class="p">,</span>  <span class="mf">8.0</span><span class="p">,</span>  <span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.0</span><span class="p">,</span>  <span class="mf">6.0</span><span class="p">,</span> <span class="mf">13.0</span><span class="p">,</span> <span class="mf">21.0</span><span class="p">]</span>
<span class="n">t_u</span> <span class="o">=</span> <span class="p">[</span><span class="mf">35.7</span><span class="p">,</span> <span class="mf">55.9</span><span class="p">,</span> <span class="mf">58.2</span><span class="p">,</span> <span class="mf">81.9</span><span class="p">,</span> <span class="mf">56.3</span><span class="p">,</span> <span class="mf">48.9</span><span class="p">,</span> <span class="mf">33.9</span><span class="p">,</span> <span class="mf">21.8</span><span class="p">,</span> <span class="mf">48.4</span><span class="p">,</span> <span class="mf">60.4</span><span class="p">,</span> <span class="mf">68.4</span><span class="p">]</span>
<span class="n">t_c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">t_c</span><span class="p">)</span>
<span class="n">t_u</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">t_u</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here, the t_c values are temperatures in Celsius, and the t_u values are our unknown units. We can expect noise in both measurements, coming from the devices themselves and from our approximate readings. For convenience, we’ve already put the data into tensors; we’ll use it in a minute.</p>
<p>A quick plot of our data tells us that it’s noisy, but we think there’s a pattern here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">dpi</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Measurement&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Temperature (°Celsius)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_u</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">t_c</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_9_0.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_9_0.png" />
</div>
</div>
<p>In the absence of further knowledge, we assume the simplest possible model for converting between the two sets of measurements, just like Kepler might have done. The two may be linearly related–that is, multiplying t_u by a factor and adding a constant, we may get the temperature in Celsius (up to an error that we omit):</p>
<p>Is this a reasonable assumption? Probably; we’ll see how well the final model performs. We chose to name w and b after weight and bias, two very common terms for linear scaling and the additive constant–we’ll bump into those all the time.</p>
<p>OK, now we need to estimate w and b, the parameters in our model, based on the data we have. We must do it so that temperatures we obtain from running the unknown temperatures t_u through the model are close to temperatures we actually measured in Celsius. If that sounds like fitting a line through a set of measurements, well, yes, because that’s exactly what we’re doing. We’ll go through this simple example using PyTorch and realize that training a neural network will essentially involve changing the model for a slightly more elaborate one, with a few (or a metric ton) more parameters.</p>
<p>Let’s flesh it out again: we have a model with some unknown parameters, and we need to estimate those parameters so that the error between predicted outputs and measured values is as low as possible. We notice that we still need to exactly define a measure of the error. Such a measure, which we refer to as the loss function, should be high if the error is high and should ideally be as low as possible for a perfect match. Our optimization process should therefore aim at finding w and b so that the loss function is at a minimum.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w</span> <span class="o">*</span> <span class="n">t_u</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
</div>
</div>
<p>A loss function (or cost function) is a function that computes a single numerical value that the learning process will attempt to minimize. The calculation of loss typically involves taking the difference between the desired outputs for some training samples and the outputs actually produced by the model when fed those samples. In our case, that would be the difference between the predicted temperatures t_p output by our model and the actual measurements: t_p - t_c.</p>
<p>We need to make sure the loss function makes the loss positive both when t_p is greater than and when it is less than the true t_c, since the goal is for t_p to match t_c. We have a few choices, the most straightforward being |t_p - t_c| and (t_p - t_c)^2. Based on the mathematical expression we choose, we can emphasize or discount certain errors. Conceptually, a loss function is a way of prioritizing which errors to fix from our training samples, so that our parameter updates result in adjustments to the outputs for the highly weighted samples instead of changes to some other samples’ output that had a smaller loss.</p>
<p>Both of the example loss functions have a clear minimum at zero and grow monotonically as the predicted value moves further from the true value in either direction. Because the steepness of the growth also monotonically increases away from the minimum, both of them are said to be convex. Since our model is linear, the loss as a function of w and b is also convex. Cases where the loss is a convex function of the model parameters are usually great to deal with because we can find a minimum very efficiently through specialized algorithms. However, we will instead use less powerful but more generally applicable methods in this chapter. We do so because for the deep neural networks we are ultimately interested in, the loss is not a convex function of the inputs.</p>
<p>For our two loss functions |t_p - t_c| and (t_p - t_c)^2, we notice that the square of the differences behaves more nicely around the minimum: the derivative of the error-squared loss with respect to t_p is zero when t_p equals t_c. The absolute value, on the other hand, has an undefined derivative right where we’d like to converge. This is less of an issue in practice than it looks like, but we’ll stick to the square of differences for the time being.</p>
<img src='https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781617295263/files/OEBPS/Images/CH05_F04_Stevens2_GS.png'><p>We’re expecting t_u, w, and b to be the input tensor, weight parameter, and bias parameter, respectively. In our model, the parameters will be PyTorch scalars (aka zero-dimensional tensors), and the product operation will use broadcasting to yield the returned tensors. Anyway, time to define our loss:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">t_p</span><span class="p">,</span> <span class="n">t_c</span><span class="p">):</span>
    <span class="n">squared_diffs</span> <span class="o">=</span> <span class="p">(</span><span class="n">t_p</span> <span class="o">-</span> <span class="n">t_c</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">squared_diffs</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Note that we are building a tensor of differences, taking their square element-wise, and finally producing a scalar loss function by averaging all of the elements in the resulting tensor. It is a mean square loss.</p>
<p>We can now initialize the parameters, invoke the model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(())</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(())</span>

<span class="n">t_p</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">t_p</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000,
        21.8000, 48.4000, 60.4000, 68.4000])
</pre></div>
</div>
</div>
</div>
<p>and check the value of the loss:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">t_p</span><span class="p">,</span> <span class="n">t_c</span><span class="p">)</span>
<span class="n">loss</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1763.8848)
</pre></div>
</div>
</div>
</div>
<p>We implemented the model and the loss in this section. We’ve finally reached the meat of the example: how do we estimate w and b such that the loss reaches a minimum? We’ll first work things out by hand and then learn how to use PyTorch’s superpowers to solve the same problem in a more general, off-the-shelf way.</p>
<p>We’ll optimize the loss function with respect to the parameters using the gradient descent algorithm. Gradient descent is actually a very simple idea, and it scales up surprisingly well to large neural network models with millions of parameters. Let’s start with a mental image. Suppose we are in front of a machine sporting two knobs, labeled w and b. We are allowed to see the value of the loss on a screen, and we are told to minimize that value. Not knowing the effect of the knobs on the loss, we start fiddling with them and decide for each knob which direction makes the loss decrease. We decide to rotate both knobs in their direction of decreasing loss. Suppose we’re far from the optimal value: we’d likely see the loss decrease quickly and then slow down as it gets closer to the minimum. We notice that at some point, the loss climbs back up again, so we invert the direction of rotation for one or both knobs. We also learn that when the loss changes slowly, it’s a good idea to adjust the knobs more finely, to avoid reaching the point where the loss goes back up. After a while, eventually, we converge to a minimum.</p>
<img src='https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781617295263/files/OEBPS/Images/CH05_F06_Stevens2_GS.png'><p>Gradient descent is not that different from the scenario we just described. The idea is to compute the rate of change of the loss with respect to each parameter, and modify each parameter in the direction of decreasing loss. Just like when we were fiddling with the knobs, we can estimate the rate of change by adding a small number to w and b and seeing how much the loss changes in that neighborhood:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">delta</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">loss_rate_of_change_w</span> <span class="o">=</span> \
    <span class="p">(</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="n">w</span> <span class="o">+</span> <span class="n">delta</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">t_c</span><span class="p">)</span> <span class="o">-</span> 
     <span class="n">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="n">w</span> <span class="o">-</span> <span class="n">delta</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">t_c</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">delta</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This is saying that in the neighborhood of the current values of w and b, a unit increase in w leads to some change in the loss. If the change is negative, then we need to increase w to minimize the loss, whereas if the change is positive, we need to decrease w. By how much? Applying a change to w that is proportional to the rate of change of the loss is a good idea, especially when the loss has several parameters: we apply a change to those that exert a significant change on the loss. It is also wise to change the parameters slowly in general, because the rate of change could be dramatically different at a distance from the neighborhood of the current w value. Therefore, we typically should scale the rate of change by a small factor. This scaling factor has many names; the one we use in machine learning is learning_rate:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span>
 
<span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">loss_rate_of_change_w</span>
</pre></div>
</div>
</div>
</div>
<p>We can do the same with b:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loss_rate_of_change_b</span> <span class="o">=</span> \
    <span class="p">(</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">+</span> <span class="n">delta</span><span class="p">),</span> <span class="n">t_c</span><span class="p">)</span> <span class="o">-</span>
     <span class="n">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">-</span> <span class="n">delta</span><span class="p">),</span> <span class="n">t_c</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">delta</span><span class="p">)</span>
 
<span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">loss_rate_of_change_b</span>
</pre></div>
</div>
</div>
</div>
<p>This represents the basic parameter-update step for gradient descent. By reiterating these evaluations (and provided we choose a small enough learning rate), we will converge to an optimal value of the parameters for which the loss computed on the given data is minimal. We’ll show the complete iterative process soon, but the way we just computed our rates of change is rather crude and needs an upgrade before we move on. Let’s see why and how.</p>
<p>Computing the rate of change by using repeated evaluations of the model and loss in order to probe the behavior of the loss function in the neighborhood of w and b doesn’t scale well to models with many parameters. Also, it is not always clear how large the neighborhood should be. We chose delta equal to 0.1 in the previous section, but it all depends on the shape of the loss as a function of w and b. If the loss changes too quickly compared to delta, we won’t have a very good idea of in which direction the loss is decreasing the most.</p>
<p>What if we could make the neighborhood infinitesimally small? That’s exactly what happens when we analytically take the derivative of the loss with respect to a parameter. In a model with two or more parameters like the one we’re dealing with, we compute the individual derivatives of the loss with respect to each parameter and put them in a vector of derivatives: the gradient.</p>
<p>In order to compute the derivative of the loss with respect to a parameter, we can apply the chain rule and compute the derivative of the loss with respect to its input (which is the output of the model), times the derivative of the model with respect to the parameter:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dloss_fn</span><span class="p">(</span><span class="n">t_p</span><span class="p">,</span> <span class="n">t_c</span><span class="p">):</span>
    <span class="n">dsq_diffs</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">t_p</span> <span class="o">-</span> <span class="n">t_c</span><span class="p">)</span> <span class="o">/</span> <span class="n">t_p</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dsq_diffs</span>

<span class="k">def</span> <span class="nf">dmodel_dw</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">t_u</span>
 
<span class="k">def</span> <span class="nf">dmodel_db</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.0</span>
</pre></div>
</div>
</div>
</div>
<img src='https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781617295263/files/OEBPS/Images/CH05_F07_Stevens2_GSII.png'><p>Putting all of this together, the function returning the gradient of the loss with respect to w and b is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">grad_fn</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="n">t_c</span><span class="p">,</span> <span class="n">t_p</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">dloss_dtp</span> <span class="o">=</span> <span class="n">dloss_fn</span><span class="p">(</span><span class="n">t_p</span><span class="p">,</span> <span class="n">t_c</span><span class="p">)</span>
    <span class="n">dloss_dw</span> <span class="o">=</span> <span class="n">dloss_dtp</span> <span class="o">*</span> <span class="n">dmodel_dw</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">dloss_db</span> <span class="o">=</span> <span class="n">dloss_dtp</span> <span class="o">*</span> <span class="n">dmodel_db</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">dloss_dw</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">dloss_db</span><span class="o">.</span><span class="n">sum</span><span class="p">()])</span>
</pre></div>
</div>
</div>
</div>
<p>We now have everything in place to optimize our parameters. Starting from a tentative value for a parameter, we can iteratively apply updates to it for a fixed number of iterations, or until w and b stop changing. There are several stopping criteria; for now, we’ll stick to a fixed number of iterations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_loop</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">t_u</span><span class="p">,</span> <span class="n">t_c</span><span class="p">,</span>
                  <span class="n">print_params</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">params</span>

        <span class="n">t_p</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">t_p</span><span class="p">,</span> <span class="n">t_c</span><span class="p">)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="n">t_c</span><span class="p">,</span> <span class="n">t_p</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

        <span class="n">params</span> <span class="o">=</span> <span class="n">params</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">99</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">4000</span><span class="p">,</span> <span class="mi">5000</span><span class="p">}:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch </span><span class="si">%d</span><span class="s1">, Loss </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">loss</span><span class="p">)))</span>
            <span class="k">if</span> <span class="n">print_params</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;    Params:&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;    Grad:  &#39;</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="p">{</span><span class="mi">4</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">101</span><span class="p">}:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;...&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
            <span class="k">break</span>
            
    <span class="k">return</span> <span class="n">params</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_loop</span><span class="p">(</span>
    <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> 
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span><span class="p">,</span> 
    <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]),</span> 
    <span class="n">t_u</span> <span class="o">=</span> <span class="n">t_u</span><span class="p">,</span> 
    <span class="n">t_c</span> <span class="o">=</span> <span class="n">t_c</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1, Loss 1763.884766
    Params: tensor([-44.1730,  -0.8260])
    Grad:   tensor([4517.2964,   82.6000])
Epoch 2, Loss 5802484.500000
    Params: tensor([2568.4011,   45.1637])
    Grad:   tensor([-261257.4062,   -4598.9702])
Epoch 3, Loss 19408029696.000000
    Params: tensor([-148527.7344,   -2616.3931])
    Grad:   tensor([15109614.0000,   266155.6875])
...
Epoch 10, Loss 90901105189019073810297959556841472.000000
    Params: tensor([3.2144e+17, 5.6621e+15])
    Grad:   tensor([-3.2700e+19, -5.7600e+17])
Epoch 11, Loss inf
    Params: tensor([-1.8590e+19, -3.2746e+17])
    Grad:   tensor([1.8912e+21, 3.3313e+19])
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-1.8590e+19, -3.2746e+17])
</pre></div>
</div>
</div>
</div>
<p>Wait, what happened? Our training process literally blew up, leading to losses becoming inf. This is a clear sign that params is receiving updates that are too large, and their values start oscillating back and forth as each update overshoots and the next overcorrects even more. The optimization process is unstable: it diverges instead of converging to a minimum. We want to see smaller and smaller updates to params, not larger.</p>
<p>How can we limit the magnitude of learning_rate * grad? Well, that looks easy. We could simply choose a smaller learning_rate, and indeed, the learning rate is one of the things we typically change when training does not go as well as we would like. We usually change learning rates by orders of magnitude, so we might try with 1e-3 or 1e-4, which would decrease the magnitude of the updates by orders of magnitude. Let’s go with 1e-4 and see how it works out:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_loop</span><span class="p">(</span>
    <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]),</span>
    <span class="n">t_u</span> <span class="o">=</span> <span class="n">t_u</span><span class="p">,</span>
    <span class="n">t_c</span> <span class="o">=</span> <span class="n">t_c</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1, Loss 1763.884766
    Params: tensor([ 0.5483, -0.0083])
    Grad:   tensor([4517.2964,   82.6000])
Epoch 2, Loss 323.090515
    Params: tensor([ 0.3623, -0.0118])
    Grad:   tensor([1859.5493,   35.7843])
Epoch 3, Loss 78.929634
    Params: tensor([ 0.2858, -0.0135])
    Grad:   tensor([765.4666,  16.5122])
...
Epoch 10, Loss 29.105247
    Params: tensor([ 0.2324, -0.0166])
    Grad:   tensor([1.4803, 3.0544])
Epoch 11, Loss 29.104168
    Params: tensor([ 0.2323, -0.0169])
    Grad:   tensor([0.5781, 3.0384])
...
Epoch 99, Loss 29.023582
    Params: tensor([ 0.2327, -0.0435])
    Grad:   tensor([-0.0533,  3.0226])
Epoch 100, Loss 29.022667
    Params: tensor([ 0.2327, -0.0438])
    Grad:   tensor([-0.0532,  3.0226])
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0.2327, -0.0438])
</pre></div>
</div>
</div>
</div>
<p>Nice–the behavior is now stable. But there’s another problem: the updates to parameters are very small, so the loss decreases very slowly and eventually stalls. We could obviate this issue by making learning_rate adaptive: that is, change according to the magnitude of updates.</p>
<p>However, there’s another potential troublemaker in the update term: the gradient itself. Let’s go back and look at grad at epoch 1 during optimization.</p>
<p>We can see that the first-epoch gradient for the weight is about 50 times larger than the gradient for the bias. This means the weight and bias live in differently scaled spaces. If this is the case, a learning rate that’s large enough to meaningfully update one will be so large as to be unstable for the other; and a rate that’s appropriate for the other won’t be large enough to meaningfully change the first. That means we’re not going to be able to update our parameters unless we change something about our formulation of the problem. We could have individual learning rates for each parameter, but for models with many parameters, this would be too much to bother with; it’s babysitting of the kind we don’t like.</p>
<p>There’s a simpler way to keep things in check: changing the inputs so that the gradients aren’t quite so different. We can make sure the range of the input doesn’t get too far from the range of -1.0 to 1.0, roughly speaking. In our case, we can achieve something close enough to that by simply multiplying t_u by 0.1:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t_un</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">t_u</span>
</pre></div>
</div>
</div>
</div>
<p>Here, we denote the normalized version of t_u by appending an n to the variable name. At this point, we can run the training loop on our normalized input:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_loop</span><span class="p">(</span>
    <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span><span class="p">,</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]),</span>
    <span class="n">t_u</span> <span class="o">=</span> <span class="n">t_un</span><span class="p">,</span>
    <span class="n">t_c</span> <span class="o">=</span> <span class="n">t_c</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1, Loss 80.364342
    Params: tensor([1.7761, 0.1064])
    Grad:   tensor([-77.6140, -10.6400])
Epoch 2, Loss 37.574913
    Params: tensor([2.0848, 0.1303])
    Grad:   tensor([-30.8623,  -2.3864])
Epoch 3, Loss 30.871077
    Params: tensor([2.2094, 0.1217])
    Grad:   tensor([-12.4631,   0.8587])
...
Epoch 10, Loss 29.030489
    Params: tensor([ 2.3232, -0.0710])
    Grad:   tensor([-0.5355,  2.9295])
Epoch 11, Loss 28.941877
    Params: tensor([ 2.3284, -0.1003])
    Grad:   tensor([-0.5240,  2.9264])
...
Epoch 99, Loss 22.214186
    Params: tensor([ 2.7508, -2.4910])
    Grad:   tensor([-0.4453,  2.5208])
Epoch 100, Loss 22.148710
    Params: tensor([ 2.7553, -2.5162])
    Grad:   tensor([-0.4446,  2.5165])
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 2.7553, -2.5162])
</pre></div>
</div>
</div>
</div>
<p>Let’s run the loop for enough iterations to see the changes in params get small. We’ll change n_epochs to 5,000:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">training_loop</span><span class="p">(</span>
    <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">,</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span><span class="p">,</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]),</span>
    <span class="n">t_u</span> <span class="o">=</span> <span class="n">t_un</span><span class="p">,</span>
    <span class="n">t_c</span> <span class="o">=</span> <span class="n">t_c</span><span class="p">,</span>
    <span class="n">print_params</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1, Loss 80.364342
Epoch 2, Loss 37.574913
Epoch 3, Loss 30.871077
...
Epoch 10, Loss 29.030489
Epoch 11, Loss 28.941877
...
Epoch 99, Loss 22.214186
Epoch 100, Loss 22.148710
...
Epoch 4000, Loss 2.927680
Epoch 5000, Loss 2.927648
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;W=</span><span class="si">{}</span><span class="s1"> and b=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W=5.367083549499512 and b=-17.301189422607422
</pre></div>
</div>
</div>
</div>
<p>Good: our loss decreases while we change parameters along the direction of gradient descent. It doesn’t go exactly to zero; this could mean there aren’t enough iterations to converge to zero, or that the data points don’t sit exactly on a line. As we anticipated, our measurements were not perfectly accurate, or there was noise involved in the reading.</p>
<p>But look: the values for w and b look an awful lot like the numbers we need to use to convert Celsius to Fahrenheit (after accounting for our earlier normalization when we multiplied our inputs by 0.1). The exact values would be w=5.5556 and b=-17.7778. Our fancy thermometer was showing temperatures in Fahrenheit the whole time. No big discovery, except that our gradient descent optimization process works!</p>
<p>Let’s revisit something we did right at the start: plotting our data. Seriously, this is the first thing anyone doing data science should do. Always plot the heck out of the data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t_p</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">t_un</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">dpi</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Temperature (°Fahrenheit)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Temperature (°Celsius)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_u</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">t_p</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_u</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">t_c</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_53_0.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_53_0.png" />
</div>
</div>
<p>In our little adventure, we just saw a simple example of backpropagation: we computed the gradient of a composition of functions–the model and the loss–with respect to their innermost parameters (w and b) by propagating derivatives backward using the chain rule. The basic requirement here is that all functions we’re dealing with can be differentiated analytically. If this is the case, we can compute the gradient–what we earlier called “the rate of change of the loss”–with respect to the parameters in one sweep.</p>
<p>Even if we have a complicated model with millions of parameters, as long as our model is differentiable, computing the gradient of the loss with respect to the parameters amounts to writing the analytical expression for the derivatives and evaluating them once. Granted, writing the analytical expression for the derivatives of a very deep composition of linear and nonlinear functions is not a lot of fun.9 It isn’t particularly quick, either.</p>
<p>This is when PyTorch tensors come to the rescue, with a PyTorch component called autograd. PyTorch tensors can remember where they come from, in terms of the operations and parent tensors that originated them, and they can automatically provide the chain of derivatives of such operations with respect to their inputs. This means we won’t need to derive our model by hand; given a forward expression, no matter how nested, PyTorch will automatically provide the gradient of that expression with respect to its input parameters.</p>
<p>At this point, the best way to proceed is to rewrite our thermometer calibration code, this time using autograd, and see what happens. First, we recall our model and loss function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t_c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">14.0</span><span class="p">,</span> <span class="mf">15.0</span><span class="p">,</span> <span class="mf">28.0</span><span class="p">,</span> <span class="mf">11.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span>
                    <span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">13.0</span><span class="p">,</span> <span class="mf">21.0</span><span class="p">])</span>
<span class="n">t_u</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">35.7</span><span class="p">,</span> <span class="mf">55.9</span><span class="p">,</span> <span class="mf">58.2</span><span class="p">,</span> <span class="mf">81.9</span><span class="p">,</span> <span class="mf">56.3</span><span class="p">,</span> <span class="mf">48.9</span><span class="p">,</span>
                    <span class="mf">33.9</span><span class="p">,</span> <span class="mf">21.8</span><span class="p">,</span> <span class="mf">48.4</span><span class="p">,</span> <span class="mf">60.4</span><span class="p">,</span> <span class="mf">68.4</span><span class="p">])</span>

<span class="n">t_un</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">t_u</span>

<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w</span> <span class="o">*</span> <span class="n">t_u</span> <span class="o">+</span> <span class="n">b</span>
    
<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">t_p</span><span class="p">,</span> <span class="n">t_c</span><span class="p">):</span>
    <span class="n">squared_diffs</span> <span class="o">=</span> <span class="p">(</span><span class="n">t_p</span> <span class="o">-</span> <span class="n">t_c</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">squared_diffs</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s again initialize a parameters tensor:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">params</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
<p>All we have to do to populate it is to start with a tensor with requires_grad set to True, then call the model and compute the loss, and then call backward on the loss tensor:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">),</span> <span class="n">t_c</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="n">params</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([4517.2969,   82.6000])
</pre></div>
</div>
</div>
</div>
<p>When we compute our loss while the parameters w and b require gradients, in addition to performing the actual computation, PyTorch creates the autograd graph with the operations (in black circles) as nodes, as shown in the top row of fig-ure 5.10. When we call loss.backward(), PyTorch traverses this graph in the reverse direction to compute the gradients, as shown by the arrows in the bottom row of the figure.</p>
<img src='https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781617295263/files/OEBPS/Images/CH05_F11_Stevens2_GS.png'><p>Calling backward will lead derivatives to accumulate at leaf nodes. We need to zero the gradient explicitly after using it for parameter updates. In order to prevent this from occurring, we need to zero the gradient explicitly at each iteration. We can do this easily using the in-place zero_ method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">params</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

<span class="n">params</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0., 0.])
</pre></div>
</div>
</div>
</div>
<p>Having this reminder drilled into our heads, let’s see what our autograd-enabled training code looks like, start to finish:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_loop</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">t_u</span><span class="p">,</span> <span class="n">t_c</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">params</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        
        <span class="n">t_p</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span> 
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">t_p</span><span class="p">,</span> <span class="n">t_c</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">params</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">params</span><span class="o">.</span><span class="n">grad</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch </span><span class="si">%d</span><span class="s1">, Loss </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">loss</span><span class="p">)))</span>
            
    <span class="k">return</span> <span class="n">params</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_loop</span><span class="p">(</span>
    <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">,</span> 
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span><span class="p">,</span> 
    <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">t_u</span> <span class="o">=</span> <span class="n">t_un</span><span class="p">,</span>
    <span class="n">t_c</span> <span class="o">=</span> <span class="n">t_c</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 500, Loss 7.860115
Epoch 1000, Loss 3.828538
Epoch 1500, Loss 3.092191
Epoch 2000, Loss 2.957698
Epoch 2500, Loss 2.933134
Epoch 3000, Loss 2.928648
Epoch 3500, Loss 2.927830
Epoch 4000, Loss 2.927679
Epoch 4500, Loss 2.927652
Epoch 5000, Loss 2.927647
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([  5.3671, -17.3012], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>The result is the same as we got previously. Good for us! It means that while we are capable of computing derivatives by hand, we no longer need to.</p>
<p>Earlier, we used vanilla gradient descent for optimization, which worked fine for our simple case. Needless to say, there are several optimization strategies and tricks that can assist convergence, especially when models get complicated. This saves us from the boilerplate busywork of having to update each and every parameter to our model ourselves. The torch module has an optim submodule where we can find classes implementing different optimization algorithms.</p>
<p>Each optimizer exposes two methods: zero_grad and step. zero_grad zeroes the grad attribute of all the parameters passed to the optimizer upon construction. step updates the value of those parameters according to the optimization strategy implemented by the specific optimizer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="nb">dir</span><span class="p">(</span><span class="n">optim</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;ASGD&#39;,
 &#39;Adadelta&#39;,
 &#39;Adagrad&#39;,
 &#39;Adam&#39;,
 &#39;AdamW&#39;,
 &#39;Adamax&#39;,
 &#39;LBFGS&#39;,
 &#39;Optimizer&#39;,
 &#39;RMSprop&#39;,
 &#39;Rprop&#39;,
 &#39;SGD&#39;,
 &#39;SparseAdam&#39;,
 &#39;__builtins__&#39;,
 &#39;__cached__&#39;,
 &#39;__doc__&#39;,
 &#39;__file__&#39;,
 &#39;__loader__&#39;,
 &#39;__name__&#39;,
 &#39;__package__&#39;,
 &#39;__path__&#39;,
 &#39;__spec__&#39;,
 &#39;_functional&#39;,
 &#39;_multi_tensor&#39;,
 &#39;lr_scheduler&#39;,
 &#39;swa_utils&#39;]
</pre></div>
</div>
</div>
</div>
<p>Let’s create params and instantiate a gradient descent optimizer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-5</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span><span class="n">params</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Anyway, let’s take our fancy new optimizer for a spin:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t_p</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">t_p</span><span class="p">,</span> <span class="n">t_c</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="n">params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>The value of params is updated upon calling step without us having to touch it ourselves! What happens is that the optimizer looks into params.grad and updates params, subtracting learning_rate times grad from it, exactly as in our former hand-rolled code.</p>
<p>Ready to stick this code in a training loop? Nope! The big gotcha almost got us–we forgot to zero out the gradients. Had we called the previous code in a loop, gradients would have accumulated in the leaves at every call to backward, and our gradient descent would have been all over the place! Here’s the loop-ready code, with the extra zero_grad at the correct spot (right before the call to backward):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span><span class="n">params</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="n">t_p</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">t_un</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">t_p</span><span class="p">,</span> <span class="n">t_c</span><span class="p">)</span>

<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="n">params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([1.7761, 0.1064], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>Perfect! See how the optim module helps us abstract away the specific optimization scheme? All we have to do is provide a list of params to it (that list can be extremely long, as is needed for very deep neural network models), and we can forget about the details.</p>
<p>Let’s update our training loop accordingly:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_loop</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">t_u</span><span class="p">,</span> <span class="n">t_c</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">t_p</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">t_u</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span> 
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">t_p</span><span class="p">,</span> <span class="n">t_c</span><span class="p">)</span>
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch </span><span class="si">%d</span><span class="s1">, Loss </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">loss</span><span class="p">)))</span>
            
    <span class="k">return</span> <span class="n">params</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span><span class="n">params</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="n">training_loop</span><span class="p">(</span>
    <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">,</span> 
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">params</span><span class="p">,</span>
    <span class="n">t_u</span> <span class="o">=</span> <span class="n">t_un</span><span class="p">,</span>
    <span class="n">t_c</span> <span class="o">=</span> <span class="n">t_c</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 500, Loss 7.860120
Epoch 1000, Loss 3.828538
Epoch 1500, Loss 3.092191
Epoch 2000, Loss 2.957698
Epoch 2500, Loss 2.933134
Epoch 3000, Loss 2.928648
Epoch 3500, Loss 2.927830
Epoch 4000, Loss 2.927679
Epoch 4500, Loss 2.927652
Epoch 5000, Loss 2.927647
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([  5.3671, -17.3012], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>Again, we get the same result as before. Great: this is further confirmation that we know how to descend a gradient by hand!</p>
<p>In order to test more optimizers, all we have to do is instantiate a different optimizer, say Adam, instead of SGD. The rest of the code stays as it is. Pretty handy stuff.</p>
<p>We won’t go into much detail about Adam; suffice to say that it is a more sophisticated optimizer in which the learning rate is set adaptively. In addition, it is a lot less sensitive to the scaling of the parameters–so insensitive that we can go back to using the original (non-normalized) input t_u, and even increase the learning rate to 1e-1, and Adam won’t even blink:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-1</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">params</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="n">training_loop</span><span class="p">(</span>
    <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span> 
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">params</span><span class="p">,</span>
    <span class="n">t_u</span> <span class="o">=</span> <span class="n">t_u</span><span class="p">,</span>
    <span class="n">t_c</span> <span class="o">=</span> <span class="n">t_c</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 500, Loss 7.612900
Epoch 1000, Loss 3.086700
Epoch 1500, Loss 2.928579
Epoch 2000, Loss 2.927644
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([  0.5367, -17.3021], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>Now, we will change another thing: the model architecture. Logically, linear model makes sense but we will anyway fit a non-linear neural net to see how things change. Recall that t_u and t_c were two 1D tensors of size B. Thanks to broadcasting, we could write our linear model as w * x + b, where w and b were two scalar parameters. This worked because we had a single input feature: if we had two, we would need to add an extra dimension to turn that 1D tensor into a matrix with samples in the rows and features in the columns.</p>
<p>That’s exactly what we need to do to switch to using nn.Linear. We reshape our B inputs to B × Nin, where Nin is 1. That is easily done with unsqueeze:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t_c</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span>  <span class="mf">14.0</span><span class="p">,</span> <span class="mf">15.0</span><span class="p">,</span> <span class="mf">28.0</span><span class="p">,</span> <span class="mf">11.0</span><span class="p">,</span>  <span class="mf">8.0</span><span class="p">,</span>  <span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.0</span><span class="p">,</span>  <span class="mf">6.0</span><span class="p">,</span> <span class="mf">13.0</span><span class="p">,</span> <span class="mf">21.0</span><span class="p">]</span>
<span class="n">t_u</span> <span class="o">=</span> <span class="p">[</span><span class="mf">35.7</span><span class="p">,</span> <span class="mf">55.9</span><span class="p">,</span> <span class="mf">58.2</span><span class="p">,</span> <span class="mf">81.9</span><span class="p">,</span> <span class="mf">56.3</span><span class="p">,</span> <span class="mf">48.9</span><span class="p">,</span> <span class="mf">33.9</span><span class="p">,</span> <span class="mf">21.8</span><span class="p">,</span> <span class="mf">48.4</span><span class="p">,</span> <span class="mf">60.4</span><span class="p">,</span> <span class="mf">68.4</span><span class="p">]</span>
<span class="n">t_c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">t_c</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">t_u</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">t_u</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">t_u</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([11, 1])
</pre></div>
</div>
</div>
</div>
<p>To avoid the overfitting we will do spliting and shuffling also:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n_samples</span> <span class="o">=</span> <span class="n">t_u</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">n_val</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span>

<span class="n">shuffled_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

<span class="n">train_indices</span> <span class="o">=</span> <span class="n">shuffled_indices</span><span class="p">[:</span><span class="o">-</span><span class="n">n_val</span><span class="p">]</span>
<span class="n">val_indices</span> <span class="o">=</span> <span class="n">shuffled_indices</span><span class="p">[</span><span class="o">-</span><span class="n">n_val</span><span class="p">:]</span>

<span class="n">train_indices</span><span class="p">,</span> <span class="n">val_indices</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([ 0,  5,  9,  7,  8, 10,  1,  3,  4]), tensor([2, 6]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t_u_train</span> <span class="o">=</span> <span class="n">t_u</span><span class="p">[</span><span class="n">train_indices</span><span class="p">]</span>
<span class="n">t_c_train</span> <span class="o">=</span> <span class="n">t_c</span><span class="p">[</span><span class="n">train_indices</span><span class="p">]</span>

<span class="n">t_u_val</span> <span class="o">=</span> <span class="n">t_u</span><span class="p">[</span><span class="n">val_indices</span><span class="p">]</span>
<span class="n">t_c_val</span> <span class="o">=</span> <span class="n">t_c</span><span class="p">[</span><span class="n">val_indices</span><span class="p">]</span>

<span class="n">t_un_train</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">t_u_train</span>
<span class="n">t_un_val</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">t_u_val</span>
</pre></div>
</div>
</div>
</div>
<p>We’re done; let’s update our training code. First, we replace our handmade model with nn.Linear(1,1), and then we need to pass the linear model parameters to the optimizer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">linear_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">linear_model</span><span class="p">(</span><span class="n">t_un_val</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1.7170],
        [1.1148]], grad_fn=&lt;AddmmBackward&gt;)
</pre></div>
</div>
</div>
</div>
<p>Earlier, it was our responsibility to create parameters and pass them as the first argument to optim.SGD. Now we can use the parameters method to ask any nn.Module for a list of parameters owned by it or any of its submodules:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">linear_model</span><span class="o">.</span><span class="n">weight</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([[0.2478]], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">linear_model</span><span class="o">.</span><span class="n">bias</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([0.2748], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>At this point, the SGD optimizer has everything it needs. When optimizer.step() is called, it will iterate through each Parameter and change it by an amount proportional to what is stored in its grad attribute. Pretty clean design.</p>
<p>Let’s take a look a the training loop now:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_loop</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">t_u_train</span><span class="p">,</span> <span class="n">t_u_val</span><span class="p">,</span>
                  <span class="n">t_c_train</span><span class="p">,</span> <span class="n">t_c_val</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">t_p_train</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">t_u_train</span><span class="p">)</span>
        <span class="n">loss_train</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">t_p_train</span><span class="p">,</span> <span class="n">t_c_train</span><span class="p">)</span>

        <span class="n">t_p_val</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">t_u_val</span><span class="p">)</span>
        <span class="n">loss_val</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">t_p_val</span><span class="p">,</span> <span class="n">t_c_val</span><span class="p">)</span>
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss_train</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, Training loss </span><span class="si">{</span><span class="n">loss_train</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">,&quot;</span>
                  <span class="sa">f</span><span class="s2">&quot; Validation loss </span><span class="si">{</span><span class="n">loss_val</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">t_p</span><span class="p">,</span> <span class="n">t_c</span><span class="p">):</span>
    <span class="n">squared_diffs</span> <span class="o">=</span> <span class="p">(</span><span class="n">t_p</span> <span class="o">-</span> <span class="n">t_c</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">squared_diffs</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">linear_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">linear_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>

<span class="n">training_loop</span><span class="p">(</span>
    <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">3000</span><span class="p">,</span> 
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="p">,</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">,</span>
    <span class="n">t_u_train</span> <span class="o">=</span> <span class="n">t_un_train</span><span class="p">,</span>
    <span class="n">t_u_val</span> <span class="o">=</span> <span class="n">t_un_val</span><span class="p">,</span> 
    <span class="n">t_c_train</span> <span class="o">=</span> <span class="n">t_c_train</span><span class="p">,</span>
    <span class="n">t_c_val</span> <span class="o">=</span> <span class="n">t_c_val</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">linear_model</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">linear_model</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1, Training loss 397.8210, Validation loss 253.9088
Epoch 1000, Training loss 3.7300, Validation loss 1.8505
Epoch 2000, Training loss 2.8095, Validation loss 4.0023
Epoch 3000, Training loss 2.7798, Validation loss 4.5213

Parameter containing:
tensor([[5.4919]], requires_grad=True)
Parameter containing:
tensor([-18.3074], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">linear_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">linear_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>

<span class="n">training_loop</span><span class="p">(</span>
    <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">3000</span><span class="p">,</span> 
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="p">,</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span>
    <span class="n">t_u_train</span> <span class="o">=</span> <span class="n">t_un_train</span><span class="p">,</span>
    <span class="n">t_u_val</span> <span class="o">=</span> <span class="n">t_un_val</span><span class="p">,</span> 
    <span class="n">t_c_train</span> <span class="o">=</span> <span class="n">t_c_train</span><span class="p">,</span>
    <span class="n">t_c_val</span> <span class="o">=</span> <span class="n">t_c_val</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">linear_model</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">linear_model</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1, Training loss 149.1001, Validation loss 82.8564
Epoch 1000, Training loss 3.7170, Validation loss 1.8634
Epoch 2000, Training loss 2.8091, Validation loss 4.0064
Epoch 3000, Training loss 2.7798, Validation loss 4.5222

Parameter containing:
tensor([[5.4920]], requires_grad=True)
Parameter containing:
tensor([-18.3081], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>It’s been a long journey–there has been a lot to explore for these 20-something lines of code we require to define and train a model. Hopefully by now the magic involved in training has vanished and left room for the mechanics. What we learned so far will allow us to own the code we write instead of merely poking at a black box when things get more complicated.</p>
<p>There’s one last step left to take: replacing our linear model with a neural network as our approximating function. We said earlier that using a neural network will not result in a higher-quality model, since the process underlying our calibration problem was fundamentally linear. However, it’s good to make the leap from linear to neural network in a controlled environment so we won’t feel lost later.</p>
<p><code class="docutils literal notranslate"><span class="pre">nn</span></code> provides a simple way to concatenate modules through the nn.Sequential container:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">seq_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">13</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">seq_model</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): Linear(in_features=1, out_features=13, bias=True)
  (1): Tanh()
  (2): Linear(in_features=13, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
<p>Calling model.parameters() will collect weight and bias from both the first and second linear modules. It’s instructive to inspect the parameters in this case by printing their shapes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">seq_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[torch.Size([13, 1]), torch.Size([13]), torch.Size([1, 13]), torch.Size([1])]
</pre></div>
</div>
</div>
</div>
<p>These are the tensors that the optimizer will get. Again, after we call model.backward(), all parameters are populated with their grad, and the optimizer then updates their values accordingly during the optimizer.step() call. Not that different from our previous linear model, eh? After all, they’re both differentiable models that can be trained using gradient descent.</p>
<p>A few notes on parameters of nn.Modules. When inspecting parameters of a model made up of several submodules, it is handy to be able to identify parameters by name. There’s a method for that, called named_parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">seq_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.weight torch.Size([13, 1])
0.bias torch.Size([13])
2.weight torch.Size([1, 13])
2.bias torch.Size([1])
</pre></div>
</div>
</div>
</div>
<p>The name of each module in Sequential is just the ordinal with which the module appears in the arguments. Interestingly, Sequential also accepts an OrderedDict, in which we can name each module passed to Sequential:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
 
<span class="n">seq_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;hidden_linear&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;hidden_activation&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;output_linear&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="p">]))</span>
 
<span class="n">seq_model</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (hidden_linear): Linear(in_features=1, out_features=8, bias=True)
  (hidden_activation): Tanh()
  (output_linear): Linear(in_features=8, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
<p>This allows us to get more explanatory names for submodules:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">seq_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>hidden_linear.weight torch.Size([8, 1])
hidden_linear.bias torch.Size([8])
output_linear.weight torch.Size([1, 8])
output_linear.bias torch.Size([1])
</pre></div>
</div>
</div>
</div>
<p>Running the training loop:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">seq_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span> <span class="c1"># &lt;1&gt;</span>

<span class="n">training_loop</span><span class="p">(</span>
    <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">,</span> 
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">seq_model</span><span class="p">,</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span>
    <span class="n">t_u_train</span> <span class="o">=</span> <span class="n">t_un_train</span><span class="p">,</span>
    <span class="n">t_u_val</span> <span class="o">=</span> <span class="n">t_un_val</span><span class="p">,</span> 
    <span class="n">t_c_train</span> <span class="o">=</span> <span class="n">t_c_train</span><span class="p">,</span>
    <span class="n">t_c_val</span> <span class="o">=</span> <span class="n">t_c_val</span><span class="p">)</span>
    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="n">seq_model</span><span class="p">(</span><span class="n">t_un_val</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;answer&#39;</span><span class="p">,</span> <span class="n">t_c_val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;hidden&#39;</span><span class="p">,</span> <span class="n">seq_model</span><span class="o">.</span><span class="n">hidden_linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1, Training loss 195.6297, Validation loss 110.7736
Epoch 1000, Training loss 3.8279, Validation loss 7.3894
Epoch 2000, Training loss 3.4167, Validation loss 11.3598
Epoch 3000, Training loss 1.9194, Validation loss 8.5613
Epoch 4000, Training loss 1.3862, Validation loss 6.8235
Epoch 5000, Training loss 1.2873, Validation loss 6.1687
output tensor([[13.2770],
        [-0.0607]], grad_fn=&lt;AddmmBackward&gt;)
answer tensor([[15.],
        [ 3.]])
hidden tensor([[-0.0094],
        [-0.0067],
        [ 0.0140],
        [-0.0049],
        [ 0.0139],
        [ 0.0015],
        [-0.0025],
        [-0.0173]])
</pre></div>
</div>
</div>
</div>
<p>We can also evaluate the model on all of the data and see how it differs from a line:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t_range</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">20.</span><span class="p">,</span> <span class="mf">90.</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">dpi</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Fahrenheit&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Celsius&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_u</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">t_c</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_range</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">seq_model</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">t_range</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;c-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_u</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">seq_model</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">t_u</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;kx&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_113_0.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_113_0.png" />
</div>
</div>
<p>We can appreciate that the neural network has a tendency to overfit, as we discussed in chapter 5, since it tries to chase the measurements, including the noisy ones. Even our tiny neural network has too many parameters to fit the few measurements we have. It doesn’t do a bad job, though, overall.</p>
<p>Let’s also try on some other settings:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">neuron_count</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">seq_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;hidden_linear&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">neuron_count</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;hidden_activation&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;output_linear&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">neuron_count</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="p">]))</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">seq_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

<span class="n">training_loop</span><span class="p">(</span>
    <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">,</span> 
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">seq_model</span><span class="p">,</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span>
    <span class="n">t_u_train</span> <span class="o">=</span> <span class="n">t_un_train</span><span class="p">,</span>
    <span class="n">t_u_val</span> <span class="o">=</span> <span class="n">t_un_val</span><span class="p">,</span> 
    <span class="n">t_c_train</span> <span class="o">=</span> <span class="n">t_c_train</span><span class="p">,</span>
    <span class="n">t_c_val</span> <span class="o">=</span> <span class="n">t_c_val</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1, Training loss 203.5335, Validation loss 118.4287
Epoch 1000, Training loss 59.5160, Validation loss 22.2755
Epoch 2000, Training loss 41.0913, Validation loss 12.3683
Epoch 3000, Training loss 24.4838, Validation loss 4.2068
Epoch 4000, Training loss 14.3510, Validation loss 1.4790
Epoch 5000, Training loss 8.6575, Validation loss 1.7794
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t_range</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">20.</span><span class="p">,</span> <span class="mf">90.</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">dpi</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Fahrenheit&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Celsius&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_u</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">t_c</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_range</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">seq_model</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">t_range</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;c-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_u</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">seq_model</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">t_u</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;kx&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_117_0.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_117_0.png" />
</div>
</div>
</div>
<div class="section" id="dataset-classes">
<h2>Dataset Classes<a class="headerlink" href="#dataset-classes" title="Permalink to this headline">¶</a></h2>
<p>PyTorch supports map- and iterable-style dataset classes. A map-style dataset is derived from the abstract class torch.utils.data.Dataset. It implements the getitem() and len() functions, and represents a map from (possibly nonintegral) indices/keys to data samples. For example, such a dataset, when accessed with dataset[idx], could read the idx-th image and its corresponding label from a folder on the disk. Map-style datasets are more commonly used than iterable-style datasets, and all datasets that represent a map made from keys or data samples should use this subclass.</p>
<p>The simplest way to create your own dataset class is to subclass the map-style torch.utils.data.Dataset class and override the getitem() and len() functions with your own code.</p>
<p>All subclasses should overwrite getitem(), which fetches a data sample for a given key. Subclasses can also optionally overwrite len(), which returns the size of the dataset by many Sampler implementations and the default options of DataLoader.</p>
<p>An iterable-style dataset, on the other hand, is derived from the torch.utils.data.IterableDataset abstract class. It implements the iter() protocol and represents an iterable over data samples. This type of dataset is typically used when reading data from a database or a remote server, as well as data generated in real time. Iterable datasets are useful when random reads are expensive or uncertain, and when the batch size depends on fetched data.</p>
<p>The Dataset class returns a dataset object that includes data and information about the data. The dataset and sampler objects are not iterables, meaning you cannot run a for loop on them. The dataloader object solves this problem. The DataLoader class combines a dataset with a sampler and returns an iterable.</p>
</div>
<div class="section" id="the-pytorch-nn-module-torch-nn">
<h2>The PyTorch NN Module (torch.nn)<a class="headerlink" href="#the-pytorch-nn-module-torch-nn" title="Permalink to this headline">¶</a></h2>
<p>One of the most powerful features of PyTorch is its Python module torch.nn, which makes it easy to design and experiment with new models. The following code illustrates how you can create a simple model with torch.nn. In this example, we will create a fully connected model called SimpleNet. It consists of an input layer, a hidden layer, and an output layer that takes in 2,048 input values and returns 2 output values for classification:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">class</span> <span class="nc">SimpleNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>Creating a model in PyTorch is said to be very “Pythonic,” meaning it creates objects in the preferred Python fashion. We first create a new subclass called SimpleNet that inherits from the nn.Module class, and then we define the <strong>init</strong>() and forward() methods. The <strong>init</strong>() function initializes the model parameters and the forward() function defines how data is passed through our model.</p>
<p>In <strong>init</strong>(), we call the super() function to execute the parent nn.Module class’s <strong>init</strong>() method to initialize the class parameters. Then we define some layers using the nn.Linear module.</p>
<p>The forward() function defines how data is passed through the network. In the forward() function, we first use view() to reshape the input into a 2,048-element vector, then we process the input through each layer and apply relu() activation functions. Finally, we apply the softmax() function and return the output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">simplenet</span> <span class="o">=</span> <span class="n">SimpleNet</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">simplenet</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>SimpleNet(
  (fc1): Linear(in_features=2048, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=2, bias=True)
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2048</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">simplenet</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="the-perceptron-the-simplest-neural-network">
<h2>The Perceptron: The Simplest Neural Network<a class="headerlink" href="#the-perceptron-the-simplest-neural-network" title="Permalink to this headline">¶</a></h2>
<p>The simplest neural network unit is a perceptron. The perceptron was historically and very loosely modeled after the biological neuron. As with a biological neuron, there is input and output, and “signals” flow from the inputs to the outputs.</p>
<p>Each perceptron unit has an input (x), an output (y), and three “knobs”: a set of weights (w), a bias (b), and an activation function (f). The weights and the bias are learned from the data, and the activation function is handpicked depending on the network designer’s intuition of the network and its target outputs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># Global Settings</span>
<span class="n">LEFT_CENTER</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">RIGHT_CENTER</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>

<span class="n">seed</span> <span class="o">=</span> <span class="mi">1337</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Perceptron</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; A Perceptron is one Linear layer &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            input_dim (int): size of the input features</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Perceptron</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_in</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The forward pass of the MLP</span>

<span class="sd">        Args:</span>
<span class="sd">            x_in (torch.Tensor): an input data tensor. </span>
<span class="sd">                x_in.shape should be (batch, input_dim)</span>
<span class="sd">        Returns:</span>
<span class="sd">            the resulting tensor. tensor.shape should be (batch, 1)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x_in</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>In machine learning, it is a common practice to create synthetic data with well-understood properties when trying to understand an algorithm. For this section, we use synthetic data for the task of classifying two-dimensional points into one of two classes. To construct the data, we sample the points from two different parts of the xy-plane, creating an easy-to-learn situation for the model. The goal of the model is to classify the stars (⋆) as one class, and the circles (◯) as another class. This is visualized on the righthand side, where everything above the line is classified differently than everything below the line.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_toy_data</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">left_center</span><span class="o">=</span><span class="n">LEFT_CENTER</span><span class="p">,</span> <span class="n">right_center</span><span class="o">=</span><span class="n">RIGHT_CENTER</span><span class="p">):</span>
    <span class="n">x_data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">y_targets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">batch_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
            <span class="n">x_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">left_center</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">right_center</span><span class="p">))</span>
            <span class="n">y_targets</span><span class="p">[</span><span class="n">batch_i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_targets</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_truth</span> <span class="o">=</span> <span class="n">get_toy_data</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">x_data</span> <span class="o">=</span> <span class="n">x_data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">y_truth</span> <span class="o">=</span> <span class="n">y_truth</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">left_x</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">right_x</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">left_colors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">right_colors</span> <span class="o">=</span>  <span class="p">[]</span>

<span class="k">for</span> <span class="n">x_i</span><span class="p">,</span> <span class="n">y_true_i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_truth</span><span class="p">):</span>
    <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span>

    <span class="k">if</span> <span class="n">y_true_i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">left_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_i</span><span class="p">)</span>
        <span class="n">left_colors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">color</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">right_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_i</span><span class="p">)</span>
        <span class="n">right_colors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">color</span><span class="p">)</span>

<span class="n">left_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">left_x</span><span class="p">)</span>
<span class="n">right_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">right_x</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">left_x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">left_x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">left_colors</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">right_x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">right_x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="n">right_colors</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_133_0.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_133_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">visualize_results</span><span class="p">(</span><span class="n">perceptron</span><span class="p">,</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">y_truth</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                      <span class="n">title</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">linestyles</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">]):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">perceptron</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">x_data</span> <span class="o">=</span> <span class="n">x_data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">y_truth</span> <span class="o">=</span> <span class="n">y_truth</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="n">all_x</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)]</span>
    <span class="n">all_colors</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)]</span>
    
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="s1">&#39;white&#39;</span><span class="p">]</span>
    <span class="n">markers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;*&#39;</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">x_i</span><span class="p">,</span> <span class="n">y_pred_i</span><span class="p">,</span> <span class="n">y_true_i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_truth</span><span class="p">):</span>
        <span class="n">all_x</span><span class="p">[</span><span class="n">y_true_i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_i</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">y_pred_i</span> <span class="o">==</span> <span class="n">y_true_i</span><span class="p">:</span>
            <span class="n">all_colors</span><span class="p">[</span><span class="n">y_true_i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">all_colors</span><span class="p">[</span><span class="n">y_true_i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
        <span class="c1">#all_colors[y_true_i].append(colors[y_pred_i])</span>

    <span class="n">all_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">x_list</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_list</span> <span class="ow">in</span> <span class="n">all_x</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
        
    <span class="k">for</span> <span class="n">x_list</span><span class="p">,</span> <span class="n">color_list</span><span class="p">,</span> <span class="n">marker</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">all_x</span><span class="p">,</span> <span class="n">all_colors</span><span class="p">,</span> <span class="n">markers</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_list</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_list</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="n">color_list</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
    
        
    <span class="n">xlim</span> <span class="o">=</span> <span class="p">(</span><span class="nb">min</span><span class="p">([</span><span class="n">x_list</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="k">for</span> <span class="n">x_list</span> <span class="ow">in</span> <span class="n">all_x</span><span class="p">]),</span> 
            <span class="nb">max</span><span class="p">([</span><span class="n">x_list</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="k">for</span> <span class="n">x_list</span> <span class="ow">in</span> <span class="n">all_x</span><span class="p">]))</span>
            
    <span class="n">ylim</span> <span class="o">=</span> <span class="p">(</span><span class="nb">min</span><span class="p">([</span><span class="n">x_list</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="k">for</span> <span class="n">x_list</span> <span class="ow">in</span> <span class="n">all_x</span><span class="p">]),</span> 
            <span class="nb">max</span><span class="p">([</span><span class="n">x_list</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="k">for</span> <span class="n">x_list</span> <span class="ow">in</span> <span class="n">all_x</span><span class="p">]))</span>
            
    <span class="c1"># hyperplane</span>
    
    <span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ylim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">YY</span><span class="p">,</span> <span class="n">XX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">yy</span><span class="p">,</span> <span class="n">xx</span><span class="p">)</span>
    <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">XX</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">YY</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span>
    
    <span class="n">Z</span> <span class="o">=</span> <span class="n">perceptron</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">XX</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">YY</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">levels</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="n">linestyles</span><span class="p">)</span>    
    
    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;Epoch = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">epoch</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">input_dim</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">n_batches</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">perceptron</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">perceptron</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">bce_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>

<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">x_data_static</span><span class="p">,</span> <span class="n">y_truth_static</span> <span class="o">=</span> <span class="n">get_toy_data</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">visualize_results</span><span class="p">(</span><span class="n">perceptron</span><span class="p">,</span> <span class="n">x_data_static</span><span class="p">,</span> <span class="n">y_truth_static</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Initial Model State&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="c1">#plt.savefig(&#39;initial.png&#39;)</span>

<span class="n">change</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">last</span> <span class="o">=</span> <span class="mf">10.0</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">epoch</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="n">change</span> <span class="o">&gt;</span> <span class="n">epsilon</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="n">n_epochs</span> <span class="ow">or</span> <span class="n">last</span> <span class="o">&gt;</span> <span class="mf">0.3</span><span class="p">:</span>
<span class="c1">#for epoch in range(n_epochs):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_batches</span><span class="p">):</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">x_data</span><span class="p">,</span> <span class="n">y_target</span> <span class="o">=</span> <span class="n">get_toy_data</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">perceptron</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">bce_loss</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        
        <span class="n">loss_value</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_value</span><span class="p">)</span>

        <span class="n">change</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">last</span> <span class="o">-</span> <span class="n">loss_value</span><span class="p">)</span>
        <span class="n">last</span> <span class="o">=</span> <span class="n">loss_value</span>
               
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
    <span class="n">visualize_results</span><span class="p">(</span><span class="n">perceptron</span><span class="p">,</span> <span class="n">x_data_static</span><span class="p">,</span> <span class="n">y_truth_static</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> 
                      <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">loss_value</span><span class="si">}</span><span class="s2">; </span><span class="si">{</span><span class="n">change</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">epoch</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="c1">#plt.savefig(&#39;epoch{}_toylearning.png&#39;.format(epoch))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_135_0.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_135_0.png" />
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_135_1.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_135_1.png" />
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_135_2.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_135_2.png" />
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_135_3.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_135_3.png" />
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_135_4.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_135_4.png" />
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_135_5.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_135_5.png" />
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_135_6.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_135_6.png" />
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_135_7.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_135_7.png" />
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_135_8.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_135_8.png" />
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_135_9.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_135_9.png" />
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_135_10.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_135_10.png" />
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_135_11.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_135_11.png" />
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_135_12.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_135_12.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">left_x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">left_x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">right_x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">right_x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">);</span>
<span class="n">visualize_results</span><span class="p">(</span><span class="n">perceptron</span><span class="p">,</span> <span class="n">x_data_static</span><span class="p">,</span> <span class="n">y_truth_static</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;perceptron_final.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;perceptron_final.pdf&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_136_0.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_136_0.png" />
</div>
</div>
</div>
<div class="section" id="the-multilayer-perceptron">
<h2>The Multilayer Perceptron<a class="headerlink" href="#the-multilayer-perceptron" title="Permalink to this headline">¶</a></h2>
<p>The multilayer perceptron is considered one of the most basic neural network building blocks. The simplest MLP is an extension to the perceptron. The perceptron takes the data vector as input and computes a single output value. In an MLP, many perceptrons are grouped so that the output of a single layer is a new vector instead of a single output value. In PyTorch, this is done simply by setting the number of output features in the Linear layer. An additional aspect of an MLP is that it combines multiple layers with a nonlinearity in between each layer.</p>
<p>The simplest MLP is composed of three stages of representation and two Linear layers. The first stage is the input vector. This is the vector that is given to the model. Given the input vector, the first Linear layer computes a hidden vector—the second stage of representation. The hidden vector is called such because it is the output of a layer that’s between the input and the output. What do we mean by “output of a layer”? One way to understand this is that the values in the hidden vector are the output of different perceptrons that make up that layer. Using this hidden vector, the second Linear layer computes an output vector. In a multiclass setting, the size of the output vector is equal to the number of classes. Always, the final hidden vector is mapped to the output vector using a combination of Linear layer and a nonlinearity.</p>
<p>Let’s take a look at the XOR example described earlier and see what would happen with a perceptron versus an MLP. In this example, we train both the perceptron and an MLP in a binary classification task: identifying stars and circles. Each data point is a 2D coordinate. Without diving into the implementation details yet, the final model predictions are shown in figure below. In this plot, incorrectly classified data points are filled in with black, whereas correctly classified data points are not filled in. In the left panel, you can see that the perceptron has difficulty in learning a decision boundary that can separate the stars and circles, as evidenced by the filled in shapes. However, the MLP (right panel) learns a decision boundary that classifies the stars and circles much more accurately.</p>
<img src='https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491978221/files/assets/nlpp_0403.png'><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="n">seed</span> <span class="o">=</span> <span class="mi">1337</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MultilayerPerceptron</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            input_dim (int): the size of the input vectors</span>
<span class="sd">            hidden_dim (int): the output size of the first Linear layer</span>
<span class="sd">            output_dim (int): the output size of the second Linear layer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultilayerPerceptron</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_in</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The forward pass of the MLP</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            x_in (torch.Tensor): an input data tensor. </span>
<span class="sd">                x_in.shape should be (batch, input_dim)</span>
<span class="sd">            apply_softmax (bool): a flag for the softmax activation</span>
<span class="sd">                should be false if used with the Cross Entropy losses</span>
<span class="sd">        Returns:</span>
<span class="sd">            the resulting tensor. tensor.shape should be (batch, output_dim)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">intermediate</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x_in</span><span class="p">))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">intermediate</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">apply_softmax</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s instantiate the MLP. Due to the generality of the MLP implementation, we can model inputs of any size. To demonstrate, we use an input dimension of size 3, an output dimension of size 4, and a hidden dimension of size 100. Notice how in the output of the print statement, the number of units in each layer nicely line up to produce an output of dimension 4 for an input of dimension 3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># number of samples input at once</span>
<span class="n">input_dim</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># Initialize model</span>
<span class="n">mlp</span> <span class="o">=</span> <span class="n">MultilayerPerceptron</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mlp</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MultilayerPerceptron(
  (fc1): Linear(in_features=3, out_features=100, bias=True)
  (fc2): Linear(in_features=100, out_features=4, bias=True)
)
</pre></div>
</div>
</div>
</div>
<p>We can quickly test the “wiring” of the model by passing some random inputs. Because the model is not yet trained, the outputs are random. Doing this is a useful sanity check before spending time training a model. Notice how PyTorch’s interactivity allows us to do all this in real time during development, in a way not much different from using NumPy or Pandas.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">describe</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">type</span><span class="p">()))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape/size: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Values: </span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">x_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>
<span class="n">describe</span><span class="p">(</span><span class="n">x_input</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Type: torch.FloatTensor
Shape/size: torch.Size([2, 3])
Values: 
tensor([[0.8329, 0.4277, 0.4363],
        [0.9686, 0.6316, 0.8494]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y_output</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">(</span><span class="n">x_input</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">describe</span><span class="p">(</span><span class="n">y_output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Type: torch.FloatTensor
Shape/size: torch.Size([2, 4])
Values: 
tensor([[-0.2456,  0.0723,  0.1589, -0.3294],
        [-0.3497,  0.0828,  0.3391, -0.4271]], grad_fn=&lt;AddmmBackward&gt;)
</pre></div>
</div>
</div>
</div>
<p>It is important to learn how to read inputs and outputs of PyTorch models. In the preceding example, the output of the MLP model is a tensor that has two rows and four columns. The rows in this tensor correspond to the batch dimension, which is the number of data points in the minibatch. The columns are the final feature vectors for each data point. In some cases, such as in a classification setting, the feature vector is a prediction vector. The name “prediction vector” means that it corresponds to a probability distribution. What happens with the prediction vector depends on whether we are currently conducting training or performing inference. During training, the outputs are used as is with a loss function and a representation of the target class labels.</p>
<p>However, if you want to turn the prediction vector into probabilities, an extra step is required. Specifically, you require the softmax activation function, which is used to transform a vector of values into probabilities. The softmax function has many roots. In physics, it is known as the Boltzmann or Gibbs distribution; in statistics, it’s multinomial logistic regression; and in the natural language processing (NLP) community it’s known as the maximum entropy (MaxEnt) classifier.7 Whatever the name, the intuition underlying the function is that large positive values will result in higher probabilities, and lower negative values will result in smaller probabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y_output</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">(</span><span class="n">x_input</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">describe</span><span class="p">(</span><span class="n">y_output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Type: torch.FloatTensor
Shape/size: torch.Size([2, 4])
Values: 
tensor([[0.2087, 0.2868, 0.3127, 0.1919],
        [0.1832, 0.2824, 0.3649, 0.1696]], grad_fn=&lt;SoftmaxBackward&gt;)
</pre></div>
</div>
</div>
</div>
<p>To conclude, MLPs are stacked Linear layers that map tensors to other tensors. Nonlinearities are used between each pair of Linear layers to break the linear relationship and allow for the model to twist the vector space around. In a classification setting, this twisting should result in linear separability between classes. Additionally, you can use the softmax function to interpret MLP outputs as probabilities, but you should not use softmax with specific loss functions, because the underlying implementations can leverage superior mathematical/computational shortcuts.</p>
</div>
<div class="section" id="image-neural-model">
<h2>Image Neural Model<a class="headerlink" href="#image-neural-model" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>


<span class="c1"># Define transforms for data preprocessing</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="p">])</span>

<span class="c1"># Datasets</span>
<span class="n">trainset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">testset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>

<span class="c1"># Dataloaders to feed the data in batches</span>
<span class="n">trainloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">testloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">testset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;No. of train images: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">trainset</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;No. of test images: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">testset</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;No. of train batches: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">trainloader</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;No. of test batches: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">testloader</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>No. of train images: 60000
No. of test images: 10000
No. of train batches: 60
No. of test batches: 100
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Network</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">12</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">60</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">60</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">find_acc</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;pixelwise accuracy&quot;&quot;&quot;</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">correct</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="k">return</span> <span class="n">accuracy</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">trainloader</span><span class="p">):</span>
    <span class="n">loss_train</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">acc_train</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">network</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">trainloader</span><span class="p">)):</span>

        <span class="n">images</span> <span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">trainloader</span><span class="p">))</span>
        
        <span class="c1"># move the images and labels to GPU</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">pred</span> <span class="o">=</span> <span class="n">network</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        
        <span class="c1"># clear all the gradients before calculating them</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        
        <span class="c1"># find the loss for the current step</span>
        <span class="n">loss_train_step</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">pred</span> <span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        
        <span class="c1"># find accuracy</span>
        <span class="n">acc_train_step</span> <span class="o">=</span> <span class="n">find_acc</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        
        <span class="c1"># calculate the gradients</span>
        <span class="n">loss_train_step</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        
        <span class="c1"># update the parameters</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="n">loss_train</span> <span class="o">+=</span> <span class="n">loss_train_step</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">acc_train</span> <span class="o">+=</span> <span class="n">acc_train_step</span>  
            
        <span class="n">loss_train</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainloader</span><span class="p">)</span>
        <span class="n">acc_train</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">testloader</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss_train</span><span class="p">,</span> <span class="n">acc_train</span>  
        
        
<span class="k">def</span> <span class="nf">validate</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">testloader</span><span class="p">):</span> 
    <span class="n">loss_valid</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">acc_valid</span> <span class="o">=</span> <span class="mi">0</span>       
    <span class="n">network</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  

    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">testloader</span><span class="p">)):</span>

        <span class="n">images</span> <span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">testloader</span><span class="p">))</span>
        
        <span class="c1"># move the images and labels to GPU</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">pred</span> <span class="o">=</span> <span class="n">network</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        
        <span class="c1"># clear all the gradients before calculating them</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        
        <span class="c1"># find the loss and acc for the current step</span>
        <span class="n">loss_valid_step</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">pred</span> <span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">acc_valid_step</span> <span class="o">=</span> <span class="n">find_acc</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
      
        <span class="n">loss_valid</span> <span class="o">+=</span> <span class="n">loss_valid_step</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">acc_valid</span> <span class="o">+=</span> <span class="n">acc_valid_step</span>

        <span class="n">loss_valid</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainloader</span><span class="p">)</span>
        <span class="n">acc_valid</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">testloader</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss_valid</span><span class="p">,</span> <span class="n">acc_valid</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.005</span><span class="p">)</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    
    <span class="n">loss_train</span><span class="p">,</span> <span class="n">acc_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">trainloader</span><span class="p">)</span>
    <span class="n">loss_valid</span><span class="p">,</span> <span class="n">acc_valid</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">testloader</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch: </span><span class="si">{}</span><span class="s1">  Train Loss: </span><span class="si">{:.4f}</span><span class="s1">  Train Acc: </span><span class="si">{:.4f}</span><span class="s1">  Valid Loss: </span><span class="si">{:.4f}</span><span class="s1">  Valid Acc: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">loss_train</span><span class="p">,</span> <span class="n">acc_train</span><span class="p">,</span> <span class="n">loss_valid</span><span class="p">,</span> <span class="n">acc_valid</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Time Elapsed : </span><span class="si">{:.4f}</span><span class="s2">s&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">))</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">&quot;model.h5&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1  Train Loss: 0.0384  Train Acc: 0.1000  Valid Loss: 0.0382  Valid Acc: 0.1500
Epoch: 2  Train Loss: 0.0383  Train Acc: 0.1350  Valid Loss: 0.0379  Valid Acc: 0.2300
Epoch: 3  Train Loss: 0.0380  Train Acc: 0.1840  Valid Loss: 0.0374  Valid Acc: 0.3500
Epoch: 4  Train Loss: 0.0375  Train Acc: 0.3170  Valid Loss: 0.0366  Valid Acc: 0.2700
Epoch: 5  Train Loss: 0.0366  Train Acc: 0.3000  Valid Loss: 0.0353  Valid Acc: 0.3400
Epoch: 6  Train Loss: 0.0354  Train Acc: 0.3720  Valid Loss: 0.0334  Valid Acc: 0.4000
Epoch: 7  Train Loss: 0.0335  Train Acc: 0.5070  Valid Loss: 0.0309  Valid Acc: 0.3900
Epoch: 8  Train Loss: 0.0312  Train Acc: 0.4660  Valid Loss: 0.0286  Valid Acc: 0.5200
Epoch: 9  Train Loss: 0.0290  Train Acc: 0.5050  Valid Loss: 0.0264  Valid Acc: 0.4900
Epoch: 10  Train Loss: 0.0264  Train Acc: 0.5470  Valid Loss: 0.0243  Valid Acc: 0.5400
Time Elapsed : 6.4947s
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>

    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="n">num_correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>

        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">testloader</span><span class="p">:</span>

            <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span>
            <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="n">total_images</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">testset</span><span class="p">)</span>

            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

            <span class="n">num_correct_batch</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="n">accuracy_batch</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="n">num_correct</span> <span class="o">+=</span> <span class="n">num_correct_batch</span>
            <span class="n">accuracy</span> <span class="o">+=</span> <span class="n">accuracy_batch</span>

        <span class="n">accuracy</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">testloader</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of test images: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">total_images</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of correct predictions: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_correct</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Time Elapsed : </span><span class="si">{:.4f}</span><span class="s2">s&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">))</span>
    
<span class="c1"># test the trained network    </span>
<span class="n">test_model</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>    
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of test images: 10000
Number of correct predictions: 5580
Accuracy: 55.799999803304665
Time Elapsed : 1.5537s
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="linear-regression">
<h2>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">colors</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;font.size&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.0247, 0.5804, 0.6351, 0.7535, 0.0089],
        [0.6606, 0.2379, 0.5669, 0.4678, 0.8564],
        [0.8111, 0.2537, 0.9507, 0.5712, 0.0985],
        [0.1759, 0.1709, 0.6917, 0.0379, 0.5138],
        [0.3299, 0.8764, 0.4889, 0.9616, 0.3959],
        [0.8186, 0.3144, 0.0861, 0.4295, 0.6138],
        [0.1872, 0.0836, 0.3833, 0.1820, 0.4921],
        [0.6746, 0.6487, 0.3425, 0.7972, 0.7711],
        [0.9162, 0.2550, 0.9624, 0.9223, 0.6067],
        [0.9402, 0.3548, 0.6309, 0.9777, 0.8523],
        [0.9641, 0.6582, 0.4960, 0.8282, 0.9987],
        [0.5216, 0.1469, 0.1933, 0.0558, 0.5462],
        [0.1168, 0.3665, 0.2524, 0.5927, 0.5074],
        [0.9839, 0.5169, 0.8867, 0.7454, 0.9383],
        [0.0682, 0.7628, 0.6876, 0.8078, 0.1992],
        [0.7100, 0.7376, 0.5474, 0.9188, 0.1415],
        [0.3615, 0.9600, 0.7234, 0.0230, 0.9815],
        [0.7484, 0.2812, 0.6441, 0.6448, 0.4908],
        [0.0400, 0.0779, 0.4354, 0.0694, 0.3190],
        [0.9839, 0.3148, 0.9563, 0.3599, 0.2827]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">input_dim</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">W</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">output_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">output_dim</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

<span class="n">true_model</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n_train</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">noise_level</span> <span class="o">=</span> <span class="mf">0.04</span>

<span class="c1"># Generate a random set of n_train samples</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_train</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">true_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_train</span><span class="p">])</span>

<span class="c1"># Add some noise</span>
<span class="n">y_train</span> <span class="o">+=</span> <span class="n">noise_level</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">input_dim</span> <span class="o">==</span> <span class="n">output_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X_train&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y_train&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_163_0.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_163_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">VectorialDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">output_data</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VectorialDataset</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">input_data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;f&#39;</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">output_data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;f&#39;</span><span class="p">))</span>
        
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">idx</span><span class="p">):</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">idx</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_data</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:],</span> 
                  <span class="bp">self</span><span class="o">.</span><span class="n">output_data</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:])</span>  
        <span class="k">return</span> <span class="n">sample</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_set</span> <span class="o">=</span> <span class="n">VectorialDataset</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">output_data</span><span class="o">=</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">training_set</span><span class="p">[</span><span class="mi">10</span><span class="p">:</span><span class="mi">12</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[0.3506],
        [0.4893]]), tensor([[-0.1045],
        [-0.0683]]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">training_set</span><span class="p">,</span> 
                                           <span class="n">batch_size</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> 
                                           <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch</span>


<span class="k">class</span> <span class="nc">LinearModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LinearModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">LinearModel</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
<span class="n">model</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LinearModel(
  (linear): Linear(in_features=1, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Parameter containing:
 tensor([[-0.0274]], requires_grad=True), Parameter containing:
 tensor([-0.3942], requires_grad=True)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([[-0.0274]], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">bias</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([-0.3942], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.3852],
        [-0.3906],
        [-0.3929],
        [-0.3403],
        [-0.3974]], grad_fn=&lt;AddmmBackward&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span> <span class="o">@</span> <span class="n">xx</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">bias</span> <span class="k">for</span> <span class="n">xx</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[tensor([-0.3852], grad_fn=&lt;AddBackward0&gt;),
 tensor([-0.3906], grad_fn=&lt;AddBackward0&gt;),
 tensor([-0.3929], grad_fn=&lt;AddBackward0&gt;),
 tensor([-0.3403], grad_fn=&lt;AddBackward0&gt;),
 tensor([-0.3974], grad_fn=&lt;AddBackward0&gt;)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">input_dim</span> <span class="o">==</span> <span class="n">output_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">training_set</span><span class="o">.</span><span class="n">input_data</span><span class="p">,</span> <span class="n">training_set</span><span class="o">.</span><span class="n">output_data</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">training_set</span><span class="o">.</span><span class="n">input_data</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">training_set</span><span class="o">.</span><span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X_train&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;y_train&#39;</span><span class="p">,</span> <span class="s1">&#39;model(X_train)&#39;</span><span class="p">])</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_174_0.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_174_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">loss_fun</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;f&#39;</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;f&#39;</span><span class="p">))</span>
<span class="n">loss_fun</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(2.)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">input_dim</span> <span class="o">==</span> <span class="n">output_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>

    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

    <span class="n">ww</span><span class="p">,</span> <span class="n">bb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">30</span><span class="p">))</span>

    <span class="n">loss_values</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">*</span> <span class="n">ww</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ww</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ww</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;linear.weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">ww</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]]])</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;linear.bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">bb</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]])</span>
            <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
            <span class="n">loss_values</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss_fun</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">training_set</span><span class="o">.</span><span class="n">input_data</span><span class="p">),</span>  <span class="n">training_set</span><span class="o">.</span><span class="n">output_data</span><span class="p">)</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">levels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">loss_values</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">loss_values</span><span class="p">)),</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">c</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">ww</span><span class="p">,</span> <span class="n">bb</span><span class="p">,</span> <span class="n">loss_values</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">levels</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">colors</span><span class="o">.</span><span class="n">LogNorm</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">b</span><span class="p">,</span> <span class="s1">&#39;r*&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;(W, b)&#39;</span><span class="p">])</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>  
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_176_0.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_176_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span>  <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fun</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>  <span class="n">y</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="k">if</span> <span class="n">input_dim</span> <span class="o">==</span> <span class="n">output_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">bias</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">bias</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-1.6953]])
tensor([[-1.6953]], grad_fn=&lt;MulBackward0&gt;)
tensor([4.3905])
tensor([[4.3905]], grad_fn=&lt;MulBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">input_dim</span> <span class="o">==</span> <span class="n">output_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>

    <span class="n">num_iter</span> <span class="o">=</span> <span class="mi">200</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># 0.01</span>

    <span class="n">train_hist</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">model</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iter</span><span class="p">):</span>

        <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fun</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">training_set</span><span class="o">.</span><span class="n">input_data</span><span class="p">),</span> <span class="n">training_set</span><span class="o">.</span><span class="n">output_data</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">w</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="n">dw</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">db</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;linear.weight&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">dw</span><span class="p">])</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;linear.bias&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">db</span><span class="p">])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">train_hist</span><span class="p">:</span>
        <span class="n">train_hist</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_hist</span><span class="p">[</span><span class="n">label</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">input_dim</span> <span class="o">==</span> <span class="n">output_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">levels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">loss_values</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">loss_values</span><span class="p">)),</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">ww</span><span class="p">,</span> <span class="n">bb</span><span class="p">,</span> <span class="n">loss_values</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">levels</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">colors</span><span class="o">.</span><span class="n">LogNorm</span><span class="p">())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> 
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> 
    
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">],</span> <span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">],</span> <span class="s1">&#39;.-b&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">b</span><span class="p">,</span> <span class="s1">&#39;r*&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;optim&#39;</span><span class="p">,</span> <span class="s1">&#39;(W, b)&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_179_0.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_179_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">5e-4</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>

<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">train_hist</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">if</span> <span class="n">input_dim</span> <span class="o">==</span> <span class="n">output_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Initialize training</span>
<span class="n">model</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fun</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>  <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">input_dim</span> <span class="o">==</span> <span class="n">output_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;[Epoch </span><span class="si">%4d</span><span class="s1">/</span><span class="si">%4d</span><span class="s1">] [Batch </span><span class="si">%4d</span><span class="s1">/</span><span class="si">%4d</span><span class="s1">] Loss: </span><span class="si">% 2.2e</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> 
                                                                <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span> 
                                                                <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
        
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Epoch    1/ 100] [Batch    1/   9] Loss:  2.32e-01
[Epoch    1/ 100] [Batch    2/   9] Loss:  9.57e-02
[Epoch    1/ 100] [Batch    3/   9] Loss:  6.26e-02
[Epoch    1/ 100] [Batch    4/   9] Loss:  4.00e-02
[Epoch    1/ 100] [Batch    5/   9] Loss:  5.43e-02
[Epoch    1/ 100] [Batch    6/   9] Loss:  7.25e-02
[Epoch    1/ 100] [Batch    7/   9] Loss:  7.60e-02
[Epoch    1/ 100] [Batch    8/   9] Loss:  7.37e-02
[Epoch    1/ 100] [Batch    9/   9] Loss:  5.14e-02
[Epoch    2/ 100] [Batch    1/   9] Loss:  4.57e-02
[Epoch    2/ 100] [Batch    2/   9] Loss:  3.42e-02
[Epoch    2/ 100] [Batch    3/   9] Loss:  2.39e-02
[Epoch    2/ 100] [Batch    4/   9] Loss:  1.98e-02
[Epoch    2/ 100] [Batch    5/   9] Loss:  2.25e-02
[Epoch    2/ 100] [Batch    6/   9] Loss:  2.94e-02
[Epoch    2/ 100] [Batch    7/   9] Loss:  2.43e-02
[Epoch    2/ 100] [Batch    8/   9] Loss:  2.46e-02
[Epoch    2/ 100] [Batch    9/   9] Loss:  1.77e-02
[Epoch    3/ 100] [Batch    1/   9] Loss:  1.60e-02
[Epoch    3/ 100] [Batch    2/   9] Loss:  1.12e-02
[Epoch    3/ 100] [Batch    3/   9] Loss:  5.27e-03
[Epoch    3/ 100] [Batch    4/   9] Loss:  4.79e-03
[Epoch    3/ 100] [Batch    5/   9] Loss:  5.48e-03
[Epoch    3/ 100] [Batch    6/   9] Loss:  5.94e-03
[Epoch    3/ 100] [Batch    7/   9] Loss:  5.66e-03
[Epoch    3/ 100] [Batch    8/   9] Loss:  7.04e-03
[Epoch    3/ 100] [Batch    9/   9] Loss:  7.22e-03
[Epoch    4/ 100] [Batch    1/   9] Loss:  5.63e-03
[Epoch    4/ 100] [Batch    2/   9] Loss:  3.97e-03
[Epoch    4/ 100] [Batch    3/   9] Loss:  1.98e-03
[Epoch    4/ 100] [Batch    4/   9] Loss:  2.49e-03
[Epoch    4/ 100] [Batch    5/   9] Loss:  2.11e-03
[Epoch    4/ 100] [Batch    6/   9] Loss:  3.53e-03
[Epoch    4/ 100] [Batch    7/   9] Loss:  3.84e-03
[Epoch    4/ 100] [Batch    8/   9] Loss:  3.83e-03
[Epoch    4/ 100] [Batch    9/   9] Loss:  3.59e-03
[Epoch    5/ 100] [Batch    1/   9] Loss:  2.75e-03
[Epoch    5/ 100] [Batch    2/   9] Loss:  2.28e-03
[Epoch    5/ 100] [Batch    3/   9] Loss:  1.80e-03
[Epoch    5/ 100] [Batch    4/   9] Loss:  2.23e-03
[Epoch    5/ 100] [Batch    5/   9] Loss:  2.46e-03
[Epoch    5/ 100] [Batch    6/   9] Loss:  2.86e-03
[Epoch    5/ 100] [Batch    7/   9] Loss:  2.78e-03
[Epoch    5/ 100] [Batch    8/   9] Loss:  2.23e-03
[Epoch    5/ 100] [Batch    9/   9] Loss:  2.84e-03
[Epoch    6/ 100] [Batch    1/   9] Loss:  2.83e-03
[Epoch    6/ 100] [Batch    2/   9] Loss:  2.31e-03
[Epoch    6/ 100] [Batch    3/   9] Loss:  2.20e-03
[Epoch    6/ 100] [Batch    4/   9] Loss:  2.49e-03
[Epoch    6/ 100] [Batch    5/   9] Loss:  2.17e-03
[Epoch    6/ 100] [Batch    6/   9] Loss:  1.98e-03
[Epoch    6/ 100] [Batch    7/   9] Loss:  2.88e-03
[Epoch    6/ 100] [Batch    8/   9] Loss:  2.50e-03
[Epoch    6/ 100] [Batch    9/   9] Loss:  1.69e-03
[Epoch    7/ 100] [Batch    1/   9] Loss:  1.97e-03
[Epoch    7/ 100] [Batch    2/   9] Loss:  1.89e-03
[Epoch    7/ 100] [Batch    3/   9] Loss:  1.52e-03
[Epoch    7/ 100] [Batch    4/   9] Loss:  2.36e-03
[Epoch    7/ 100] [Batch    5/   9] Loss:  2.16e-03
[Epoch    7/ 100] [Batch    6/   9] Loss:  1.76e-03
[Epoch    7/ 100] [Batch    7/   9] Loss:  1.94e-03
[Epoch    7/ 100] [Batch    8/   9] Loss:  1.75e-03
[Epoch    7/ 100] [Batch    9/   9] Loss:  2.23e-03
[Epoch    8/ 100] [Batch    1/   9] Loss:  1.93e-03
[Epoch    8/ 100] [Batch    2/   9] Loss:  1.54e-03
[Epoch    8/ 100] [Batch    3/   9] Loss:  2.09e-03
[Epoch    8/ 100] [Batch    4/   9] Loss:  1.95e-03
[Epoch    8/ 100] [Batch    5/   9] Loss:  2.52e-03
[Epoch    8/ 100] [Batch    6/   9] Loss:  1.65e-03
[Epoch    8/ 100] [Batch    7/   9] Loss:  1.32e-03
[Epoch    8/ 100] [Batch    8/   9] Loss:  2.08e-03
[Epoch    8/ 100] [Batch    9/   9] Loss:  1.30e-03
[Epoch    9/ 100] [Batch    1/   9] Loss:  1.86e-03
[Epoch    9/ 100] [Batch    2/   9] Loss:  1.76e-03
[Epoch    9/ 100] [Batch    3/   9] Loss:  1.44e-03
[Epoch    9/ 100] [Batch    4/   9] Loss:  1.65e-03
[Epoch    9/ 100] [Batch    5/   9] Loss:  1.66e-03
[Epoch    9/ 100] [Batch    6/   9] Loss:  2.05e-03
[Epoch    9/ 100] [Batch    7/   9] Loss:  1.52e-03
[Epoch    9/ 100] [Batch    8/   9] Loss:  1.94e-03
[Epoch    9/ 100] [Batch    9/   9] Loss:  2.45e-03
[Epoch   10/ 100] [Batch    1/   9] Loss:  1.76e-03
[Epoch   10/ 100] [Batch    2/   9] Loss:  1.97e-03
[Epoch   10/ 100] [Batch    3/   9] Loss:  2.19e-03
[Epoch   10/ 100] [Batch    4/   9] Loss:  1.84e-03
[Epoch   10/ 100] [Batch    5/   9] Loss:  1.58e-03
[Epoch   10/ 100] [Batch    6/   9] Loss:  1.72e-03
[Epoch   10/ 100] [Batch    7/   9] Loss:  1.60e-03
[Epoch   10/ 100] [Batch    8/   9] Loss:  1.47e-03
[Epoch   10/ 100] [Batch    9/   9] Loss:  2.05e-03
[Epoch   11/ 100] [Batch    1/   9] Loss:  1.69e-03
[Epoch   11/ 100] [Batch    2/   9] Loss:  1.66e-03
[Epoch   11/ 100] [Batch    3/   9] Loss:  1.93e-03
[Epoch   11/ 100] [Batch    4/   9] Loss:  1.79e-03
[Epoch   11/ 100] [Batch    5/   9] Loss:  1.68e-03
[Epoch   11/ 100] [Batch    6/   9] Loss:  1.70e-03
[Epoch   11/ 100] [Batch    7/   9] Loss:  1.86e-03
[Epoch   11/ 100] [Batch    8/   9] Loss:  1.47e-03
[Epoch   11/ 100] [Batch    9/   9] Loss:  2.25e-03
[Epoch   12/ 100] [Batch    1/   9] Loss:  1.65e-03
[Epoch   12/ 100] [Batch    2/   9] Loss:  1.60e-03
[Epoch   12/ 100] [Batch    3/   9] Loss:  1.90e-03
[Epoch   12/ 100] [Batch    4/   9] Loss:  1.80e-03
[Epoch   12/ 100] [Batch    5/   9] Loss:  1.47e-03
[Epoch   12/ 100] [Batch    6/   9] Loss:  1.86e-03
[Epoch   12/ 100] [Batch    7/   9] Loss:  2.05e-03
[Epoch   12/ 100] [Batch    8/   9] Loss:  1.58e-03
[Epoch   12/ 100] [Batch    9/   9] Loss:  2.15e-03
[Epoch   13/ 100] [Batch    1/   9] Loss:  1.61e-03
[Epoch   13/ 100] [Batch    2/   9] Loss:  2.10e-03
[Epoch   13/ 100] [Batch    3/   9] Loss:  1.79e-03
[Epoch   13/ 100] [Batch    4/   9] Loss:  1.53e-03
[Epoch   13/ 100] [Batch    5/   9] Loss:  1.53e-03
[Epoch   13/ 100] [Batch    6/   9] Loss:  2.02e-03
[Epoch   13/ 100] [Batch    7/   9] Loss:  1.74e-03
[Epoch   13/ 100] [Batch    8/   9] Loss:  1.85e-03
[Epoch   13/ 100] [Batch    9/   9] Loss:  1.43e-03
[Epoch   14/ 100] [Batch    1/   9] Loss:  1.63e-03
[Epoch   14/ 100] [Batch    2/   9] Loss:  1.94e-03
[Epoch   14/ 100] [Batch    3/   9] Loss:  1.89e-03
[Epoch   14/ 100] [Batch    4/   9] Loss:  1.86e-03
[Epoch   14/ 100] [Batch    5/   9] Loss:  1.77e-03
[Epoch   14/ 100] [Batch    6/   9] Loss:  1.47e-03
[Epoch   14/ 100] [Batch    7/   9] Loss:  1.76e-03
[Epoch   14/ 100] [Batch    8/   9] Loss:  1.68e-03
[Epoch   14/ 100] [Batch    9/   9] Loss:  1.87e-03
[Epoch   15/ 100] [Batch    1/   9] Loss:  2.04e-03
[Epoch   15/ 100] [Batch    2/   9] Loss:  1.73e-03
[Epoch   15/ 100] [Batch    3/   9] Loss:  1.46e-03
[Epoch   15/ 100] [Batch    4/   9] Loss:  1.54e-03
[Epoch   15/ 100] [Batch    5/   9] Loss:  1.90e-03
[Epoch   15/ 100] [Batch    6/   9] Loss:  1.50e-03
[Epoch   15/ 100] [Batch    7/   9] Loss:  2.03e-03
[Epoch   15/ 100] [Batch    8/   9] Loss:  1.83e-03
[Epoch   15/ 100] [Batch    9/   9] Loss:  1.75e-03
[Epoch   16/ 100] [Batch    1/   9] Loss:  2.04e-03
[Epoch   16/ 100] [Batch    2/   9] Loss:  1.64e-03
[Epoch   16/ 100] [Batch    3/   9] Loss:  1.91e-03
[Epoch   16/ 100] [Batch    4/   9] Loss:  1.82e-03
[Epoch   16/ 100] [Batch    5/   9] Loss:  1.85e-03
[Epoch   16/ 100] [Batch    6/   9] Loss:  1.41e-03
[Epoch   16/ 100] [Batch    7/   9] Loss:  1.69e-03
[Epoch   16/ 100] [Batch    8/   9] Loss:  1.59e-03
[Epoch   16/ 100] [Batch    9/   9] Loss:  1.50e-03
[Epoch   17/ 100] [Batch    1/   9] Loss:  1.54e-03
[Epoch   17/ 100] [Batch    2/   9] Loss:  1.64e-03
[Epoch   17/ 100] [Batch    3/   9] Loss:  1.60e-03
[Epoch   17/ 100] [Batch    4/   9] Loss:  1.81e-03
[Epoch   17/ 100] [Batch    5/   9] Loss:  1.57e-03
[Epoch   17/ 100] [Batch    6/   9] Loss:  2.02e-03
[Epoch   17/ 100] [Batch    7/   9] Loss:  1.95e-03
[Epoch   17/ 100] [Batch    8/   9] Loss:  1.83e-03
[Epoch   17/ 100] [Batch    9/   9] Loss:  1.50e-03
[Epoch   18/ 100] [Batch    1/   9] Loss:  1.85e-03
[Epoch   18/ 100] [Batch    2/   9] Loss:  1.80e-03
[Epoch   18/ 100] [Batch    3/   9] Loss:  1.90e-03
[Epoch   18/ 100] [Batch    4/   9] Loss:  1.52e-03
[Epoch   18/ 100] [Batch    5/   9] Loss:  1.58e-03
[Epoch   18/ 100] [Batch    6/   9] Loss:  1.59e-03
[Epoch   18/ 100] [Batch    7/   9] Loss:  1.59e-03
[Epoch   18/ 100] [Batch    8/   9] Loss:  1.92e-03
[Epoch   18/ 100] [Batch    9/   9] Loss:  1.91e-03
[Epoch   19/ 100] [Batch    1/   9] Loss:  1.48e-03
[Epoch   19/ 100] [Batch    2/   9] Loss:  1.87e-03
[Epoch   19/ 100] [Batch    3/   9] Loss:  1.48e-03
[Epoch   19/ 100] [Batch    4/   9] Loss:  1.78e-03
[Epoch   19/ 100] [Batch    5/   9] Loss:  1.95e-03
[Epoch   19/ 100] [Batch    6/   9] Loss:  1.72e-03
[Epoch   19/ 100] [Batch    7/   9] Loss:  1.83e-03
[Epoch   19/ 100] [Batch    8/   9] Loss:  1.74e-03
[Epoch   19/ 100] [Batch    9/   9] Loss:  1.58e-03
[Epoch   20/ 100] [Batch    1/   9] Loss:  1.76e-03
[Epoch   20/ 100] [Batch    2/   9] Loss:  1.67e-03
[Epoch   20/ 100] [Batch    3/   9] Loss:  1.57e-03
[Epoch   20/ 100] [Batch    4/   9] Loss:  1.50e-03
[Epoch   20/ 100] [Batch    5/   9] Loss:  2.14e-03
[Epoch   20/ 100] [Batch    6/   9] Loss:  1.85e-03
[Epoch   20/ 100] [Batch    7/   9] Loss:  1.93e-03
[Epoch   20/ 100] [Batch    8/   9] Loss:  1.68e-03
[Epoch   20/ 100] [Batch    9/   9] Loss:  1.59e-03
[Epoch   21/ 100] [Batch    1/   9] Loss:  1.71e-03
[Epoch   21/ 100] [Batch    2/   9] Loss:  2.24e-03
[Epoch   21/ 100] [Batch    3/   9] Loss:  1.47e-03
[Epoch   21/ 100] [Batch    4/   9] Loss:  1.66e-03
[Epoch   21/ 100] [Batch    5/   9] Loss:  1.84e-03
[Epoch   21/ 100] [Batch    6/   9] Loss:  1.62e-03
[Epoch   21/ 100] [Batch    7/   9] Loss:  1.66e-03
[Epoch   21/ 100] [Batch    8/   9] Loss:  1.76e-03
[Epoch   21/ 100] [Batch    9/   9] Loss:  1.85e-03
[Epoch   22/ 100] [Batch    1/   9] Loss:  1.56e-03
[Epoch   22/ 100] [Batch    2/   9] Loss:  1.81e-03
[Epoch   22/ 100] [Batch    3/   9] Loss:  1.87e-03
[Epoch   22/ 100] [Batch    4/   9] Loss:  1.63e-03
[Epoch   22/ 100] [Batch    5/   9] Loss:  1.68e-03
[Epoch   22/ 100] [Batch    6/   9] Loss:  2.19e-03
[Epoch   22/ 100] [Batch    7/   9] Loss:  1.85e-03
[Epoch   22/ 100] [Batch    8/   9] Loss:  1.62e-03
[Epoch   22/ 100] [Batch    9/   9] Loss:  1.66e-03
[Epoch   23/ 100] [Batch    1/   9] Loss:  2.01e-03
[Epoch   23/ 100] [Batch    2/   9] Loss:  1.87e-03
[Epoch   23/ 100] [Batch    3/   9] Loss:  1.80e-03
[Epoch   23/ 100] [Batch    4/   9] Loss:  1.33e-03
[Epoch   23/ 100] [Batch    5/   9] Loss:  1.72e-03
[Epoch   23/ 100] [Batch    6/   9] Loss:  1.43e-03
[Epoch   23/ 100] [Batch    7/   9] Loss:  1.80e-03
[Epoch   23/ 100] [Batch    8/   9] Loss:  1.83e-03
[Epoch   23/ 100] [Batch    9/   9] Loss:  1.86e-03
[Epoch   24/ 100] [Batch    1/   9] Loss:  2.08e-03
[Epoch   24/ 100] [Batch    2/   9] Loss:  1.52e-03
[Epoch   24/ 100] [Batch    3/   9] Loss:  1.72e-03
[Epoch   24/ 100] [Batch    4/   9] Loss:  1.74e-03
[Epoch   24/ 100] [Batch    5/   9] Loss:  1.78e-03
[Epoch   24/ 100] [Batch    6/   9] Loss:  1.81e-03
[Epoch   24/ 100] [Batch    7/   9] Loss:  1.57e-03
[Epoch   24/ 100] [Batch    8/   9] Loss:  1.58e-03
[Epoch   24/ 100] [Batch    9/   9] Loss:  1.82e-03
[Epoch   25/ 100] [Batch    1/   9] Loss:  1.78e-03
[Epoch   25/ 100] [Batch    2/   9] Loss:  1.75e-03
[Epoch   25/ 100] [Batch    3/   9] Loss:  1.81e-03
[Epoch   25/ 100] [Batch    4/   9] Loss:  1.55e-03
[Epoch   25/ 100] [Batch    5/   9] Loss:  1.24e-03
[Epoch   25/ 100] [Batch    6/   9] Loss:  1.57e-03
[Epoch   25/ 100] [Batch    7/   9] Loss:  1.98e-03
[Epoch   25/ 100] [Batch    8/   9] Loss:  2.05e-03
[Epoch   25/ 100] [Batch    9/   9] Loss:  1.98e-03
[Epoch   26/ 100] [Batch    1/   9] Loss:  1.64e-03
[Epoch   26/ 100] [Batch    2/   9] Loss:  2.04e-03
[Epoch   26/ 100] [Batch    3/   9] Loss:  1.53e-03
[Epoch   26/ 100] [Batch    4/   9] Loss:  1.97e-03
[Epoch   26/ 100] [Batch    5/   9] Loss:  1.65e-03
[Epoch   26/ 100] [Batch    6/   9] Loss:  1.75e-03
[Epoch   26/ 100] [Batch    7/   9] Loss:  1.62e-03
[Epoch   26/ 100] [Batch    8/   9] Loss:  1.66e-03
[Epoch   26/ 100] [Batch    9/   9] Loss:  1.81e-03
[Epoch   27/ 100] [Batch    1/   9] Loss:  1.37e-03
[Epoch   27/ 100] [Batch    2/   9] Loss:  2.17e-03
[Epoch   27/ 100] [Batch    3/   9] Loss:  1.57e-03
[Epoch   27/ 100] [Batch    4/   9] Loss:  1.62e-03
[Epoch   27/ 100] [Batch    5/   9] Loss:  1.47e-03
[Epoch   27/ 100] [Batch    6/   9] Loss:  1.50e-03
[Epoch   27/ 100] [Batch    7/   9] Loss:  1.91e-03
[Epoch   27/ 100] [Batch    8/   9] Loss:  2.00e-03
[Epoch   27/ 100] [Batch    9/   9] Loss:  2.39e-03
[Epoch   28/ 100] [Batch    1/   9] Loss:  2.00e-03
[Epoch   28/ 100] [Batch    2/   9] Loss:  1.81e-03
[Epoch   28/ 100] [Batch    3/   9] Loss:  1.81e-03
[Epoch   28/ 100] [Batch    4/   9] Loss:  1.76e-03
[Epoch   28/ 100] [Batch    5/   9] Loss:  1.82e-03
[Epoch   28/ 100] [Batch    6/   9] Loss:  1.30e-03
[Epoch   28/ 100] [Batch    7/   9] Loss:  1.59e-03
[Epoch   28/ 100] [Batch    8/   9] Loss:  1.74e-03
[Epoch   28/ 100] [Batch    9/   9] Loss:  1.77e-03
[Epoch   29/ 100] [Batch    1/   9] Loss:  1.64e-03
[Epoch   29/ 100] [Batch    2/   9] Loss:  1.71e-03
[Epoch   29/ 100] [Batch    3/   9] Loss:  1.41e-03
[Epoch   29/ 100] [Batch    4/   9] Loss:  2.12e-03
[Epoch   29/ 100] [Batch    5/   9] Loss:  1.69e-03
[Epoch   29/ 100] [Batch    6/   9] Loss:  2.01e-03
[Epoch   29/ 100] [Batch    7/   9] Loss:  1.65e-03
[Epoch   29/ 100] [Batch    8/   9] Loss:  1.78e-03
[Epoch   29/ 100] [Batch    9/   9] Loss:  1.22e-03
[Epoch   30/ 100] [Batch    1/   9] Loss:  1.66e-03
[Epoch   30/ 100] [Batch    2/   9] Loss:  1.63e-03
[Epoch   30/ 100] [Batch    3/   9] Loss:  1.93e-03
[Epoch   30/ 100] [Batch    4/   9] Loss:  1.71e-03
[Epoch   30/ 100] [Batch    5/   9] Loss:  1.79e-03
[Epoch   30/ 100] [Batch    6/   9] Loss:  1.68e-03
[Epoch   30/ 100] [Batch    7/   9] Loss:  2.00e-03
[Epoch   30/ 100] [Batch    8/   9] Loss:  1.58e-03
[Epoch   30/ 100] [Batch    9/   9] Loss:  1.22e-03
[Epoch   31/ 100] [Batch    1/   9] Loss:  1.58e-03
[Epoch   31/ 100] [Batch    2/   9] Loss:  1.66e-03
[Epoch   31/ 100] [Batch    3/   9] Loss:  2.10e-03
[Epoch   31/ 100] [Batch    4/   9] Loss:  2.00e-03
[Epoch   31/ 100] [Batch    5/   9] Loss:  1.71e-03
[Epoch   31/ 100] [Batch    6/   9] Loss:  1.94e-03
[Epoch   31/ 100] [Batch    7/   9] Loss:  1.61e-03
[Epoch   31/ 100] [Batch    8/   9] Loss:  1.63e-03
[Epoch   31/ 100] [Batch    9/   9] Loss:  1.45e-03
[Epoch   32/ 100] [Batch    1/   9] Loss:  1.63e-03
[Epoch   32/ 100] [Batch    2/   9] Loss:  1.85e-03
[Epoch   32/ 100] [Batch    3/   9] Loss:  1.60e-03
[Epoch   32/ 100] [Batch    4/   9] Loss:  1.72e-03
[Epoch   32/ 100] [Batch    5/   9] Loss:  1.87e-03
[Epoch   32/ 100] [Batch    6/   9] Loss:  1.90e-03
[Epoch   32/ 100] [Batch    7/   9] Loss:  1.92e-03
[Epoch   32/ 100] [Batch    8/   9] Loss:  1.70e-03
[Epoch   32/ 100] [Batch    9/   9] Loss:  2.04e-03
[Epoch   33/ 100] [Batch    1/   9] Loss:  1.82e-03
[Epoch   33/ 100] [Batch    2/   9] Loss:  1.87e-03
[Epoch   33/ 100] [Batch    3/   9] Loss:  1.80e-03
[Epoch   33/ 100] [Batch    4/   9] Loss:  1.49e-03
[Epoch   33/ 100] [Batch    5/   9] Loss:  1.72e-03
[Epoch   33/ 100] [Batch    6/   9] Loss:  1.94e-03
[Epoch   33/ 100] [Batch    7/   9] Loss:  1.76e-03
[Epoch   33/ 100] [Batch    8/   9] Loss:  1.78e-03
[Epoch   33/ 100] [Batch    9/   9] Loss:  1.33e-03
[Epoch   34/ 100] [Batch    1/   9] Loss:  2.00e-03
[Epoch   34/ 100] [Batch    2/   9] Loss:  1.31e-03
[Epoch   34/ 100] [Batch    3/   9] Loss:  1.92e-03
[Epoch   34/ 100] [Batch    4/   9] Loss:  1.55e-03
[Epoch   34/ 100] [Batch    5/   9] Loss:  1.49e-03
[Epoch   34/ 100] [Batch    6/   9] Loss:  2.07e-03
[Epoch   34/ 100] [Batch    7/   9] Loss:  1.83e-03
[Epoch   34/ 100] [Batch    8/   9] Loss:  1.84e-03
[Epoch   34/ 100] [Batch    9/   9] Loss:  1.66e-03
[Epoch   35/ 100] [Batch    1/   9] Loss:  1.88e-03
[Epoch   35/ 100] [Batch    2/   9] Loss:  1.31e-03
[Epoch   35/ 100] [Batch    3/   9] Loss:  1.72e-03
[Epoch   35/ 100] [Batch    4/   9] Loss:  1.78e-03
[Epoch   35/ 100] [Batch    5/   9] Loss:  1.87e-03
[Epoch   35/ 100] [Batch    6/   9] Loss:  2.08e-03
[Epoch   35/ 100] [Batch    7/   9] Loss:  1.85e-03
[Epoch   35/ 100] [Batch    8/   9] Loss:  1.60e-03
[Epoch   35/ 100] [Batch    9/   9] Loss:  1.96e-03
[Epoch   36/ 100] [Batch    1/   9] Loss:  1.52e-03
[Epoch   36/ 100] [Batch    2/   9] Loss:  1.37e-03
[Epoch   36/ 100] [Batch    3/   9] Loss:  2.02e-03
[Epoch   36/ 100] [Batch    4/   9] Loss:  1.83e-03
[Epoch   36/ 100] [Batch    5/   9] Loss:  1.96e-03
[Epoch   36/ 100] [Batch    6/   9] Loss:  1.78e-03
[Epoch   36/ 100] [Batch    7/   9] Loss:  1.81e-03
[Epoch   36/ 100] [Batch    8/   9] Loss:  1.63e-03
[Epoch   36/ 100] [Batch    9/   9] Loss:  1.66e-03
[Epoch   37/ 100] [Batch    1/   9] Loss:  1.64e-03
[Epoch   37/ 100] [Batch    2/   9] Loss:  1.86e-03
[Epoch   37/ 100] [Batch    3/   9] Loss:  1.71e-03
[Epoch   37/ 100] [Batch    4/   9] Loss:  1.71e-03
[Epoch   37/ 100] [Batch    5/   9] Loss:  1.59e-03
[Epoch   37/ 100] [Batch    6/   9] Loss:  1.93e-03
[Epoch   37/ 100] [Batch    7/   9] Loss:  1.71e-03
[Epoch   37/ 100] [Batch    8/   9] Loss:  1.97e-03
[Epoch   37/ 100] [Batch    9/   9] Loss:  1.56e-03
[Epoch   38/ 100] [Batch    1/   9] Loss:  1.55e-03
[Epoch   38/ 100] [Batch    2/   9] Loss:  1.68e-03
[Epoch   38/ 100] [Batch    3/   9] Loss:  1.62e-03
[Epoch   38/ 100] [Batch    4/   9] Loss:  1.35e-03
[Epoch   38/ 100] [Batch    5/   9] Loss:  1.67e-03
[Epoch   38/ 100] [Batch    6/   9] Loss:  1.66e-03
[Epoch   38/ 100] [Batch    7/   9] Loss:  2.35e-03
[Epoch   38/ 100] [Batch    8/   9] Loss:  1.89e-03
[Epoch   38/ 100] [Batch    9/   9] Loss:  1.95e-03
[Epoch   39/ 100] [Batch    1/   9] Loss:  2.02e-03
[Epoch   39/ 100] [Batch    2/   9] Loss:  2.13e-03
[Epoch   39/ 100] [Batch    3/   9] Loss:  1.46e-03
[Epoch   39/ 100] [Batch    4/   9] Loss:  1.87e-03
[Epoch   39/ 100] [Batch    5/   9] Loss:  1.45e-03
[Epoch   39/ 100] [Batch    6/   9] Loss:  1.79e-03
[Epoch   39/ 100] [Batch    7/   9] Loss:  1.62e-03
[Epoch   39/ 100] [Batch    8/   9] Loss:  1.81e-03
[Epoch   39/ 100] [Batch    9/   9] Loss:  1.30e-03
[Epoch   40/ 100] [Batch    1/   9] Loss:  2.15e-03
[Epoch   40/ 100] [Batch    2/   9] Loss:  1.63e-03
[Epoch   40/ 100] [Batch    3/   9] Loss:  1.49e-03
[Epoch   40/ 100] [Batch    4/   9] Loss:  1.51e-03
[Epoch   40/ 100] [Batch    5/   9] Loss:  1.92e-03
[Epoch   40/ 100] [Batch    6/   9] Loss:  1.85e-03
[Epoch   40/ 100] [Batch    7/   9] Loss:  1.99e-03
[Epoch   40/ 100] [Batch    8/   9] Loss:  1.69e-03
[Epoch   40/ 100] [Batch    9/   9] Loss:  1.49e-03
[Epoch   41/ 100] [Batch    1/   9] Loss:  1.67e-03
[Epoch   41/ 100] [Batch    2/   9] Loss:  1.45e-03
[Epoch   41/ 100] [Batch    3/   9] Loss:  1.81e-03
[Epoch   41/ 100] [Batch    4/   9] Loss:  1.78e-03
[Epoch   41/ 100] [Batch    5/   9] Loss:  1.80e-03
[Epoch   41/ 100] [Batch    6/   9] Loss:  1.85e-03
[Epoch   41/ 100] [Batch    7/   9] Loss:  1.52e-03
[Epoch   41/ 100] [Batch    8/   9] Loss:  1.87e-03
[Epoch   41/ 100] [Batch    9/   9] Loss:  2.05e-03
[Epoch   42/ 100] [Batch    1/   9] Loss:  1.51e-03
[Epoch   42/ 100] [Batch    2/   9] Loss:  1.70e-03
[Epoch   42/ 100] [Batch    3/   9] Loss:  1.75e-03
[Epoch   42/ 100] [Batch    4/   9] Loss:  2.05e-03
[Epoch   42/ 100] [Batch    5/   9] Loss:  1.24e-03
[Epoch   42/ 100] [Batch    6/   9] Loss:  1.93e-03
[Epoch   42/ 100] [Batch    7/   9] Loss:  2.03e-03
[Epoch   42/ 100] [Batch    8/   9] Loss:  1.78e-03
[Epoch   42/ 100] [Batch    9/   9] Loss:  1.48e-03
[Epoch   43/ 100] [Batch    1/   9] Loss:  1.60e-03
[Epoch   43/ 100] [Batch    2/   9] Loss:  1.76e-03
[Epoch   43/ 100] [Batch    3/   9] Loss:  1.84e-03
[Epoch   43/ 100] [Batch    4/   9] Loss:  1.86e-03
[Epoch   43/ 100] [Batch    5/   9] Loss:  1.68e-03
[Epoch   43/ 100] [Batch    6/   9] Loss:  1.67e-03
[Epoch   43/ 100] [Batch    7/   9] Loss:  1.87e-03
[Epoch   43/ 100] [Batch    8/   9] Loss:  1.67e-03
[Epoch   43/ 100] [Batch    9/   9] Loss:  2.14e-03
[Epoch   44/ 100] [Batch    1/   9] Loss:  1.68e-03
[Epoch   44/ 100] [Batch    2/   9] Loss:  2.01e-03
[Epoch   44/ 100] [Batch    3/   9] Loss:  1.79e-03
[Epoch   44/ 100] [Batch    4/   9] Loss:  1.85e-03
[Epoch   44/ 100] [Batch    5/   9] Loss:  1.56e-03
[Epoch   44/ 100] [Batch    6/   9] Loss:  1.48e-03
[Epoch   44/ 100] [Batch    7/   9] Loss:  1.96e-03
[Epoch   44/ 100] [Batch    8/   9] Loss:  1.57e-03
[Epoch   44/ 100] [Batch    9/   9] Loss:  1.57e-03
[Epoch   45/ 100] [Batch    1/   9] Loss:  2.02e-03
[Epoch   45/ 100] [Batch    2/   9] Loss:  1.65e-03
[Epoch   45/ 100] [Batch    3/   9] Loss:  1.67e-03
[Epoch   45/ 100] [Batch    4/   9] Loss:  1.75e-03
[Epoch   45/ 100] [Batch    5/   9] Loss:  2.24e-03
[Epoch   45/ 100] [Batch    6/   9] Loss:  1.75e-03
[Epoch   45/ 100] [Batch    7/   9] Loss:  1.16e-03
[Epoch   45/ 100] [Batch    8/   9] Loss:  1.78e-03
[Epoch   45/ 100] [Batch    9/   9] Loss:  1.39e-03
[Epoch   46/ 100] [Batch    1/   9] Loss:  1.56e-03
[Epoch   46/ 100] [Batch    2/   9] Loss:  1.58e-03
[Epoch   46/ 100] [Batch    3/   9] Loss:  1.70e-03
[Epoch   46/ 100] [Batch    4/   9] Loss:  2.07e-03
[Epoch   46/ 100] [Batch    5/   9] Loss:  1.83e-03
[Epoch   46/ 100] [Batch    6/   9] Loss:  1.92e-03
[Epoch   46/ 100] [Batch    7/   9] Loss:  1.70e-03
[Epoch   46/ 100] [Batch    8/   9] Loss:  1.46e-03
[Epoch   46/ 100] [Batch    9/   9] Loss:  1.89e-03
[Epoch   47/ 100] [Batch    1/   9] Loss:  1.59e-03
[Epoch   47/ 100] [Batch    2/   9] Loss:  1.87e-03
[Epoch   47/ 100] [Batch    3/   9] Loss:  1.84e-03
[Epoch   47/ 100] [Batch    4/   9] Loss:  1.84e-03
[Epoch   47/ 100] [Batch    5/   9] Loss:  1.85e-03
[Epoch   47/ 100] [Batch    6/   9] Loss:  1.84e-03
[Epoch   47/ 100] [Batch    7/   9] Loss:  1.62e-03
[Epoch   47/ 100] [Batch    8/   9] Loss:  1.61e-03
[Epoch   47/ 100] [Batch    9/   9] Loss:  1.35e-03
[Epoch   48/ 100] [Batch    1/   9] Loss:  1.45e-03
[Epoch   48/ 100] [Batch    2/   9] Loss:  1.74e-03
[Epoch   48/ 100] [Batch    3/   9] Loss:  1.62e-03
[Epoch   48/ 100] [Batch    4/   9] Loss:  1.73e-03
[Epoch   48/ 100] [Batch    5/   9] Loss:  1.64e-03
[Epoch   48/ 100] [Batch    6/   9] Loss:  1.82e-03
[Epoch   48/ 100] [Batch    7/   9] Loss:  1.80e-03
[Epoch   48/ 100] [Batch    8/   9] Loss:  1.90e-03
[Epoch   48/ 100] [Batch    9/   9] Loss:  2.12e-03
[Epoch   49/ 100] [Batch    1/   9] Loss:  1.91e-03
[Epoch   49/ 100] [Batch    2/   9] Loss:  1.76e-03
[Epoch   49/ 100] [Batch    3/   9] Loss:  1.54e-03
[Epoch   49/ 100] [Batch    4/   9] Loss:  1.45e-03
[Epoch   49/ 100] [Batch    5/   9] Loss:  1.64e-03
[Epoch   49/ 100] [Batch    6/   9] Loss:  1.84e-03
[Epoch   49/ 100] [Batch    7/   9] Loss:  1.97e-03
[Epoch   49/ 100] [Batch    8/   9] Loss:  1.80e-03
[Epoch   49/ 100] [Batch    9/   9] Loss:  1.71e-03
[Epoch   50/ 100] [Batch    1/   9] Loss:  1.80e-03
[Epoch   50/ 100] [Batch    2/   9] Loss:  1.65e-03
[Epoch   50/ 100] [Batch    3/   9] Loss:  1.68e-03
[Epoch   50/ 100] [Batch    4/   9] Loss:  1.51e-03
[Epoch   50/ 100] [Batch    5/   9] Loss:  1.87e-03
[Epoch   50/ 100] [Batch    6/   9] Loss:  1.54e-03
[Epoch   50/ 100] [Batch    7/   9] Loss:  1.75e-03
[Epoch   50/ 100] [Batch    8/   9] Loss:  2.36e-03
[Epoch   50/ 100] [Batch    9/   9] Loss:  8.08e-04
[Epoch   51/ 100] [Batch    1/   9] Loss:  1.86e-03
[Epoch   51/ 100] [Batch    2/   9] Loss:  1.94e-03
[Epoch   51/ 100] [Batch    3/   9] Loss:  1.91e-03
[Epoch   51/ 100] [Batch    4/   9] Loss:  1.92e-03
[Epoch   51/ 100] [Batch    5/   9] Loss:  1.57e-03
[Epoch   51/ 100] [Batch    6/   9] Loss:  1.59e-03
[Epoch   51/ 100] [Batch    7/   9] Loss:  1.60e-03
[Epoch   51/ 100] [Batch    8/   9] Loss:  1.66e-03
[Epoch   51/ 100] [Batch    9/   9] Loss:  1.46e-03
[Epoch   52/ 100] [Batch    1/   9] Loss:  1.92e-03
[Epoch   52/ 100] [Batch    2/   9] Loss:  1.98e-03
[Epoch   52/ 100] [Batch    3/   9] Loss:  1.76e-03
[Epoch   52/ 100] [Batch    4/   9] Loss:  1.50e-03
[Epoch   52/ 100] [Batch    5/   9] Loss:  2.04e-03
[Epoch   52/ 100] [Batch    6/   9] Loss:  1.72e-03
[Epoch   52/ 100] [Batch    7/   9] Loss:  1.81e-03
[Epoch   52/ 100] [Batch    8/   9] Loss:  1.63e-03
[Epoch   52/ 100] [Batch    9/   9] Loss:  1.69e-03
[Epoch   53/ 100] [Batch    1/   9] Loss:  2.19e-03
[Epoch   53/ 100] [Batch    2/   9] Loss:  1.59e-03
[Epoch   53/ 100] [Batch    3/   9] Loss:  1.54e-03
[Epoch   53/ 100] [Batch    4/   9] Loss:  1.88e-03
[Epoch   53/ 100] [Batch    5/   9] Loss:  1.65e-03
[Epoch   53/ 100] [Batch    6/   9] Loss:  1.78e-03
[Epoch   53/ 100] [Batch    7/   9] Loss:  1.88e-03
[Epoch   53/ 100] [Batch    8/   9] Loss:  1.73e-03
[Epoch   53/ 100] [Batch    9/   9] Loss:  2.00e-03
[Epoch   54/ 100] [Batch    1/   9] Loss:  1.83e-03
[Epoch   54/ 100] [Batch    2/   9] Loss:  2.10e-03
[Epoch   54/ 100] [Batch    3/   9] Loss:  1.81e-03
[Epoch   54/ 100] [Batch    4/   9] Loss:  1.38e-03
[Epoch   54/ 100] [Batch    5/   9] Loss:  1.93e-03
[Epoch   54/ 100] [Batch    6/   9] Loss:  1.86e-03
[Epoch   54/ 100] [Batch    7/   9] Loss:  1.53e-03
[Epoch   54/ 100] [Batch    8/   9] Loss:  1.83e-03
[Epoch   54/ 100] [Batch    9/   9] Loss:  1.85e-03
[Epoch   55/ 100] [Batch    1/   9] Loss:  1.58e-03
[Epoch   55/ 100] [Batch    2/   9] Loss:  1.71e-03
[Epoch   55/ 100] [Batch    3/   9] Loss:  1.86e-03
[Epoch   55/ 100] [Batch    4/   9] Loss:  1.97e-03
[Epoch   55/ 100] [Batch    5/   9] Loss:  1.81e-03
[Epoch   55/ 100] [Batch    6/   9] Loss:  1.70e-03
[Epoch   55/ 100] [Batch    7/   9] Loss:  1.71e-03
[Epoch   55/ 100] [Batch    8/   9] Loss:  1.80e-03
[Epoch   55/ 100] [Batch    9/   9] Loss:  1.57e-03
[Epoch   56/ 100] [Batch    1/   9] Loss:  1.84e-03
[Epoch   56/ 100] [Batch    2/   9] Loss:  1.47e-03
[Epoch   56/ 100] [Batch    3/   9] Loss:  2.13e-03
[Epoch   56/ 100] [Batch    4/   9] Loss:  1.58e-03
[Epoch   56/ 100] [Batch    5/   9] Loss:  1.67e-03
[Epoch   56/ 100] [Batch    6/   9] Loss:  2.05e-03
[Epoch   56/ 100] [Batch    7/   9] Loss:  2.01e-03
[Epoch   56/ 100] [Batch    8/   9] Loss:  1.46e-03
[Epoch   56/ 100] [Batch    9/   9] Loss:  1.75e-03
[Epoch   57/ 100] [Batch    1/   9] Loss:  1.87e-03
[Epoch   57/ 100] [Batch    2/   9] Loss:  2.10e-03
[Epoch   57/ 100] [Batch    3/   9] Loss:  1.61e-03
[Epoch   57/ 100] [Batch    4/   9] Loss:  2.24e-03
[Epoch   57/ 100] [Batch    5/   9] Loss:  1.53e-03
[Epoch   57/ 100] [Batch    6/   9] Loss:  1.88e-03
[Epoch   57/ 100] [Batch    7/   9] Loss:  1.35e-03
[Epoch   57/ 100] [Batch    8/   9] Loss:  1.37e-03
[Epoch   57/ 100] [Batch    9/   9] Loss:  1.91e-03
[Epoch   58/ 100] [Batch    1/   9] Loss:  1.78e-03
[Epoch   58/ 100] [Batch    2/   9] Loss:  1.82e-03
[Epoch   58/ 100] [Batch    3/   9] Loss:  1.79e-03
[Epoch   58/ 100] [Batch    4/   9] Loss:  1.79e-03
[Epoch   58/ 100] [Batch    5/   9] Loss:  1.73e-03
[Epoch   58/ 100] [Batch    6/   9] Loss:  1.82e-03
[Epoch   58/ 100] [Batch    7/   9] Loss:  1.65e-03
[Epoch   58/ 100] [Batch    8/   9] Loss:  1.83e-03
[Epoch   58/ 100] [Batch    9/   9] Loss:  2.53e-03
[Epoch   59/ 100] [Batch    1/   9] Loss:  1.80e-03
[Epoch   59/ 100] [Batch    2/   9] Loss:  1.68e-03
[Epoch   59/ 100] [Batch    3/   9] Loss:  1.99e-03
[Epoch   59/ 100] [Batch    4/   9] Loss:  1.87e-03
[Epoch   59/ 100] [Batch    5/   9] Loss:  2.04e-03
[Epoch   59/ 100] [Batch    6/   9] Loss:  1.53e-03
[Epoch   59/ 100] [Batch    7/   9] Loss:  1.53e-03
[Epoch   59/ 100] [Batch    8/   9] Loss:  1.67e-03
[Epoch   59/ 100] [Batch    9/   9] Loss:  1.86e-03
[Epoch   60/ 100] [Batch    1/   9] Loss:  1.77e-03
[Epoch   60/ 100] [Batch    2/   9] Loss:  1.70e-03
[Epoch   60/ 100] [Batch    3/   9] Loss:  1.63e-03
[Epoch   60/ 100] [Batch    4/   9] Loss:  2.01e-03
[Epoch   60/ 100] [Batch    5/   9] Loss:  1.75e-03
[Epoch   60/ 100] [Batch    6/   9] Loss:  1.71e-03
[Epoch   60/ 100] [Batch    7/   9] Loss:  1.72e-03
[Epoch   60/ 100] [Batch    8/   9] Loss:  1.71e-03
[Epoch   60/ 100] [Batch    9/   9] Loss:  2.32e-03
[Epoch   61/ 100] [Batch    1/   9] Loss:  1.65e-03
[Epoch   61/ 100] [Batch    2/   9] Loss:  1.76e-03
[Epoch   61/ 100] [Batch    3/   9] Loss:  1.39e-03
[Epoch   61/ 100] [Batch    4/   9] Loss:  1.77e-03
[Epoch   61/ 100] [Batch    5/   9] Loss:  1.69e-03
[Epoch   61/ 100] [Batch    6/   9] Loss:  1.55e-03
[Epoch   61/ 100] [Batch    7/   9] Loss:  2.12e-03
[Epoch   61/ 100] [Batch    8/   9] Loss:  1.83e-03
[Epoch   61/ 100] [Batch    9/   9] Loss:  1.89e-03
[Epoch   62/ 100] [Batch    1/   9] Loss:  1.77e-03
[Epoch   62/ 100] [Batch    2/   9] Loss:  2.02e-03
[Epoch   62/ 100] [Batch    3/   9] Loss:  1.62e-03
[Epoch   62/ 100] [Batch    4/   9] Loss:  1.82e-03
[Epoch   62/ 100] [Batch    5/   9] Loss:  1.60e-03
[Epoch   62/ 100] [Batch    6/   9] Loss:  1.94e-03
[Epoch   62/ 100] [Batch    7/   9] Loss:  1.76e-03
[Epoch   62/ 100] [Batch    8/   9] Loss:  1.49e-03
[Epoch   62/ 100] [Batch    9/   9] Loss:  1.75e-03
[Epoch   63/ 100] [Batch    1/   9] Loss:  1.51e-03
[Epoch   63/ 100] [Batch    2/   9] Loss:  1.86e-03
[Epoch   63/ 100] [Batch    3/   9] Loss:  1.70e-03
[Epoch   63/ 100] [Batch    4/   9] Loss:  1.65e-03
[Epoch   63/ 100] [Batch    5/   9] Loss:  1.84e-03
[Epoch   63/ 100] [Batch    6/   9] Loss:  1.75e-03
[Epoch   63/ 100] [Batch    7/   9] Loss:  2.08e-03
[Epoch   63/ 100] [Batch    8/   9] Loss:  2.11e-03
[Epoch   63/ 100] [Batch    9/   9] Loss:  1.47e-03
[Epoch   64/ 100] [Batch    1/   9] Loss:  2.13e-03
[Epoch   64/ 100] [Batch    2/   9] Loss:  1.30e-03
[Epoch   64/ 100] [Batch    3/   9] Loss:  1.74e-03
[Epoch   64/ 100] [Batch    4/   9] Loss:  1.94e-03
[Epoch   64/ 100] [Batch    5/   9] Loss:  1.70e-03
[Epoch   64/ 100] [Batch    6/   9] Loss:  1.87e-03
[Epoch   64/ 100] [Batch    7/   9] Loss:  2.15e-03
[Epoch   64/ 100] [Batch    8/   9] Loss:  2.04e-03
[Epoch   64/ 100] [Batch    9/   9] Loss:  1.74e-03
[Epoch   65/ 100] [Batch    1/   9] Loss:  1.55e-03
[Epoch   65/ 100] [Batch    2/   9] Loss:  1.70e-03
[Epoch   65/ 100] [Batch    3/   9] Loss:  1.52e-03
[Epoch   65/ 100] [Batch    4/   9] Loss:  2.02e-03
[Epoch   65/ 100] [Batch    5/   9] Loss:  1.98e-03
[Epoch   65/ 100] [Batch    6/   9] Loss:  1.65e-03
[Epoch   65/ 100] [Batch    7/   9] Loss:  2.05e-03
[Epoch   65/ 100] [Batch    8/   9] Loss:  2.03e-03
[Epoch   65/ 100] [Batch    9/   9] Loss:  1.87e-03
[Epoch   66/ 100] [Batch    1/   9] Loss:  1.85e-03
[Epoch   66/ 100] [Batch    2/   9] Loss:  1.68e-03
[Epoch   66/ 100] [Batch    3/   9] Loss:  1.85e-03
[Epoch   66/ 100] [Batch    4/   9] Loss:  1.72e-03
[Epoch   66/ 100] [Batch    5/   9] Loss:  1.96e-03
[Epoch   66/ 100] [Batch    6/   9] Loss:  1.49e-03
[Epoch   66/ 100] [Batch    7/   9] Loss:  1.79e-03
[Epoch   66/ 100] [Batch    8/   9] Loss:  1.70e-03
[Epoch   66/ 100] [Batch    9/   9] Loss:  1.82e-03
[Epoch   67/ 100] [Batch    1/   9] Loss:  1.68e-03
[Epoch   67/ 100] [Batch    2/   9] Loss:  1.55e-03
[Epoch   67/ 100] [Batch    3/   9] Loss:  1.80e-03
[Epoch   67/ 100] [Batch    4/   9] Loss:  1.95e-03
[Epoch   67/ 100] [Batch    5/   9] Loss:  1.54e-03
[Epoch   67/ 100] [Batch    6/   9] Loss:  2.22e-03
[Epoch   67/ 100] [Batch    7/   9] Loss:  1.68e-03
[Epoch   67/ 100] [Batch    8/   9] Loss:  1.78e-03
[Epoch   67/ 100] [Batch    9/   9] Loss:  1.86e-03
[Epoch   68/ 100] [Batch    1/   9] Loss:  1.32e-03
[Epoch   68/ 100] [Batch    2/   9] Loss:  1.76e-03
[Epoch   68/ 100] [Batch    3/   9] Loss:  1.66e-03
[Epoch   68/ 100] [Batch    4/   9] Loss:  1.64e-03
[Epoch   68/ 100] [Batch    5/   9] Loss:  2.23e-03
[Epoch   68/ 100] [Batch    6/   9] Loss:  1.81e-03
[Epoch   68/ 100] [Batch    7/   9] Loss:  1.64e-03
[Epoch   68/ 100] [Batch    8/   9] Loss:  1.86e-03
[Epoch   68/ 100] [Batch    9/   9] Loss:  1.76e-03
[Epoch   69/ 100] [Batch    1/   9] Loss:  1.44e-03
[Epoch   69/ 100] [Batch    2/   9] Loss:  1.56e-03
[Epoch   69/ 100] [Batch    3/   9] Loss:  1.82e-03
[Epoch   69/ 100] [Batch    4/   9] Loss:  1.96e-03
[Epoch   69/ 100] [Batch    5/   9] Loss:  1.53e-03
[Epoch   69/ 100] [Batch    6/   9] Loss:  1.84e-03
[Epoch   69/ 100] [Batch    7/   9] Loss:  1.71e-03
[Epoch   69/ 100] [Batch    8/   9] Loss:  1.71e-03
[Epoch   69/ 100] [Batch    9/   9] Loss:  2.58e-03
[Epoch   70/ 100] [Batch    1/   9] Loss:  2.18e-03
[Epoch   70/ 100] [Batch    2/   9] Loss:  1.61e-03
[Epoch   70/ 100] [Batch    3/   9] Loss:  1.88e-03
[Epoch   70/ 100] [Batch    4/   9] Loss:  1.93e-03
[Epoch   70/ 100] [Batch    5/   9] Loss:  1.86e-03
[Epoch   70/ 100] [Batch    6/   9] Loss:  1.74e-03
[Epoch   70/ 100] [Batch    7/   9] Loss:  1.86e-03
[Epoch   70/ 100] [Batch    8/   9] Loss:  1.59e-03
[Epoch   70/ 100] [Batch    9/   9] Loss:  2.09e-03
[Epoch   71/ 100] [Batch    1/   9] Loss:  1.83e-03
[Epoch   71/ 100] [Batch    2/   9] Loss:  1.89e-03
[Epoch   71/ 100] [Batch    3/   9] Loss:  1.55e-03
[Epoch   71/ 100] [Batch    4/   9] Loss:  1.74e-03
[Epoch   71/ 100] [Batch    5/   9] Loss:  1.78e-03
[Epoch   71/ 100] [Batch    6/   9] Loss:  1.91e-03
[Epoch   71/ 100] [Batch    7/   9] Loss:  2.02e-03
[Epoch   71/ 100] [Batch    8/   9] Loss:  1.79e-03
[Epoch   71/ 100] [Batch    9/   9] Loss:  1.65e-03
[Epoch   72/ 100] [Batch    1/   9] Loss:  1.80e-03
[Epoch   72/ 100] [Batch    2/   9] Loss:  1.73e-03
[Epoch   72/ 100] [Batch    3/   9] Loss:  2.09e-03
[Epoch   72/ 100] [Batch    4/   9] Loss:  1.84e-03
[Epoch   72/ 100] [Batch    5/   9] Loss:  1.68e-03
[Epoch   72/ 100] [Batch    6/   9] Loss:  1.85e-03
[Epoch   72/ 100] [Batch    7/   9] Loss:  1.87e-03
[Epoch   72/ 100] [Batch    8/   9] Loss:  1.59e-03
[Epoch   72/ 100] [Batch    9/   9] Loss:  1.94e-03
[Epoch   73/ 100] [Batch    1/   9] Loss:  2.07e-03
[Epoch   73/ 100] [Batch    2/   9] Loss:  1.69e-03
[Epoch   73/ 100] [Batch    3/   9] Loss:  2.02e-03
[Epoch   73/ 100] [Batch    4/   9] Loss:  1.68e-03
[Epoch   73/ 100] [Batch    5/   9] Loss:  1.56e-03
[Epoch   73/ 100] [Batch    6/   9] Loss:  1.83e-03
[Epoch   73/ 100] [Batch    7/   9] Loss:  1.81e-03
[Epoch   73/ 100] [Batch    8/   9] Loss:  1.44e-03
[Epoch   73/ 100] [Batch    9/   9] Loss:  1.53e-03
[Epoch   74/ 100] [Batch    1/   9] Loss:  1.75e-03
[Epoch   74/ 100] [Batch    2/   9] Loss:  1.71e-03
[Epoch   74/ 100] [Batch    3/   9] Loss:  1.50e-03
[Epoch   74/ 100] [Batch    4/   9] Loss:  1.88e-03
[Epoch   74/ 100] [Batch    5/   9] Loss:  1.75e-03
[Epoch   74/ 100] [Batch    6/   9] Loss:  1.39e-03
[Epoch   74/ 100] [Batch    7/   9] Loss:  2.07e-03
[Epoch   74/ 100] [Batch    8/   9] Loss:  1.89e-03
[Epoch   74/ 100] [Batch    9/   9] Loss:  1.54e-03
[Epoch   75/ 100] [Batch    1/   9] Loss:  1.93e-03
[Epoch   75/ 100] [Batch    2/   9] Loss:  1.95e-03
[Epoch   75/ 100] [Batch    3/   9] Loss:  1.52e-03
[Epoch   75/ 100] [Batch    4/   9] Loss:  1.85e-03
[Epoch   75/ 100] [Batch    5/   9] Loss:  2.12e-03
[Epoch   75/ 100] [Batch    6/   9] Loss:  1.65e-03
[Epoch   75/ 100] [Batch    7/   9] Loss:  1.65e-03
[Epoch   75/ 100] [Batch    8/   9] Loss:  1.70e-03
[Epoch   75/ 100] [Batch    9/   9] Loss:  2.04e-03
[Epoch   76/ 100] [Batch    1/   9] Loss:  1.57e-03
[Epoch   76/ 100] [Batch    2/   9] Loss:  1.87e-03
[Epoch   76/ 100] [Batch    3/   9] Loss:  1.84e-03
[Epoch   76/ 100] [Batch    4/   9] Loss:  1.65e-03
[Epoch   76/ 100] [Batch    5/   9] Loss:  1.72e-03
[Epoch   76/ 100] [Batch    6/   9] Loss:  1.61e-03
[Epoch   76/ 100] [Batch    7/   9] Loss:  1.93e-03
[Epoch   76/ 100] [Batch    8/   9] Loss:  1.72e-03
[Epoch   76/ 100] [Batch    9/   9] Loss:  1.57e-03
[Epoch   77/ 100] [Batch    1/   9] Loss:  2.35e-03
[Epoch   77/ 100] [Batch    2/   9] Loss:  1.61e-03
[Epoch   77/ 100] [Batch    3/   9] Loss:  2.19e-03
[Epoch   77/ 100] [Batch    4/   9] Loss:  1.55e-03
[Epoch   77/ 100] [Batch    5/   9] Loss:  1.73e-03
[Epoch   77/ 100] [Batch    6/   9] Loss:  1.72e-03
[Epoch   77/ 100] [Batch    7/   9] Loss:  1.70e-03
[Epoch   77/ 100] [Batch    8/   9] Loss:  1.89e-03
[Epoch   77/ 100] [Batch    9/   9] Loss:  1.17e-03
[Epoch   78/ 100] [Batch    1/   9] Loss:  1.64e-03
[Epoch   78/ 100] [Batch    2/   9] Loss:  1.51e-03
[Epoch   78/ 100] [Batch    3/   9] Loss:  1.75e-03
[Epoch   78/ 100] [Batch    4/   9] Loss:  1.83e-03
[Epoch   78/ 100] [Batch    5/   9] Loss:  1.95e-03
[Epoch   78/ 100] [Batch    6/   9] Loss:  1.80e-03
[Epoch   78/ 100] [Batch    7/   9] Loss:  1.69e-03
[Epoch   78/ 100] [Batch    8/   9] Loss:  1.70e-03
[Epoch   78/ 100] [Batch    9/   9] Loss:  2.29e-03
[Epoch   79/ 100] [Batch    1/   9] Loss:  1.82e-03
[Epoch   79/ 100] [Batch    2/   9] Loss:  1.57e-03
[Epoch   79/ 100] [Batch    3/   9] Loss:  1.80e-03
[Epoch   79/ 100] [Batch    4/   9] Loss:  2.26e-03
[Epoch   79/ 100] [Batch    5/   9] Loss:  1.48e-03
[Epoch   79/ 100] [Batch    6/   9] Loss:  1.86e-03
[Epoch   79/ 100] [Batch    7/   9] Loss:  1.80e-03
[Epoch   79/ 100] [Batch    8/   9] Loss:  1.82e-03
[Epoch   79/ 100] [Batch    9/   9] Loss:  1.75e-03
[Epoch   80/ 100] [Batch    1/   9] Loss:  1.56e-03
[Epoch   80/ 100] [Batch    2/   9] Loss:  1.76e-03
[Epoch   80/ 100] [Batch    3/   9] Loss:  1.56e-03
[Epoch   80/ 100] [Batch    4/   9] Loss:  1.96e-03
[Epoch   80/ 100] [Batch    5/   9] Loss:  1.93e-03
[Epoch   80/ 100] [Batch    6/   9] Loss:  1.80e-03
[Epoch   80/ 100] [Batch    7/   9] Loss:  2.17e-03
[Epoch   80/ 100] [Batch    8/   9] Loss:  1.25e-03
[Epoch   80/ 100] [Batch    9/   9] Loss:  1.64e-03
[Epoch   81/ 100] [Batch    1/   9] Loss:  1.79e-03
[Epoch   81/ 100] [Batch    2/   9] Loss:  2.16e-03
[Epoch   81/ 100] [Batch    3/   9] Loss:  1.55e-03
[Epoch   81/ 100] [Batch    4/   9] Loss:  1.80e-03
[Epoch   81/ 100] [Batch    5/   9] Loss:  1.57e-03
[Epoch   81/ 100] [Batch    6/   9] Loss:  1.85e-03
[Epoch   81/ 100] [Batch    7/   9] Loss:  1.95e-03
[Epoch   81/ 100] [Batch    8/   9] Loss:  1.62e-03
[Epoch   81/ 100] [Batch    9/   9] Loss:  2.13e-03
[Epoch   82/ 100] [Batch    1/   9] Loss:  1.78e-03
[Epoch   82/ 100] [Batch    2/   9] Loss:  1.75e-03
[Epoch   82/ 100] [Batch    3/   9] Loss:  2.16e-03
[Epoch   82/ 100] [Batch    4/   9] Loss:  1.65e-03
[Epoch   82/ 100] [Batch    5/   9] Loss:  1.90e-03
[Epoch   82/ 100] [Batch    6/   9] Loss:  1.55e-03
[Epoch   82/ 100] [Batch    7/   9] Loss:  1.89e-03
[Epoch   82/ 100] [Batch    8/   9] Loss:  2.18e-03
[Epoch   82/ 100] [Batch    9/   9] Loss:  1.64e-03
[Epoch   83/ 100] [Batch    1/   9] Loss:  1.71e-03
[Epoch   83/ 100] [Batch    2/   9] Loss:  1.64e-03
[Epoch   83/ 100] [Batch    3/   9] Loss:  1.76e-03
[Epoch   83/ 100] [Batch    4/   9] Loss:  2.15e-03
[Epoch   83/ 100] [Batch    5/   9] Loss:  1.87e-03
[Epoch   83/ 100] [Batch    6/   9] Loss:  1.97e-03
[Epoch   83/ 100] [Batch    7/   9] Loss:  2.18e-03
[Epoch   83/ 100] [Batch    8/   9] Loss:  1.67e-03
[Epoch   83/ 100] [Batch    9/   9] Loss:  1.89e-03
[Epoch   84/ 100] [Batch    1/   9] Loss:  1.87e-03
[Epoch   84/ 100] [Batch    2/   9] Loss:  1.42e-03
[Epoch   84/ 100] [Batch    3/   9] Loss:  1.64e-03
[Epoch   84/ 100] [Batch    4/   9] Loss:  2.09e-03
[Epoch   84/ 100] [Batch    5/   9] Loss:  1.74e-03
[Epoch   84/ 100] [Batch    6/   9] Loss:  1.53e-03
[Epoch   84/ 100] [Batch    7/   9] Loss:  1.68e-03
[Epoch   84/ 100] [Batch    8/   9] Loss:  1.72e-03
[Epoch   84/ 100] [Batch    9/   9] Loss:  3.11e-03
[Epoch   85/ 100] [Batch    1/   9] Loss:  1.86e-03
[Epoch   85/ 100] [Batch    2/   9] Loss:  1.87e-03
[Epoch   85/ 100] [Batch    3/   9] Loss:  1.35e-03
[Epoch   85/ 100] [Batch    4/   9] Loss:  1.61e-03
[Epoch   85/ 100] [Batch    5/   9] Loss:  1.98e-03
[Epoch   85/ 100] [Batch    6/   9] Loss:  1.72e-03
[Epoch   85/ 100] [Batch    7/   9] Loss:  1.94e-03
[Epoch   85/ 100] [Batch    8/   9] Loss:  2.06e-03
[Epoch   85/ 100] [Batch    9/   9] Loss:  1.38e-03
[Epoch   86/ 100] [Batch    1/   9] Loss:  1.84e-03
[Epoch   86/ 100] [Batch    2/   9] Loss:  1.59e-03
[Epoch   86/ 100] [Batch    3/   9] Loss:  1.68e-03
[Epoch   86/ 100] [Batch    4/   9] Loss:  2.01e-03
[Epoch   86/ 100] [Batch    5/   9] Loss:  1.49e-03
[Epoch   86/ 100] [Batch    6/   9] Loss:  1.81e-03
[Epoch   86/ 100] [Batch    7/   9] Loss:  1.67e-03
[Epoch   86/ 100] [Batch    8/   9] Loss:  2.21e-03
[Epoch   86/ 100] [Batch    9/   9] Loss:  1.89e-03
[Epoch   87/ 100] [Batch    1/   9] Loss:  1.80e-03
[Epoch   87/ 100] [Batch    2/   9] Loss:  1.71e-03
[Epoch   87/ 100] [Batch    3/   9] Loss:  2.06e-03
[Epoch   87/ 100] [Batch    4/   9] Loss:  1.50e-03
[Epoch   87/ 100] [Batch    5/   9] Loss:  1.85e-03
[Epoch   87/ 100] [Batch    6/   9] Loss:  1.65e-03
[Epoch   87/ 100] [Batch    7/   9] Loss:  1.81e-03
[Epoch   87/ 100] [Batch    8/   9] Loss:  1.83e-03
[Epoch   87/ 100] [Batch    9/   9] Loss:  1.51e-03
[Epoch   88/ 100] [Batch    1/   9] Loss:  1.69e-03
[Epoch   88/ 100] [Batch    2/   9] Loss:  1.81e-03
[Epoch   88/ 100] [Batch    3/   9] Loss:  1.71e-03
[Epoch   88/ 100] [Batch    4/   9] Loss:  1.59e-03
[Epoch   88/ 100] [Batch    5/   9] Loss:  2.03e-03
[Epoch   88/ 100] [Batch    6/   9] Loss:  1.89e-03
[Epoch   88/ 100] [Batch    7/   9] Loss:  2.03e-03
[Epoch   88/ 100] [Batch    8/   9] Loss:  1.80e-03
[Epoch   88/ 100] [Batch    9/   9] Loss:  2.02e-03
[Epoch   89/ 100] [Batch    1/   9] Loss:  1.83e-03
[Epoch   89/ 100] [Batch    2/   9] Loss:  2.28e-03
[Epoch   89/ 100] [Batch    3/   9] Loss:  1.93e-03
[Epoch   89/ 100] [Batch    4/   9] Loss:  1.56e-03
[Epoch   89/ 100] [Batch    5/   9] Loss:  2.68e-03
[Epoch   89/ 100] [Batch    6/   9] Loss:  2.57e-03
[Epoch   89/ 100] [Batch    7/   9] Loss:  1.77e-03
[Epoch   89/ 100] [Batch    8/   9] Loss:  1.53e-03
[Epoch   89/ 100] [Batch    9/   9] Loss:  2.28e-03
[Epoch   90/ 100] [Batch    1/   9] Loss:  2.49e-03
[Epoch   90/ 100] [Batch    2/   9] Loss:  1.87e-03
[Epoch   90/ 100] [Batch    3/   9] Loss:  2.21e-03
[Epoch   90/ 100] [Batch    4/   9] Loss:  2.30e-03
[Epoch   90/ 100] [Batch    5/   9] Loss:  1.77e-03
[Epoch   90/ 100] [Batch    6/   9] Loss:  2.09e-03
[Epoch   90/ 100] [Batch    7/   9] Loss:  2.49e-03
[Epoch   90/ 100] [Batch    8/   9] Loss:  1.99e-03
[Epoch   90/ 100] [Batch    9/   9] Loss:  1.49e-03
[Epoch   91/ 100] [Batch    1/   9] Loss:  2.34e-03
[Epoch   91/ 100] [Batch    2/   9] Loss:  2.27e-03
[Epoch   91/ 100] [Batch    3/   9] Loss:  1.84e-03
[Epoch   91/ 100] [Batch    4/   9] Loss:  1.82e-03
[Epoch   91/ 100] [Batch    5/   9] Loss:  1.95e-03
[Epoch   91/ 100] [Batch    6/   9] Loss:  1.66e-03
[Epoch   91/ 100] [Batch    7/   9] Loss:  1.55e-03
[Epoch   91/ 100] [Batch    8/   9] Loss:  1.95e-03
[Epoch   91/ 100] [Batch    9/   9] Loss:  2.48e-03
[Epoch   92/ 100] [Batch    1/   9] Loss:  1.48e-03
[Epoch   92/ 100] [Batch    2/   9] Loss:  1.79e-03
[Epoch   92/ 100] [Batch    3/   9] Loss:  1.99e-03
[Epoch   92/ 100] [Batch    4/   9] Loss:  2.00e-03
[Epoch   92/ 100] [Batch    5/   9] Loss:  1.51e-03
[Epoch   92/ 100] [Batch    6/   9] Loss:  1.92e-03
[Epoch   92/ 100] [Batch    7/   9] Loss:  1.53e-03
[Epoch   92/ 100] [Batch    8/   9] Loss:  2.25e-03
[Epoch   92/ 100] [Batch    9/   9] Loss:  2.00e-03
[Epoch   93/ 100] [Batch    1/   9] Loss:  1.65e-03
[Epoch   93/ 100] [Batch    2/   9] Loss:  1.57e-03
[Epoch   93/ 100] [Batch    3/   9] Loss:  2.12e-03
[Epoch   93/ 100] [Batch    4/   9] Loss:  1.93e-03
[Epoch   93/ 100] [Batch    5/   9] Loss:  1.52e-03
[Epoch   93/ 100] [Batch    6/   9] Loss:  2.08e-03
[Epoch   93/ 100] [Batch    7/   9] Loss:  1.97e-03
[Epoch   93/ 100] [Batch    8/   9] Loss:  1.82e-03
[Epoch   93/ 100] [Batch    9/   9] Loss:  9.41e-04
[Epoch   94/ 100] [Batch    1/   9] Loss:  1.92e-03
[Epoch   94/ 100] [Batch    2/   9] Loss:  1.82e-03
[Epoch   94/ 100] [Batch    3/   9] Loss:  1.91e-03
[Epoch   94/ 100] [Batch    4/   9] Loss:  1.88e-03
[Epoch   94/ 100] [Batch    5/   9] Loss:  1.46e-03
[Epoch   94/ 100] [Batch    6/   9] Loss:  1.77e-03
[Epoch   94/ 100] [Batch    7/   9] Loss:  1.75e-03
[Epoch   94/ 100] [Batch    8/   9] Loss:  1.80e-03
[Epoch   94/ 100] [Batch    9/   9] Loss:  1.41e-03
[Epoch   95/ 100] [Batch    1/   9] Loss:  1.70e-03
[Epoch   95/ 100] [Batch    2/   9] Loss:  1.91e-03
[Epoch   95/ 100] [Batch    3/   9] Loss:  1.47e-03
[Epoch   95/ 100] [Batch    4/   9] Loss:  1.62e-03
[Epoch   95/ 100] [Batch    5/   9] Loss:  1.66e-03
[Epoch   95/ 100] [Batch    6/   9] Loss:  1.85e-03
[Epoch   95/ 100] [Batch    7/   9] Loss:  1.77e-03
[Epoch   95/ 100] [Batch    8/   9] Loss:  1.95e-03
[Epoch   95/ 100] [Batch    9/   9] Loss:  1.80e-03
[Epoch   96/ 100] [Batch    1/   9] Loss:  1.58e-03
[Epoch   96/ 100] [Batch    2/   9] Loss:  1.53e-03
[Epoch   96/ 100] [Batch    3/   9] Loss:  1.74e-03
[Epoch   96/ 100] [Batch    4/   9] Loss:  2.01e-03
[Epoch   96/ 100] [Batch    5/   9] Loss:  1.72e-03
[Epoch   96/ 100] [Batch    6/   9] Loss:  2.08e-03
[Epoch   96/ 100] [Batch    7/   9] Loss:  1.58e-03
[Epoch   96/ 100] [Batch    8/   9] Loss:  1.89e-03
[Epoch   96/ 100] [Batch    9/   9] Loss:  1.27e-03
[Epoch   97/ 100] [Batch    1/   9] Loss:  1.45e-03
[Epoch   97/ 100] [Batch    2/   9] Loss:  1.98e-03
[Epoch   97/ 100] [Batch    3/   9] Loss:  1.87e-03
[Epoch   97/ 100] [Batch    4/   9] Loss:  2.47e-03
[Epoch   97/ 100] [Batch    5/   9] Loss:  1.60e-03
[Epoch   97/ 100] [Batch    6/   9] Loss:  1.59e-03
[Epoch   97/ 100] [Batch    7/   9] Loss:  1.62e-03
[Epoch   97/ 100] [Batch    8/   9] Loss:  1.92e-03
[Epoch   97/ 100] [Batch    9/   9] Loss:  1.59e-03
[Epoch   98/ 100] [Batch    1/   9] Loss:  1.25e-03
[Epoch   98/ 100] [Batch    2/   9] Loss:  1.97e-03
[Epoch   98/ 100] [Batch    3/   9] Loss:  1.96e-03
[Epoch   98/ 100] [Batch    4/   9] Loss:  2.04e-03
[Epoch   98/ 100] [Batch    5/   9] Loss:  2.22e-03
[Epoch   98/ 100] [Batch    6/   9] Loss:  1.89e-03
[Epoch   98/ 100] [Batch    7/   9] Loss:  1.63e-03
[Epoch   98/ 100] [Batch    8/   9] Loss:  1.89e-03
[Epoch   98/ 100] [Batch    9/   9] Loss:  2.40e-03
[Epoch   99/ 100] [Batch    1/   9] Loss:  1.76e-03
[Epoch   99/ 100] [Batch    2/   9] Loss:  1.98e-03
[Epoch   99/ 100] [Batch    3/   9] Loss:  1.96e-03
[Epoch   99/ 100] [Batch    4/   9] Loss:  1.71e-03
[Epoch   99/ 100] [Batch    5/   9] Loss:  1.70e-03
[Epoch   99/ 100] [Batch    6/   9] Loss:  1.49e-03
[Epoch   99/ 100] [Batch    7/   9] Loss:  1.74e-03
[Epoch   99/ 100] [Batch    8/   9] Loss:  1.91e-03
[Epoch   99/ 100] [Batch    9/   9] Loss:  1.04e-03
[Epoch  100/ 100] [Batch    1/   9] Loss:  1.68e-03
[Epoch  100/ 100] [Batch    2/   9] Loss:  1.81e-03
[Epoch  100/ 100] [Batch    3/   9] Loss:  1.40e-03
[Epoch  100/ 100] [Batch    4/   9] Loss:  1.87e-03
[Epoch  100/ 100] [Batch    5/   9] Loss:  1.91e-03
[Epoch  100/ 100] [Batch    6/   9] Loss:  1.88e-03
[Epoch  100/ 100] [Batch    7/   9] Loss:  1.72e-03
[Epoch  100/ 100] [Batch    8/   9] Loss:  2.13e-03
[Epoch  100/ 100] [Batch    9/   9] Loss:  1.90e-03
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LinearModel(
  (linear): Linear(in_features=1, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">input_dim</span> <span class="o">==</span> <span class="n">output_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">n_test</span> <span class="o">=</span> <span class="mi">500</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_test</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">])):</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;linear.weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">][</span><span class="n">idx</span><span class="p">]]])</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;linear.bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">][</span><span class="n">idx</span><span class="p">]])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="n">y_pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;f&#39;</span><span class="p">)))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>    
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">input_dim</span> <span class="o">==</span> <span class="n">output_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
    
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">levels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">loss_values</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">loss_values</span><span class="p">)),</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">ww</span><span class="p">,</span> <span class="n">bb</span><span class="p">,</span> <span class="n">loss_values</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">levels</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">colors</span><span class="o">.</span><span class="n">LogNorm</span><span class="p">())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">],</span> <span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">],</span> <span class="s1">&#39;.-b&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">b</span><span class="p">,</span> <span class="s1">&#39;r*&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;optim&#39;</span><span class="p">,</span> <span class="s1">&#39;(W, b)&#39;</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> 
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> 
    
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iter&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="n">a</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">y_pred</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_color</span><span class="p">())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>   

    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="k">else</span><span class="p">:</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">train_hist</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iter&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/T632722_PyTorch_Fundamentals_Part_1_182_0.png" src="../_images/T632722_PyTorch_Fundamentals_Part_1_182_0.png" />
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./nbs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="T966055_OBP_Library_Workshop_Tutorials.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">OBP Library Workshop Tutorials</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="T472467_PyTorch_Fundamentals_Part_2.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">PyTorch Fundamentals Part 2</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Sparsh A.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>