
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Evaluation of Multiple Off-Policy Estimators on Synthetic Dataset &#8212; Reco Book</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset" href="T874693_Evaluating_Standard_Off_Policy_Estimators_with_Small_Sample_Open_Bandit_Dataset.html" />
    <link rel="prev" title="Evaluating the Robustness of Off-Policy Evaluation" href="T902666_Evaluating_the_Robustness_of_Off_Policy_Evaluation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Reco Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Tutorials in Jupyter notebook format
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  User Stories
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="US780867_Transformer_based_Recommenders.html">
   Transformer-based Recommenders
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="T034923_BERT4Rec_on_ML1M_in_PyTorch.html">
     BERT4Rec on ML-1M in PyTorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T595874_BERT4Rec_on_ML25M_in_PyTorch_Lightning.html">
     BERT4Rec on ML-25M
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T088416_BST_Implementation_in_MXNet.html">
     BST Implementation in MXNet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T602245_BST_implementation_in_PyTorch.html">
     BST implementation in PyTorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T007665_BST_on_ML1M_in_Keras.html">
     A Transformer-based recommendation system
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T881207_BST_PTLightning_ML1M.html">
     Rating prediction using the Behavior Sequence Transformer (BST) model on ML-1M dataset in PyTorch Lightning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T025247_BST_using_Deepctr_library.html">
     BST using Deepctr library
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T757997_SASRec_PyTorch.html">
     SASRec implementation with PyTorch Library
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T225287_SASRec_PaddlePaddle.html">
     SASRec implementation with Paddle Library
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T701627_SR_SAN_Session_based_Model.html">
     SR-SAN Session-based Recommender
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T975104_SSEPT_ML1M_Tensorflow1x.html">
     SSE-PT Personalized Transformer Recommender on ML-1M in Tensorflow 1.x
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T472955_GCSAN_Session_based_Model.html">
     GCSAN Session-based Recommender
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T970274_Transformers4Rec_Session_based_Recommender_on_Yoochoose.html">
     End-to-end session-based recommendation with Transformers4Rec
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T382183_Transformers4Rec_XLNet_on_Synthetic_data.html">
     Transformers4Rec XLNet on Synthetic data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="T793395_Session_based_recommendation_on_REES46_Dataset.html">
     End-to-end Session-based recommendation on REES46 Dataset
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Prototypes
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="T382881_DeepWalk_Karateclub.html">
   DeepWalk from scratch referencing Karateclub library
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T384270_DeepWalk_pure_python.html">
   DeepWalk in pure python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T677598_Jaccard_Cosine_SVD_DeepWalk_ML100K.html">
   Recommender System with DeepWalk Graph Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T815556_Node2vec_Karateclub.html">
   Node2vec from scratch referencing Karateclub library
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T894941_Node2vec_MovieLens_Keras.html">
   Graph representation learning with node2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T611050_Node2vec_PyG.html">
   Node2vec from scratch in PyTorch Geometric
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T186367_Node2vec_library.html">
   Node2vec from scratch referencing node2vec library
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T331379_bayesian_personalized_ranking.html">
   BPR from scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T081831_Data_Poisoning_Attacks_on_Factorization_Based_Collaborative_Filtering.html">
   Data Poisoning Attacks on Factorization-Based Collaborative Filtering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T102448_Adversarial_Learning_for_Recommendation.html">
   Adversarial Training (Regularization) on a Recommender System
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T865035_Simulating_Data_Poisoning_Attacks_against_Twitter_Recommender.html">
   Load and process dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T711285_Data_Poisoning_Attack_using_LFM_and_ItemAE_on_Synthetic_Dataset.html">
   Injection attack using LFM and ItemAE model trained on Toy dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T355514_Black_box_Attack_on_Sequential_Recs.html">
   Black-box Attack on Sequential Recs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T873451_Statistics_fundamentals.html">
   Statictics Fundamentals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T890478_Batch_Learning_from_Bandit_Feedback_%28BLBF%29.html">
   Imports
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T257798_Off_Policy_Learning_in_Two_stage_Recommender_Systems.html">
   Off-Policy Learning in Two-stage Recommender Systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T471827_Adaptive_Estimator_Selection_for_Off_Policy_Evaluation.html">
   Adaptive Estimator Selection for Off-Policy Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T902666_Evaluating_the_Robustness_of_Off_Policy_Evaluation.html">
   Evaluating the Robustness of Off-Policy Evaluation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Evaluation of Multiple Off-Policy Estimators on Synthetic Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T874693_Evaluating_Standard_Off_Policy_Estimators_with_Small_Sample_Open_Bandit_Dataset.html">
   Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T792262_Optimal_Off_Policy_Evaluation_from_Multiple_Logging_Policies.html">
   Optimal Off-Policy Evaluation from Multiple Logging Policies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T167249_Offline_Policy_Evaluation_with_VW_Command_Line.html">
   Offline Policy Evaluation with VW Command Line
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T966055_OBP_Library_Workshop_Tutorials.html">
   OBP Library Workshop Tutorials
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T632722_PyTorch_Fundamentals_Part_1.html">
   PyTorch Fundamentals Part 1
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T472467_PyTorch_Fundamentals_Part_2.html">
   PyTorch Fundamentals Part 2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T206654_PyTorch_Fundamentals_Part_3.html">
   PyTorch Fundamentals Part 3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T536348_Attention_Mechanisms.html">
   Imports
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T500796_Agricultural_Satellite_Image_Segmentation.html">
   Agricultural Satellite Image Segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T611432_Image_Analysis_with_Tensorflow.html">
   Image Analysis with Tensorflow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T925716_MongoDB_to_CSV_Conversion.html">
   MongoDB to CSV conversion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T396469_PDF_to_Word_Cloud_via_Email.html">
   PDF to WordCloud via Email
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T030890_Job_Scraping_and_Clustering.html">
   Job scraping and clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T897054_Scene_Text_Recognition.html">
   Scene Text Recognition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T034809_Large_scale_Document_Retrieval_with_Elastic_Search.html">
   Large-scale Document Retrieval with ElasticSearch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T467251_vowpal_wabbit_contextual_recommender.html">
   Simulating a news personalization scenario using Contextual Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T686684_similar_product_recommender.html">
   Similar Product Recommender system using Deep Learning for an online e-commerce store
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T132203_Retail_Product_Recommendations_using_Word2vec.html">
   Retail Product Recommendations using word2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T501828_Recommender_Implicit_Negative_Feedback.html">
   Retail Product Recommendation with Negative Implicit Feedback
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T315965_Sequence_Aware_Recommenders_Music.html">
   Sequence Aware Recommender Systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T051777_image_similarity_recommendations.html">
   Similar Product Recommendations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990172_recobook_diversity_aware_book_recommender.html">
   Diversity Aware Book Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T488549_Goodreads_Diversity_Aware_Book_Recommender.html">
   Diversity Aware Book Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T023535_Kafka_MongoDB_Real_time_Streaming.html">
   Kafka MongoDB Real-time Streaming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T622304_Session_based_Recommender_Using_Word2vec.html">
   Session-based recommendation using word2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T416854_bandit_based_recommender_using_thompson_sampling_app.html">
   Bandit-based Online Learning using Thompson Sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T198578_Booking_dot_com_Trip_Recommendation.html">
   Booking.com Trip Recommendation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T519734_Vowpal_Wabbit_Contextual_Bandit.html">
   Vowpal Wabbit Contextual Bandit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T871537_Recommendation_Systems_using_Olist_Dataset.html">
   Recommendation systems using Olist dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T057885_Offline_Replayer_Evaluation.html">
   Offline Replayer Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T915054_method_for_effective_online_testing.html">
   Methods for effective online testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T513987_Recsys_2020_Feature_Engineering_Tutorial.html">
   Recsys’20 Feature Engineering Tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T227901_amazon_personalize_batch_job.html">
   Amazon Personalize Batch Job
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T022961_Amazon_Personalize_Workshop.html">
   Amazon Personalize Workshop
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T424437_Collaborative_Filtering_on_ML_latest_small.html">
   Collaborative Filtering on ML-latest-small
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T539160_Building_and_Deploying_ASOS_Fashion_Recommender.html">
   Building and deploying ASOS fashion recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T757697_Simple_Similarity_based_Recommender.html">
   Simple Similarity based Recommmendations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T051594_Analytics_Zoo.html">
   Analytics Zoo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T313645_A_B_Testing.html">
   A/B Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T475711_PinSage_Graph_based_Recommender.html">
   PinSage Graph-based Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T516490_Graph_Embeddings.html">
   Learn Embeddings using Graph Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T822164_movielens_milvus_redis_efficient_retrieval.html">
   Recommender with Redis and Milvus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T845186_Anime_Recommender.html">
   RekoNet Anime Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T855843_kafka_spark_streaming_colab.html">
   Kafka and Spark Streaming in Colab
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T460437_Building_Models_From_Scratch.html">
   Building Models from scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T855971_Conet_Model_for_Movie_Recommender.html">
   CoNet model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T996996_content_based_and_collaborative_movielens.html">
   Movie Recommendation with Content-Based and Collaborative Filtering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T239418_Simple_Movie_Recommenders.html">
   Simple Movie Recommenders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T138337_Simple_Movie_Recommender.html">
   Simple movie recommender in implicit, explicit, and cold-start settings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T935440_The_importance_of_Rating_Normalization.html">
   The importance of Rating Normalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T612622_cornac_examples.html">
   Cornac Examples
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T561435_Flower_Classification.html">
   Flower classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T680910_Trivago_Session_based_Recommender.html">
   Trivago Session-based Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_ads_selection_using_bandits.html">
   Best Ads detection using bandit methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_book_crossing_surprise_svd_nmf.html">
   Book-Crossing Recommendation System
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_book_recommender_kubeflow.html">
   Books recommendations with Kubeflow Pipelines on Scaleway Kapsule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_build_a_kubeflow_pipeline.html">
   Build a Kubeflow Pipeline
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_causal_inference.html">
   Causal Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_concept_embedding_nlp.html">
   Exploring Word Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_concept_neural_net.html">
   Neural Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_concept_nlp_basics.html">
   Natural Language Processing 101
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_concept_transformer_lm.html">
   TransformerLM Quick Start and Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_content_based_music_recommender_lyricsfreak.html">
   Content-based method for song recommendation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_evaluation_metrics_basics.html">
   Recommender System Evaluations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_implicit_synthetic.html">
   Comparing Implicit Models on Synthetic Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_movie_recommender_tensorflow.html">
   Recommendation Systems with TensorFlow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_movie_recommender_tensorflow_sagemaker.html">
   Movie recommender using Tensorflow in Sagemaker
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_movielens_eda_modeling.html">
   Movielens EDA and Modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_read_data_from_cassandra_into_pandas.html">
   Read Cassandra Data Snapshot as DataFrame
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_rl_in_action.html">
   Reinforcement Learning fundamentals in action
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_rnn_cnn_basics.html">
   Processing sequences using RNNs and CNNs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_tf_serving_in_action.html">
   TF Serving in action
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990000_training_indexing_movie_recommender.html">
   Training and indexing movie recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T207114_EduRec_MOOCCube_Course_Recommender.html">
   EduRec MOOCCube Course Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T273184_Multi_Task_Learning.html">
   Multi-task Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T661108_Book_Recommender_API.html">
   Book Recommender API
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T912764_Simple_Movie_Recommender_App.html">
   Simple Movie Recommender App
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T964554_Career_Village_Questions_Recommendation.html">
   CareerVillage Questions Recommendation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_amazon_women_apparel_tfidf_word2vec.html">
   Amazon Product Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_anime_recommender_graph_network.html">
   Anime Recommender with Bi-partite Graph Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_concept_self_attention.html">
   Self-Attention
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_course_recommender_svd_flask.html">
   Course Recommender with SVD based similarity
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_data_mining_similarity_measures.html">
   Concept - Data Mining Similarity Measures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_jaccard_recommender.html">
   Jaccard Similarity based Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_live_streamer_recommender.html">
   Live Streamer Recommender with Implicit feedback
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_songs_embedding_skipgram_recommender.html">
   Song Embeddings - Skipgram Recommender
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_toy_example_car_recommender_knn.html">
   Toy example - Car Recommender using KNN method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T990001_wikirecs_recommender.html">
   WikiRecs
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/nbs/T705904_Evaluation_of_Multiple_Off_Policy_Estimators_on_Synthetic_Dataset.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sparsh-ai/reco-book/main?urlpath=tree/nbs/T705904_Evaluation_of_Multiple_Off_Policy_Estimators_on_Synthetic_Dataset.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#utils">
   Utils
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset">
   Dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#base-models">
   Base Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimators">
   Estimators
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy">
   Policy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#main">
   Main
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparams">
     Hyperparams
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Base Models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ope-estimators">
     OPE Estimators
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#arg-parse">
     Arg Parse
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#process">
     Process
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#run">
     Run
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><a href="https://colab.research.google.com/github/sparsh-ai/reco-book/blob/stage/nbs/T705904_Evaluation_of_Multiple_Off_Policy_Estimators_on_Synthetic_Dataset.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<div class="section" id="evaluation-of-multiple-off-policy-estimators-on-synthetic-dataset">
<h1>Evaluation of Multiple Off-Policy Estimators on Synthetic Dataset<a class="headerlink" href="#evaluation-of-multiple-off-policy-estimators-on-synthetic-dataset" title="Permalink to this headline">¶</a></h1>
<p>We evaluate the estimation performances of the following off-policy estimators using the ground-truth policy value of an evaluation policy calculable with synthetic data.</p>
<ul class="simple">
<li><p>Direct Method (DM)</p></li>
<li><p>Inverse Probability Weighting (IPW)</p></li>
<li><p>Self-Normalized Inverse Probability Weighting (SNIPW)</p></li>
<li><p>Doubly Robust (DR)</p></li>
<li><p>Self-Normalized Doubly Robust (SNDR)</p></li>
<li><p>Switch Doubly Robust (Switch-DR)</p></li>
<li><p>Doubly Robust with Optimistic Shrinkage (DRos)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># !git clone https://github.com/st-tech/zr-obp.git</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="imports">
<h2>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABCMeta</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABCMeta</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">abstractmethod</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>

<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">delayed</span>
<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">DataFrame</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">import</span> <span class="nn">yaml</span>

<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="nn">enum</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">truncnorm</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">softmax</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">ClassifierMixin</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">clone</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">is_classifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="kn">import</span> <span class="n">mse_loss</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">clone</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">is_classifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>

<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">logging</span> <span class="kn">import</span> <span class="n">getLogger</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">DataFrame</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="utils">
<h2>Utils<a class="headerlink" href="#utils" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown main_utils</span>
<span class="k">def</span> <span class="nf">check_confidence_interval_arguments</span><span class="p">(</span>
    <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">ValueError</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Check confidence interval arguments.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    alpha: float, default=0.05</span>
<span class="sd">        Significance level.</span>
<span class="sd">    n_bootstrap_samples: int, default=10000</span>
<span class="sd">        Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        Controls the random seed in bootstrap sampling.</span>
<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    estimated_confidence_interval: Dict[str, float]</span>
<span class="sd">        Dictionary storing the estimated mean and upper-lower confidence bounds.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">check_scalar</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">check_scalar</span><span class="p">(</span><span class="n">n_bootstrap_samples</span><span class="p">,</span> <span class="s2">&quot;n_bootstrap_samples&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">estimate_confidence_interval_by_bootstrap</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Estimate confidence interval by nonparametric bootstrap-like procedure.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    samples: array-like</span>
<span class="sd">        Empirical observed samples to be used to estimate cumulative distribution function.</span>
<span class="sd">    alpha: float, default=0.05</span>
<span class="sd">        Significance level.</span>
<span class="sd">    n_bootstrap_samples: int, default=10000</span>
<span class="sd">        Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        Controls the random seed in bootstrap sampling.</span>
<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    estimated_confidence_interval: Dict[str, float]</span>
<span class="sd">        Dictionary storing the estimated mean and upper-lower confidence bounds.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_confidence_interval_arguments</span><span class="p">(</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span>
    <span class="p">)</span>

    <span class="n">boot_samples</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="n">random_</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_bootstrap_samples</span><span class="p">):</span>
        <span class="n">boot_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">random_</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
    <span class="n">lower_bound</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">boot_samples</span><span class="p">,</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">upper_bound</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">boot_samples</span><span class="p">,</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">boot_samples</span><span class="p">),</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span><span class="si">}</span><span class="s2">% CI (lower)&quot;</span><span class="p">:</span> <span class="n">lower_bound</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span><span class="si">}</span><span class="s2">% CI (upper)&quot;</span><span class="p">:</span> <span class="n">upper_bound</span><span class="p">,</span>
    <span class="p">}</span>


<span class="k">def</span> <span class="nf">sample_action_fast</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Sample actions faster based on a given action distribution.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    action_dist: array-like, shape (n_rounds, n_actions)</span>
<span class="sd">        Distribution over actions.</span>
<span class="sd">    random_state: Optional[int], default=None</span>
<span class="sd">        Controls the random seed in sampling synthetic bandit dataset.</span>
<span class="sd">    Returns</span>
<span class="sd">    ---------</span>
<span class="sd">    sampled_action: array-like, shape (n_rounds,)</span>
<span class="sd">        Actions sampled based on `action_dist`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">random_</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">uniform_rvs</span> <span class="o">=</span> <span class="n">random_</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="n">cum_action_dist</span> <span class="o">=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">flg</span> <span class="o">=</span> <span class="n">cum_action_dist</span> <span class="o">&gt;</span> <span class="n">uniform_rvs</span>
    <span class="n">sampled_action</span> <span class="o">=</span> <span class="n">flg</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sampled_action</span>


<span class="k">def</span> <span class="nf">convert_to_action_dist</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">selected_actions</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Convert selected actions (output of `run_bandit_simulation`) to distribution over actions.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_actions: int</span>
<span class="sd">        Number of actions.</span>
<span class="sd">    selected_actions: array-like, shape (n_rounds, len_list)</span>
<span class="sd">            Sequence of actions selected by evaluation policy</span>
<span class="sd">            at each round in offline bandit simulation.</span>
<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">        Action choice probabilities (can be deterministic).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_rounds</span><span class="p">,</span> <span class="n">len_list</span> <span class="o">=</span> <span class="n">selected_actions</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">action_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_rounds</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">,</span> <span class="n">len_list</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">len_list</span><span class="p">):</span>
        <span class="n">selected_actions_</span> <span class="o">=</span> <span class="n">selected_actions</span><span class="p">[:,</span> <span class="n">pos</span><span class="p">]</span>
        <span class="n">action_dist</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span>
            <span class="n">selected_actions_</span><span class="p">,</span>
            <span class="n">pos</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="nb">int</span><span class="p">),</span>
        <span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">action_dist</span>


<span class="k">def</span> <span class="nf">check_array</span><span class="p">(</span>
    <span class="n">array</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">expected_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="ne">ValueError</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Input validation on an array.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -------------</span>
<span class="sd">    array: object</span>
<span class="sd">        Input object to check.</span>
<span class="sd">    name: str</span>
<span class="sd">        Name of the input array.</span>
<span class="sd">    expected_dim: int, default=1</span>
<span class="sd">        Expected dimension of the input array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must be </span><span class="si">{</span><span class="n">expected_dim</span><span class="si">}</span><span class="s2">D array, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">array</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">array</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="n">expected_dim</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must be </span><span class="si">{</span><span class="n">expected_dim</span><span class="si">}</span><span class="s2">D array, but got </span><span class="si">{</span><span class="n">array</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">D array&quot;</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">check_tensor</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">expected_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="ne">ValueError</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Input validation on a tensor.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -------------</span>
<span class="sd">    array: object</span>
<span class="sd">        Input object to check.</span>
<span class="sd">    name: str</span>
<span class="sd">        Name of the input array.</span>
<span class="sd">    expected_dim: int, default=1</span>
<span class="sd">        Expected dimension of the input array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must be </span><span class="si">{</span><span class="n">expected_dim</span><span class="si">}</span><span class="s2">D tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="n">expected_dim</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must be </span><span class="si">{</span><span class="n">expected_dim</span><span class="si">}</span><span class="s2">D tensor, but got </span><span class="si">{</span><span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">D tensor&quot;</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">check_bandit_feedback_inputs</span><span class="p">(</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">expected_reward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pscore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">action_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">ValueError</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Check inputs for bandit learning or simulation.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    context: array-like, shape (n_rounds, dim_context)</span>
<span class="sd">        Context vectors in each round, i.e., :math:`x_t`.</span>
<span class="sd">    action: array-like, shape (n_rounds,)</span>
<span class="sd">        Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">    reward: array-like, shape (n_rounds,)</span>
<span class="sd">        Observed rewards (or outcome) in each round, i.e., :math:`r_t`.</span>
<span class="sd">    expected_reward: array-like, shape (n_rounds, n_actions), default=None</span>
<span class="sd">        Expected rewards (or outcome) in each round, i.e., :math:`\\mathbb{E}[r_t]`.</span>
<span class="sd">    position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">    pscore: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Propensity scores, the probability of selecting each action by behavior policy,</span>
<span class="sd">        in the given logged bandit data.</span>
<span class="sd">    action_context: array-like, shape (n_actions, dim_action_context)</span>
<span class="sd">        Context vectors characterizing each action.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)</span> <span class="ow">and</span> <span class="n">action</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;action elements must be non-negative integers&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">expected_reward</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">expected_reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;expected_reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">expected_reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `context.shape[0] == action.shape[0] == reward.shape[0] == expected_reward.shape[0]`&quot;</span>
                <span class="s2">&quot;, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">action</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">expected_reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;action elements must be smaller than `expected_reward.shape[1]`&quot;</span>
            <span class="p">)</span>
    <span class="k">if</span> <span class="n">pscore</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">pscore</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `context.shape[0] == action.shape[0] == reward.shape[0] == pscore.shape[0]`&quot;</span>
                <span class="s2">&quot;, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">pscore</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;pscore must be positive&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">position</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;position&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">position</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `context.shape[0] == action.shape[0] == reward.shape[0] == position.shape[0]`&quot;</span>
                <span class="s2">&quot;, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">position</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)</span> <span class="ow">and</span> <span class="n">position</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;position elements must be non-negative integers&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `context.shape[0] == action.shape[0] == reward.shape[0]`&quot;</span>
                <span class="s2">&quot;, but found it False&quot;</span>
            <span class="p">)</span>
    <span class="k">if</span> <span class="n">action_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action_context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action_context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">action</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">action_context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;action elements must be smaller than `action_context.shape[0]`&quot;</span>
            <span class="p">)</span>


<span class="k">def</span> <span class="nf">check_ope_inputs</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">action</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pscore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">ValueError</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Check inputs for ope.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">        Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">    position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">    action: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">    reward: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Observed rewards (or outcome) in each round, i.e., :math:`r_t`.</span>
<span class="sd">    pscore: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Propensity scores, the probability of selecting each action by behavior policy,</span>
<span class="sd">        in the given logged bandit data.</span>
<span class="sd">    estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list), default=None</span>
<span class="sd">        Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># action_dist</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action_dist&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;action_dist must be a probability distribution&quot;</span><span class="p">)</span>

    <span class="c1"># position</span>
    <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">position</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;position&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">position</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `position.shape[0] == action_dist.shape[0]`, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">position</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)</span> <span class="ow">and</span> <span class="n">position</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;position elements must be non-negative integers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;position elements must be smaller than `action_dist.shape[2]`&quot;</span>
            <span class="p">)</span>
    <span class="k">elif</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;position elements must be given when `action_dist.shape[2] &gt; 1`&quot;</span>
        <span class="p">)</span>

    <span class="c1"># estimated_rewards_by_reg_model</span>
    <span class="k">if</span> <span class="n">estimated_rewards_by_reg_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">estimated_rewards_by_reg_model</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `estimated_rewards_by_reg_model.shape == action_dist.shape`, but found it False&quot;</span>
            <span class="p">)</span>

    <span class="c1"># action, reward</span>
    <span class="k">if</span> <span class="n">action</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reward</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `action.shape[0] == reward.shape[0]`, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)</span> <span class="ow">and</span> <span class="n">action</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;action elements must be non-negative integers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">action</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;action elements must be smaller than `action_dist.shape[1]`&quot;</span>
            <span class="p">)</span>

    <span class="c1"># pscore</span>
    <span class="k">if</span> <span class="n">pscore</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pscore</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;pscore must be 1-dimensional&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">pscore</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `action.shape[0] == reward.shape[0] == pscore.shape[0]`, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">pscore</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;pscore must be positive&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">check_continuous_bandit_feedback_inputs</span><span class="p">(</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">action_by_behavior_policy</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">expected_reward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pscore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">ValueError</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Check inputs for bandit learning or simulation with continuous actions.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    context: array-like, shape (n_rounds, dim_context)</span>
<span class="sd">        Context vectors in each round, i.e., :math:`x_t`.</span>
<span class="sd">    action_by_behavior_policy: array-like, shape (n_rounds,)</span>
<span class="sd">        Continuous action values sampled by a behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">    reward: array-like, shape (n_rounds,)</span>
<span class="sd">        Observed rewards (or outcome) in each round, i.e., :math:`r_t`.</span>
<span class="sd">    expected_reward: array-like, shape (n_rounds, n_actions), default=None</span>
<span class="sd">        Expected rewards (or outcome) in each round, i.e., :math:`\\mathbb{E}[r_t]`.</span>
<span class="sd">    pscore: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Probability densities of the continuous action values sampled by a behavior policy</span>
<span class="sd">        (generalized propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">check_array</span><span class="p">(</span>
        <span class="n">array</span><span class="o">=</span><span class="n">action_by_behavior_policy</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action_by_behavior_policy&quot;</span><span class="p">,</span>
        <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">expected_reward</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">expected_reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;expected_reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">action_by_behavior_policy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">expected_reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `context.shape[0] == action_by_behavior_policy.shape[0]&quot;</span>
                <span class="s2">&quot;== reward.shape[0] == expected_reward.shape[0]`, but found it False&quot;</span>
            <span class="p">)</span>
    <span class="k">if</span> <span class="n">pscore</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">action_by_behavior_policy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">pscore</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `context.shape[0] == action_by_behavior_policy.shape[0]&quot;</span>
                <span class="s2">&quot;== reward.shape[0] == pscore.shape[0]`, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">pscore</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;pscore must be positive&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">check_continuous_ope_inputs</span><span class="p">(</span>
    <span class="n">action_by_evaluation_policy</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">action_by_behavior_policy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pscore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">ValueError</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Check inputs for OPE with continuous actions.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    action_by_evaluation_policy: array-like, shape (n_rounds,)</span>
<span class="sd">        Continuous action values given by the evaluation policy (can be deterministic), i.e., :math:`\\pi_e(x_t)`.</span>
<span class="sd">    action_by_behavior_policy: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Continuous action values sampled by a behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">    reward: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Observed rewards (or outcome) in each round, i.e., :math:`r_t`.</span>
<span class="sd">    pscore: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Probability densities of the continuous action values sampled by a behavior policy</span>
<span class="sd">        (generalized propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">    estimated_rewards_by_reg_model: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Expected rewards given context and action estimated by a regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># action_by_evaluation_policy</span>
    <span class="n">check_array</span><span class="p">(</span>
        <span class="n">array</span><span class="o">=</span><span class="n">action_by_evaluation_policy</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action_by_evaluation_policy&quot;</span><span class="p">,</span>
        <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># estimated_rewards_by_reg_model</span>
    <span class="k">if</span> <span class="n">estimated_rewards_by_reg_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span>
            <span class="n">array</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span><span class="p">,</span>
            <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">!=</span> <span class="n">action_by_evaluation_policy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `estimated_rewards_by_reg_model.shape[0] == action_by_evaluation_policy.shape[0]`&quot;</span>
                <span class="s2">&quot;, but found if False&quot;</span>
            <span class="p">)</span>

    <span class="c1"># action, reward</span>
    <span class="k">if</span> <span class="n">action_by_behavior_policy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reward</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span>
            <span class="n">array</span><span class="o">=</span><span class="n">action_by_behavior_policy</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action_by_behavior_policy&quot;</span><span class="p">,</span>
            <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">action_by_behavior_policy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `action_by_behavior_policy.shape[0] == reward.shape[0]`&quot;</span>
                <span class="s2">&quot;, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="n">action_by_behavior_policy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">action_by_evaluation_policy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `action_by_behavior_policy.shape[0] == action_by_evaluation_policy.shape[0]`&quot;</span>
                <span class="s2">&quot;, but found it False&quot;</span>
            <span class="p">)</span>

    <span class="c1"># pscore</span>
    <span class="k">if</span> <span class="n">pscore</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="n">action_by_behavior_policy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">pscore</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `action_by_behavior_policy.shape[0] == reward.shape[0] == pscore.shape[0]`&quot;</span>
                <span class="s2">&quot;, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">pscore</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;pscore must be positive&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_slate_ope_inputs</span><span class="p">(</span>
    <span class="n">slate_id</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">position</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">evaluation_policy_pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">pscore_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">ValueError</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Check inputs of Slate OPE estimators.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    slate_id: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Slate id observed in each round of the logged bandit feedback.</span>
<span class="sd">    reward: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Reward observed at each slot in each round of the logged bandit feedback, i.e., :math:`r_{t}(k)`.</span>
<span class="sd">    position: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Positions of each round and slot in the given logged bandit data.</span>
<span class="sd">    pscore: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Action choice probabilities of behavior policy (propensity scores).</span>
<span class="sd">    evaluation_policy_pscore: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Action choice probabilities of evaluation policy.</span>
<span class="sd">    pscore_type: str</span>
<span class="sd">        Either &quot;pscore&quot;, &quot;pscore_item_position&quot;, or &quot;pscore_cascade&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># position</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">position</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;position&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">position</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="nb">int</span> <span class="ow">and</span> <span class="n">position</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;position elements must be non-negative integers&quot;</span><span class="p">)</span>

    <span class="c1"># reward</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># pscore</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pscore_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">pscore</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">pscore</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pscore_type</span><span class="si">}</span><span class="s2"> must be in the range of (0, 1]&quot;</span><span class="p">)</span>

    <span class="c1"># evaluation_policy_pscore</span>
    <span class="n">check_array</span><span class="p">(</span>
        <span class="n">array</span><span class="o">=</span><span class="n">evaluation_policy_pscore</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;evaluation_policy_</span><span class="si">{</span><span class="n">pscore_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">evaluation_policy_pscore</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">evaluation_policy_pscore</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;evaluation_policy_</span><span class="si">{</span><span class="n">pscore_type</span><span class="si">}</span><span class="s2"> must be in the range of [0, 1]&quot;</span>
        <span class="p">)</span>

    <span class="c1"># slate id</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">slate_id</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;slate_id&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">slate_id</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="nb">int</span> <span class="ow">and</span> <span class="n">slate_id</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;slate_id elements must be non-negative integers&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
        <span class="n">slate_id</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="o">==</span> <span class="n">position</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="o">==</span> <span class="n">pscore</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="o">==</span> <span class="n">evaluation_policy_pscore</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;slate_id, position, reward, </span><span class="si">{</span><span class="n">pscore_type</span><span class="si">}</span><span class="s2">, and evaluation_policy_</span><span class="si">{</span><span class="n">pscore_type</span><span class="si">}</span><span class="s2"> &quot;</span>
            <span class="s2">&quot;must have the same number of samples.&quot;</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">check_sips_inputs</span><span class="p">(</span>
    <span class="n">slate_id</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">position</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">evaluation_policy_pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">ValueError</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Check inputs of SlateStandardIPS.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    slate_id: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Slate id observed in each round of the logged bandit feedback.</span>
<span class="sd">    reward: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Reward observed at each slot in each round of the logged bandit feedback, i.e., :math:`r_{t}(k)`.</span>
<span class="sd">    position: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Positions of each round and slot in the given logged bandit data.</span>
<span class="sd">    pscore: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">    evaluation_policy_pscore: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Action choice probabilities of evaluation policy, i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_slate_ope_inputs</span><span class="p">(</span>
        <span class="n">slate_id</span><span class="o">=</span><span class="n">slate_id</span><span class="p">,</span>
        <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
        <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
        <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
        <span class="n">evaluation_policy_pscore</span><span class="o">=</span><span class="n">evaluation_policy_pscore</span><span class="p">,</span>
        <span class="n">pscore_type</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">bandit_feedback_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">bandit_feedback_df</span><span class="p">[</span><span class="s2">&quot;slate_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">slate_id</span>
    <span class="n">bandit_feedback_df</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward</span>
    <span class="n">bandit_feedback_df</span><span class="p">[</span><span class="s2">&quot;position&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">position</span>
    <span class="n">bandit_feedback_df</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pscore</span>
    <span class="n">bandit_feedback_df</span><span class="p">[</span><span class="s2">&quot;evaluation_policy_pscore&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">evaluation_policy_pscore</span>
    <span class="c1"># check uniqueness</span>
    <span class="k">if</span> <span class="n">bandit_feedback_df</span><span class="o">.</span><span class="n">duplicated</span><span class="p">([</span><span class="s2">&quot;slate_id&quot;</span><span class="p">,</span> <span class="s2">&quot;position&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;position must not be duplicated in each slate&quot;</span><span class="p">)</span>
    <span class="c1"># check pscore uniqueness</span>
    <span class="n">distinct_count_pscore_in_slate</span> <span class="o">=</span> <span class="n">bandit_feedback_df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;slate_id&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">distinct_count_pscore_in_slate</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;pscore must be unique in each slate&quot;</span><span class="p">)</span>
    <span class="c1"># check pscore uniqueness of evaluation policy</span>
    <span class="n">distinct_count_evaluation_policy_pscore_in_slate</span> <span class="o">=</span> <span class="n">bandit_feedback_df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span>
        <span class="s2">&quot;slate_id&quot;</span>
    <span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;evaluation_policy_pscore&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">distinct_count_evaluation_policy_pscore_in_slate</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;evaluation_policy_pscore must be unique in each slate&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">check_iips_inputs</span><span class="p">(</span>
    <span class="n">slate_id</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">position</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">pscore_item_position</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">evaluation_policy_pscore_item_position</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">ValueError</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Check inputs of SlateIndependentIPS.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    slate_id: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Slate id observed in each round of the logged bandit feedback.</span>
<span class="sd">    reward: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Reward observed at each slot in each round of the logged bandit feedback, i.e., :math:`r_{t}(k)`.</span>
<span class="sd">    position: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Positions of each round and slot in the given logged bandit data.</span>
<span class="sd">    pscore_item_position: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Marginal action choice probabilities of the slot (:math:`k`) by a behavior policy (propensity scores), i.e., :math:`\\pi_b(a_{t}(k) |x_t)`.</span>
<span class="sd">    evaluation_policy_pscore_item_position: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Marginal action choice probabilities of the slot (:math:`k`) by the evaluation policy, i.e., :math:`\\pi_e(a_{t}(k) |x_t)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_slate_ope_inputs</span><span class="p">(</span>
        <span class="n">slate_id</span><span class="o">=</span><span class="n">slate_id</span><span class="p">,</span>
        <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
        <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
        <span class="n">pscore</span><span class="o">=</span><span class="n">pscore_item_position</span><span class="p">,</span>
        <span class="n">evaluation_policy_pscore</span><span class="o">=</span><span class="n">evaluation_policy_pscore_item_position</span><span class="p">,</span>
        <span class="n">pscore_type</span><span class="o">=</span><span class="s2">&quot;pscore_item_position&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">bandit_feedback_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">bandit_feedback_df</span><span class="p">[</span><span class="s2">&quot;slate_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">slate_id</span>
    <span class="n">bandit_feedback_df</span><span class="p">[</span><span class="s2">&quot;position&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">position</span>
    <span class="c1"># check uniqueness</span>
    <span class="k">if</span> <span class="n">bandit_feedback_df</span><span class="o">.</span><span class="n">duplicated</span><span class="p">([</span><span class="s2">&quot;slate_id&quot;</span><span class="p">,</span> <span class="s2">&quot;position&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;position must not be duplicated in each slate&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">check_rips_inputs</span><span class="p">(</span>
    <span class="n">slate_id</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">position</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">pscore_cascade</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">evaluation_policy_pscore_cascade</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">ValueError</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Check inputs of SlateRewardInteractionIPS.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    slate_id: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Slate id observed in each round of the logged bandit feedback.</span>
<span class="sd">    reward: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Reward observed at each slot in each round of the logged bandit feedback, i.e., :math:`r_{t}(k)`.</span>
<span class="sd">    position: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Positions of each round and slot in the given logged bandit data.</span>
<span class="sd">    pscore_cascade: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">    evaluation_policy_pscore_cascade: array-like, shape (&lt;= n_rounds * len_list,)</span>
<span class="sd">        Action choice probabilities above the slot (:math:`k`) by the evaluation policy, i.e., :math:`\\pi_e(\\{a_{t, j}\\}_{j \\le k}|x_t)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_slate_ope_inputs</span><span class="p">(</span>
        <span class="n">slate_id</span><span class="o">=</span><span class="n">slate_id</span><span class="p">,</span>
        <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
        <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
        <span class="n">pscore</span><span class="o">=</span><span class="n">pscore_cascade</span><span class="p">,</span>
        <span class="n">evaluation_policy_pscore</span><span class="o">=</span><span class="n">evaluation_policy_pscore_cascade</span><span class="p">,</span>
        <span class="n">pscore_type</span><span class="o">=</span><span class="s2">&quot;pscore_cascade&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">bandit_feedback_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">bandit_feedback_df</span><span class="p">[</span><span class="s2">&quot;slate_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">slate_id</span>
    <span class="n">bandit_feedback_df</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward</span>
    <span class="n">bandit_feedback_df</span><span class="p">[</span><span class="s2">&quot;position&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">position</span>
    <span class="n">bandit_feedback_df</span><span class="p">[</span><span class="s2">&quot;pscore_cascade&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pscore_cascade</span>
    <span class="n">bandit_feedback_df</span><span class="p">[</span>
        <span class="s2">&quot;evaluation_policy_pscore_cascade&quot;</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">evaluation_policy_pscore_cascade</span>
    <span class="c1"># sort dataframe</span>
    <span class="n">bandit_feedback_df</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">bandit_feedback_df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">([</span><span class="s2">&quot;slate_id&quot;</span><span class="p">,</span> <span class="s2">&quot;position&quot;</span><span class="p">])</span>
        <span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="c1"># check uniqueness</span>
    <span class="k">if</span> <span class="n">bandit_feedback_df</span><span class="o">.</span><span class="n">duplicated</span><span class="p">([</span><span class="s2">&quot;slate_id&quot;</span><span class="p">,</span> <span class="s2">&quot;position&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;position must not be duplicated in each slate&quot;</span><span class="p">)</span>
    <span class="c1"># check pscore_cascade structure</span>
    <span class="n">previous_minimum_pscore_cascade</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">bandit_feedback_df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;slate_id&quot;</span><span class="p">)[</span><span class="s2">&quot;pscore_cascade&quot;</span><span class="p">]</span>
        <span class="o">.</span><span class="n">expanding</span><span class="p">()</span>
        <span class="o">.</span><span class="n">min</span><span class="p">()</span>
        <span class="o">.</span><span class="n">values</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="n">previous_minimum_pscore_cascade</span> <span class="o">&lt;</span> <span class="n">bandit_feedback_df</span><span class="p">[</span><span class="s2">&quot;pscore_cascade&quot;</span><span class="p">]</span>
    <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;pscore_cascade must be non-increasing sequence in each slate&quot;</span><span class="p">)</span>
    <span class="c1"># check pscore_cascade structure of evaluation policy</span>
    <span class="n">previous_minimum_evaluation_policy_pscore_cascade</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">bandit_feedback_df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;slate_id&quot;</span><span class="p">)[</span><span class="s2">&quot;evaluation_policy_pscore_cascade&quot;</span><span class="p">]</span>
        <span class="o">.</span><span class="n">expanding</span><span class="p">()</span>
        <span class="o">.</span><span class="n">min</span><span class="p">()</span>
        <span class="o">.</span><span class="n">values</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="n">previous_minimum_evaluation_policy_pscore_cascade</span>
        <span class="o">&lt;</span> <span class="n">bandit_feedback_df</span><span class="p">[</span><span class="s2">&quot;evaluation_policy_pscore_cascade&quot;</span><span class="p">]</span>
    <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;evaluation_policy_pscore_cascade must be non-increasing sequence in each slate&quot;</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Calculate sigmoid function.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Calculate softmax function.&quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">numerator</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown helpers</span>
<span class="k">def</span> <span class="nf">estimate_bias_in_ope</span><span class="p">(</span>
    <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">iw</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">iw_hat</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">q_hat</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Helper to estimate a bias in OPE.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    reward: array-like, shape (n_rounds,)</span>
<span class="sd">        Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">    iw: array-like, shape (n_rounds,)</span>
<span class="sd">        Importance weight in each round of the logged bandit feedback, i.e., :math:`w(x,a)=\\pi_e(a|x)/ \\pi_b(a|x)`.</span>
<span class="sd">    iw_hat: array-like, shape (n_rounds,)</span>
<span class="sd">        Importance weight (IW) modified by a hyparpareter. How IW is modified depends on the estimator as follows.</span>
<span class="sd">            - clipping: :math:`\\hat{w}(x,a) := \\min \\{ \\lambda, w(x,a) \\}`</span>
<span class="sd">            - switching: :math:`\\hat{w}(x,a) := w(x,a) \\cdot \\mathbb{I} \\{ w(x,a) &lt; \\lambda \\}`</span>
<span class="sd">            - shrinkage: :math:`\\hat{w}(x,a) := (\\lambda w(x,a)) / (\\lambda + w^2(x,a))`</span>
<span class="sd">        where :math:`\\lambda` is a hyperparameter value.</span>
<span class="sd">    q_hat: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Estimated expected reward given context :math:`x_t` and action :math:`a_t`.</span>
<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    estimated_bias: float</span>
<span class="sd">        Estimated the bias in OPE.</span>
<span class="sd">        This is based on the direct bias estimation stated on page 17 of Su et al.(2020).</span>
<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.</span>
<span class="sd">    &quot;Doubly Robust Off-Policy Evaluation with Shrinkage.&quot;, 2020.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">q_hat</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">q_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">)</span>
    <span class="n">estimated_bias_arr</span> <span class="o">=</span> <span class="p">(</span><span class="n">iw</span> <span class="o">-</span> <span class="n">iw_hat</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">q_hat</span><span class="p">)</span>
    <span class="n">estimated_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">estimated_bias_arr</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">estimated_bias</span>


<span class="k">def</span> <span class="nf">estimate_high_probability_upper_bound_bias</span><span class="p">(</span>
    <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">iw</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">iw_hat</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">q_hat</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Helper to estimate a high probability upper bound of bias in OPE.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    reward: array-like, shape (n_rounds,)</span>
<span class="sd">        Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">    iw: array-like, shape (n_rounds,)</span>
<span class="sd">        Importance weight in each round of the logged bandit feedback, i.e., :math:`w(x,a)=\\pi_e(a|x)/ \\pi_b(a|x)`.</span>
<span class="sd">    iw_hat: array-like, shape (n_rounds,)</span>
<span class="sd">        Importance weight (IW) modified by a hyparpareter. How IW is modified depends on the estimator as follows.</span>
<span class="sd">            - clipping: :math:`\\hat{w}(x,a) := \\min \\{ \\lambda, w(x,a) \\}`</span>
<span class="sd">            - switching: :math:`\\hat{w}(x,a) := w(x,a) \\cdot \\mathbb{I} \\{ w(x,a) &lt; \\lambda \\}`</span>
<span class="sd">            - shrinkage: :math:`\\hat{w}(x,a) := (\\lambda w(x,a)) / (\\lambda + w^2(x,a))`</span>
<span class="sd">        where :math:`\\lambda` and :math:`\\lambda` are hyperparameters.</span>
<span class="sd">    q_hat: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Estimated expected reward given context :math:`x_t` and action :math:`a_t`.</span>
<span class="sd">    delta: float, default=0.05</span>
<span class="sd">        A confidence delta to construct a high probability upper bound based on the Bernstein’s inequality.</span>
<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    bias_upper_bound: float</span>
<span class="sd">        Estimated (high probability) upper bound of the bias.</span>
<span class="sd">        This upper bound is based on the direct bias estimation</span>
<span class="sd">        stated on page 17 of Su et al.(2020).</span>
<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.</span>
<span class="sd">    &quot;Doubly Robust Off-Policy Evaluation with Shrinkage.&quot;, 2020.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_scalar</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="s2">&quot;delta&quot;</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">),</span> <span class="n">min_val</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

    <span class="n">bias_upper_bound</span> <span class="o">=</span> <span class="n">estimate_bias_in_ope</span><span class="p">(</span>
        <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
        <span class="n">iw</span><span class="o">=</span><span class="n">iw</span><span class="p">,</span>
        <span class="n">iw_hat</span><span class="o">=</span><span class="n">iw_hat</span><span class="p">,</span>
        <span class="n">q_hat</span><span class="o">=</span><span class="n">q_hat</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">bias_upper_bound</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">iw</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">delta</span><span class="p">))</span> <span class="o">/</span> <span class="n">n_rounds</span><span class="p">)</span>
    <span class="n">bias_upper_bound</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">iw</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">delta</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">n_rounds</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">bias_upper_bound</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown reward type</span>
<span class="k">class</span> <span class="nc">RewardType</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Reward type.</span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    BINARY:</span>
<span class="sd">        The reward type is binary.</span>
<span class="sd">    CONTINUOUS:</span>
<span class="sd">        The reward type is continuous.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">BINARY</span> <span class="o">=</span> <span class="s2">&quot;binary&quot;</span>
    <span class="n">CONTINUOUS</span> <span class="o">=</span> <span class="s2">&quot;continuous&quot;</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>

        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown policy type</span>
<span class="k">class</span> <span class="nc">PolicyType</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Policy type.</span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    CONTEXT_FREE:</span>
<span class="sd">        The policy type is contextfree.</span>
<span class="sd">    CONTEXTUAL:</span>
<span class="sd">        The policy type is contextual.</span>
<span class="sd">    OFFLINE:</span>
<span class="sd">        The policy type is offline.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">CONTEXT_FREE</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>
    <span class="n">CONTEXTUAL</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>
    <span class="n">OFFLINE</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>

        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># from ..types import BanditFeedback</span>
<span class="c1"># from ..utils import check_array</span>
<span class="c1"># from ..utils import sample_action_fast</span>
<span class="c1"># from ..utils import sigmoid</span>
<span class="c1"># from ..utils import softmax</span>
<span class="c1"># from .base import BaseBanditDataset</span>
<span class="c1"># from .reward_type import RewardType</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BaseBanditDataset</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base Class for Synthetic Bandit Dataset.&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Obtain batch logged bandit feedback.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>


<span class="k">class</span> <span class="nc">BaseRealBanditDataset</span><span class="p">(</span><span class="n">BaseBanditDataset</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base Class for Real-World Bandit Dataset.&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">load_raw_data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Load raw dataset.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">pre_process</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Preprocess raw dataset.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># dataset type</span>
<span class="n">BanditFeedback</span> <span class="o">=</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown synthetic dataset</span>
<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">SyntheticBanditDataset</span><span class="p">(</span><span class="n">BaseBanditDataset</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Class for generating synthetic bandit dataset.</span>
<span class="sd">    Note</span>
<span class="sd">    -----</span>
<span class="sd">    By calling the `obtain_batch_bandit_feedback` method several times,</span>
<span class="sd">    we have different bandit samples with the same setting.</span>
<span class="sd">    This can be used to estimate confidence intervals of the performances of OPE estimators.</span>
<span class="sd">    If None is set as `behavior_policy_function`, the synthetic data will be context-free bandit feedback.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    n_actions: int</span>
<span class="sd">        Number of actions.</span>
<span class="sd">    dim_context: int, default=1</span>
<span class="sd">        Number of dimensions of context vectors.</span>
<span class="sd">    reward_type: str, default=&#39;binary&#39;</span>
<span class="sd">        Type of reward variable, which must be either &#39;binary&#39; or &#39;continuous&#39;.</span>
<span class="sd">        When &#39;binary&#39; is given, rewards are sampled from the Bernoulli distribution.</span>
<span class="sd">        When &#39;continuous&#39; is given, rewards are sampled from the truncated Normal distribution with `scale=1`.</span>
<span class="sd">        The mean parameter of the reward distribution is determined by the `reward_function` specified by the next argument.</span>
<span class="sd">    reward_function: Callable[[np.ndarray, np.ndarray], np.ndarray]], default=None</span>
<span class="sd">        Function generating expected reward for each given action-context pair,</span>
<span class="sd">        i.e., :math:`\\mu: \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\mathbb{R}`.</span>
<span class="sd">        If None is set, context **independent** expected reward for each action will be</span>
<span class="sd">        sampled from the uniform distribution automatically.</span>
<span class="sd">    reward_std: float, default=1.0</span>
<span class="sd">        Standard deviation of the reward distribution.</span>
<span class="sd">        A larger value leads to a noisy reward distribution.</span>
<span class="sd">        This argument is valid only when `reward_type=&quot;continuous&quot;`.</span>
<span class="sd">    behavior_policy_function: Callable[[np.ndarray, np.ndarray], np.ndarray], default=None</span>
<span class="sd">        Function generating probability distribution over action space,</span>
<span class="sd">        i.e., :math:`\\pi: \\mathcal{X} \\rightarrow \\Delta(\\mathcal{A})`.</span>
<span class="sd">        If None is set, context **independent** uniform distribution will be used (uniform random behavior policy).</span>
<span class="sd">    tau: float, default=1.0</span>
<span class="sd">        A temperature hyperparameer which controls the behavior policy.</span>
<span class="sd">        A large value leads to a near-uniform behavior policy,</span>
<span class="sd">        while a small value leads to a near-deterministic behavior policy.</span>
<span class="sd">    random_state: int, default=12345</span>
<span class="sd">        Controls the random seed in sampling synthetic bandit dataset.</span>
<span class="sd">    dataset_name: str, default=&#39;synthetic_bandit_dataset&#39;</span>
<span class="sd">        Name of the dataset.</span>
<span class="sd">    Examples</span>
<span class="sd">    ----------</span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from obp.dataset import (</span>
<span class="sd">            SyntheticBanditDataset,</span>
<span class="sd">            linear_reward_function,</span>
<span class="sd">            linear_behavior_policy</span>
<span class="sd">        )</span>
<span class="sd">        # generate synthetic contextual bandit feedback with 10 actions.</span>
<span class="sd">        &gt;&gt;&gt; dataset = SyntheticBanditDataset(</span>
<span class="sd">                n_actions=10,</span>
<span class="sd">                dim_context=5,</span>
<span class="sd">                reward_function=logistic_reward_function,</span>
<span class="sd">                behavior_policy=linear_behavior_policy,</span>
<span class="sd">                random_state=12345</span>
<span class="sd">            )</span>
<span class="sd">        &gt;&gt;&gt; bandit_feedback = dataset.obtain_batch_bandit_feedback(n_rounds=100000)</span>
<span class="sd">        &gt;&gt;&gt; bandit_feedback</span>
<span class="sd">        {</span>
<span class="sd">            &#39;n_rounds&#39;: 100000,</span>
<span class="sd">            &#39;n_actions&#39;: 10,</span>
<span class="sd">            &#39;context&#39;: array([[-0.20470766,  0.47894334, -0.51943872, -0.5557303 ,  1.96578057],</span>
<span class="sd">                    [ 1.39340583,  0.09290788,  0.28174615,  0.76902257,  1.24643474],</span>
<span class="sd">                    [ 1.00718936, -1.29622111,  0.27499163,  0.22891288,  1.35291684],</span>
<span class="sd">                    ...,</span>
<span class="sd">                    [ 1.36946256,  0.58727761, -0.69296769, -0.27519988, -2.10289159],</span>
<span class="sd">                    [-0.27428715,  0.52635353,  1.02572168, -0.18486381,  0.72464834],</span>
<span class="sd">                    [-1.25579833, -1.42455203, -0.26361242,  0.27928604,  1.21015571]]),</span>
<span class="sd">            &#39;action_context&#39;: array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="sd">                    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="sd">                    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="sd">                    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],</span>
<span class="sd">                    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],</span>
<span class="sd">                    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],</span>
<span class="sd">                    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],</span>
<span class="sd">                    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],</span>
<span class="sd">                    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],</span>
<span class="sd">                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]),</span>
<span class="sd">            &#39;action&#39;: array([7, 4, 0, ..., 7, 9, 6]),</span>
<span class="sd">            &#39;position&#39;: None,</span>
<span class="sd">            &#39;reward&#39;: array([0, 1, 1, ..., 0, 1, 0]),</span>
<span class="sd">            &#39;expected_reward&#39;: array([[0.80210203, 0.73828559, 0.83199558, ..., 0.81190503, 0.70617705,</span>
<span class="sd">                    0.68985306],</span>
<span class="sd">                    [0.94119582, 0.93473317, 0.91345213, ..., 0.94140688, 0.93152449,</span>
<span class="sd">                    0.90132868],</span>
<span class="sd">                    [0.87248862, 0.67974991, 0.66965669, ..., 0.79229752, 0.82712978,</span>
<span class="sd">                    0.74923536],</span>
<span class="sd">                    ...,</span>
<span class="sd">                    [0.64856003, 0.38145901, 0.84476094, ..., 0.40962057, 0.77114661,</span>
<span class="sd">                    0.65752798],</span>
<span class="sd">                    [0.73208527, 0.82012699, 0.78161352, ..., 0.72361416, 0.8652249 ,</span>
<span class="sd">                    0.82571751],</span>
<span class="sd">                    [0.40348366, 0.24485417, 0.24037926, ..., 0.49613133, 0.30714854,</span>
<span class="sd">                    0.5527749 ]]),</span>
<span class="sd">            &#39;pscore&#39;: array([0.05423855, 0.10339675, 0.09756788, ..., 0.05423855, 0.07250876,</span>
<span class="sd">                    0.14065505])</span>
<span class="sd">        }</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">n_actions</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dim_context</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">reward_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">RewardType</span><span class="o">.</span><span class="n">BINARY</span><span class="o">.</span><span class="n">value</span>
    <span class="n">reward_function</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">reward_std</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">behavior_policy_function</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Callable</span><span class="p">[[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12345</span>
    <span class="n">dataset_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;synthetic_bandit_dataset&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Class.&quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="s2">&quot;n_actions&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_context</span><span class="p">,</span> <span class="s2">&quot;dim_context&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">RewardType</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_type</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">RewardType</span><span class="o">.</span><span class="n">BINARY</span><span class="p">,</span>
            <span class="n">RewardType</span><span class="o">.</span><span class="n">CONTINUOUS</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;reward_type must be either &#39;</span><span class="si">{</span><span class="n">RewardType</span><span class="o">.</span><span class="n">BINARY</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2">&#39; or &#39;</span><span class="si">{</span><span class="n">RewardType</span><span class="o">.</span><span class="n">CONTINUOUS</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2">&#39;, but </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_type</span><span class="si">}</span><span class="s2"> is given.&#39;&quot;</span>
            <span class="p">)</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_std</span><span class="p">,</span> <span class="s2">&quot;reward_std&quot;</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">),</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">,</span> <span class="s2">&quot;tau&quot;</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">),</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`random_state` must be given&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">expected_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_contextfree_expected_reward</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">behavior_policy_function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">behavior_policy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span>
        <span class="k">if</span> <span class="n">RewardType</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_type</span><span class="p">)</span> <span class="o">==</span> <span class="n">RewardType</span><span class="o">.</span><span class="n">CONTINUOUS</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reward_min</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reward_max</span> <span class="o">=</span> <span class="mf">1e10</span>
        <span class="c1"># one-hot encoding representations characterizing each action</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_context</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">len_list</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Length of recommendation lists.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">sample_contextfree_expected_reward</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sample expected reward for each action from the uniform distribution.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">calc_expected_reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sample expected rewards given contexts&quot;&quot;&quot;</span>
        <span class="c1"># sample reward for each round based on the reward function</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">expected_reward_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">expected_reward</span><span class="p">,</span> <span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">expected_reward_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_function</span><span class="p">(</span>
                <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
                <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">expected_reward_</span>

    <span class="k">def</span> <span class="nf">sample_reward_given_expected_reward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">expected_reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sample reward given expected rewards&quot;&quot;&quot;</span>
        <span class="n">expected_reward_factual</span> <span class="o">=</span> <span class="n">expected_reward</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">action</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">RewardType</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_type</span><span class="p">)</span> <span class="o">==</span> <span class="n">RewardType</span><span class="o">.</span><span class="n">BINARY</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">expected_reward_factual</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">RewardType</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_type</span><span class="p">)</span> <span class="o">==</span> <span class="n">RewardType</span><span class="o">.</span><span class="n">CONTINUOUS</span><span class="p">:</span>
            <span class="n">mean</span> <span class="o">=</span> <span class="n">expected_reward_factual</span>
            <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_min</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_std</span>
            <span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_max</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_std</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="n">truncnorm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span>
                <span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">,</span>
                <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">,</span>
                <span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span>
                <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_std</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="k">return</span> <span class="n">reward</span>

    <span class="k">def</span> <span class="nf">sample_reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sample rewards given contexts and actions, i.e., :math:`r \\sim p(r \\mid x, a)`.</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        context: array-like, shape (n_rounds, dim_context)</span>
<span class="sd">            Context vectors characterizing each round (such as user information).</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Selected actions to the contexts.</span>
<span class="sd">        Returns</span>
<span class="sd">        ---------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Sampled rewards given contexts and actions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `context.shape[0] == action.shape[0]`, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;the dtype of action must be a subdtype of int&quot;</span><span class="p">)</span>

        <span class="n">expected_reward_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calc_expected_reward</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_reward_given_expected_reward</span><span class="p">(</span><span class="n">expected_reward_</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_rounds</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BanditFeedback</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Obtain batch logged bandit feedback.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        n_rounds: int</span>
<span class="sd">            Number of rounds for synthetic bandit feedback data.</span>
<span class="sd">        Returns</span>
<span class="sd">        ---------</span>
<span class="sd">        bandit_feedback: BanditFeedback</span>
<span class="sd">            Generated synthetic bandit feedback dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="s2">&quot;n_rounds&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_context</span><span class="p">))</span>
        <span class="c1"># sample actions for each round based on the behavior policy</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">behavior_policy_function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">behavior_policy_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">behavior_policy</span><span class="p">,</span> <span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">behavior_policy_</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">behavior_policy_</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">behavior_policy</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_rounds</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">behavior_policy_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">behavior_policy_function</span><span class="p">(</span>
                <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
                <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">behavior_policy_</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">behavior_policy_</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">sample_action_fast</span><span class="p">(</span>
                <span class="n">behavior_policy_</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span>
            <span class="p">)</span>
        <span class="n">pscore</span> <span class="o">=</span> <span class="n">behavior_policy_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">]</span>

        <span class="c1"># sample reward based on the context and action</span>
        <span class="n">expected_reward_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calc_expected_reward</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">RewardType</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_type</span><span class="p">)</span> <span class="o">==</span> <span class="n">RewardType</span><span class="o">.</span><span class="n">CONTINUOUS</span><span class="p">:</span>
            <span class="c1"># correct expected_reward_, as we use truncated normal distribution here</span>
            <span class="n">mean</span> <span class="o">=</span> <span class="n">expected_reward_</span>
            <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_min</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_std</span>
            <span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_max</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_std</span>
            <span class="n">expected_reward_</span> <span class="o">=</span> <span class="n">truncnorm</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span>
                <span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_std</span><span class="p">,</span> <span class="n">moments</span><span class="o">=</span><span class="s2">&quot;m&quot;</span>
            <span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_reward_given_expected_reward</span><span class="p">(</span><span class="n">expected_reward_</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">n_rounds</span><span class="o">=</span><span class="n">n_rounds</span><span class="p">,</span>
            <span class="n">n_actions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># position effect is not considered in synthetic data</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">expected_reward</span><span class="o">=</span><span class="n">expected_reward_</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">calc_ground_truth_policy_value</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">expected_reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Calculate the policy value of given action distribution on the given expected_reward.</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        expected_reward: array-like, shape (n_rounds, n_actions)</span>
<span class="sd">            Expected reward given context (:math:`x`) and action (:math:`a`), i.e., :math:`q(x,a):=\\mathbb{E}[r|x,a]`.</span>
<span class="sd">            This is often the expected_reward of the test set of logged bandit feedback data.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        policy_value: float</span>
<span class="sd">            The policy value of the given action distribution on the given bandit feedback data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">expected_reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;expected_reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action_dist&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">expected_reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `expected_reward.shape[0] = action_dist.shape[0]`, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">expected_reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `expected_reward.shape[1] = action_dist.shape[1]`, but found it False&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">expected_reward</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">action_dist</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">logistic_reward_function</span><span class="p">(</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">action_context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Logistic mean reward function for synthetic bandit datasets.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    context: array-like, shape (n_rounds, dim_context)</span>
<span class="sd">        Context vectors characterizing each round (such as user information).</span>
<span class="sd">    action_context: array-like, shape (n_actions, dim_action_context)</span>
<span class="sd">        Vector representation for each action.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        Controls the random seed in sampling dataset.</span>
<span class="sd">    Returns</span>
<span class="sd">    ---------</span>
<span class="sd">    expected_reward: array-like, shape (n_rounds, n_actions)</span>
<span class="sd">        Expected reward given context (:math:`x`) and action (:math:`a`), i.e., :math:`q(x,a):=\\mathbb{E}[r|x,a]`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action_context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action_context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">random_</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">action_context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="c1"># each arm has different coefficient vectors</span>
    <span class="n">coef_</span> <span class="o">=</span> <span class="n">random_</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">action_context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">action_coef_</span> <span class="o">=</span> <span class="n">random_</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">action_context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">action_context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">logits</span><span class="p">[:,</span> <span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">context</span> <span class="o">@</span> <span class="n">coef_</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span> <span class="n">action_context</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">@</span> <span class="n">action_coef_</span>

    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">linear_reward_function</span><span class="p">(</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">action_context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Linear mean reward function for synthetic bandit datasets.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    context: array-like, shape (n_rounds, dim_context)</span>
<span class="sd">        Context vectors characterizing each round (such as user information).</span>
<span class="sd">    action_context: array-like, shape (n_actions, dim_action_context)</span>
<span class="sd">        Vector representation for each action.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        Controls the random seed in sampling dataset.</span>
<span class="sd">    Returns</span>
<span class="sd">    ---------</span>
<span class="sd">    expected_reward: array-like, shape (n_rounds, n_actions)</span>
<span class="sd">        Expected reward given context (:math:`x`) and action (:math:`a`), i.e., :math:`q(x,a):=\\mathbb{E}[r|x,a]`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action_context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action_context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">random_</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">expected_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">action_context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="c1"># each arm has different coefficient vectors</span>
    <span class="n">coef_</span> <span class="o">=</span> <span class="n">random_</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">action_context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">action_coef_</span> <span class="o">=</span> <span class="n">random_</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">action_context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">action_context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">expected_reward</span><span class="p">[:,</span> <span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">context</span> <span class="o">@</span> <span class="n">coef_</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span> <span class="n">action_context</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">@</span> <span class="n">action_coef_</span>

    <span class="k">return</span> <span class="n">expected_reward</span>


<span class="k">def</span> <span class="nf">linear_behavior_policy</span><span class="p">(</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">action_context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Linear contextual behavior policy for synthetic bandit datasets.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    context: array-like, shape (n_rounds, dim_context)</span>
<span class="sd">        Context vectors characterizing each round (such as user information).</span>
<span class="sd">    action_context: array-like, shape (n_actions, dim_action_context)</span>
<span class="sd">        Vector representation for each action.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        Controls the random seed in sampling dataset.</span>
<span class="sd">    Returns</span>
<span class="sd">    ---------</span>
<span class="sd">    behavior_policy: array-like, shape (n_rounds, n_actions)</span>
<span class="sd">        Logit values given context (:math:`x`), i.e., :math:`\\pi: \\mathcal{X} \\rightarrow \\Delta(\\mathcal{A})`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action_context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action_context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">random_</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">action_context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">coef_</span> <span class="o">=</span> <span class="n">random_</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">action_coef_</span> <span class="o">=</span> <span class="n">random_</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">action_context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">action_context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">logits</span><span class="p">[:,</span> <span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">context</span> <span class="o">@</span> <span class="n">coef_</span> <span class="o">+</span> <span class="n">action_context</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">@</span> <span class="n">action_coef_</span>

    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="base-models">
<h2>Base Models<a class="headerlink" href="#base-models" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown Regression model</span>
<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">RegressionModel</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Machine learning model to estimate the mean reward function (:math:`q(x,a):= \\mathbb{E}[r|x,a]`).</span>
<span class="sd">    Note</span>
<span class="sd">    -------</span>
<span class="sd">    Reward (or outcome) :math:`r` must be either binary or continuous.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ------------</span>
<span class="sd">    base_model: BaseEstimator</span>
<span class="sd">        A machine learning model used to estimate the mean reward function.</span>
<span class="sd">    n_actions: int</span>
<span class="sd">        Number of actions.</span>
<span class="sd">    len_list: int, default=1</span>
<span class="sd">        Length of a list of actions recommended in each impression.</span>
<span class="sd">        When Open Bandit Dataset is used, 3 should be set.</span>
<span class="sd">    action_context: array-like, shape (n_actions, dim_action_context), default=None</span>
<span class="sd">        Context vector characterizing action (i.e., vector representation of each action).</span>
<span class="sd">        If not given, one-hot encoding of the action variable is used as default.</span>
<span class="sd">    fitting_method: str, default=&#39;normal&#39;</span>
<span class="sd">        Method to fit the regression model.</span>
<span class="sd">        Must be one of [&#39;normal&#39;, &#39;iw&#39;, &#39;mrdr&#39;] where &#39;iw&#39; stands for importance weighting and</span>
<span class="sd">        &#39;mrdr&#39; stands for more robust doubly robust.</span>
<span class="sd">    References</span>
<span class="sd">    -----------</span>
<span class="sd">    Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh.</span>
<span class="sd">    &quot;More Robust Doubly Robust Off-policy Evaluation.&quot;, 2018.</span>
<span class="sd">    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.</span>
<span class="sd">    &quot;Doubly Robust Off-Policy Evaluation with Shrinkage.&quot;, 2020.</span>
<span class="sd">    Yusuke Narita, Shota Yasui, and Kohei Yata.</span>
<span class="sd">    &quot;Off-policy Bandit and Reinforcement Learning.&quot;, 2020.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">base_model</span><span class="p">:</span> <span class="n">BaseEstimator</span>
    <span class="n">n_actions</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">len_list</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">action_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">fitting_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;normal&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Class.&quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="s2">&quot;n_actions&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">,</span> <span class="s2">&quot;len_list&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="s2">&quot;iw&quot;</span><span class="p">,</span> <span class="s2">&quot;mrdr&quot;</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;fitting_method must be one of &#39;normal&#39;, &#39;iw&#39;, or &#39;mrdr&#39;, but </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span><span class="si">}</span><span class="s2"> is given&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_model</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;base_model must be BaseEstimator or a child class of BaseEstimator&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_model</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_context</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">action_context</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Fit the regression model on given logged bandit feedback data.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        context: array-like, shape (n_rounds, dim_context)</span>
<span class="sd">            Context vectors observed in each round of the logged bandit feedback, i.e., :math:`x_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">            When None is given, behavior policy is assumed to be uniform.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            If None is set, a regression model assumes that there is only one position.</span>
<span class="sd">            When `len_list` &gt; 1, this position argument has to be set.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list), default=None</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">            When either of &#39;iw&#39; or &#39;mrdr&#39; is used as the &#39;fitting_method&#39; argument, then `action_dist` must be given.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_bandit_feedback_inputs</span><span class="p">(</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">position</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;position elements must be smaller than len_list, but the maximum value is </span><span class="si">{</span><span class="n">position</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">}</span><span class="s2"> (&gt;= </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;iw&quot;</span><span class="p">,</span> <span class="s2">&quot;mrdr&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;when fitting_method is either &#39;iw&#39; or &#39;mrdr&#39;, action_dist (a 3-dimensional ndarray) must be given&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;shape of action_dist must be (n_rounds, n_actions, len_list)=(</span><span class="si">{</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="si">}</span><span class="s2">), but is </span><span class="si">{</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;action_dist must be a probability distribution&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pscore</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pscore</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span>

        <span class="k">for</span> <span class="n">position_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">position</span> <span class="o">==</span> <span class="n">position_</span>
            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_process_for_reg_model</span><span class="p">(</span>
                <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
                <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No training data at position </span><span class="si">{</span><span class="n">position_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="c1"># train the base model according to the given `fitting method`</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="o">==</span> <span class="s2">&quot;normal&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">reward</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">action_dist_at_position</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span>
                    <span class="n">action</span><span class="p">,</span>
                    <span class="n">position_</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
                <span class="p">][</span><span class="n">idx</span><span class="p">]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="o">==</span> <span class="s2">&quot;iw&quot;</span><span class="p">:</span>
                    <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">action_dist_at_position</span> <span class="o">/</span> <span class="n">pscore</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                        <span class="n">X</span><span class="p">,</span> <span class="n">reward</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="o">==</span> <span class="s2">&quot;mrdr&quot;</span><span class="p">:</span>
                    <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">action_dist_at_position</span>
                    <span class="n">sample_weight</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">pscore</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                    <span class="n">sample_weight</span> <span class="o">/=</span> <span class="n">pscore</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                        <span class="n">X</span><span class="p">,</span> <span class="n">reward</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span>
                    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Predict the mean reward function.</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        context: array-like, shape (n_rounds_of_new_data, dim_context)</span>
<span class="sd">            Context vectors of new data.</span>
<span class="sd">        Returns</span>
<span class="sd">        -----------</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds_of_new_data, n_actions, len_list)</span>
<span class="sd">            Expected rewards of new data estimated by the regression model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_rounds_of_new_data</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">ones_n_rounds_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_rounds_of_new_data</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
        <span class="n">estimated_rewards_by_reg_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">n_rounds_of_new_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">action_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">position_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
                <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_process_for_reg_model</span><span class="p">(</span>
                    <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
                    <span class="n">action</span><span class="o">=</span><span class="n">action_</span> <span class="o">*</span> <span class="n">ones_n_rounds_arr</span><span class="p">,</span>
                    <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">estimated_rewards_</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">]</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">])</span>
                    <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds_of_new_data</span><span class="p">),</span>
                    <span class="n">action_</span> <span class="o">*</span> <span class="n">ones_n_rounds_arr</span><span class="p">,</span>
                    <span class="n">position_</span> <span class="o">*</span> <span class="n">ones_n_rounds_arr</span><span class="p">,</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">estimated_rewards_</span>
        <span class="k">return</span> <span class="n">estimated_rewards_by_reg_model</span>

    <span class="k">def</span> <span class="nf">fit_predict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_folds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Fit the regression model on given logged bandit feedback data and predict the reward function of the same data.</span>
<span class="sd">        Note</span>
<span class="sd">        ------</span>
<span class="sd">        When `n_folds` is larger than 1, then the cross-fitting procedure is applied.</span>
<span class="sd">        See the reference for the details about the cross-fitting technique.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        context: array-like, shape (n_rounds, dim_context)</span>
<span class="sd">            Context vectors observed in each round of the logged bandit feedback, i.e., :math:`x_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Observed rewards (or outcome) in each round, i.e., :math:`r_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Action choice probabilities (propensity score) of a behavior policy</span>
<span class="sd">            in the training logged bandit feedback.</span>
<span class="sd">            When None is given, the the behavior policy is assumed to be a uniform one.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            If None is set, a regression model assumes that there is only one position.</span>
<span class="sd">            When `len_list` &gt; 1, this position argument has to be set.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list), default=None</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">            When either of &#39;iw&#39; or &#39;mrdr&#39; is used as the &#39;fitting_method&#39; argument, then `action_dist` must be given.</span>
<span class="sd">        n_folds: int, default=1</span>
<span class="sd">            Number of folds in the cross-fitting procedure.</span>
<span class="sd">            When 1 is given, the regression model is trained on the whole logged bandit feedback data.</span>
<span class="sd">            Please refer to https://arxiv.org/abs/2002.08536 about the details of the cross-fitting procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            `random_state` affects the ordering of the indices, which controls the randomness of each fold.</span>
<span class="sd">            See https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html for the details.</span>
<span class="sd">        Returns</span>
<span class="sd">        -----------</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards of new data estimated by the regression model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_bandit_feedback_inputs</span><span class="p">(</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">check_scalar</span><span class="p">(</span><span class="n">n_folds</span><span class="p">,</span> <span class="s2">&quot;n_folds&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">position</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;position elements must be smaller than len_list, but the maximum value is </span><span class="si">{</span><span class="n">position</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">}</span><span class="s2"> (&gt;= </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;iw&quot;</span><span class="p">,</span> <span class="s2">&quot;mrdr&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;when fitting_method is either &#39;iw&#39; or &#39;mrdr&#39;, action_dist (a 3-dimensional ndarray) must be given&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;shape of action_dist must be (n_rounds, n_actions, len_list)=(</span><span class="si">{</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="si">}</span><span class="s2">), but is </span><span class="si">{</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="n">pscore</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pscore</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span>

        <span class="k">if</span> <span class="n">n_folds</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
                <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">estimated_rewards_by_reg_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_folds</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">kf</span><span class="o">.</span><span class="n">get_n_splits</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">context</span><span class="p">):</span>
            <span class="n">action_dist_tr</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">action_dist</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span> <span class="k">if</span> <span class="n">action_dist</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">action_dist</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span>
                <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span>
                <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_tr</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span><span class="n">test_idx</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
                <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">estimated_rewards_by_reg_model</span>

    <span class="k">def</span> <span class="nf">_pre_process_for_reg_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Preprocess feature vectors to train a regression model.</span>
<span class="sd">        Note</span>
<span class="sd">        -----</span>
<span class="sd">        Please override this method if you want to use another feature enginnering</span>
<span class="sd">        for training the regression model.</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        context: array-like, shape (n_rounds,)</span>
<span class="sd">            Context vectors observed in each round of the logged bandit feedback, i.e., :math:`x_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        action_context: array-like, shape shape (n_actions, dim_action_context)</span>
<span class="sd">            Context vector characterizing action (i.e., vector representation of each action).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">context</span><span class="p">,</span> <span class="n">action_context</span><span class="p">[</span><span class="n">action</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="estimators">
<h2>Estimators<a class="headerlink" href="#estimators" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># from ..utils import check_array</span>
<span class="c1"># from ..utils import check_ope_inputs</span>
<span class="c1"># from ..utils import estimate_confidence_interval_by_bootstrap</span>
<span class="c1"># from .helper import estimate_bias_in_ope</span>
<span class="c1"># from .helper import estimate_high_probability_upper_bound_bias</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown estimator classes</span>
<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseOffPolicyEstimator</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for OPE estimators.&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_estimate_round_rewards</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate round-wise (or sample-wise) rewards.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">estimate_policy_value</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">estimate_interval</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate confidence interval of policy value by nonparametric bootstrap procedure.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">ReplayMethod</span><span class="p">(</span><span class="n">BaseOffPolicyEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Relpay Method (RM).</span>

<span class="sd">    Note</span>
<span class="sd">    -------</span>
<span class="sd">    Replay Method (RM) estimates the policy value of evaluation policy :math:`\\pi_e` by</span>

<span class="sd">    .. math::</span>

<span class="sd">        \\hat{V}_{\\mathrm{RM}} (\\pi_e; \\mathcal{D}) :=</span>
<span class="sd">        \\frac{\\mathbb{E}_{\\mathcal{D}}[\\mathbb{I} \\{ \\pi_e (x_t) = a_t \\} r_t ]}{\\mathbb{E}_{\\mathcal{D}}[\\mathbb{I} \\{ \\pi_e (x_t) = a_t \\}]},</span>

<span class="sd">    where :math:`\\mathcal{D}=\\{(x_t,a_t,r_t)\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by</span>
<span class="sd">    a behavior policy :math:`\\pi_b`. :math:`\\pi_e: \\mathcal{X} \\rightarrow \\mathcal{A}` is the function</span>
<span class="sd">    representing action choices by the evaluation policy realized during offline bandit simulation.</span>
<span class="sd">    :math:`\\mathbb{E}_{\\mathcal{D}}[\\cdot]` is the empirical average over :math:`T` observations in :math:`\\mathcal{D}`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator_name: str, default=&#39;rm&#39;.</span>
<span class="sd">        Name of the estimator.</span>

<span class="sd">    References</span>
<span class="sd">    ------------</span>
<span class="sd">    Lihong Li, Wei Chu, John Langford, and Xuanhui Wang.</span>
<span class="sd">    &quot;Unbiased Offline Evaluation of Contextual-bandit-based News Article Recommendation Algorithms.&quot;, 2011.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">estimator_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rm&quot;</span>

    <span class="k">def</span> <span class="nf">_estimate_round_rewards</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate round-wise (or sample-wise) rewards.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ------------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>

<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>

<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (must be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_rewards: array-like, shape (n_rounds,)</span>
<span class="sd">            Rewards of each round estimated by the Replay Method.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">action_match</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">estimated_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">action_match</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">action_match</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="n">estimated_rewards</span> <span class="o">=</span> <span class="n">action_match</span> <span class="o">*</span> <span class="n">reward</span> <span class="o">/</span> <span class="n">action_match</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">estimated_rewards</span>

    <span class="k">def</span> <span class="nf">estimate_policy_value</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ------------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>

<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>

<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (must be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        V_hat: float</span>
<span class="sd">            Estimated policy value (performance) of a given evaluation policy.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="o">=</span><span class="n">reward</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">estimate_interval</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate confidence interval of policy value by nonparametric bootstrap procedure.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>

<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>

<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (must be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>

<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>

<span class="sd">        n_bootstrap_samples: int, default=10000</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>

<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_confidence_interval: Dict[str, float]</span>
<span class="sd">            Dictionary storing the estimated mean and upper-lower confidence bounds.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="o">=</span><span class="n">reward</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">estimated_round_rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">estimate_confidence_interval_by_bootstrap</span><span class="p">(</span>
            <span class="n">samples</span><span class="o">=</span><span class="n">estimated_round_rewards</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">InverseProbabilityWeighting</span><span class="p">(</span><span class="n">BaseOffPolicyEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Inverse Probability Weighting (IPW) Estimator.</span>

<span class="sd">    Note</span>
<span class="sd">    -------</span>
<span class="sd">    Inverse Probability Weighting (IPW) estimates the policy value of evaluation policy :math:`\\pi_e` by</span>

<span class="sd">    .. math::</span>

<span class="sd">        \\hat{V}_{\\mathrm{IPW}} (\\pi_e; \\mathcal{D}) := \\mathbb{E}_{\\mathcal{D}} [ w(x_t,a_t) r_t],</span>

<span class="sd">    where :math:`\\mathcal{D}=\\{(x_t,a_t,r_t)\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by</span>
<span class="sd">    a behavior policy :math:`\\pi_b`. :math:`w(x,a):=\\pi_e (a|x)/\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\mathbb{E}_{\\mathcal{D}}[\\cdot]` is the empirical average over :math:`T` observations in :math:`\\mathcal{D}`.</span>
<span class="sd">    When the weight-clipping is applied, a large importance weight is clipped as :math:`\\hat{w}(x,a) := \\min \\{ \\lambda, w(x,a) \\}`</span>
<span class="sd">    where :math:`\\lambda (&gt;0)` is a hyperparameter that decides a maximum allowed importance weight.</span>

<span class="sd">    IPW re-weights the rewards by the ratio of the evaluation policy and behavior policy (importance weight).</span>
<span class="sd">    When the behavior policy is known, IPW is unbiased and consistent for the true policy value.</span>
<span class="sd">    However, it can have a large variance, especially when the evaluation policy significantly deviates from the behavior policy.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ------------</span>
<span class="sd">    lambda_: float, default=np.inf</span>
<span class="sd">        A maximum possible value of the importance weight.</span>
<span class="sd">        When a positive finite value is given, importance weights larger than `lambda_` will be clipped.</span>

<span class="sd">    estimator_name: str, default=&#39;ipw&#39;.</span>
<span class="sd">        Name of the estimator.</span>

<span class="sd">    References</span>
<span class="sd">    ------------</span>
<span class="sd">    Alex Strehl, John Langford, Lihong Li, and Sham M Kakade.</span>
<span class="sd">    &quot;Learning from Logged Implicit Exploration Data&quot;., 2010.</span>

<span class="sd">    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.</span>
<span class="sd">    &quot;Doubly Robust Policy Evaluation and Optimization.&quot;, 2014.</span>

<span class="sd">    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.</span>
<span class="sd">    &quot;Doubly Robust Off-Policy Evaluation with Shrinkage.&quot;, 2020.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">lambda_</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    <span class="n">estimator_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;ipw&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Class.&quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;lambda_&quot;</span><span class="p">,</span>
            <span class="n">target_type</span><span class="o">=</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">),</span>
            <span class="n">min_val</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;lambda_ must not be nan&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_estimate_round_rewards</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate round-wise (or sample-wise) rewards.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>

<span class="sd">        action: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>

<span class="sd">        pscore: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>

<span class="sd">        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        position: array-like or Tensor, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_rewards: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Rewards of each round estimated by IPW.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="c1"># weight clipping</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">iw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">reward</span> <span class="o">*</span> <span class="n">iw</span>

    <span class="k">def</span> <span class="nf">estimate_policy_value</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>

<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>

<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>

<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        V_hat: float</span>
<span class="sd">            Estimated policy value (performance) of a given evaluation policy.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">estimate_interval</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate confidence interval of policy value by nonparametric bootstrap procedure.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>

<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>

<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>

<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>

<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>

<span class="sd">        n_bootstrap_samples: int, default=10000</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>

<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_confidence_interval: Dict[str, float]</span>
<span class="sd">            Dictionary storing the estimated mean and upper-lower confidence bounds.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">estimated_round_rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">estimate_confidence_interval_by_bootstrap</span><span class="p">(</span>
            <span class="n">samples</span><span class="o">=</span><span class="n">estimated_round_rewards</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_estimate_mse_score</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_bias_upper_bound</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the MSE score of a given clipping hyperparameter to conduct hyperparameter tuning.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>

<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>

<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>

<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>

<span class="sd">        use_bias_upper_bound: bool, default=True</span>
<span class="sd">            Whether to use bias upper bound in hyperparameter tuning.</span>
<span class="sd">            If False, direct bias estimator is used to estimate the MSE.</span>

<span class="sd">        delta: float, default=0.05</span>
<span class="sd">            A confidence delta to construct a high probability upper bound based on the Bernstein’s inequality.</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_mse_score: float</span>
<span class="sd">            Estimated MSE score of a given clipping hyperparameter `lambda_`.</span>
<span class="sd">            MSE score is the sum of (high probability) upper bound of bias and the sample variance.</span>
<span class="sd">            This is estimated using the automatic hyperparameter tuning procedure</span>
<span class="sd">            based on Section 5 of Su et al.(2020).</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># estimate the sample variance of IPW with clipping</span>
        <span class="n">sample_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
                <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">sample_variance</span> <span class="o">/=</span> <span class="n">n_rounds</span>

        <span class="c1"># estimate the (high probability) upper bound of the bias of IPW with clipping</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="k">if</span> <span class="n">use_bias_upper_bound</span><span class="p">:</span>
            <span class="n">bias_term</span> <span class="o">=</span> <span class="n">estimate_high_probability_upper_bound_bias</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">iw</span><span class="o">=</span><span class="n">iw</span><span class="p">,</span> <span class="n">iw_hat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">),</span> <span class="n">delta</span><span class="o">=</span><span class="n">delta</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bias_term</span> <span class="o">=</span> <span class="n">estimate_bias_in_ope</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">iw</span><span class="o">=</span><span class="n">iw</span><span class="p">,</span>
                <span class="n">iw_hat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="n">estimated_mse_score</span> <span class="o">=</span> <span class="n">sample_variance</span> <span class="o">+</span> <span class="p">(</span><span class="n">bias_term</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">estimated_mse_score</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">SelfNormalizedInverseProbabilityWeighting</span><span class="p">(</span><span class="n">InverseProbabilityWeighting</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Self-Normalized Inverse Probability Weighting (SNIPW) Estimator.</span>

<span class="sd">    Note</span>
<span class="sd">    -------</span>
<span class="sd">    Self-Normalized Inverse Probability Weighting (SNIPW) estimates the policy value of evaluation policy :math:`\\pi_e` by</span>

<span class="sd">    .. math::</span>

<span class="sd">        \\hat{V}_{\\mathrm{SNIPW}} (\\pi_e; \\mathcal{D}) :=</span>
<span class="sd">        \\frac{\\mathbb{E}_{\\mathcal{D}} [w(x_t,a_t) r_t]}{ \\mathbb{E}_{\\mathcal{D}} [w(x_t,a_t)]},</span>

<span class="sd">    where :math:`\\mathcal{D}=\\{(x_t,a_t,r_t)\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by</span>
<span class="sd">    a behavior policy :math:`\\pi_b`. :math:`w(x,a):=\\pi_e (a|x)/\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\mathbb{E}_{\\mathcal{D}}[\\cdot]` is the empirical average over :math:`T` observations in :math:`\\mathcal{D}`.</span>

<span class="sd">    SNIPW re-weights the observed rewards by the self-normalized importance weihgt.</span>
<span class="sd">    This estimator is not unbiased even when the behavior policy is known.</span>
<span class="sd">    However, it is still consistent for the true policy value and increases the stability in some senses.</span>
<span class="sd">    See the references for the detailed discussions.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator_name: str, default=&#39;snipw&#39;.</span>
<span class="sd">        Name of the estimator.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Adith Swaminathan and Thorsten Joachims.</span>
<span class="sd">    &quot;The Self-normalized Estimator for Counterfactual Learning.&quot;, 2015.</span>

<span class="sd">    Nathan Kallus and Masatoshi Uehara.</span>
<span class="sd">    &quot;Intrinsically Efficient, Stable, and Bounded Off-Policy Evaluation for Reinforcement Learning.&quot;, 2019.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">estimator_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;snipw&quot;</span>

    <span class="k">def</span> <span class="nf">_estimate_round_rewards</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate round-wise (or sample-wise) rewards.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>

<span class="sd">        action: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>

<span class="sd">        pscore: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>

<span class="sd">        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        position: array-like or Tensor, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_rewards: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Rewards of each round estimated by the SNIPW estimator.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="k">return</span> <span class="n">reward</span> <span class="o">*</span> <span class="n">iw</span> <span class="o">/</span> <span class="n">iw</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">DirectMethod</span><span class="p">(</span><span class="n">BaseOffPolicyEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Direct Method (DM).</span>

<span class="sd">    Note</span>
<span class="sd">    -------</span>
<span class="sd">    DM first learns a supervised machine learning model, such as ridge regression and gradient boosting,</span>
<span class="sd">    to estimate the mean reward function (:math:`q(x,a) = \\mathbb{E}[r|x,a]`).</span>
<span class="sd">    It then uses it to estimate the policy value as follows.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \\hat{V}_{\\mathrm{DM}} (\\pi_e; \\mathcal{D}, \\hat{q})</span>
<span class="sd">        &amp;:= \\mathbb{E}_{\\mathcal{D}} \\left[ \\sum_{a \\in \\mathcal{A}} \\hat{q} (x_t,a) \\pi_e(a|x_t) \\right],    \\\\</span>
<span class="sd">        &amp; =  \\mathbb{E}_{\\mathcal{D}}[\\hat{q} (x_t,\\pi_e)],</span>

<span class="sd">    where :math:`\\mathcal{D}=\\{(x_t,a_t,r_t)\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by</span>
<span class="sd">    a behavior policy :math:`\\pi_b`. :math:`\\mathbb{E}_{\\mathcal{D}}[\\cdot]` is the empirical average over :math:`T` observations in :math:`\\mathcal{D}`.</span>
<span class="sd">    :math:`\\hat{q} (x,a)` is an estimated expected reward given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\hat{q} (x_t,\\pi):= \\mathbb{E}_{a \\sim \\pi(a|x)}[\\hat{q}(x,a)]` is the expectation of the estimated reward function over :math:`\\pi`.</span>
<span class="sd">    To estimate the mean reward function, please use `obp.ope.regression_model.RegressionModel`, which supports several fitting methods specific to OPE.</span>

<span class="sd">    If the regression model (:math:`\\hat{q}`) is a good approximation to the true mean reward function,</span>
<span class="sd">    this estimator accurately estimates the policy value of the evaluation policy.</span>
<span class="sd">    If the regression function fails to approximate the mean reward function well,</span>
<span class="sd">    however, the final estimator is no longer consistent.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator_name: str, default=&#39;dm&#39;.</span>
<span class="sd">        Name of the estimator.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Alina Beygelzimer and John Langford.</span>
<span class="sd">    &quot;The offset tree for learning with partial labels.&quot;, 2009.</span>

<span class="sd">    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.</span>
<span class="sd">    &quot;Doubly Robust Policy Evaluation and Optimization.&quot;, 2014.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">estimator_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;dm&quot;</span>

    <span class="k">def</span> <span class="nf">_estimate_round_rewards</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        estimated_rewards_by_reg_model: array-like or Tensor, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>

<span class="sd">        position: array-like or Tensor, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_rewards: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Rewards of each round estimated by the DM estimator.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">position</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">q_hat_at_position</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span>
        <span class="p">]</span>
        <span class="n">pi_e_at_position</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
                <span class="n">q_hat_at_position</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="n">pi_e_at_position</span><span class="p">,</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;action must be 1D array&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">estimate_policy_value</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>

<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        V_hat: float</span>
<span class="sd">            Estimated policy value (performance) of a given evaluation policy.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span>
            <span class="n">array</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span><span class="p">,</span>
            <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">estimate_interval</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate confidence interval of policy value by nonparametric bootstrap procedure.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>

<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>

<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>

<span class="sd">        n_bootstrap_samples: int, default=10000</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>

<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_confidence_interval: Dict[str, float]</span>
<span class="sd">            Dictionary storing the estimated mean and upper-lower confidence bounds.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span>
            <span class="n">array</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span><span class="p">,</span>
            <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">estimated_round_rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">estimate_confidence_interval_by_bootstrap</span><span class="p">(</span>
            <span class="n">samples</span><span class="o">=</span><span class="n">estimated_round_rewards</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">DoublyRobust</span><span class="p">(</span><span class="n">BaseOffPolicyEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Doubly Robust (DR) Estimator.</span>

<span class="sd">    Note</span>
<span class="sd">    -------</span>
<span class="sd">    Similar to DM, DR first learns a supervised machine learning model, such as ridge regression and gradient boosting,</span>
<span class="sd">    to estimate the mean reward function (:math:`q(x,a) = \\mathbb{E}[r|x,a]`).</span>
<span class="sd">    It then uses it to estimate the policy value as follows.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \\hat{V}_{\\mathrm{DR}} (\\pi_e; \\mathcal{D}, \\hat{q})</span>
<span class="sd">        := \\mathbb{E}_{\\mathcal{D}}[\\hat{q}(x_t,\\pi_e) +  w(x_t,a_t) (r_t - \\hat{q}(x_t,a_t))],</span>

<span class="sd">    where :math:`\\mathcal{D}=\\{(x_t,a_t,r_t)\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by</span>
<span class="sd">    a behavior policy :math:`\\pi_b`.</span>
<span class="sd">    :math:`w(x,a):=\\pi_e (a|x)/\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\mathbb{E}_{\\mathcal{D}}[\\cdot]` is the empirical average over :math:`T` observations in :math:`\\mathcal{D}`.</span>
<span class="sd">    :math:`\\hat{q} (x,a)` is an estimated expected reward given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\hat{q} (x_t,\\pi):= \\mathbb{E}_{a \\sim \\pi(a|x)}[\\hat{q}(x,a)]` is the expectation of the estimated reward function over :math:`\\pi`.</span>
<span class="sd">    When the weight-clipping is applied, a large importance weight is clipped as :math:`\\hat{w}(x,a) := \\min \\{ \\lambda, w(x,a) \\}`</span>
<span class="sd">    where :math:`\\lambda (&gt;0)` is a hyperparameter that decides a maximum allowed importance weight.</span>

<span class="sd">    To estimate the mean reward function, please use `obp.ope.regression_model.RegressionModel`,</span>
<span class="sd">    which supports several fitting methods specific to OPE such as *more robust doubly robust*.</span>

<span class="sd">    DR mimics IPW to use a weighted version of rewards, but DR also uses the estimated mean reward</span>
<span class="sd">    function (the regression model) as a control variate to decrease the variance.</span>
<span class="sd">    It preserves the consistency of IPW if either the importance weight or</span>
<span class="sd">    the mean reward estimator is accurate (a property called double robustness).</span>
<span class="sd">    Moreover, DR is semiparametric efficient when the mean reward estimator is correctly specified.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    lambda_: float, default=np.inf</span>
<span class="sd">        A maximum possible value of the importance weight.</span>
<span class="sd">        When a positive finite value is given, importance weights larger than `lambda_` will be clipped.</span>
<span class="sd">        DoublyRobust with a finite positive `lambda_` corresponds to Doubly Robust with Pessimistic Shrinkage of Su et al.(2020) or CAB-DR of Su et al.(2019).</span>

<span class="sd">    estimator_name: str, default=&#39;dr&#39;.</span>
<span class="sd">        Name of the estimator.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.</span>
<span class="sd">    &quot;Doubly Robust Policy Evaluation and Optimization.&quot;, 2014.</span>

<span class="sd">    Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh.</span>
<span class="sd">    &quot;More Robust Doubly Robust Off-policy Evaluation.&quot;, 2018.</span>

<span class="sd">    Yi Su, Lequn Wang, Michele Santacatterina, and Thorsten Joachims.</span>
<span class="sd">    &quot;CAB: Continuous Adaptive Blending Estimator for Policy Evaluation and Learning&quot;, 2019.</span>

<span class="sd">    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudík.</span>
<span class="sd">    &quot;Doubly robust off-policy evaluation with shrinkage.&quot;, 2020.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">lambda_</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    <span class="n">estimator_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;dr&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Class.&quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;lambda_&quot;</span><span class="p">,</span>
            <span class="n">target_type</span><span class="o">=</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">),</span>
            <span class="n">min_val</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;lambda_ must not be nan&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_estimate_round_rewards</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate round-wise (or sample-wise) rewards.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>

<span class="sd">        action: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>

<span class="sd">        pscore: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>

<span class="sd">        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        estimated_rewards_by_reg_model or Tensor: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>

<span class="sd">        position: array-like or Tensor, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_rewards: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Rewards of each round estimated by the DR estimator.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="c1"># weight clipping</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">iw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">)</span>
        <span class="n">q_hat_at_position</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span>
        <span class="p">]</span>
        <span class="n">q_hat_factual</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span>
        <span class="p">]</span>
        <span class="n">pi_e_at_position</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">estimated_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
                <span class="n">q_hat_at_position</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="n">pi_e_at_position</span><span class="p">,</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;reward must be 1D array&quot;</span><span class="p">)</span>

        <span class="n">estimated_rewards</span> <span class="o">+=</span> <span class="n">iw</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">q_hat_factual</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">estimated_rewards</span>

    <span class="k">def</span> <span class="nf">estimate_policy_value</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>

<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>

<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>

<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>

<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        V_hat: float</span>
<span class="sd">            Policy value estimated by the DR estimator.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span>
            <span class="n">array</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span><span class="p">,</span>
            <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">estimate_interval</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate confidence interval of policy value by nonparametric bootstrap procedure.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>

<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>

<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>

<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>

<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>

<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>

<span class="sd">        n_bootstrap_samples: int, default=10000</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>

<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_confidence_interval: Dict[str, float]</span>
<span class="sd">            Dictionary storing the estimated mean and upper-lower confidence bounds.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span>
            <span class="n">array</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span><span class="p">,</span>
            <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">estimated_round_rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">estimate_confidence_interval_by_bootstrap</span><span class="p">(</span>
            <span class="n">samples</span><span class="o">=</span><span class="n">estimated_round_rewards</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_estimate_mse_score</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_bias_upper_bound</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the MSE score of a given clipping hyperparameter to conduct hyperparameter tuning.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>

<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>

<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>

<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>

<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>

<span class="sd">        use_bias_upper_bound: bool, default=True</span>
<span class="sd">            Whether to use bias upper bound in hyperparameter tuning.</span>
<span class="sd">            If False, direct bias estimator is used to estimate the MSE.</span>

<span class="sd">        delta: float, default=0.05</span>
<span class="sd">            A confidence delta to construct a high probability upper bound based on the Bernstein’s inequality.</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_mse_score: float</span>
<span class="sd">            Estimated MSE score of a given clipping hyperparameter `lambda_`.</span>
<span class="sd">            MSE score is the sum of (high probability) upper bound of bias and the sample variance.</span>
<span class="sd">            This is estimated using the automatic hyperparameter tuning procedure</span>
<span class="sd">            based on Section 5 of Su et al.(2020).</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># estimate the sample variance of DR with clipping</span>
        <span class="n">sample_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
                <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
                <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">sample_variance</span> <span class="o">/=</span> <span class="n">n_rounds</span>

        <span class="c1"># estimate the (high probability) upper bound of the bias of DR with clipping</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="k">if</span> <span class="n">use_bias_upper_bound</span><span class="p">:</span>
            <span class="n">bias_term</span> <span class="o">=</span> <span class="n">estimate_high_probability_upper_bound_bias</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">iw</span><span class="o">=</span><span class="n">iw</span><span class="p">,</span>
                <span class="n">iw_hat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">),</span>
                <span class="n">q_hat</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span>
                <span class="p">],</span>
                <span class="n">delta</span><span class="o">=</span><span class="n">delta</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bias_term</span> <span class="o">=</span> <span class="n">estimate_bias_in_ope</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">iw</span><span class="o">=</span><span class="n">iw</span><span class="p">,</span>
                <span class="n">iw_hat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">),</span>
                <span class="n">q_hat</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span>
                <span class="p">],</span>
            <span class="p">)</span>
        <span class="n">estimated_mse_score</span> <span class="o">=</span> <span class="n">sample_variance</span> <span class="o">+</span> <span class="p">(</span><span class="n">bias_term</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">estimated_mse_score</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">SelfNormalizedDoublyRobust</span><span class="p">(</span><span class="n">DoublyRobust</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Self-Normalized Doubly Robust (SNDR) Estimator.</span>

<span class="sd">    Note</span>
<span class="sd">    -------</span>
<span class="sd">    Self-Normalized Doubly Robust estimates the policy value of evaluation policy :math:`\\pi_e` by</span>

<span class="sd">    .. math::</span>

<span class="sd">        \\hat{V}_{\\mathrm{SNDR}} (\\pi_e; \\mathcal{D}, \\hat{q}) :=</span>
<span class="sd">        \\mathbb{E}_{\\mathcal{D}} \\left[\\hat{q}(x_t,\\pi_e) +  \\frac{w(x_t,a_t) (r_t - \\hat{q}(x_t,a_t))}{\\mathbb{E}_{\\mathcal{D}}[ w(x_t,a_t) ]} \\right],</span>

<span class="sd">    where :math:`\\mathcal{D}=\\{(x_t,a_t,r_t)\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by</span>
<span class="sd">    a behavior policy :math:`\\pi_b`. :math:`w(x,a):=\\pi_e (a|x)/\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\mathbb{E}_{\\mathcal{D}}[\\cdot]` is the empirical average over :math:`T` observations in :math:`\\mathcal{D}`.</span>
<span class="sd">    :math:`\\hat{q} (x,a)` is an estimated expected reward given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\hat{q} (x_t,\\pi):= \\mathbb{E}_{a \\sim \\pi(a|x)}[\\hat{q}(x,a)]` is the expectation of the estimated reward function over :math:`\\pi`.</span>
<span class="sd">    To estimate the mean reward function, please use `obp.ope.regression_model.RegressionModel`.</span>

<span class="sd">    Similar to Self-Normalized Inverse Probability Weighting, SNDR estimator applies the self-normalized importance weighting technique to</span>
<span class="sd">    increase the stability of the original Doubly Robust estimator.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator_name: str, default=&#39;sndr&#39;.</span>
<span class="sd">        Name of the estimator.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.</span>
<span class="sd">    &quot;Doubly Robust Policy Evaluation and Optimization.&quot;, 2014.</span>

<span class="sd">    Nathan Kallus and Masatoshi Uehara.</span>
<span class="sd">    &quot;Intrinsically Efficient, Stable, and Bounded Off-Policy Evaluation for Reinforcement Learning.&quot;, 2019.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">estimator_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sndr&quot;</span>

    <span class="k">def</span> <span class="nf">_estimate_round_rewards</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate round-wise (or sample-wise) rewards.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>

<span class="sd">        action: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>

<span class="sd">        pscore: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>

<span class="sd">        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        estimated_rewards_by_reg_model: array-like or Tensor, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>

<span class="sd">        position: array-like or Tensor, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_rewards: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Rewards of each round estimated by the SNDR estimator.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="n">q_hat_at_position</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span>
        <span class="p">]</span>
        <span class="n">pi_e_at_position</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">estimated_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
                <span class="n">q_hat_at_position</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="n">pi_e_at_position</span><span class="p">,</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;reward must be 1D array&quot;</span><span class="p">)</span>

        <span class="n">q_hat_factual</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span>
        <span class="p">]</span>
        <span class="n">estimated_rewards</span> <span class="o">+=</span> <span class="n">iw</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">q_hat_factual</span><span class="p">)</span> <span class="o">/</span> <span class="n">iw</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">estimated_rewards</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">SwitchDoublyRobust</span><span class="p">(</span><span class="n">DoublyRobust</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Switch Doubly Robust (Switch-DR) Estimator.</span>

<span class="sd">    Note</span>
<span class="sd">    -------</span>
<span class="sd">    Switch-DR aims to reduce the variance of the DR estimator by using direct method when the importance weight is large.</span>
<span class="sd">    This estimator estimates the policy value of evaluation policy :math:`\\pi_e` by</span>

<span class="sd">    .. math::</span>

<span class="sd">        \\hat{V}_{\\mathrm{SwitchDR}} (\\pi_e; \\mathcal{D}, \\hat{q}, \\lambda)</span>
<span class="sd">        := \\mathbb{E}_{\\mathcal{D}} [\\hat{q}(x_t,\\pi_e) +  w(x_t,a_t) (r_t - \\hat{q}(x_t,a_t)) \\mathbb{I} \\{ w(x_t,a_t) \\le \\lambda \\}],</span>

<span class="sd">    where :math:`\\mathcal{D}=\\{(x_t,a_t,r_t)\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by</span>
<span class="sd">    a behavior policy :math:`\\pi_b`. :math:`w(x,a):=\\pi_e (a|x)/\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\mathbb{E}_{\\mathcal{D}}[\\cdot]` is the empirical average over :math:`T` observations in :math:`\\mathcal{D}`.</span>
<span class="sd">    :math:`\\lambda (\\ge 0)` is a switching hyperparameter, which decides the threshold for the importance weight.</span>
<span class="sd">    :math:`\\hat{q} (x,a)` is an estimated expected reward given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\hat{q} (x_t,\\pi):= \\mathbb{E}_{a \\sim \\pi(a|x)}[\\hat{q}(x,a)]` is the expectation of the estimated reward function over :math:`\\pi`.</span>
<span class="sd">    To estimate the mean reward function, please use `obp.ope.regression_model.RegressionModel`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    lambda_: float, default=np.inf</span>
<span class="sd">        Switching hyperparameter. When importance weight is larger than this parameter, DM is applied, otherwise DR is used.</span>
<span class="sd">        This hyperparameter should be larger than or equal to 0., otherwise it is meaningless.</span>

<span class="sd">    estimator_name: str, default=&#39;switch-dr&#39;.</span>
<span class="sd">        Name of the estimator.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.</span>
<span class="sd">    &quot;Doubly Robust Policy Evaluation and Optimization.&quot;, 2014.</span>

<span class="sd">    Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudík.</span>
<span class="sd">    &quot;Optimal and Adaptive Off-policy Evaluation in Contextual Bandits&quot;, 2016.</span>

<span class="sd">    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.</span>
<span class="sd">    &quot;Doubly Robust Off-Policy Evaluation with Shrinkage.&quot;, 2020.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">lambda_</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    <span class="n">estimator_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;switch-dr&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Class.&quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;lambda_&quot;</span><span class="p">,</span>
            <span class="n">target_type</span><span class="o">=</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">),</span>
            <span class="n">min_val</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;lambda_ must not be nan&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_estimate_round_rewards</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate round-wise (or sample-wise) rewards.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>

<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>

<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>

<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>

<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_rewards: array-like, shape (n_rounds,)</span>
<span class="sd">            Rewards of each round estimated by the Switch-DR estimator.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="n">switch_indicator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">iw</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">q_hat_at_position</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span>
        <span class="p">]</span>
        <span class="n">q_hat_factual</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span>
        <span class="p">]</span>
        <span class="n">pi_e_at_position</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span><span class="p">]</span>
        <span class="n">estimated_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
            <span class="n">q_hat_at_position</span><span class="p">,</span>
            <span class="n">weights</span><span class="o">=</span><span class="n">pi_e_at_position</span><span class="p">,</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">estimated_rewards</span> <span class="o">+=</span> <span class="n">switch_indicator</span> <span class="o">*</span> <span class="n">iw</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">q_hat_factual</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">estimated_rewards</span>

    <span class="k">def</span> <span class="nf">_estimate_mse_score</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_bias_upper_bound</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the MSE score of a given switching hyperparameter to conduct hyperparameter tuning.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>

<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>

<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>

<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>

<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>

<span class="sd">        use_bias_upper_bound: bool, default=True</span>
<span class="sd">            Whether to use bias upper bound in hyperparameter tuning.</span>
<span class="sd">            If False, direct bias estimator is used to estimate the MSE.</span>

<span class="sd">        delta: float, default=0.05</span>
<span class="sd">            A confidence delta to construct a high probability upper bound based on the Bernstein’s inequality.</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_mse_score: float</span>
<span class="sd">            Estimated MSE score of a given switching hyperparameter `lambda_`.</span>
<span class="sd">            MSE score is the sum of (high probability) upper bound of bias and the sample variance.</span>
<span class="sd">            This is estimated using the automatic hyperparameter tuning procedure</span>
<span class="sd">            based on Section 5 of Su et al.(2020).</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># estimate the sample variance of Switch-DR (Eq.(8) of Wang et al.(2017))</span>
        <span class="n">sample_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
                <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
                <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">sample_variance</span> <span class="o">/=</span> <span class="n">n_rounds</span>

        <span class="c1"># estimate the (high probability) upper bound of the bias of Switch-DR</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="k">if</span> <span class="n">use_bias_upper_bound</span><span class="p">:</span>
            <span class="n">bias_term</span> <span class="o">=</span> <span class="n">estimate_high_probability_upper_bound_bias</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">iw</span><span class="o">=</span><span class="n">iw</span><span class="p">,</span>
                <span class="n">iw_hat</span><span class="o">=</span><span class="n">iw</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">iw</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
                <span class="n">q_hat</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span>
                <span class="p">],</span>
                <span class="n">delta</span><span class="o">=</span><span class="n">delta</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bias_term</span> <span class="o">=</span> <span class="n">estimate_bias_in_ope</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">iw</span><span class="o">=</span><span class="n">iw</span><span class="p">,</span>
                <span class="n">iw_hat</span><span class="o">=</span><span class="n">iw</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">iw</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
                <span class="n">q_hat</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span>
                <span class="p">],</span>
            <span class="p">)</span>
        <span class="n">estimated_mse_score</span> <span class="o">=</span> <span class="n">sample_variance</span> <span class="o">+</span> <span class="p">(</span><span class="n">bias_term</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">estimated_mse_score</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">DoublyRobustWithShrinkage</span><span class="p">(</span><span class="n">DoublyRobust</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Doubly Robust with optimistic shrinkage (DRos) Estimator.</span>

<span class="sd">    Note</span>
<span class="sd">    ------</span>
<span class="sd">    DR with (optimistic) shrinkage replaces the importance weight in the original DR estimator with a new weight mapping</span>
<span class="sd">    found by directly optimizing sharp bounds on the resulting MSE.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \\hat{V}_{\\mathrm{DRos}} (\\pi_e; \\mathcal{D}, \\hat{q}, \\lambda)</span>
<span class="sd">        := \\mathbb{E}_{\\mathcal{D}} [\\hat{q}(x_t,\\pi_e) +  w_o(x_t,a_t;\\lambda) (r_t - \\hat{q}(x_t,a_t))],</span>

<span class="sd">    where :math:`\\mathcal{D}=\\{(x_t,a_t,r_t)\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by</span>
<span class="sd">    a behavior policy :math:`\\pi_b`.</span>
<span class="sd">    :math:`w(x,a):=\\pi_e (a|x)/\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\hat{q} (x_t,\\pi):= \\mathbb{E}_{a \\sim \\pi(a|x)}[\\hat{q}(x,a)]` is the expectation of the estimated reward function over :math:`\\pi`.</span>
<span class="sd">    :math:`\\mathbb{E}_{\\mathcal{D}}[\\cdot]` is the empirical average over :math:`T` observations in :math:`\\mathcal{D}`.</span>
<span class="sd">    :math:`\\hat{q} (x,a)` is an estimated expected reward given :math:`x` and :math:`a`.</span>
<span class="sd">    To estimate the mean reward function, please use `obp.ope.regression_model.RegressionModel`.</span>

<span class="sd">    :math:`w_{o} (x_t,a_t;\\lambda)` is a new weight by the shrinkage technique which is defined as</span>

<span class="sd">    .. math::</span>

<span class="sd">        w_{o} (x_t,a_t;\\lambda) := \\frac{\\lambda}{w^2(x_t,a_t) + \\lambda} w(x_t,a_t).</span>

<span class="sd">    When :math:`\\lambda=0`, we have :math:`w_{o} (x,a;\\lambda)=0` corresponding to the DM estimator.</span>
<span class="sd">    In contrast, as :math:`\\lambda \\rightarrow \\infty`, :math:`w_{o} (x,a;\\lambda)` increases and in the limit becomes equal to the original importance weight, corresponding to the standard DR estimator.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    lambda_: float</span>
<span class="sd">        Shrinkage hyperparameter.</span>
<span class="sd">        This hyperparameter should be larger than or equal to 0., otherwise it is meaningless.</span>

<span class="sd">    estimator_name: str, default=&#39;dr-os&#39;.</span>
<span class="sd">        Name of the estimator.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.</span>
<span class="sd">    &quot;Doubly Robust Policy Evaluation and Optimization.&quot;, 2014.</span>

<span class="sd">    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.</span>
<span class="sd">    &quot;Doubly Robust Off-Policy Evaluation with Shrinkage.&quot;, 2020.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">lambda_</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">estimator_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;dr-os&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Class.&quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;lambda_&quot;</span><span class="p">,</span>
            <span class="n">target_type</span><span class="o">=</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">),</span>
            <span class="n">min_val</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;lambda_ must not be nan&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_estimate_round_rewards</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate round-wise (or sample-wise) rewards.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>

<span class="sd">        action: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>

<span class="sd">        pscore: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>

<span class="sd">        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        estimated_rewards_by_reg_model: array-like or Tensor, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>

<span class="sd">        position: array-like or Tensor, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_rewards: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Rewards of each round estimated by the DRos estimator.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span>
            <span class="n">iw_hat</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">*</span> <span class="n">iw</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">iw</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">iw_hat</span> <span class="o">=</span> <span class="n">iw</span>
        <span class="n">q_hat_at_position</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span>
        <span class="p">]</span>
        <span class="n">q_hat_factual</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span>
        <span class="p">]</span>
        <span class="n">pi_e_at_position</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">estimated_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
                <span class="n">q_hat_at_position</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="n">pi_e_at_position</span><span class="p">,</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;reward must be 1D array&quot;</span><span class="p">)</span>

        <span class="n">estimated_rewards</span> <span class="o">+=</span> <span class="n">iw_hat</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">q_hat_factual</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">estimated_rewards</span>

    <span class="k">def</span> <span class="nf">_estimate_mse_score</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_bias_upper_bound</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the MSE score of a given shrinkage hyperparameter to conduct hyperparameter tuning.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>

<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>

<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>

<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>

<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>

<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>

<span class="sd">        use_bias_upper_bound: bool, default=True</span>
<span class="sd">            Whether to use bias upper bound in hyperparameter tuning.</span>
<span class="sd">            If False, direct bias estimator is used to estimate the MSE.</span>

<span class="sd">        delta: float, default=0.05</span>
<span class="sd">            A confidence delta to construct a high probability upper bound based on the Bernstein’s inequality.</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_mse_score: float</span>
<span class="sd">            Estimated MSE score of a given shrinkage hyperparameter `lambda_`.</span>
<span class="sd">            MSE score is the sum of (high probability) upper bound of bias and the sample variance.</span>
<span class="sd">            This is estimated using the automatic hyperparameter tuning procedure</span>
<span class="sd">            based on Section 5 of Su et al.(2020).</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># estimate the sample variance of DRos</span>
        <span class="n">sample_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
                <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
                <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">sample_variance</span> <span class="o">/=</span> <span class="n">n_rounds</span>

        <span class="c1"># estimate the (high probability) upper bound of the bias of DRos</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span>
            <span class="n">iw_hat</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">*</span> <span class="n">iw</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">iw</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">iw_hat</span> <span class="o">=</span> <span class="n">iw</span>
        <span class="k">if</span> <span class="n">use_bias_upper_bound</span><span class="p">:</span>
            <span class="n">bias_term</span> <span class="o">=</span> <span class="n">estimate_high_probability_upper_bound_bias</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">iw</span><span class="o">=</span><span class="n">iw</span><span class="p">,</span>
                <span class="n">iw_hat</span><span class="o">=</span><span class="n">iw_hat</span><span class="p">,</span>
                <span class="n">q_hat</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span>
                <span class="p">],</span>
                <span class="n">delta</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bias_term</span> <span class="o">=</span> <span class="n">estimate_bias_in_ope</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">iw</span><span class="o">=</span><span class="n">iw</span><span class="p">,</span>
                <span class="n">iw_hat</span><span class="o">=</span><span class="n">iw_hat</span><span class="p">,</span>
                <span class="n">q_hat</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span>
                <span class="p">],</span>
            <span class="p">)</span>
        <span class="n">estimated_mse_score</span> <span class="o">=</span> <span class="n">sample_variance</span> <span class="o">+</span> <span class="p">(</span><span class="n">bias_term</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">estimated_mse_score</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown off-policy estimator</span>
<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">OffPolicyEvaluation</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Class to conduct OPE by multiple estimators simultaneously.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    bandit_feedback: BanditFeedback</span>
<span class="sd">        Logged bandit feedback data used to conduct OPE.</span>
<span class="sd">    ope_estimators: List[BaseOffPolicyEstimator]</span>
<span class="sd">        List of OPE estimators used to evaluate the policy value of evaluation policy.</span>
<span class="sd">        Estimators must follow the interface of `obp.ope.BaseOffPolicyEstimator`.</span>
<span class="sd">    Examples</span>
<span class="sd">    ----------</span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">        # a case for implementing OPE of the BernoulliTS policy</span>
<span class="sd">        # using log data generated by the Random policy</span>
<span class="sd">        &gt;&gt;&gt; from obp.dataset import OpenBanditDataset</span>
<span class="sd">        &gt;&gt;&gt; from obp.policy import BernoulliTS</span>
<span class="sd">        &gt;&gt;&gt; from obp.ope import OffPolicyEvaluation, InverseProbabilityWeighting as IPW</span>
<span class="sd">        # (1) Data loading and preprocessing</span>
<span class="sd">        &gt;&gt;&gt; dataset = OpenBanditDataset(behavior_policy=&#39;random&#39;, campaign=&#39;all&#39;)</span>
<span class="sd">        &gt;&gt;&gt; bandit_feedback = dataset.obtain_batch_bandit_feedback()</span>
<span class="sd">        &gt;&gt;&gt; bandit_feedback.keys()</span>
<span class="sd">        dict_keys([&#39;n_rounds&#39;, &#39;n_actions&#39;, &#39;action&#39;, &#39;position&#39;, &#39;reward&#39;, &#39;pscore&#39;, &#39;context&#39;, &#39;action_context&#39;])</span>
<span class="sd">        # (2) Off-Policy Learning</span>
<span class="sd">        &gt;&gt;&gt; evaluation_policy = BernoulliTS(</span>
<span class="sd">            n_actions=dataset.n_actions,</span>
<span class="sd">            len_list=dataset.len_list,</span>
<span class="sd">            is_zozotown_prior=True, # replicate the policy in the ZOZOTOWN production</span>
<span class="sd">            campaign=&quot;all&quot;,</span>
<span class="sd">            random_state=12345</span>
<span class="sd">        )</span>
<span class="sd">        &gt;&gt;&gt; action_dist = evaluation_policy.compute_batch_action_dist(</span>
<span class="sd">            n_sim=100000, n_rounds=bandit_feedback[&quot;n_rounds&quot;]</span>
<span class="sd">        )</span>
<span class="sd">        # (3) Off-Policy Evaluation</span>
<span class="sd">        &gt;&gt;&gt; ope = OffPolicyEvaluation(bandit_feedback=bandit_feedback, ope_estimators=[IPW()])</span>
<span class="sd">        &gt;&gt;&gt; estimated_policy_value = ope.estimate_policy_values(action_dist=action_dist)</span>
<span class="sd">        &gt;&gt;&gt; estimated_policy_value</span>
<span class="sd">        {&#39;ipw&#39;: 0.004553...}</span>
<span class="sd">        # policy value improvement of BernoulliTS over the Random policy estimated by IPW</span>
<span class="sd">        &gt;&gt;&gt; estimated_policy_value_improvement = estimated_policy_value[&#39;ipw&#39;] / bandit_feedback[&#39;reward&#39;].mean()</span>
<span class="sd">        # our OPE procedure suggests that BernoulliTS improves Random by 19.81%</span>
<span class="sd">        &gt;&gt;&gt; print(estimated_policy_value_improvement)</span>
<span class="sd">        1.198126...</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">bandit_feedback</span><span class="p">:</span> <span class="n">BanditFeedback</span>
    <span class="n">ope_estimators</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseOffPolicyEstimator</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize class.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">key_</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="s2">&quot;position&quot;</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="s2">&quot;pscore&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">key_</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit_feedback</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Missing key of </span><span class="si">{</span><span class="n">key_</span><span class="si">}</span><span class="s2"> in &#39;bandit_feedback&#39;.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_model_dependent</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="p">[</span><span class="n">estimator</span><span class="o">.</span><span class="n">estimator_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">DirectMethod</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">DoublyRobust</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">is_model_dependent</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_create_estimator_inputs</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Create input dictionary to estimate policy value using subclasses of `BaseOffPolicyEstimator`&quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action_dist&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">estimated_rewards_by_reg_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">estimated_rewards_by_reg_model</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">check_array</span><span class="p">(</span>
                    <span class="n">array</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;estimated_rewards_by_reg_model[</span><span class="si">{</span><span class="n">estimator_name</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">,</span>
                    <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Expected `estimated_rewards_by_reg_model[</span><span class="si">{</span><span class="n">estimator_name</span><span class="si">}</span><span class="s2">].shape == action_dist.shape`, but found it False.&quot;</span>
                    <span class="p">)</span>
        <span class="k">elif</span> <span class="n">estimated_rewards_by_reg_model</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `estimated_rewards_by_reg_model.shape == action_dist.shape`, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">estimator_name</span><span class="p">:</span> <span class="p">{</span>
                <span class="n">input_</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="n">input_</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">input_</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="s2">&quot;position&quot;</span><span class="p">,</span> <span class="s2">&quot;pscore&quot;</span><span class="p">]</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">estimator_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span>
        <span class="p">}</span>

        <span class="k">for</span> <span class="n">estimator_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="p">:</span>
            <span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span><span class="s2">&quot;action_dist&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">action_dist</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">estimator_name</span> <span class="ow">in</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span>
                    <span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span>
                        <span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span>
                    <span class="p">]</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span>
                        <span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span>
                    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span>
                    <span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span>

        <span class="k">return</span> <span class="n">estimator_inputs</span>

    <span class="k">def</span> <span class="nf">estimate_policy_values</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ------------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given each round, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        policy_value_dict: Dict[str, float]</span>
<span class="sd">            Dictionary containing estimated policy values by OPE estimators.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_model_dependent</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">estimated_rewards_by_reg_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;When model dependent estimators such as DM or DR are used, `estimated_rewards_by_reg_model` must be given&quot;</span>
                <span class="p">)</span>

        <span class="n">policy_value_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_estimator_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">policy_value_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">estimate_policy_value</span><span class="p">(</span>
                <span class="o">**</span><span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">policy_value_dict</span>

    <span class="k">def</span> <span class="nf">estimate_intervals</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate confidence intervals of policy values using nonparametric bootstrap procedure.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ------------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=100</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        policy_value_interval_dict: Dict[str, Dict[str, float]]</span>
<span class="sd">            Dictionary containing confidence intervals of estimated policy value estimated</span>
<span class="sd">            using nonparametric bootstrap procedure.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_model_dependent</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">estimated_rewards_by_reg_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;When model dependent estimators such as DM or DR are used, `estimated_rewards_by_reg_model` must be given&quot;</span>
                <span class="p">)</span>

        <span class="n">check_confidence_interval_arguments</span><span class="p">(</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">policy_value_interval_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_estimator_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">policy_value_interval_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">estimate_interval</span><span class="p">(</span>
                <span class="o">**</span><span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">],</span>
                <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">policy_value_interval_dict</span>

    <span class="k">def</span> <span class="nf">summarize_off_policy_estimates</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Summarize policy values and their confidence intervals estimated by OPE estimators.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ------------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given each round, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=100</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        (policy_value_df, policy_value_interval_df): Tuple[DataFrame, DataFrame]</span>
<span class="sd">            Policy values and their confidence intervals Estimated by OPE estimators.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">policy_value_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimate_policy_values</span><span class="p">(</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;estimated_policy_value&quot;</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">policy_value_interval_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimate_intervals</span><span class="p">(</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">policy_value_of_behavior_policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">policy_value_df</span> <span class="o">=</span> <span class="n">policy_value_df</span><span class="o">.</span><span class="n">T</span>
        <span class="k">if</span> <span class="n">policy_value_of_behavior_policy</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Policy value of the behavior policy is </span><span class="si">{</span><span class="n">policy_value_of_behavior_policy</span><span class="si">}</span><span class="s2"> (&lt;=0); relative estimated policy value is set to np.nan&quot;</span>
            <span class="p">)</span>
            <span class="n">policy_value_df</span><span class="p">[</span><span class="s2">&quot;relative_estimated_policy_value&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">policy_value_df</span><span class="p">[</span><span class="s2">&quot;relative_estimated_policy_value&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">policy_value_df</span><span class="o">.</span><span class="n">estimated_policy_value</span> <span class="o">/</span> <span class="n">policy_value_of_behavior_policy</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">policy_value_df</span><span class="p">,</span> <span class="n">policy_value_interval_df</span><span class="o">.</span><span class="n">T</span>

    <span class="k">def</span> <span class="nf">visualize_off_policy_estimates</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">is_relative</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">fig_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Path</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">fig_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;estimated_policy_value.png&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Visualize policy values estimated by OPE estimators.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=100</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        is_relative: bool, default=False,</span>
<span class="sd">            If True, the method visualizes the estimated policy values of evaluation policy</span>
<span class="sd">            relative to the ground-truth policy value of behavior policy.</span>
<span class="sd">        fig_dir: Path, default=None</span>
<span class="sd">            Path to store the bar figure.</span>
<span class="sd">            If &#39;None&#39; is given, the figure will not be saved.</span>
<span class="sd">        fig_name: str, default=&quot;estimated_policy_value.png&quot;</span>
<span class="sd">            Name of the bar figure.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">fig_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fig_dir</span><span class="p">,</span> <span class="n">Path</span><span class="p">),</span> <span class="s2">&quot;fig_dir must be a Path&quot;</span>
        <span class="k">if</span> <span class="n">fig_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fig_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">),</span> <span class="s2">&quot;fig_dir must be a string&quot;</span>

        <span class="n">estimated_round_rewards_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_estimator_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">estimated_round_rewards_dict</span><span class="p">[</span>
                <span class="n">estimator_name</span>
            <span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span><span class="o">**</span><span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">])</span>
        <span class="n">estimated_round_rewards_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">estimated_round_rewards_dict</span><span class="p">)</span>
        <span class="n">estimated_round_rewards_df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span>
            <span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">key</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">estimated_round_rewards_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()},</span>
            <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">is_relative</span><span class="p">:</span>
            <span class="n">estimated_round_rewards_df</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;ggplot&quot;</span><span class="p">)</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span>
            <span class="n">data</span><span class="o">=</span><span class="n">estimated_round_rewards_df</span><span class="p">,</span>
            <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
            <span class="n">ci</span><span class="o">=</span><span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">),</span>
            <span class="n">n_boot</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;OPE Estimators&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Estimated Policy Value (± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">))</span><span class="si">}</span><span class="s2">% CI)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span>
        <span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">fig_dir</span><span class="p">:</span>
            <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">fig_dir</span> <span class="o">/</span> <span class="n">fig_name</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">evaluate_performance_of_estimators</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ground_truth_policy_value</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relative-ee&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Evaluate estimation performance of OPE estimators.</span>
<span class="sd">        Note</span>
<span class="sd">        ------</span>
<span class="sd">        Evaluate the estimation performance of OPE estimators by relative estimation error (relative-EE) or squared error (SE):</span>
<span class="sd">        .. math ::</span>
<span class="sd">            \\text{Relative-EE} (\\hat{V}; \\mathcal{D}) = \\left|  \\frac{\\hat{V}(\\pi; \\mathcal{D}) - V(\\pi)}{V(\\pi)} \\right|,</span>
<span class="sd">        .. math ::</span>
<span class="sd">            \\text{SE} (\\hat{V}; \\mathcal{D}) = \\left(\\hat{V}(\\pi; \\mathcal{D}) - V(\\pi) \\right)^2,</span>
<span class="sd">        where :math:`V({\\pi})` is the ground-truth policy value of the evalation policy :math:`\\pi_e` (often estimated using on-policy estimation).</span>
<span class="sd">        :math:`\\hat{V}(\\pi; \\mathcal{D})` is an estimated policy value by an OPE estimator :math:`\\hat{V}` and logged bandit feedback :math:`\\mathcal{D}`.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ground_truth policy value: float</span>
<span class="sd">            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\pi_e)`.</span>
<span class="sd">            With Open Bandit Dataset, we use an on-policy estimate of the policy value as its ground-truth.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        metric: str, default=&quot;relative-ee&quot;</span>
<span class="sd">            Evaluation metric used to evaluate and compare the estimation performance of OPE estimators.</span>
<span class="sd">            Must be &quot;relative-ee&quot; or &quot;se&quot;.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        eval_metric_ope_dict: Dict[str, float]</span>
<span class="sd">            Dictionary containing evaluation metric for evaluating the estimation performance of OPE estimators.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span>
            <span class="n">ground_truth_policy_value</span><span class="p">,</span>
            <span class="s2">&quot;ground_truth_policy_value&quot;</span><span class="p">,</span>
            <span class="nb">float</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">metric</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;relative-ee&quot;</span><span class="p">,</span> <span class="s2">&quot;se&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;metric must be either &#39;relative-ee&#39; or &#39;se&#39;, but </span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2"> is given&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;relative-ee&quot;</span> <span class="ow">and</span> <span class="n">ground_truth_policy_value</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;ground_truth_policy_value must be non-zero when metric is relative-ee&quot;</span>
            <span class="p">)</span>

        <span class="n">eval_metric_ope_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_estimator_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">estimated_policy_value</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">estimate_policy_value</span><span class="p">(</span>
                <span class="o">**</span><span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;relative-ee&quot;</span><span class="p">:</span>
                <span class="n">relative_ee_</span> <span class="o">=</span> <span class="n">estimated_policy_value</span> <span class="o">-</span> <span class="n">ground_truth_policy_value</span>
                <span class="n">relative_ee_</span> <span class="o">/=</span> <span class="n">ground_truth_policy_value</span>
                <span class="n">eval_metric_ope_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">relative_ee_</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;se&quot;</span><span class="p">:</span>
                <span class="n">se_</span> <span class="o">=</span> <span class="p">(</span><span class="n">estimated_policy_value</span> <span class="o">-</span> <span class="n">ground_truth_policy_value</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
                <span class="n">eval_metric_ope_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">se_</span>
        <span class="k">return</span> <span class="n">eval_metric_ope_dict</span>

    <span class="k">def</span> <span class="nf">summarize_estimators_comparison</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ground_truth_policy_value</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relative-ee&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataFrame</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Summarize performance comparisons of OPE estimators.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ground_truth policy value: float</span>
<span class="sd">            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\pi_e)`.</span>
<span class="sd">            With Open Bandit Dataset, we use an on-policy estimate of the policy value as ground-truth.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list), default=None</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        metric: str, default=&quot;relative-ee&quot;</span>
<span class="sd">            Evaluation metric used to evaluate and compare the estimation performance of OPE estimators.</span>
<span class="sd">            Must be either &quot;relative-ee&quot; or &quot;se&quot;.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        eval_metric_ope_df: DataFrame</span>
<span class="sd">            Evaluation metric to evaluate and compare the estimation performance of OPE estimators.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">eval_metric_ope_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_performance_of_estimators</span><span class="p">(</span>
                <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">ground_truth_policy_value</span><span class="p">,</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
                <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="n">metric</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">eval_metric_ope_df</span><span class="o">.</span><span class="n">T</span>

    <span class="k">def</span> <span class="nf">visualize_off_policy_estimates_of_multiple_policies</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">policy_name_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">action_dist_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">is_relative</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">fig_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Path</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">fig_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;estimated_policy_value.png&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Visualize policy values estimated by OPE estimators.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        policy_name_list: List[str]</span>
<span class="sd">            List of the names of evaluation policies.</span>
<span class="sd">        action_dist_list: List[array-like, shape (n_rounds, n_actions, len_list)]</span>
<span class="sd">            List of action choice probabilities by the evaluation policies (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of an estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=100</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        is_relative: bool, default=False,</span>
<span class="sd">            If True, the method visualizes the estimated policy values of evaluation policy</span>
<span class="sd">            relative to the ground-truth policy value of behavior policy.</span>
<span class="sd">        fig_dir: Path, default=None</span>
<span class="sd">            Path to store the bar figure.</span>
<span class="sd">            If &#39;None&#39; is given, the figure will not be saved.</span>
<span class="sd">        fig_name: str, default=&quot;estimated_policy_value.png&quot;</span>
<span class="sd">            Name of the bar figure.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">policy_name_list</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">action_dist_list</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;the length of policy_name_list must be the same as action_dist_list&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">fig_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fig_dir</span><span class="p">,</span> <span class="n">Path</span><span class="p">),</span> <span class="s2">&quot;fig_dir must be a Path&quot;</span>
        <span class="k">if</span> <span class="n">fig_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fig_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">),</span> <span class="s2">&quot;fig_dir must be a string&quot;</span>

        <span class="n">estimated_round_rewards_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">estimator_name</span><span class="p">:</span> <span class="p">{}</span> <span class="k">for</span> <span class="n">estimator_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span>
        <span class="p">}</span>

        <span class="k">for</span> <span class="n">policy_name</span><span class="p">,</span> <span class="n">action_dist</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">policy_name_list</span><span class="p">,</span> <span class="n">action_dist_list</span><span class="p">):</span>
            <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_estimator_inputs</span><span class="p">(</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">estimated_round_rewards_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span>
                    <span class="n">policy_name</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
                    <span class="o">**</span><span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span>
                <span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;ggplot&quot;</span><span class="p">)</span>
        <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mf">6.2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="p">)))</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">estimator_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="p">):</span>
            <span class="n">estimated_round_rewards_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span>
                <span class="n">estimated_round_rewards_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">is_relative</span><span class="p">:</span>
                <span class="n">estimated_round_rewards_df</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

            <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">action_dist_list</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span>
                <span class="n">data</span><span class="o">=</span><span class="n">estimated_round_rewards_df</span><span class="p">,</span>
                <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                <span class="n">ci</span><span class="o">=</span><span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">),</span>
                <span class="n">n_boot</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
                <span class="n">seed</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Estimated Policy Value (± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">))</span><span class="si">}</span><span class="s2">% CI)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span>
            <span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">policy_name_list</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">fig_dir</span><span class="p">:</span>
            <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">fig_dir</span> <span class="o">/</span> <span class="n">fig_name</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="policy">
<h2>Policy<a class="headerlink" href="#policy" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseOfflinePolicyLearner</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for off-policy learners.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    n_actions: int</span>
<span class="sd">        Number of actions.</span>
<span class="sd">    len_list: int, default=1</span>
<span class="sd">        Length of a list of actions recommended in each impression.</span>
<span class="sd">        When Open Bandit Dataset is used, 3 should be set.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">n_actions</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">len_list</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize class.&quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="s2">&quot;n_actions&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">,</span> <span class="s2">&quot;len_list&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">policy_type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PolicyType</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Type of the bandit policy.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">PolicyType</span><span class="o">.</span><span class="n">OFFLINE</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Fits an offline bandit policy using the given logged bandit feedback data.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Predict best action for new data.</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        context: array-like, shape (n_rounds_of_new_data, dim_context)</span>
<span class="sd">            Context vectors for new data.</span>
<span class="sd">        Returns</span>
<span class="sd">        -----------</span>
<span class="sd">        action: array-like, shape (n_rounds_of_new_data, n_actions, len_list)</span>
<span class="sd">            Action choices by a policy trained by calling the `fit` method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown IPWLearner Policy</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">IPWLearner</span><span class="p">(</span><span class="n">BaseOfflinePolicyLearner</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Off-policy learner with Inverse Probability Weighting.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    n_actions: int</span>
<span class="sd">        Number of actions.</span>
<span class="sd">    len_list: int, default=1</span>
<span class="sd">        Length of a list of actions recommended in each impression.</span>
<span class="sd">        When Open Bandit Dataset is used, 3 should be set.</span>
<span class="sd">    base_classifier: ClassifierMixin</span>
<span class="sd">        Machine learning classifier used to train an offline decision making policy.</span>
<span class="sd">    References</span>
<span class="sd">    ------------</span>
<span class="sd">    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.</span>
<span class="sd">    &quot;Doubly Robust Policy Evaluation and Optimization.&quot;, 2014.</span>
<span class="sd">    Damien Lefortier, Adith Swaminathan, Xiaotao Gu, Thorsten Joachims, and Maarten de Rijke.</span>
<span class="sd">    &quot;Large-scale Validation of Counterfactual Learning Methods: A Test-Bed.&quot;, 2016.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">base_classifier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ClassifierMixin</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize class.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__post_init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_classifier</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">base_classifier</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">12345</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_classifier</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;base_classifier must be a classifier&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_classifier_list</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_classifier</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_create_train_data_for_opl</span><span class="p">(</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Create training data for off-policy learning.</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        context: array-like, shape (n_rounds, dim_context)</span>
<span class="sd">            Context vectors in each round, i.e., :math:`x_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Observed rewards (or outcome) in each round, i.e., :math:`r_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Propensity scores, the probability of selecting each action by behavior policy,</span>
<span class="sd">            in the given logged bandit data.</span>
<span class="sd">        Returns</span>
<span class="sd">        --------</span>
<span class="sd">        (X, sample_weight, y): Tuple[np.ndarray, np.ndarray, np.ndarray]</span>
<span class="sd">            Feature vectors, sample weights, and outcome for training the base machine learning model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">context</span><span class="p">,</span> <span class="p">(</span><span class="n">reward</span> <span class="o">/</span> <span class="n">pscore</span><span class="p">),</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Fits an offline bandit policy using the given logged bandit feedback data.</span>
<span class="sd">        Note</span>
<span class="sd">        --------</span>
<span class="sd">        This `fit` method trains a deterministic policy :math:`\\pi: \\mathcal{X} \\rightarrow \\mathcal{A}`</span>
<span class="sd">        via a cost-sensitive classification reduction as follows:</span>
<span class="sd">        .. math::</span>
<span class="sd">            \\hat{\\pi}</span>
<span class="sd">            &amp; \\in \\arg \\max_{\\pi \\in \\Pi} \\hat{V}_{\\mathrm{IPW}} (\\pi ; \\mathcal{D}) \\\\</span>
<span class="sd">            &amp; = \\arg \\max_{\\pi \\in \\Pi} \\mathbb{E}_{\\mathcal{D}} \\left[\\frac{\\mathbb{I} \\{\\pi (x_{i})=a_{i} \\}}{\\pi_{b}(a_{i} | x_{i})} r_{i} \\right] \\\\</span>
<span class="sd">            &amp; = \\arg \\min_{\\pi \\in \\Pi} \\mathbb{E}_{\\mathcal{D}} \\left[\\frac{r_i}{\\pi_{b}(a_{i} | x_{i})} \\mathbb{I} \\{\\pi (x_{i}) \\neq a_{i} \\} \\right],</span>
<span class="sd">        where :math:`\\mathbb{E}_{\\mathcal{D}} [\cdot]` is the empirical average over observations in :math:`\\mathcal{D}`.</span>
<span class="sd">        See the reference for the details.</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        context: array-like, shape (n_rounds, dim_context)</span>
<span class="sd">            Context vectors in each round, i.e., :math:`x_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Observed rewards (or outcome) in each round, i.e., :math:`r_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            If None is given, a learner assumes that there is only one position.</span>
<span class="sd">            When `len_list` &gt; 1, position has to be set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_bandit_feedback_inputs</span><span class="p">(</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">reward</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;A negative value is found in `reward`.&quot;</span>
                <span class="s2">&quot;`obp.policy.IPWLearner` cannot handle negative rewards,&quot;</span>
                <span class="s2">&quot;and please use `obp.policy.NNPolicyLearner` instead.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">pscore</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">n_actions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">pscore</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_actions</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;When `self.len_list=1`, `position` must be given.&quot;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">position_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_train_data_for_opl</span><span class="p">(</span>
                <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">[</span><span class="n">position</span> <span class="o">==</span> <span class="n">position_</span><span class="p">],</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">[</span><span class="n">position</span> <span class="o">==</span> <span class="n">position_</span><span class="p">],</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">[</span><span class="n">position</span> <span class="o">==</span> <span class="n">position_</span><span class="p">],</span>
                <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">[</span><span class="n">position</span> <span class="o">==</span> <span class="n">position_</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">base_classifier_list</span><span class="p">[</span><span class="n">position_</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Predict best actions for new data.</span>
<span class="sd">        Note</span>
<span class="sd">        --------</span>
<span class="sd">        Action set predicted by this `predict` method can contain duplicate items.</span>
<span class="sd">        If you want a non-repetitive action set, then please use the `sample_action` method.</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        context: array-like, shape (n_rounds_of_new_data, dim_context)</span>
<span class="sd">            Context vectors for new data.</span>
<span class="sd">        Returns</span>
<span class="sd">        -----------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds_of_new_data, n_actions, len_list)</span>
<span class="sd">            Action choices by a classifier, which can contain duplicate items.</span>
<span class="sd">            If you want a non-repetitive action set, please use the `sample_action` method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">action_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">position_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
            <span class="n">predicted_actions_at_position</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_classifier_list</span><span class="p">[</span>
                <span class="n">position_</span>
            <span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
            <span class="n">action_dist</span><span class="p">[</span>
                <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span>
                <span class="n">predicted_actions_at_position</span><span class="p">,</span>
                <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span> <span class="o">*</span> <span class="n">position_</span><span class="p">,</span>
            <span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">action_dist</span>

    <span class="k">def</span> <span class="nf">predict_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Predict non-negative scores for all possible products of action and position.</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        context: array-like, shape (n_rounds_of_new_data, dim_context)</span>
<span class="sd">            Context vectors for new data.</span>
<span class="sd">        Returns</span>
<span class="sd">        -----------</span>
<span class="sd">        score_predicted: array-like, shape (n_rounds_of_new_data, n_actions, len_list)</span>
<span class="sd">            Scores for all possible pairs of action and position predicted by a classifier.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">score_predicted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">position_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
            <span class="n">score_predicteds_at_position</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_classifier_list</span><span class="p">[</span>
                <span class="n">position_</span>
            <span class="p">]</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
            <span class="n">score_predicted</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">position_</span><span class="p">]</span> <span class="o">=</span> <span class="n">score_predicteds_at_position</span>
        <span class="k">return</span> <span class="n">score_predicted</span>

    <span class="k">def</span> <span class="nf">sample_action</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">tau</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sample (non-repetitive) actions based on scores predicted by a classifier.</span>
<span class="sd">        Note</span>
<span class="sd">        --------</span>
<span class="sd">        This `sample_action` method samples a **non-repetitive** set of actions for new data :math:`x \\in \\mathcal{X}`</span>
<span class="sd">        by first computing non-negative scores for all possible candidate products of action and position</span>
<span class="sd">        :math:`(a, k) \\in \\mathcal{A} \\times \\mathcal{K}` (where :math:`\\mathcal{A}` is an action set and</span>
<span class="sd">        :math:`\\mathcal{K}` is a position set), and using softmax function as follows:</span>
<span class="sd">        .. math::</span>
<span class="sd">            &amp; P (A_1 = a_1 | x) = \\frac{\\mathrm{exp}(f(x,a_1,1) / \\tau)}{\\sum_{a^{\\prime} \\in \\mathcal{A}} \\mathrm{exp}( f(x,a^{\\prime},1) / \\tau)} , \\\\</span>
<span class="sd">            &amp; P (A_2 = a_2 | A_1 = a_1, x) = \\frac{\\mathrm{exp}(f(x,a_2,2) / \\tau)}{\\sum_{a^{\\prime} \\in \\mathcal{A} \\backslash \\{a_1\\}} \\mathrm{exp}(f(x,a^{\\prime},2) / \\tau )} ,</span>
<span class="sd">            \\ldots</span>
<span class="sd">        where :math:`A_k` is a random variable representing an action at a position :math:`k`.</span>
<span class="sd">        :math:`\\tau` is a temperature hyperparameter.</span>
<span class="sd">        :math:`f: \\mathcal{X} \\times \\mathcal{A} \\times \\mathcal{K} \\rightarrow \\mathbb{R}_{+}`</span>
<span class="sd">        is a scoring function which is now implemented in the `predict_score` method.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------------</span>
<span class="sd">        context: array-like, shape (n_rounds_of_new_data, dim_context)</span>
<span class="sd">            Context vectors for new data.</span>
<span class="sd">        tau: int or float, default=1.0</span>
<span class="sd">            A temperature parameter, controlling the randomness of the action choice.</span>
<span class="sd">            As :math:`\\tau \\rightarrow \\infty`, the algorithm will select arms uniformly at random.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in sampling actions.</span>
<span class="sd">        Returns</span>
<span class="sd">        -----------</span>
<span class="sd">        action: array-like, shape (n_rounds_of_new_data, n_actions, len_list)</span>
<span class="sd">            Action sampled by a trained classifier.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;tau&quot;</span><span class="p">,</span> <span class="n">target_type</span><span class="o">=</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">),</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">random_</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">))</span>
        <span class="n">score_predicted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_score</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;[sample_action]&quot;</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="n">n_rounds</span><span class="p">):</span>
            <span class="n">action_set</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">position_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
                <span class="n">score_</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">score_predicted</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">action_set</span><span class="p">,</span> <span class="n">position_</span><span class="p">]</span> <span class="o">/</span> <span class="n">tau</span><span class="p">)</span>
                <span class="n">action_sampled</span> <span class="o">=</span> <span class="n">random_</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">action_set</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">score_</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">action</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">action_sampled</span><span class="p">,</span> <span class="n">position_</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="n">action_set</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">action_set</span><span class="p">,</span> <span class="n">action_set</span> <span class="o">==</span> <span class="n">action_sampled</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">tau</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Obtains action choice probabilities for new data based on scores predicted by a classifier.</span>
<span class="sd">        Note</span>
<span class="sd">        --------</span>
<span class="sd">        This `predict_proba` method obtains action choice probabilities for new data :math:`x \\in \\mathcal{X}`</span>
<span class="sd">        by first computing non-negative scores for all possible candidate actions</span>
<span class="sd">        :math:`a \\in \\mathcal{A}` (where :math:`\\mathcal{A}` is an action set),</span>
<span class="sd">        and using a Plackett-Luce ranking model as follows:</span>
<span class="sd">        .. math::</span>
<span class="sd">            P (A = a | x) = \\frac{\\mathrm{exp}(f(x,a) / \\tau)}{\\sum_{a^{\\prime} \\in \\mathcal{A}} \\mathrm{exp}(f(x,a^{\\prime}) / \\tau)},</span>
<span class="sd">        where :math:`A` is a random variable representing an action, and :math:`\\tau` is a temperature hyperparameter.</span>
<span class="sd">        :math:`f: \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\mathbb{R}_{+}`</span>
<span class="sd">        is a scoring function which is now implemented in the `predict_score` method.</span>
<span class="sd">        **Note that this method can be used only when `len_list=1`, please use the `sample_action` method otherwise.**</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------------</span>
<span class="sd">        context: array-like, shape (n_rounds_of_new_data, dim_context)</span>
<span class="sd">            Context vectors for new data.</span>
<span class="sd">        tau: int or float, default=1.0</span>
<span class="sd">            A temperature parameter, controlling the randomness of the action choice.</span>
<span class="sd">            As :math:`\\tau \\rightarrow \\infty`, the algorithm will select arms uniformly at random.</span>
<span class="sd">        Returns</span>
<span class="sd">        -----------</span>
<span class="sd">        choice_prob: array-like, shape (n_rounds_of_new_data, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities obtained by a trained classifier.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="p">),</span> <span class="s2">&quot;predict_proba method cannot be used when `len_list != 1`&quot;</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;tau&quot;</span><span class="p">,</span> <span class="n">target_type</span><span class="o">=</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">),</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">score_predicted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_score</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
        <span class="n">choice_prob</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">score_predicted</span> <span class="o">/</span> <span class="n">tau</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">choice_prob</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="main">
<h2>Main<a class="headerlink" href="#main" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># from obp.dataset import linear_behavior_policy</span>
<span class="c1"># from obp.dataset import logistic_reward_function</span>
<span class="c1"># from obp.dataset import SyntheticBanditDataset</span>
<span class="c1"># from obp.ope import DirectMethod</span>
<span class="c1"># from obp.ope import DoublyRobust</span>
<span class="c1"># from obp.ope import DoublyRobustWithShrinkage</span>
<span class="c1"># from obp.ope import InverseProbabilityWeighting</span>
<span class="c1"># from obp.ope import OffPolicyEvaluation</span>
<span class="c1"># from obp.ope import RegressionModel</span>
<span class="c1"># from obp.ope import SelfNormalizedDoublyRobust</span>
<span class="c1"># from obp.ope import SelfNormalizedInverseProbabilityWeighting</span>
<span class="c1"># from obp.ope import SwitchDoublyRobust</span>
<span class="c1"># from obp.policy import IPWLearner</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="hyperparams">
<h3>Hyperparams<a class="headerlink" href="#hyperparams" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">writefile</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">yaml</span>
<span class="n">lightgbm</span><span class="p">:</span>
  <span class="n">n_estimators</span><span class="p">:</span> <span class="mi">30</span>
  <span class="n">learning_rate</span><span class="p">:</span> <span class="mf">0.01</span>
  <span class="n">max_depth</span><span class="p">:</span> <span class="mi">5</span>
  <span class="n">min_samples_leaf</span><span class="p">:</span> <span class="mi">10</span>
  <span class="n">random_state</span><span class="p">:</span> <span class="mi">12345</span>
<span class="n">logistic_regression</span><span class="p">:</span>
  <span class="n">max_iter</span><span class="p">:</span> <span class="mi">10000</span>
  <span class="n">C</span><span class="p">:</span> <span class="mi">100</span>
  <span class="n">random_state</span><span class="p">:</span> <span class="mi">12345</span>
<span class="n">random_forest</span><span class="p">:</span>
  <span class="n">n_estimators</span><span class="p">:</span> <span class="mi">30</span>
  <span class="n">max_depth</span><span class="p">:</span> <span class="mi">5</span>
  <span class="n">min_samples_leaf</span><span class="p">:</span> <span class="mi">10</span>
  <span class="n">random_state</span><span class="p">:</span> <span class="mi">12345</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Overwriting hyperparams.yaml
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># hyperparameters of the regression model used in model dependent OPE estimators</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;hyperparams.yaml&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">hyperparams</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">safe_load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id1">
<h3>Base Models<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">base_model_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">logistic_regression</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">,</span>
    <span class="n">lightgbm</span><span class="o">=</span><span class="n">GradientBoostingClassifier</span><span class="p">,</span>
    <span class="n">random_forest</span><span class="o">=</span><span class="n">RandomForestClassifier</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="ope-estimators">
<h3>OPE Estimators<a class="headerlink" href="#ope-estimators" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># compared OPE estimators</span>
<span class="n">ope_estimators</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">DirectMethod</span><span class="p">(),</span>
    <span class="n">InverseProbabilityWeighting</span><span class="p">(),</span>
    <span class="n">SelfNormalizedInverseProbabilityWeighting</span><span class="p">(),</span>
    <span class="n">DoublyRobust</span><span class="p">(),</span>
    <span class="n">SelfNormalizedDoublyRobust</span><span class="p">(),</span>
    <span class="n">SwitchDoublyRobust</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;switch-dr (lambda=1)&quot;</span><span class="p">),</span>
    <span class="n">SwitchDoublyRobust</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="mf">100.0</span><span class="p">,</span> <span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;switch-dr (lambda=100)&quot;</span><span class="p">),</span>
    <span class="n">DoublyRobustWithShrinkage</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;dr-os (lambda=1)&quot;</span><span class="p">),</span>
    <span class="n">DoublyRobustWithShrinkage</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="mf">100.0</span><span class="p">,</span> <span class="n">estimator_name</span><span class="o">=</span><span class="s2">&quot;dr-os (lambda=100)&quot;</span><span class="p">),</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="arg-parse">
<h3>Arg Parse<a class="headerlink" href="#arg-parse" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;evaluate off-policy estimators with synthetic bandit data.&quot;</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--n_runs&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;number of simulations in the experiment.&quot;</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--n_rounds&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;number of rounds for synthetic bandit feedback.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--n_actions&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;number of actions for synthetic bandit feedback.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--dim_context&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;dimensions of context vectors characterizing each round.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--base_model_for_evaluation_policy&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;logistic_regression&quot;</span><span class="p">,</span> <span class="s2">&quot;lightgbm&quot;</span><span class="p">,</span> <span class="s2">&quot;random_forest&quot;</span><span class="p">],</span>
        <span class="n">default</span><span class="o">=</span><span class="s1">&#39;random_forest&#39;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;base ML model for evaluation policy, logistic_regression, random_forest or lightgbm.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--base_model_for_reg_model&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;logistic_regression&quot;</span><span class="p">,</span> <span class="s2">&quot;lightgbm&quot;</span><span class="p">,</span> <span class="s2">&quot;random_forest&quot;</span><span class="p">],</span>
        <span class="n">default</span><span class="o">=</span><span class="s1">&#39;logistic_regression&#39;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;base ML model for regression model, logistic_regression, random_forest or lightgbm.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--n_jobs&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the maximum number of concurrently running jobs.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--random_state&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">12345</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">{})</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Namespace(base_model_for_evaluation_policy=&#39;random_forest&#39;, base_model_for_reg_model=&#39;logistic_regression&#39;, dim_context=5, n_actions=10, n_jobs=2, n_rounds=10000, n_runs=1, random_state=12345)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># configurations</span>
<span class="n">n_runs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_runs</span>
<span class="n">n_rounds</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_rounds</span>
<span class="n">n_actions</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_actions</span>
<span class="n">dim_context</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">dim_context</span>
<span class="n">base_model_for_evaluation_policy</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">base_model_for_evaluation_policy</span>
<span class="n">base_model_for_reg_model</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">base_model_for_reg_model</span>
<span class="n">n_jobs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_jobs</span>
<span class="n">random_state</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">random_state</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="process">
<h3>Process<a class="headerlink" href="#process" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="n">i</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="c1"># synthetic data generator</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">SyntheticBanditDataset</span><span class="p">(</span>
        <span class="n">n_actions</span><span class="o">=</span><span class="n">n_actions</span><span class="p">,</span>
        <span class="n">dim_context</span><span class="o">=</span><span class="n">dim_context</span><span class="p">,</span>
        <span class="n">reward_function</span><span class="o">=</span><span class="n">logistic_reward_function</span><span class="p">,</span>
        <span class="n">behavior_policy_function</span><span class="o">=</span><span class="n">linear_behavior_policy</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># define evaluation policy using IPWLearner</span>
    <span class="n">evaluation_policy</span> <span class="o">=</span> <span class="n">IPWLearner</span><span class="p">(</span>
        <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
        <span class="n">base_classifier</span><span class="o">=</span><span class="n">base_model_dict</span><span class="p">[</span><span class="n">base_model_for_evaluation_policy</span><span class="p">](</span>
            <span class="o">**</span><span class="n">hyperparams</span><span class="p">[</span><span class="n">base_model_for_evaluation_policy</span><span class="p">]</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="c1"># sample new training and test sets of synthetic logged bandit feedback</span>
    <span class="n">bandit_feedback_train</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="n">n_rounds</span><span class="o">=</span><span class="n">n_rounds</span><span class="p">)</span>
    <span class="n">bandit_feedback_test</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="n">n_rounds</span><span class="o">=</span><span class="n">n_rounds</span><span class="p">)</span>
    <span class="c1"># train the evaluation policy on the training set of the synthetic logged bandit feedback</span>
    <span class="n">evaluation_policy</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
        <span class="n">action</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span>
        <span class="n">reward</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span>
        <span class="n">pscore</span><span class="o">=</span><span class="n">bandit_feedback_train</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="c1"># predict the action decisions for the test set of the synthetic logged bandit feedback</span>
    <span class="n">action_dist</span> <span class="o">=</span> <span class="n">evaluation_policy</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
        <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback_test</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="c1"># estimate the mean reward function of the test set of synthetic bandit feedback with ML model</span>
    <span class="n">regression_model</span> <span class="o">=</span> <span class="n">RegressionModel</span><span class="p">(</span>
        <span class="n">n_actions</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
        <span class="n">action_context</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
        <span class="n">base_model</span><span class="o">=</span><span class="n">base_model_dict</span><span class="p">[</span><span class="n">base_model_for_reg_model</span><span class="p">](</span>
            <span class="o">**</span><span class="n">hyperparams</span><span class="p">[</span><span class="n">base_model_for_reg_model</span><span class="p">]</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">estimated_rewards_by_reg_model</span> <span class="o">=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span>
        <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback_test</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
        <span class="n">action</span><span class="o">=</span><span class="n">bandit_feedback_test</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span>
        <span class="n">reward</span><span class="o">=</span><span class="n">bandit_feedback_test</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span>
        <span class="n">n_folds</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>  <span class="c1"># 3-fold cross-fitting</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># evaluate estimators&#39; performances using relative estimation error (relative-ee)</span>
    <span class="n">ope</span> <span class="o">=</span> <span class="n">OffPolicyEvaluation</span><span class="p">(</span>
        <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">bandit_feedback_test</span><span class="p">,</span>
        <span class="n">ope_estimators</span><span class="o">=</span><span class="n">ope_estimators</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">relative_ee_i</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">evaluate_performance_of_estimators</span><span class="p">(</span>
        <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">calc_ground_truth_policy_value</span><span class="p">(</span>
            <span class="n">expected_reward</span><span class="o">=</span><span class="n">bandit_feedback_test</span><span class="p">[</span><span class="s2">&quot;expected_reward&quot;</span><span class="p">],</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">relative_ee_i</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="run">
<h3>Run<a class="headerlink" href="#run" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">processed</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span>
    <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
<span class="p">)([</span><span class="n">delayed</span><span class="p">(</span><span class="n">process</span><span class="p">)(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_runs</span><span class="p">)])</span>
<span class="n">relative_ee_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">est</span><span class="o">.</span><span class="n">estimator_name</span><span class="p">:</span> <span class="nb">dict</span><span class="p">()</span> <span class="k">for</span> <span class="n">est</span> <span class="ow">in</span> <span class="n">ope_estimators</span><span class="p">}</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">relative_ee_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">processed</span><span class="p">):</span>
    <span class="k">for</span> <span class="p">(</span>
        <span class="n">estimator_name</span><span class="p">,</span>
        <span class="n">relative_ee_</span><span class="p">,</span>
    <span class="p">)</span> <span class="ow">in</span> <span class="n">relative_ee_i</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">relative_ee_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">relative_ee_</span>
<span class="n">relative_ee_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">relative_ee_dict</span><span class="p">)</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">45</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;random_state=</span><span class="si">{</span><span class="n">random_state</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">45</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">relative_ee_df</span><span class="p">[[</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;std&quot;</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">45</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.
[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    2.3s
[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    2.3s finished
=============================================
random_state=12345
---------------------------------------------
                            mean  std
dm                      0.120247  NaN
ipw                     0.062459  NaN
snipw                   0.021442  NaN
dr                      0.014321  NaN
sndr                    0.009687  NaN
switch-dr (lambda=1)    0.120247  NaN
switch-dr (lambda=100)  0.014321  NaN
dr-os (lambda=1)        0.118951  NaN
dr-os (lambda=100)      0.064524  NaN
=============================================
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># save results of the evaluation of off-policy estimators in &#39;./logs&#39; directory.</span>
<span class="n">log_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;./logs&quot;</span><span class="p">)</span>
<span class="n">log_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">relative_ee_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">log_path</span> <span class="o">/</span> <span class="s2">&quot;relative_ee_of_ope_estimators.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./nbs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="T902666_Evaluating_the_Robustness_of_Off_Policy_Evaluation.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Evaluating the Robustness of Off-Policy Evaluation</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="T874693_Evaluating_Standard_Off_Policy_Estimators_with_Small_Sample_Open_Bandit_Dataset.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Sparsh A.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>