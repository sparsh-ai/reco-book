{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QryzL_Qkbr5X"
      },
      "source": [
        "# Kafka and Spark Streaming in Colab\n",
        "> Installing Kafka and Spark streaming in colab and streaming movielens dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BVQV1-5tnGK"
      },
      "source": [
        "<img src='https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4aa8ae71-e390-4259-8420-b8c16fa139d7%2FUntitled.png?table=block&id=21e35b66-9681-418d-a221-01a6eccdd3d4&spaceId=63b72b1f-0e90-4ab8-a6df-a060a6545a56&width=2000&userId=21ec183f-f0be-4b6b-9b3e-6f0d4e5c5469&cache=v2'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwaPmnI-MoPD"
      },
      "source": [
        "There are several benefits of implementing Spark-Kafka integration. You can ensure minimum data loss through Spark Streaming while saving all the received Kafka data synchronously for an easy recovery. Users can read messages from a single topic or multiple Kafka topics. \n",
        "\n",
        "Along with this level of flexibility you can also access high scalability, throughput and fault-tolerance and a range of other benefits by using Spark and Kafka in tandem. This integration can be understood with a data pipeline that functions in the methodology shown below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwWgTaSqtq97"
      },
      "source": [
        "<img src='https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fe0118a23-b20d-4206-b3d2-3ad2ee234981%2FUntitled.png?table=block&id=b40cf392-bad7-4c5a-98df-8cda81838787&spaceId=63b72b1f-0e90-4ab8-a6df-a060a6545a56&width=2000&userId=21ec183f-f0be-4b6b-9b3e-6f0d4e5c5469&cache=v2'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48B9eAMMhAgw"
      },
      "outputs": [],
      "source": [
        "!pip install kafka-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjrZNJQRJP-U"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6KXZuTBWgRm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "import threading\n",
        "import json\n",
        "from kafka import KafkaProducer\n",
        "from kafka.errors import KafkaError\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZmI7l_GykcW"
      },
      "source": [
        "## Download and setup Kafka and Zookeeper instances\n",
        "\n",
        "For demo purposes, the following instances are setup locally:\n",
        "\n",
        "- Kafka (Brokers: 127.0.0.1:9092)\n",
        "- Zookeeper (Node: 127.0.0.1:2181)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUj0878jPyz7"
      },
      "outputs": [],
      "source": [
        "!curl -sSOL https://downloads.apache.org/kafka/2.7.0/kafka_2.13-2.7.0.tgz\n",
        "!tar -xzf kafka_2.13-2.7.0.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAzfu_WiEs4F"
      },
      "source": [
        "Using the default configurations (provided by Apache Kafka) for spinning up the instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9ujlunrWgRx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for 10 secs until kafka and zookeeper services are up and running\n"
          ]
        }
      ],
      "source": [
        "!./kafka_2.13-2.7.0/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-2.7.0/config/zookeeper.properties\n",
        "!./kafka_2.13-2.7.0/bin/kafka-server-start.sh -daemon ./kafka_2.13-2.7.0/config/server.properties\n",
        "!echo \"Waiting for 10 secs until kafka and zookeeper services are up and running\"\n",
        "!sleep 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6qxCdypE1DD"
      },
      "source": [
        "Once the instances are started as daemon processes, grep for `kafka` in the processes list. The two java processes correspond to zookeeper and the kafka instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48LqMJ1BEHm5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root         406     359  5 04:12 ?        00:00:16 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -cp /content/spark-3.1.2-bin-hadoop3.2/conf/:/content/spark-3.1.2-bin-hadoop3.2/jars/* -Xmx1g org.apache.spark.deploy.SparkSubmit --conf spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 pyspark-shell\n",
            "root         901       1 11 04:17 ?        00:00:01 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Xmx512M -Xms512M -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true -Xloggc:/content/kafka_2.13-2.7.0/bin/../logs/zookeeper-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=100M -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/content/kafka_2.13-2.7.0/bin/../logs -Dlog4j.configuration=file:./kafka_2.13-2.7.0/bin/../config/log4j.properties -cp /content/kafka_2.13-2.7.0/bin/../libs/activation-1.1.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/argparse4j-0.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/audience-annotations-0.5.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/commons-cli-1.4.jar:/content/kafka_2.13-2.7.0/bin/../libs/commons-lang3-3.8.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/connect-api-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/connect-basic-auth-extension-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/connect-file-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/connect-json-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/connect-mirror-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/connect-mirror-client-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/connect-runtime-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/connect-transforms-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/hk2-api-2.6.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/hk2-locator-2.6.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/hk2-utils-2.6.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-annotations-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-core-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-databind-2.10.5.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-dataformat-csv-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-datatype-jdk8-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-jaxrs-base-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-jaxrs-json-provider-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-module-jaxb-annotations-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-module-paranamer-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-module-scala_2.13-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jakarta.activation-api-1.2.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jakarta.inject-2.6.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/content/kafka_2.13-2.7.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/content/kafka_2.13-2.7.0/bin/../libs/jakarta.xml.bind-api-2.3.2.jar:/content/kafka_2.13-2.7.0/bin/../libs/javassist-3.25.0-GA.jar:/content/kafka_2.13-2.7.0/bin/../libs/javassist-3.26.0-GA.jar:/content/kafka_2.13-2.7.0/bin/../libs/javax.servlet-api-3.1.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/jaxb-api-2.3.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/jersey-client-2.31.jar:/content/kafka_2.13-2.7.0/bin/../libs/jersey-common-2.31.jar:/content/kafka_2.13-2.7.0/bin/../libs/jersey-container-servlet-2.31.jar:/content/kafka_2.13-2.7.0/bin/../libs/jersey-container-servlet-core-2.31.jar:/content/kafka_2.13-2.7.0/bin/../libs/jersey-hk2-2.31.jar:/content/kafka_2.13-2.7.0/bin/../libs/jersey-media-jaxb-2.31.jar:/content/kafka_2.13-2.7.0/bin/../libs/jersey-server-2.31.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-client-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-continuation-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-http-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-io-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-security-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-server-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-servlet-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-servlets-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-util-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jopt-simple-5.0.4.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka_2.13-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka_2.13-2.7.0-sources.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka-clients-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka-log4j-appender-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka-raft-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka-streams-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka-streams-examples-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka-streams-scala_2.13-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka-streams-test-utils-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka-tools-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/log4j-1.2.17.jar:/content/kafka_2.13-2.7.0/bin/../libs/lz4-java-1.7.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/maven-artifact-3.6.3.jar:/content/kafka_2.13-2.7.0/bin/../libs/metrics-core-2.2.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/netty-buffer-4.1.51.Final.jar:/content/kafka_2.13-2.7.0/bin/../libs/netty-codec-4.1.51.Final.jar:/content/kafka_2.13-2.7.0/bin/../libs/netty-common-4.1.51.Final.jar:/content/kafka_2.13-2.7.0/bin/../libs/netty-handler-4.1.51.Final.jar:/content/kafka_2.13-2.7.0/bin/../libs/netty-resolver-4.1.51.Final.jar:/content/kafka_2.13-2.7.0/bin/../libs/netty-transport-4.1.51.Final.jar:/content/kafka_2.13-2.7.0/bin/../libs/netty-transport-native-epoll-4.1.51.Final.jar:/content/kafka_2.13-2.7.0/bin/../libs/netty-transport-native-unix-common-4.1.51.Final.jar:/content/kafka_2.13-2.7.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/content/kafka_2.13-2.7.0/bin/../libs/paranamer-2.8.jar:/content/kafka_2.13-2.7.0/bin/../libs/plexus-utils-3.2.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/reflections-0.9.12.jar:/content/kafka_2.13-2.7.0/bin/../libs/rocksdbjni-5.18.4.jar:/content/kafka_2.13-2.7.0/bin/../libs/scala-collection-compat_2.13-2.2.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/scala-java8-compat_2.13-0.9.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/scala-library-2.13.3.jar:/content/kafka_2.13-2.7.0/bin/../libs/scala-logging_2.13-3.9.2.jar:/content/kafka_2.13-2.7.0/bin/../libs/scala-reflect-2.13.3.jar:/content/kafka_2.13-2.7.0/bin/../libs/slf4j-api-1.7.30.jar:/content/kafka_2.13-2.7.0/bin/../libs/slf4j-log4j12-1.7.30.jar:/content/kafka_2.13-2.7.0/bin/../libs/snappy-java-1.1.7.7.jar:/content/kafka_2.13-2.7.0/bin/../libs/zookeeper-3.5.8.jar:/content/kafka_2.13-2.7.0/bin/../libs/zookeeper-jute-3.5.8.jar:/content/kafka_2.13-2.7.0/bin/../libs/zstd-jni-1.4.5-6.jar org.apache.zookeeper.server.quorum.QuorumPeerMain ./kafka_2.13-2.7.0/config/zookeeper.properties\n",
            "root        1254       1 57 04:17 ?        00:00:05 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true -Xloggc:/content/kafka_2.13-2.7.0/bin/../logs/kafkaServer-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=100M -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/content/kafka_2.13-2.7.0/bin/../logs -Dlog4j.configuration=file:./kafka_2.13-2.7.0/bin/../config/log4j.properties -cp /content/kafka_2.13-2.7.0/bin/../libs/activation-1.1.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/argparse4j-0.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/audience-annotations-0.5.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/commons-cli-1.4.jar:/content/kafka_2.13-2.7.0/bin/../libs/commons-lang3-3.8.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/connect-api-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/connect-basic-auth-extension-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/connect-file-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/connect-json-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/connect-mirror-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/connect-mirror-client-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/connect-runtime-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/connect-transforms-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/hk2-api-2.6.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/hk2-locator-2.6.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/hk2-utils-2.6.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-annotations-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-core-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-databind-2.10.5.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-dataformat-csv-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-datatype-jdk8-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-jaxrs-base-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-jaxrs-json-provider-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-module-jaxb-annotations-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-module-paranamer-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jackson-module-scala_2.13-2.10.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jakarta.activation-api-1.2.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/content/kafka_2.13-2.7.0/bin/../libs/jakarta.inject-2.6.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/content/kafka_2.13-2.7.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/content/kafka_2.13-2.7.0/bin/../libs/jakarta.xml.bind-api-2.3.2.jar:/content/kafka_2.13-2.7.0/bin/../libs/javassist-3.25.0-GA.jar:/content/kafka_2.13-2.7.0/bin/../libs/javassist-3.26.0-GA.jar:/content/kafka_2.13-2.7.0/bin/../libs/javax.servlet-api-3.1.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/jaxb-api-2.3.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/jersey-client-2.31.jar:/content/kafka_2.13-2.7.0/bin/../libs/jersey-common-2.31.jar:/content/kafka_2.13-2.7.0/bin/../libs/jersey-container-servlet-2.31.jar:/content/kafka_2.13-2.7.0/bin/../libs/jersey-container-servlet-core-2.31.jar:/content/kafka_2.13-2.7.0/bin/../libs/jersey-hk2-2.31.jar:/content/kafka_2.13-2.7.0/bin/../libs/jersey-media-jaxb-2.31.jar:/content/kafka_2.13-2.7.0/bin/../libs/jersey-server-2.31.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-client-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-continuation-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-http-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-io-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-security-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-server-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-servlet-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-servlets-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jetty-util-9.4.33.v20201020.jar:/content/kafka_2.13-2.7.0/bin/../libs/jopt-simple-5.0.4.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka_2.13-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka_2.13-2.7.0-sources.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka-clients-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka-log4j-appender-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka-raft-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka-streams-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka-streams-examples-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka-streams-scala_2.13-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka-streams-test-utils-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/kafka-tools-2.7.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/log4j-1.2.17.jar:/content/kafka_2.13-2.7.0/bin/../libs/lz4-java-1.7.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/maven-artifact-3.6.3.jar:/content/kafka_2.13-2.7.0/bin/../libs/metrics-core-2.2.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/netty-buffer-4.1.51.Final.jar:/content/kafka_2.13-2.7.0/bin/../libs/netty-codec-4.1.51.Final.jar:/content/kafka_2.13-2.7.0/bin/../libs/netty-common-4.1.51.Final.jar:/content/kafka_2.13-2.7.0/bin/../libs/netty-handler-4.1.51.Final.jar:/content/kafka_2.13-2.7.0/bin/../libs/netty-resolver-4.1.51.Final.jar:/content/kafka_2.13-2.7.0/bin/../libs/netty-transport-4.1.51.Final.jar:/content/kafka_2.13-2.7.0/bin/../libs/netty-transport-native-epoll-4.1.51.Final.jar:/content/kafka_2.13-2.7.0/bin/../libs/netty-transport-native-unix-common-4.1.51.Final.jar:/content/kafka_2.13-2.7.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/content/kafka_2.13-2.7.0/bin/../libs/paranamer-2.8.jar:/content/kafka_2.13-2.7.0/bin/../libs/plexus-utils-3.2.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/reflections-0.9.12.jar:/content/kafka_2.13-2.7.0/bin/../libs/rocksdbjni-5.18.4.jar:/content/kafka_2.13-2.7.0/bin/../libs/scala-collection-compat_2.13-2.2.0.jar:/content/kafka_2.13-2.7.0/bin/../libs/scala-java8-compat_2.13-0.9.1.jar:/content/kafka_2.13-2.7.0/bin/../libs/scala-library-2.13.3.jar:/content/kafka_2.13-2.7.0/bin/../libs/scala-logging_2.13-3.9.2.jar:/content/kafka_2.13-2.7.0/bin/../libs/scala-reflect-2.13.3.jar:/content/kafka_2.13-2.7.0/bin/../libs/slf4j-api-1.7.30.jar:/content/kafka_2.13-2.7.0/bin/../libs/slf4j-log4j12-1.7.30.jar:/content/kafka_2.13-2.7.0/bin/../libs/snappy-java-1.1.7.7.jar:/content/kafka_2.13-2.7.0/bin/../libs/zookeeper-3.5.8.jar:/content/kafka_2.13-2.7.0/bin/../libs/zookeeper-jute-3.5.8.jar:/content/kafka_2.13-2.7.0/bin/../libs/zstd-jni-1.4.5-6.jar kafka.Kafka ./kafka_2.13-2.7.0/config/server.properties\n",
            "root        1329     359  0 04:18 ?        00:00:00 /bin/bash -c ps -ef | grep kafka\n",
            "root        1331    1329  0 04:18 ?        00:00:00 grep kafka\n"
          ]
        }
      ],
      "source": [
        "!ps -ef | grep kafka"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3TntBqanQnh"
      },
      "source": [
        "Create the kafka topics with the following specs:\n",
        "\n",
        "- susy-train: partitions=1, replication-factor=1 \n",
        "- susy-test: partitions=2, replication-factor=1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXJWqMmWnPyP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created topic reco-train.\n",
            "Created topic reco-test.\n"
          ]
        }
      ],
      "source": [
        "!./kafka_2.13-2.7.0/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic reco-train\n",
        "!./kafka_2.13-2.7.0/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 2 --topic reco-test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNxf_NqjnycC"
      },
      "source": [
        "Describe the topic for details on the configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apCf9pfVnwn7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic: reco-train\tPartitionCount: 1\tReplicationFactor: 1\tConfigs: segment.bytes=1073741824\n",
            "\tTopic: reco-train\tPartition: 0\tLeader: 0\tReplicas: 0\tIsr: 0\n",
            "Topic: reco-test\tPartitionCount: 2\tReplicationFactor: 1\tConfigs: segment.bytes=1073741824\n",
            "\tTopic: reco-test\tPartition: 0\tLeader: 0\tReplicas: 0\tIsr: 0\n",
            "\tTopic: reco-test\tPartition: 1\tLeader: 0\tReplicas: 0\tIsr: 0\n"
          ]
        }
      ],
      "source": [
        "!./kafka_2.13-2.7.0/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic reco-train\n",
        "!./kafka_2.13-2.7.0/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic reco-test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKVnz3Pjot9t"
      },
      "source": [
        "The replication factor 1 indicates that the data is not being replicated. This is due to the presence of a single broker in our kafka setup.\n",
        "In production systems, the number of bootstrap servers can be in the range of 100's of nodes. That is where the fault-tolerance using replication comes into picture.\n",
        "\n",
        "Please refer to the [docs](https://kafka.apache.org/documentation/#replication) for more details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjCy3zaCQJ7-"
      },
      "source": [
        "## Movielens Dataset\n",
        "\n",
        "Kafka being an event streaming platform, enables  data from various sources to be written into it. For instance:\n",
        "\n",
        "- Web traffic logs\n",
        "- Astronomical measurements\n",
        "- IoT sensor data\n",
        "- Product reviews and many more.\n",
        "\n",
        "For the purpose of this tutorial, lets download the [Movielens](https://github.com/sparsh-ai/reco-data/blob/master/MovieLens_100K_ratings.csv?raw=true) dataset and feed the data into kafka manually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emslB2EGQMCR"
      },
      "outputs": [],
      "source": [
        "!wget -O ml_ratings.csv https://github.com/sparsh-ai/reco-data/blob/master/MovieLens_100K_ratings.csv?raw=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CfKVmCvwcL7"
      },
      "source": [
        "## Explore the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nC-yt_c9u0sH"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserId</th>\n",
              "      <th>MovieId</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>196</td>\n",
              "      <td>242</td>\n",
              "      <td>3.0</td>\n",
              "      <td>881250949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>186</td>\n",
              "      <td>302</td>\n",
              "      <td>3.0</td>\n",
              "      <td>891717742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>377</td>\n",
              "      <td>1.0</td>\n",
              "      <td>878887116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>244</td>\n",
              "      <td>51</td>\n",
              "      <td>2.0</td>\n",
              "      <td>880606923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>166</td>\n",
              "      <td>346</td>\n",
              "      <td>1.0</td>\n",
              "      <td>886397596</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserId  MovieId  Rating  Timestamp\n",
              "0     196      242     3.0  881250949\n",
              "1     186      302     3.0  891717742\n",
              "2      22      377     1.0  878887116\n",
              "3     244       51     2.0  880606923\n",
              "4     166      346     1.0  886397596"
            ]
          },
          "execution_count": 16,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movielens_df = pd.read_csv('ml_ratings.csv')\n",
        "movielens_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlNuW7xbu6o8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100000, 4)"
            ]
          },
          "execution_count": 17,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Number of datapoints and columns\n",
        "len(movielens_df), len(movielens_df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF5K9xtmlT2P"
      },
      "source": [
        "## Split the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-ku_X0Wld59"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(movielens_df, test_size=0.4, shuffle=True)\n",
        "print(\"Number of training samples: \",len(train_df))\n",
        "print(\"Number of testing sample: \",len(test_df))\n",
        "\n",
        "x_train_df = train_df.drop([\"Rating\"], axis=1)\n",
        "y_train_df = train_df[\"Rating\"]\n",
        "\n",
        "x_test_df = test_df.drop([\"Rating\"], axis=1)\n",
        "y_test_df = test_df[\"Rating\"]\n",
        "\n",
        "# The labels are set as the kafka message keys so as to store data\n",
        "# in multiple-partitions. Thus, enabling efficient data retrieval\n",
        "# using the consumer groups.\n",
        "x_train = list(filter(None, x_train_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "y_train = list(filter(None, y_train_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "\n",
        "x_test = list(filter(None, x_test_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "y_test = list(filter(None, y_test_df.to_csv(index=False).split(\"\\n\")[1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHXk0x2MXVgL"
      },
      "outputs": [],
      "source": [
        "NUM_COLUMNS = len(x_train_df.columns)\n",
        "len(x_train), len(y_train), len(x_test), len(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwP5U4GqmhoL"
      },
      "source": [
        "## Store the train and test data in kafka\n",
        "\n",
        "Storing the data in kafka simulates an environment for continuous remote data retrieval for training and inference purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhwFImSqncLE"
      },
      "outputs": [],
      "source": [
        "def error_callback(exc):\n",
        "    raise Exception('Error while sendig data to kafka: {0}'.format(str(exc)))\n",
        "\n",
        "def write_to_kafka(topic_name, items):\n",
        "  count=0\n",
        "  producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])\n",
        "  for message, key in items:\n",
        "    producer.send(topic_name, key=key.encode('utf-8'), value=message.encode('utf-8')).add_errback(error_callback)\n",
        "    count+=1\n",
        "  producer.flush()\n",
        "  print(\"Wrote {0} messages into topic: {1}\".format(count, topic_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP_Hyjy0uyPN"
      },
      "outputs": [],
      "source": [
        "write_to_kafka(\"reco-train\", zip(x_train, y_train))\n",
        "write_to_kafka(\"reco-test\", zip(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX8HRyZXSGyJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed a total of 60000 messages\n"
          ]
        }
      ],
      "source": [
        "# ! /content/kafka_2.13-2.7.0/bin/kafka-console-consumer.sh \\\n",
        "# --bootstrap-server localhost:9092 \\\n",
        "# --topic reco-train \\\n",
        "# --from-beginning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8m1_WSMSHL7"
      },
      "source": [
        "## Spark Streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xD4cawyvQun"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://downloads.apache.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!tar -xvf spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oJknXAgzMW6"
      },
      "outputs": [],
      "source": [
        "!wget \"https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.8/spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1qgT9emv57O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.8-bin-hadoop2.7\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar pyspark-shell'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_jvWtSFCFCF"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7BBe1jvwms-"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import Normalizer, StandardScaler\n",
        "import random\n",
        "import pyspark\n",
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "from uuid import uuid1\n",
        "import time\n",
        "\n",
        "kafka_topic_name = \"reco-train\"\n",
        "kafka_bootstrap_servers = 'localhost:9092'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VauODtJBtxA2"
      },
      "source": [
        "<img src='https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fea686f5e-44a4-4c36-baa2-bb13694b7773%2FUntitled.png?table=block&id=04618308-bc98-49a9-a1a5-821b4a0bd3d3&spaceId=63b72b1f-0e90-4ab8-a6df-a060a6545a56&width=2000&userId=21ec183f-f0be-4b6b-9b3e-6f0d4e5c5469&cache=v2'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxvuH_ZaKZdJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Time = 03:46:39\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "\n",
        "current_time = now.strftime(\"%H:%M:%S\")\n",
        "print(\"Current Time =\", current_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx4Ktlvr1Jfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Time: 2021-06-25 03:46:50\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2021-06-25 03:46:55\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2021-06-25 03:47:00\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2021-06-25 03:47:05\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2021-06-25 03:47:10\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2021-06-25 03:47:15\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2021-06-25 03:47:20\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2021-06-25 03:47:25\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2021-06-25 03:47:30\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2021-06-25 03:47:35\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2021-06-25 03:47:40\n",
            "-------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sc = pyspark.SparkContext()\n",
        "ssc = StreamingContext(sc,5)\n",
        "\n",
        "kafka_topic_name = \"reco-train\"\n",
        "kafka_bootstrap_servers = 'localhost:9092'\n",
        "\n",
        "kvs = KafkaUtils.createStream(ssc, kafka_bootstrap_servers, 'spark-streaming-consumer', {kafka_topic_name:1}) \n",
        "kvs = KafkaUtils.createDirectStream(ssc, [kafka_topic_name], {\"metadata.broker.list\": kafka_bootstrap_servers})\n",
        "kvs = KafkaUtils.createDirectStream(ssc, [kafka_topic_name], {\n",
        "                        'bootstrap.servers':kafka_bootstrap_servers,\n",
        "                        'group.id':'test-group',\n",
        "                        'auto.offset.reset':'largest'})\n",
        "\n",
        "lines = kvs.map(lambda x: x[1])\n",
        "counts = lines.flatMap(lambda line: line.split(' '))\n",
        "counts = lines.flatMap(lambda line: line.split(' ')).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\n",
        "counts.pprint()\n",
        "ssc.start()\n",
        "# stream will run for 50 sec\n",
        "ssc.awaitTerminationOrTimeout(50)\n",
        "ssc.stop()\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bolvdIncbjsD"
      },
      "source": [
        "## Further exploration\n",
        "- https://towardsdatascience.com/enabling-streaming-data-with-spark-structured-streaming-and-kafka-93ce91e5b435\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "T855843_kafka_spark_streaming_colab.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
