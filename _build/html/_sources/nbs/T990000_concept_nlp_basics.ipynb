{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8RREnf5Ms_I"
      },
      "source": [
        "# Natural Language Processing 101\n",
        "> Understand the basic process of pattern recognition that is needed to work with text data. This approach can be used for a large number of basic operations on text data like Document Classification (e.g. topic of an article),Sequence to Sequence Learning (e.g. translations), and Sentiment Analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX0HRp2SM6c2"
      },
      "source": [
        "Lets understand the basic process of pattern recognition that is needed to work with text data. This approach can be used for a large number of basic operations on text data like \n",
        "- Document Classification (e.g. topic of an article)\n",
        "- Sequence to Sequence Learning (e.g. translations)\n",
        "- Sentiment Analysis\n",
        "\n",
        "Lets start with seeing how a text sequence can be encoded into numbers and processed to prepare for these tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrAAWGSAMs_a"
      },
      "source": [
        "## Tokenisation\n",
        "\n",
        "Lets take the sentence - **The quick brown fox jumped over the lazy dog**\n",
        "\n",
        "We need to first break this sentence in to smaller constituents - called **tokens**. Now there are three ways of creating the tokens can happen:\n",
        "\n",
        "- **Individual character** - create tokens for each\n",
        "- **Individual word** - Create tokens for each word in the sentence\n",
        "- **N-gram** - Create tokens by taking n-grams words in the sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_247UN-8Ms_c"
      },
      "source": [
        "### Create word tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra093bFgMs_d"
      },
      "source": [
        "**Pre-processing - split, punctuation & case**\n",
        "\n",
        "There is some basic **pre-processing** that has been done in the process of creating word tokens\n",
        "- Split the sentence on **whitespace**\n",
        "- Filter **punctuations**\n",
        "- Change to **lower case** text\n",
        "\n",
        "After this pre-processing, we can get each token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkczbwgEMs_f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "import spacy\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.text import one_hot, hashing_trick"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLUmP61sMs_v"
      },
      "outputs": [],
      "source": [
        "! python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGFKq3l_Ms_k"
      },
      "outputs": [],
      "source": [
        "sentence = 'The quick brown fox jumped over the lazy dog.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-MWZVh8Ms_l"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']"
            ]
          },
          "execution_count": 5,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_to_word_sequence(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itGv8WC2Ms_x"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPvnhJ_-Ms_z"
      },
      "outputs": [],
      "source": [
        "sentence = 'The quick brown fox jumped over the lazy dog'\n",
        "doc = nlp(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRo0m0kiMs_0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The\n",
            "quick\n",
            "brown\n",
            "fox\n",
            "jumped\n",
            "over\n",
            "the\n",
            "lazy\n",
            "dog\n"
          ]
        }
      ],
      "source": [
        "for token in doc:\n",
        "    print(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dKO21nPMs_3"
      },
      "source": [
        "## Vectorisation\n",
        "\n",
        "Once you have tokens, we need to find a way to represent them as vectors. Let's look at two traditional way of representing them as vectors\n",
        "\n",
        "- Frequency Based\n",
        "    - Binary\n",
        "    - Count \n",
        "    - tfidf\n",
        "    - Co-occurence (Skipgram)\n",
        "- Prediction Based\n",
        "    - Pre-trained Vectors\n",
        "    - Learning Vectors\n",
        "    - Learning vectors with the task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBLXilXtMs_5"
      },
      "source": [
        "### One-Hot Encoding "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPf81n4XMs_7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[5, 3, 3, 7, 8, 8, 5, 9, 9]"
            ]
          },
          "execution_count": 12,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Given a size of vocabulary, do one-hot encoding\n",
        "one_hot(sentence, n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdxhaclNMs_9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[51, 13, 19, 11, 7, 95, 51, 74, 33]"
            ]
          },
          "execution_count": 13,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Given a size of vocabulary, do hash encoding (to save space)\n",
        "hashing_trick(sentence, n=100, hash_function=\"md5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhzRPwUxMs_-"
      },
      "source": [
        "> Tip: Using the Tokenizer API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP4D3xYbMtAA"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xSSfmK0MtAB"
      },
      "outputs": [],
      "source": [
        "# Instantiate the Tokenizer\n",
        "simple_tokenizer = Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDjdGU48MtAD"
      },
      "outputs": [],
      "source": [
        "# Fit the Tokenizer\n",
        "simple_tokenizer.fit_on_texts([sentence])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rs3IUXyMtAU"
      },
      "outputs": [],
      "source": [
        "def get_sentence_vectors(sentences, tokenizer, mode=\"binary\"):\n",
        "    matrix = tokenizer.texts_to_matrix(sentences, mode=mode)\n",
        "    df = pd.DataFrame(matrix)\n",
        "    df.drop(columns=0, inplace=True)\n",
        "    df.columns = tokenizer.word_index\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCzgQYTYMtAE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'brown': 3,\n",
              " 'dog': 8,\n",
              " 'fox': 4,\n",
              " 'jumped': 5,\n",
              " 'lazy': 7,\n",
              " 'over': 6,\n",
              " 'quick': 2,\n",
              " 'the': 1}"
            ]
          },
          "execution_count": 17,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# See the word vectors\n",
        "simple_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiJjAcjiMtAI"
      },
      "source": [
        "Normally we will be working with a set of text (like sentences), so it is better to use the tokenizer API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaJAHfcTMtAK"
      },
      "outputs": [],
      "source": [
        "sentences = ['The quick brown fox jumped over the lazy dog', \n",
        "             'The dog woke up lazily and barked at the fox',\n",
        "             'The fox looked back and just ignored the dog']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ_dESr1MtAL"
      },
      "outputs": [],
      "source": [
        "# Instantiate and Fit\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95uSs146RREx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'and': 4,\n",
              " 'at': 14,\n",
              " 'back': 16,\n",
              " 'barked': 13,\n",
              " 'brown': 6,\n",
              " 'dog': 3,\n",
              " 'fox': 2,\n",
              " 'ignored': 18,\n",
              " 'jumped': 7,\n",
              " 'just': 17,\n",
              " 'lazily': 12,\n",
              " 'lazy': 9,\n",
              " 'looked': 15,\n",
              " 'over': 8,\n",
              " 'quick': 5,\n",
              " 'the': 1,\n",
              " 'up': 11,\n",
              " 'woke': 10}"
            ]
          },
          "execution_count": 33,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NOA9LArMtAN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[1, 5, 6, 2, 7, 8, 1, 9, 3],\n",
              " [1, 3, 10, 11, 12, 4, 13, 14, 1, 2],\n",
              " [1, 2, 15, 16, 4, 17, 18, 1, 3]]"
            ]
          },
          "execution_count": 34,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5j3-XLnMtAP"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0.],\n",
              "       [0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
              "        0., 0., 0.],\n",
              "       [0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "        1., 1., 1.]])"
            ]
          },
          "execution_count": 35,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.texts_to_matrix(sentences, mode=\"binary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgHR2qghMtAV"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>the</th>\n",
              "      <th>fox</th>\n",
              "      <th>dog</th>\n",
              "      <th>and</th>\n",
              "      <th>quick</th>\n",
              "      <th>brown</th>\n",
              "      <th>jumped</th>\n",
              "      <th>over</th>\n",
              "      <th>lazy</th>\n",
              "      <th>woke</th>\n",
              "      <th>up</th>\n",
              "      <th>lazily</th>\n",
              "      <th>barked</th>\n",
              "      <th>at</th>\n",
              "      <th>looked</th>\n",
              "      <th>back</th>\n",
              "      <th>just</th>\n",
              "      <th>ignored</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.947512</td>\n",
              "      <td>0.559616</td>\n",
              "      <td>0.559616</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.947512</td>\n",
              "      <td>0.559616</td>\n",
              "      <td>0.559616</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.947512</td>\n",
              "      <td>0.559616</td>\n",
              "      <td>0.559616</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        the       fox       dog  ...      back      just   ignored\n",
              "0  0.947512  0.559616  0.559616  ...  0.000000  0.000000  0.000000\n",
              "1  0.947512  0.559616  0.559616  ...  0.000000  0.000000  0.000000\n",
              "2  0.947512  0.559616  0.559616  ...  0.916291  0.916291  0.916291\n",
              "\n",
              "[3 rows x 18 columns]"
            ]
          },
          "execution_count": 52,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_x = get_sentence_vectors(sentences, tokenizer, mode=\"tfidf\")\n",
        "_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0xngigzVyod"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>variable</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>the</td>\n",
              "      <td>0.947512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>the</td>\n",
              "      <td>0.947512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>the</td>\n",
              "      <td>0.947512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>fox</td>\n",
              "      <td>0.559616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>fox</td>\n",
              "      <td>0.559616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>fox</td>\n",
              "      <td>0.559616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>dog</td>\n",
              "      <td>0.559616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>dog</td>\n",
              "      <td>0.559616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>dog</td>\n",
              "      <td>0.559616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>and</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence variable     value\n",
              "0         0      the  0.947512\n",
              "1         1      the  0.947512\n",
              "2         2      the  0.947512\n",
              "3         0      fox  0.559616\n",
              "4         1      fox  0.559616\n",
              "5         2      fox  0.559616\n",
              "6         0      dog  0.559616\n",
              "7         1      dog  0.559616\n",
              "8         2      dog  0.559616\n",
              "9         0      and  0.000000"
            ]
          },
          "execution_count": 53,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_x = _x.rename_axis('sentence').reset_index().melt(id_vars=['sentence'])\n",
        "_x.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpAjw1BJMtAY"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-3caaa2dc73df4dad9aa4021cbcaffc17\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-3caaa2dc73df4dad9aa4021cbcaffc17\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-3caaa2dc73df4dad9aa4021cbcaffc17\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function loadScript(lib) {\n",
              "      return new Promise(function(resolve, reject) {\n",
              "        var s = document.createElement('script');\n",
              "        s.src = paths[lib];\n",
              "        s.async = true;\n",
              "        s.onload = () => resolve(paths[lib]);\n",
              "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "      });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else if (typeof vegaEmbed === \"function\") {\n",
              "      displayChart(vegaEmbed);\n",
              "    } else {\n",
              "      loadScript(\"vega\")\n",
              "        .then(() => loadScript(\"vega-lite\"))\n",
              "        .then(() => loadScript(\"vega-embed\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-b81da020d8d16f30b951e3d78ec7f69f\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"type\": \"quantitative\", \"field\": \"value\", \"title\": \"tfidf\"}, \"x\": {\"type\": \"nominal\", \"field\": \"variable\", \"title\": \"word\"}, \"y\": {\"type\": \"nominal\", \"field\": \"sentence\", \"title\": \"sentence\"}}, \"selection\": {\"selector007\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 700, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-b81da020d8d16f30b951e3d78ec7f69f\": [{\"sentence\": 0, \"variable\": \"the\", \"value\": 0.9475118935396932}, {\"sentence\": 1, \"variable\": \"the\", \"value\": 0.9475118935396932}, {\"sentence\": 2, \"variable\": \"the\", \"value\": 0.9475118935396932}, {\"sentence\": 0, \"variable\": \"fox\", \"value\": 0.5596157879354227}, {\"sentence\": 1, \"variable\": \"fox\", \"value\": 0.5596157879354227}, {\"sentence\": 2, \"variable\": \"fox\", \"value\": 0.5596157879354227}, {\"sentence\": 0, \"variable\": \"dog\", \"value\": 0.5596157879354227}, {\"sentence\": 1, \"variable\": \"dog\", \"value\": 0.5596157879354227}, {\"sentence\": 2, \"variable\": \"dog\", \"value\": 0.5596157879354227}, {\"sentence\": 0, \"variable\": \"and\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"and\", \"value\": 0.6931471805599453}, {\"sentence\": 2, \"variable\": \"and\", \"value\": 0.6931471805599453}, {\"sentence\": 0, \"variable\": \"quick\", \"value\": 0.9162907318741551}, {\"sentence\": 1, \"variable\": \"quick\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"quick\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"brown\", \"value\": 0.9162907318741551}, {\"sentence\": 1, \"variable\": \"brown\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"brown\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"jumped\", \"value\": 0.9162907318741551}, {\"sentence\": 1, \"variable\": \"jumped\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"jumped\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"over\", \"value\": 0.9162907318741551}, {\"sentence\": 1, \"variable\": \"over\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"over\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"lazy\", \"value\": 0.9162907318741551}, {\"sentence\": 1, \"variable\": \"lazy\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"lazy\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"woke\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"woke\", \"value\": 0.9162907318741551}, {\"sentence\": 2, \"variable\": \"woke\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"up\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"up\", \"value\": 0.9162907318741551}, {\"sentence\": 2, \"variable\": \"up\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"lazily\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"lazily\", \"value\": 0.9162907318741551}, {\"sentence\": 2, \"variable\": \"lazily\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"barked\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"barked\", \"value\": 0.9162907318741551}, {\"sentence\": 2, \"variable\": \"barked\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"at\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"at\", \"value\": 0.9162907318741551}, {\"sentence\": 2, \"variable\": \"at\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"looked\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"looked\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"looked\", \"value\": 0.9162907318741551}, {\"sentence\": 0, \"variable\": \"back\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"back\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"back\", \"value\": 0.9162907318741551}, {\"sentence\": 0, \"variable\": \"just\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"just\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"just\", \"value\": 0.9162907318741551}, {\"sentence\": 0, \"variable\": \"ignored\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"ignored\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"ignored\", \"value\": 0.9162907318741551}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "execution_count": 55,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "alt.Chart(_x).mark_rect().encode(\n",
        "    x=alt.X('variable:N', title=\"word\"),\n",
        "    y=alt.Y('sentence:N', title=\"sentence\"),\n",
        "    color=alt.Color('value:Q', title=\"tfidf\")\n",
        ").properties(\n",
        "    width=700\n",
        ").interactive()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxSiN_LsMtAb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 18 unique tokens.\n"
          ]
        }
      ],
      "source": [
        "one_hot_results = tokenizer.texts_to_matrix(sentence, mode='binary')\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "T990000_concept_nlp_basics.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
